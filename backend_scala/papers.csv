1,"The benefits provided by Web service protocols are well recognized. Deployments to date, however, have concentrated on new applications, and existing Web-based applications. A host of legacy applications and protocols continue to exist in their native forms, outdated, yet entrenched due to large installed bases. This paper details our observations in integrating a Web service infrastructure into the simple network management protocol (SNMP). SNMP has been in use for over a decade and a half, predominantly in network equipment embedded systems. Our Web service-based approach allows us to enhance our existing application with XML/SOAP interoperability, SSL/TLS security, and the potential to migrate both application and protocol layers to encompass future extensions and Web browser accessibility. The difficulty with SNMP, and many other legacy networking protocols, is that much of the extensive installed base is hosted on limited capability, legacy hardware. While the benefits of our scheme are quite tangible, the performance impact of adding these features is not well known. We examine two approaches, an integrated solution using a light-weight HTTP/SOAP stack, as well as a standard Java Web server implementation. Our tests reveal unanticipated performance results through both the integrated and proxy methods. We discuss the impact of these anomalies on the viability of our approach and address the broad issue of migrating Web services to legacy embedded architectures."
3,"Gartner has determined that “Complex Event Processing” (CEP) is one of the emerging areas on the rise in the “hype cycle” and become dominant in the area of Business Process Integration and Management (BPIM) as well as other areas (such as: command and control). According to Gartner’s prediction, within 5-10 years this area will get to maturity. Activities in this industry have already started, and it is of the interest of this community to have early acquaintance with an area that may be dominant in several of the application types. The aim of this tutorial is to educate the audience about CEP and it’s relevant to the some of the other emergent areas such as: Web services, autonomic computing, and grid management, as well as to traditional data management areas. The tutorial will cover introduction to the area, model-based approaches, architectures, several use cases, relationships to other concepts. It does not require any prior knowledge in the CEP."
4,"Recent years have witnessed a rapid growth in using Web services for data publishing and sharing among organizations. To improve the efficiency of software development and economize on human and material resources, service reuse is viewed as a powerful means which will not only reuse atomic services, but also reuse arbitrary granularities of Service Process Fragments (SPFs). However, effectively reusing arbitrary granularities of SPFs has not been solved yet. In this paper, we propose a novel method of SPF reuse, named SCKY, based on the Cocke-Kasami-Younger (CKY) algorithm. We first present an extended CKY to do SPF-query. Then we address how to do SPF-query by a probability CKY, i.e. return a SPF with maximum emergence probability. Finally, we explore the QoS Query of SPF. Through a set of experiments, the effectiveness and robustness of our approach are evaluated, where the dataset is constructed by the Web Service Challenge Testset Generator1 (CTG)."
5,"Recently, collaborative filtering has been applied to QoS-aware Web service recommendation. However, it cannot make recommendations for users that have invoked only a very small number of services because of data sparsity. In addition, these methods do not know how confident they are in their recommendations. Based on the fact that QoS values of web services are usually subject to the locations of users, a few works assume that the additional knowledge of users' locations can be used to better deal with the data sparsity issue, since a user only needs to know the users near to him/her. On the other hand, the sparsity of user-service invocations forces the location-aware method to consider the QoS experiences of users not near enough, which may decrease its precision. In order to find a good trade-off between coverage and precision, we propose a random walk method combining location-aware and collaborative filtering method for web service recommendation. The random walk method allows us to define and to measure the confidence of a recommendation. To evaluate the performance of our proposed method, we conduct a set of comprehensive experiments using a real-world web service dataset, and compared the method with existing collaborative filtering methods."
6,"As web-enabled software becomes the standard for business processes, the ways organizations, partners and customers interface with it have become a critical differentiator in the market place, i.e., API Economy. With the rapid proliferation of APIs, it is increasingly important for users to effectively manage objective APIs in kinds of API markets, e.g., ProgramableWeb (PW), Mashape, etc. In this paper, to facilitate the process of API management, we propose a graphbased recommendation approach called ATRec to automatically assign tags to unlabeled APIs by exploiting both graph structure information and semantic similarity. Specifically, ATRec first leverages the multi-type relations (i.e., among APIs, mashups, and mashup assigned tags) to construct a heterogeneous network, in which a Random Walk with Restart (RWR) model is applied to alleviate the total cold start problem where no API has ever been tagged. Furthermore, we apply the recommended API tags in two API management scenarios (API search, API recommendation). Comprehensive experiments based on a real dataset crawled from PW demonstrate the effectiveness of the proposed approach."
7,"Locality Sensitive Hashing (LSH) is a widely used similarity search technique for many web services, such as content-based retrieval services for images and videos. Due to its popularity, much research effort has been devoted to improving the search quality, and the indexing and query performance of LSH. However, most existing variants of LSH can only run on single node, which limits their applicability to large-scale data. In this paper, we present a Shuffle-Efficient Similarity Search scheme based on LSH, which can be efficiently executed in distributed environments, to serve a massive amount of data. In SES-LSH, a shuffle efficient indexing scheme is proposed to reduce the data shuffle when constructing hash tables, and a location-aware querying scheme is proposed to improve the query performance. We have implemented a prototype of SES-LSH based on Spark, and several optimizations have been utilized to improve the fine-grained hash table operations of distributed LSH. Extensive experiments using large-scale real-world datasets show that SES-LSH is remarkably more efficient than existing methods."
9,"In a short period the Web has become an important part of our lives. However, the full potential of the Web is still not realized. Two recent developments - Web services and the semantic Web - are steps in the direction of utilizing the full potential of the Web. Web services allow applications to utilize the Web for automatically extracting (and updating) information while the semantic Web enterprise promises to provide the infrastructure that allows intelligent Web services to be rapidly created and deployed. However, with this comes the task of transforming the traditional Web-based systems to Web-services over the semantic Web. In this paper, we demonstrate how an existing successful Web-based system for providing help to first responders of chemically hazardous emergencies (called E-plan) can be converted into a Web-services based model using the semantic Web and intelligent reasoning technologies. Our efforts can be regarded as a case study in converting monolithic Web-based applications to a more agile, rapidly deployable intelligent Web-services model."
10,"Workflows often operate in volatile environments in which the component services' QoS changes frequently. Optimally adapting to these changes becomes an important problem that must be addressed by the Web service composition and execution (WSCE) system being utilized. We adopt the A-WSCE framework that utilizes a three-stage approach for composing and executing Web workflows. The A-WSCE framework offers a way to adapt by defining multiple workflows and switching among them in case of component failure or changes in the QoS parameters. However, the A-WSCE framework suffers from the limitations imposed by a simple strategy of periodically checking the QoS offerings of randomly picked providers in order to decide whether the current workflow is optimal. To address these limitations, we associate the value of changed information (VOC) with each workflow and utilize the VOC to update which workflow to execute. We empirically demonstrate the improved performance of the workflows selected using the new approach in comparison to the original framework."
11,"XML plays an important role in building enterprise applications. However, most of the XML-based applications, particularly the emerging Web services, suffer from low performance caused by XML processing and thus bring negative user experience in terms of response time. We argue that by reducing the considerable overhead in garbage collection the XML processing performance can be improved. We begin by conducting a set of experiments to understand the XML parser's memory characteristics, such as heap composition, object size and type distributions, object lifetime, and so on. Then, we get the valuable findings for improving performance that XML processing, which violates the weak generational hypothesis, is a memory allocation intensive workload in which most objects are small and long-lived. The findings can benefit the design of XML parsing specific GC and related tools designed to improve XML processing performance for Web services"
12,"Traditional, centralized orchestration of composite Web services often leads to inefficient routing of messages. To solve this problem, we present a novel scheme to execute composite Web services in a fully decentralized way. We introduce service invocation triggers, a lightweight infrastructure that routes messages directly from the producing service to the consuming one, enabling fully decentralized orchestration. An evaluation confirms that decentralized orchestration can significantly reduce the network traffic when compared with centralized orchestration"
13,"The emergence of Web services represents a significant advance in the continuing evolution of e-business. In order to fully explore business opportunities provided by this paradigm, it is important to track its utilization. This can be done through the use of logging facilities. However, current Web logging approaches do not contemplate Web services utilization. This paper presents a Web services logging architecture based on SOAP intermediaries that captures comprehensive services usage information, which can be explored to improve B2B and B2C transactions by providing feedback on customer electronic behavior."
14,"In this paper, we describe an approach to discover the control flow graph of Web services for Web services analysis, verification, and testing. For this purpose, three novel methods are proposed. First, we introduce a domain independent RDF Schemas for concise resource oriented functional specification of Web services operations. Secondly, we describe the use of RDF entailment to accurately derive the control flow from the functional specifications. We developed a transformation from RDF graph to SPARQL query to facilitate the RDF entailment which offers flexibility and extensibility over the direct graph matching approach. The third is a linkage based Web services modeling and analysis framework, within which we apply an improved Google PageRank algorithm to efficiently calculate test coverage potential using the derived control flow. We justify that the proposed linkage based Web services modeling and analysis framework is particularly suitable for testing Web services. A prototype of the proposed methods has been implemented and tested on some standard based Web services. Experimental results show that the control flow analysis is quite efficient and accurate, and the coverage based test results of the proposed approach are very promising."
15,"Authorization and access control in Web services is complicated by the unique requirements of the dynamic Web services paradigm. Current authentication mechanisms for Web services do not differentiate between users in terms of fine-grained access privileges. This results in an all-or-nothing access which is not flexible enough for modern day business processes using Web services to execute. In this paper, we present a policy-based authorization framework to address this requirement. We have designed a profile of the well-known WS-policy specification tailored to meet the access control requirements in Web services by integrating WS-policy with an access control policy specification language, X-GTRBAC. The design of the profile is aimed at bridging the gap between available policy standards for Web services and existing policy specification languages for access control. The profile supports the WS-policy attachment specification, which allows separate policies to be associated with multiple components of a Web service description, and one of our key contributions is the design of an algorithm to compute the effective policy for the Web service given the multiple policy attachments. To allow Web service applications to use our solution, we have adopted a component-based design approach based on well-known UML notations. We have also prototyped our architecture, and implemented it as a loosely coupled Web service providing healthcare information services to physicians subject to applicable authorization policies."
16,"Semantic web service discovery has attracted a lot of attention in the last decade. Research conducted in this area can be (mainly) summarized as follows: (1) ""monolith"" matchmaking algorithms (and systems), and (2) schema matching-based techniques. In this paper we describe a flexible approach that takes leverage of existing schema matchers, leading to a multiple choice strategy for semantic service discovery. The approach has been implemented and validated in using the data collection provided by the S3 (Semantic Service Selection) community, and led to promising preliminary results."
17,"We present methods for optimally adapting Web processes to exogenous events while preserving inter-service constraints that necessitate coordination. For example, in a supply chain process, orders placed by a manufacturer may get delayed in arriving. In response to this event, the manufacturer has the choice of either waiting out the delay or changing the supplier. Additionally, there may be compatibility constraints between the different orders, thereby introducing the problem of coordination between them if the manufacturer chooses to change the suppliers. We focus on formulating the decision making models of the managers, who must adapt to external events while satisfying the coordination constraints, using Markov decision processes. Our methods range from being centralized and globally optimal in their adaptation but not scalable, to decentralized that is suboptimal but scalable to multiple managers. We also develop a hybrid approach that improves on the performance of the decentralized approach with a minimal loss of optimality"
18,In this paper we introduce a new modeling tool for constraint handling in the area of workflow technology. The constraint handlers can be used to improve the quality of business processes but without changing already existing business logic. Todays workflow languages provide no possibility to model constraints and the actions in case the constraints get violated explicitly. Fault and event handling mechanisms to react to events not expected in normal executions are only provided by the BPEL language. Using BPEL as workflow language we integrate the constraint handling extension without changing any existing semantics in a smart way. In our approach we use this fault and event handling mechanisms to extend the BPEL language with a constraint handling mechanism. By integrating this constraint handling tool into the BPEL language we provide an approach for quality driven process modeling with the BPEL language.
19,"This paper addresses a fundamental issue of Web service composition. We present a simple but powerful conceptual model that leads to a scalable approach to automatically constructing a composite Web service to meet its requirements by using as few services as possible. Our approach is based on a state space model that has a monotone property to allow efficient search along with efficient algorithms for pruning and simple parallelization. We provide both empirical and theoretical analyses of the proposed approach and show that it has time complexity of O(n
<sup>2</sup>
), for a repository with n services. However, the approach takes linear time for sequential compositions when service applicability is performed by service discovery and thus, it is shown to give asymptotically optimal performance. Although optimality in the number of services deployed is not guaranteed, our experiments on public benchmark data sets show correct optimized solutions 100% of the time, with a reduction in the average running time, compared to a well-performed planning-based system, of better than 35% over 207 composition problems."
20,"With the advancement of Web services technologies, online businesses have the ability to offer their capabilities to larger, lesser known communities of potential collaborators. Universal Description, Discovery, and Integration (UDDI) specification and supporting technologies support open frameworks for businesses to store, advertise and retrieve pertinent services. Many researchers investigate approaches that ubiquitously create higher-level processes by composing services discovered in and retrieved from UDDI registries. However, there are few studies that consider the impact of registry performance on future service automation. This work focuses on evaluating the performance of UDDI registries considering variability and concurrent load of publish and inquiry requests."
21,"With the proliferation of Web services, service engineers demand good automatic service composition algorithms that not only synthesize the correct work plans from thousands of services but also satisfy the quality requirements of the users. Our observation is that conventional approaches suffer from serious limitations in scalability and accuracy when addressing both requirements simultaneously. We have designed and implemented a tool QSynth to use QoS objectives of service requests as the search directives. This approach effectively prunes the search space and significantly improves the accuracy of the search results. Evaluations show that, compared to the state of the art, QSynth achieves superior scalability and accuracy with respect to a large variety of composition scenarios. Our design of QSynth won the performance championship of Web Services Challenge 2009."
22,"Service-oriented systems facilitate business workflows to span multiple organizations (e.g. by means of Web services). As a side effect, data may be more easily transferred over organizational boundaries. Thus, privacy issues arise. At the same time, there are personal, business and legal requirements for protecting privacy and IPR and allowing customers to request information about how and by whom their data was handled. Managing these requirements constitutes an unsolved technical and organizational problem. We propose to solve the information request problem by attaching meta-knowledge about how data was handled to the data itself. We present our solution, in form of an architecture, a formalization and an implemented prototype for logging and collecting logs in service-oriented and cross-organizational systems."
23,"We present a semantic optimized service discovery (SemOSD) approach capable of handling Web service search requests on a fine-grained level of detail where we augment semantic service descriptions with statistically built predictor functions. Our approach combines ontologies and mathematical functions built using statistical regression over previous Web service interactions. In the search requests we allow for arbitrary, independent and dependent constraints and user preferences expressed using objective functions. Our approach maps to standard operational research global optimization problem where algorithms of simulated annealing and differential evolution are used. It is capable of finding the optimal combination of service input and output parameters (a configuration) to a user request with rich preferences. Our approach is applied to an international package shipment scenario where real (Web) services are used and mined to create price prediction models. We show that the chosen regression method provides price prediction models of high accuracy and our approach supports expressive and complex search requests."
24,"The composition of Web-based services is a process that usually requires advanced programming skills and vast knowledge about specific technologies. One such modern technology is Web services. In this work, we propose a framework for the smooth composition of Web services that are targeted to provide particular government services that usually require interaction between several agencies. The goal is to allow government agencies to smoothly register simple services or create new ones based on other already registered simple or composed services. Our work has the additional goal of reducing the overhead involved in the execution of composite services by providing efficient mapping schemes to bypass intermediary composed services and allowing direct invocation of the basic services in the composition."
25,"This paper introduces a service selection model with the service location considered. The location of a service represents its position in the network, which determines the transmission cost of calling this service in the composite service. The more concentrated the invoking services are, the less transmission time the composite service costs. On the other hand, the more and more popular big data processing services, which need to transfer mass data as input, make the effect much more obvious than ever before. Therefore, it is necessary to introduce service location as a basic feature in service selection. The definition and membership functions of service location are presented in this paper. After that, the optimal service selection problem is represented as an optimization problem under some reasonable assumptions. A shortest-path based algorithm is proposed to solve this optimization problem. At last, the case of railway detection is studied for better understanding of our model."
26,"Advances of the computing industry in SOA infrastructure have made service oriented applications practically attainable in the field of high performance computing for earth sciences. We have applied SOA to modeling and inversion of geophysical well logs. Parallelized log simulators are accessible as a Web service to a variety of interpretation workflows. We have developed an HPC management infrastructure based on SOAP. It is a fit-for-purpose system with a low runtime overhead and a novel flexible job submission mechanism, suitable for applications running on multi-cluster environments. We describe the system architecture and discuss performance benchmarks, including a novel way of estimating performance on heterogeneous clusters."
27,"Web services provide an instantiation of the loosely coupled service–oriented architecture and facilitate the process of enterprise application integration by encapsulating information, software, and other resources. However, to exploit the true potential of web services, it is critical to develop technologies and tools for composing new services from existing ones. While numerous composition approaches have been developed in the past, very little has been done towards tooling. What is clearly lacking is an Integrated Development Environment (IDE) to ease the process of composition, thereby reducing development time and integration efforts. In this paper, we build on our previous work on service composition, and present an IDE for end–to–end composition of web services. We elaborate on the design of the IDE, describe its integration with existing technologies, and discuss its usability based on the findings of a user survey."
28,"In this paper, we formalize the concept of a delegation network for Web intermediaries and present formal semantics for the responsibility of these actors. Key properties of the network are proven and method to judge an actor's responsibility is given. This work is important because it determines the accuracy of task execution and the feasibility of content reuse in the network"
30,"The growth of the Internet has been accompanied by the growth of Web services (e.g. e-commerce, e-health). This proliferation of Web services and the increasing regulatory and legal requirements for personal privacy have fueled the need to protect the personal privacy of Web service users. We advocate a privacy policy negotiation approach to protecting personal privacy (Yee and Korba, 2003; ). We provided semiautomated approaches for deriving personal privacy policies (Yee and Korba, 2004). However, it is evident that approaches are also needed to ensure that providers of Web services comply with the privacy policies of service users. In this paper, we examine privacy legislation to derive requirements for privacy policy compliance systems. We then propose an architecture for a privacy policy compliance system that satisfies the requirements and discuss the strengths and weaknesses of our proposed architecture."
31,"This paper presents the initial findings from a series of case studies involving the enterprise transformation to service oriented architecture. Ten large enterprises were studied to determine how they were able to convert their legacy IT architecture. Particular interest was paid to business models, governance, enterprise architecture, change management, risk management, and technology. These cases were used to form a predictive model of success factors in transformation."
32,"Technologies involved in Web information distribution services are evolving to adapt themselves to new user requirements. Usually, these new technologies are used separately. ELIN (electronic newspaper initiative) project is a European Commission funded project that tries to integrate the newest standards and technologies involved in multimedia delivery applied to web newspapers. It has as objective the delivery of any type of media format to any kind of user terminal in an efficient way. In order to do that, it takes the approach of using MPEG standards: MPEG-4 for video delivery, MPEG-7 for data classification and MPEG-21 for data management and adaptation. So it integrates the totality of solutions provided for the MPEG group."
33,"A major advantage of Service-Oriented Architectures (SOA) is composition and coordination of loosely coupled services. Because the development lifecycles of services and clients are decoupled, multiple service versions have to be maintained to continue supporting older clients. Typically versions are managed within the SOA by updating service descriptions using conventions on version numbers and namespaces. In all cases, the compatibility among services description must be evaluated, which can be hard, error-prone and costly if performed manually, particularly for complex descriptions. In this paper, we describe a method to automatically determine when two service descriptions are backward compatible. We then describe a case study to illustrate how we leveraged version compatibility information in a SOA environment and present initial performance overheads of doing so. By automatically exploring compatibility information, a) service developers can assess the impact of proposed changes; b) proper versioning requirements can be put in client implementations guaranteeing that incompatibilities will not occur during run-time; and c) messages exchanged in the SOA can be validated to ensure that only expected messages or compatible ones are exchanged."
34,"With the fast development of Internet and rapid acceptance of Web Service technology, more and more service resources have emerged. Service composition which integrates the functionalities of different services is a promising technique for developing applications across multiple organizations. However, in a distributed, dynamic and autonomous environment, such as a service composition-based system, the availability and reliability are big concerns in terms of nonfunctional properties. In this paper, we propose a novel service composition method, ANGEL, with the target of the improvement of system availability. We adopt redundant mechanism in ANGEL and propose a model to improve the property of the service availability. We model the multiple services selection problem based on redundant mechanism as a nonlinear mixed integer programming problem and therefore, we propose two heuristic algorithms to select multiple feasible services that have the same functions, but with better availability. In order to maintain the availability of composite services, we further introduce monitor and detection mechanisms. Through the comprehensive experiments, we find that our proposed techniques can indeed achieve better availability as expected."
35,Presents the welcome message from the conference proceedings.
36,"The civil aviation system is a global enterprise that includes airframe, engine and component manufacturers, airlines, maintenance organizations, regulatory agencies, airports, air traffic control authorities and millions of service providers that must work together effectively to ensure cargo and passengers get to their destinations as scheduled, while traveling safely and efficiently. The system includes a bewildering array of commercial and custom developed systems for monitoring and controlling the operations of the participant."
37,"In order to accurately forecast Quality of Service (QoS) of different Web Services, this paper proposes a novel QoS forecasting approach called MulA-LMRBF (Multi-step fore-casting with Advertisement and Levenberg-Marquardt improved Radial Basis Function) based on multivariate time series. Considering the correlation among different QoS attributes, we use phase-space reconstruction to map historical multivariate QoS data into a dynamic system, use Average Dimension (AD) to estimate the embedding dimension and delay time of reconstructed phase space. We also add the short-term QoS advertisement data of service provider to form a more comprehensive data set. Then, RBF (Radial Basis Function) neural network improved by the Levenberg-Marquardt (LM) algorithm is used to update the weight of the neural network dynamically, which improves the forecasting accuracy and realizes the dynamic multiple-step forecasting. The experimental results demonstrate that MulA-LMRBF is better than previous approaches in term of precision and is more suitable for multi-step forecasting."
38,"The DynaSOAr framework presents a wholly service-oriented approach to grid and Internet-based computing that makes a clear and explicit separation of concerns between service-provision and resource-provision for each service invocation. The separation allows the dynamic deployment of code at runtime, in the form of a service implementation, between a service provider and an explicit resource provider. This paper presents work in progress towards an integrated tripartite security model and framework that enables the security constraints of each engaging party to be expressed, propagated, unified and enforced as part of a DynaSOAr service invocation."
39,"Web service composition is to integrate existing web services to provide a compound service which satisfies specified requirement. However, traditional web service compositions fail to provide different compound services under various scenarios. In this paper, we propose an approach to compose services with context. A context ontology is defined to describe the scenario for user. An abstract service description is defined to describe current three kinds of services including WSDL/Restful/Web API. The goal is to correlate context and service composition to improve the quality of the compound service from real services."
40,"Community of Web services (CWS) is a society composed by a number of functionally identical Web services. The communities always aim to increase their reputation level in order to obtain more requests. In this paper, we propose an effective mechanism dealing with reputation assessment for communities of Web services. The proposed mechanism is based on after-service feedbacks provided by the users to a run-time logging system. The proposed method defines the evaluation metrics involved in reputation assessment of a community, and supervises the logging system in order to verify the validity and soundness of the feedbacks provided by the users. In this paper, the proposed framework is described, a theoretical analysis of its assessment and its implementation along with empirical result discussions are provided. We also show how our model is efficient, particularly in very dynamic environments."
41,"Operation services are reusable and shareable units of configuration code executed by configuration management tools (CMTs), achieving continuous deployment and continuous delivery. With the prevalence of DevOps (Development and Operations), thousands of operation services have been developed for various software systems, and they are publicly available through the online repositories of popular CMTs. However, locating and retrieving desired operation services is challenging since keyword-and tag-based search provided by a repository is with low precision. In this paper, we implement a hierarchical categorization approach based search service, named OSFinder, which searches and locates desired operation services more accurately. OSFinder first constructs a category hierarchy for operation services across multiple repositories, and then it classifies over 13,000 operation services into 90 categories based on machine learning technique, finally it provides a search for users. With OSFinder, a user can narrow down his search scope by tracking the category hierarchy in a top-down way, and then searches in a small group with keywords. The evaluation shows that OSFinder outperforms keyword-and tag-based search."
42,"This paper discusses a solution based on Web services and service oriented architecture involving interorganizational information systems in the context of multichannel integration in retailing industry. Distribution intensive industries such as retailing have seen a proliferation of distribution channels. This is in part triggered by the intense competition as well as the retailers' drive to reach the customers through all possible channels. As the number of channels for a retailer increases, managing the dynamics of customer behavior in the multichannel environment becomes complex. Customers have an option of interacting with retailers across a number of channels. Acquiring and retaining customers requires that relationship management applications should be able to accommodate the various channels. In this paper, we provide an overview of the benefits of using Web services in multichannel integration and related retail processes. We propose a reference service oriented architecture for usage of Web services involving inter and intra company systems for multichannel integration."
43,"Most approaches for Web service compositions focus on the technical level as they specify ""how-to"" achieve the composition instead of providing casual users with means to express ""what-to"" achieve their business objectives. This new mindset requires extending Web services with the concept of capability, which describes what a service can do to reduce the gap between Web services and business processes. In this paper, we propose a three-layer Web service capability model to capture objectives, to specify actions that achieve objectives, and establish links between capabilities. Based on this model, the capability matching process does not only discover Web services from high level requirements expressed by Semantics of Business Vocabulary and Business Rules (SBVR), but it also derives constraints on Web services."
44,"With the advent of Web 2.0 application, and the increasing number of browsers and platforms on which the applications can be executed, cross-browser incompatibilities (XBIs) are becoming a serious problem for organizations to develop web-based software. Although some techniques and tools have been proposed to identify XBIs, they cannot assure the same execution when the application runs across different browsers as only explicit user activity is considered, and thus prone to generating both false positives and false negatives. To address this limitation, this paper describes X-Check, a platform that enables cross-browser testing as a service by leveraging record/replay technique. Comparing to existing techniques and tools, X-Check supports to detect cross-browser issues with high accuracy. It also provides useful support to developers for diagnosis and (eventually) elimination of XBIs. Our empirical evaluation shows that X-Check is effective, improves the state of the art."
45,"Web service search has been a serious concern for service-oriented software development. The challenge is how to find the right Web services efficiently and effectively for a given development task. There have been many efforts on developing techniques or systems to search services. Though effective in certain scenarios, existing techniques do not form systems for public use; or are based on one query modal--keyword query, which cannot give the matched services accurately. In this paper, we introduce proposed multimodal query search, where users can use keyword and file as query or custom the query. The multimodal query is based on an innovative similarity measure approach, which incorporates both semantic information and structural information of Web services. Our experiments and test cases validate the effectiveness of the approach. Compared with the alternative system Seekda, it is able to obtain much higher search accuracy with keyword query (with a match rate of 2-4 times higher than that of Seekda). The custom search can achieve 100% top-3 match rate, while Seekda fails in most cases using keywords."
46,"The performance of a service's QoS may evolve relatively frequently with its internal changes or the changes of dynamic Internet environment, especially when some""intentional"" deceptions are taken into consideration. Therefore, the service providers could not always deliver their services according to their ""promised"" quality. In view of this challenge, a history record-based service optimization method, named Hire Some, is investigated in this paper. This method aims at enhancing the credibility of service composition plan, taking advantage of a web service's QoShistory records, rather than using the tentative QoS values advertised by the service provider. At last, a case study and an experiment is presented for validating the method."
47,"In data-centric environments, for example, in the field of scientific computing, the transmission of large amount of structured data to Web services is required. In service-oriented environments (SOA), the Simple Object Access Protocol (SOAP) is commonly used as the main transport protocol. However, the resulting 'by value' data transmission approach is not efficiently applicable in data-centric environments. One challenging bottleneck of SOAP arises from the XML serialization and deserialization when processing large SOAP messages. In this paper, we present an extended Web service framework which explicitly considers the data aspects of functional Web services. Aside from the possibility to integrate specialized data transfer methods in SOA, this framework allows the efficient and scalable data handling and processing within Web services. In this case, we combine the advantages of the functional perspective (SOA) and the data perspective to efficiently support data-centric environments."
48,"With the development of deep learning and artificial intelligence, more and more research apply neural networks to natural language processing tasks. However, while the majority of these research take English corpus as the dataset, few studies have been done using Chinese corpus. Meanwhile, Existing Chinese processing algorithms typically regard Chinese word or Chinese character as the basic unit but ignore the deeper information into the Chinese character. In Chinese linguistic, strokes are the basic unit of Chinese character who are similar to letters of the English word. Inspired by the recent success of deep learning at character-level, we delve deeper to Chinese stroke level for Chinese language processing and developed it into service for Chinese text classification. In this paper, we dig the basic feature of the strokes considering the similar Chinese character components and propose a new method to leverage Chinese stroke for learning the continuous representation of Chinese character and develop it into a service for Chinese text classification. We develop a dedicated neural architecture based on the convolutional neural network to effectively learn character embedding and apply it to Chinese word similarity judgment and Chinese text classification. Both experiments results show that the stroke level method is effective for Chinese language processing."
49,"We propose a new framework for composing Sensor-Cloud services based on dynamic features such as spatio-temporal aspects. To evaluate spatio-temporal Sensor-Cloud services, two new quality attributes are introduced. We present a heuristic algorithm based on A* to compose Sensor-Cloud services in terms of spatio-temporal aspects. In addition, a new spatio-temporal technique based on 3D R-tree to access Sensor-Cloud services is proposed. Analytical and simulation results are presented to show the performance of the proposed approach."
50,"With the rapid development of Web2.0 and its related technologies, Mashup services (i.e., Web applications created by combining two or more Web APIs) are becoming a hot research topic. The explosion of Mashup services, especially the functionally similar or equivalent services, however, make services discovery more difficult than ever. In this paper, we present an approach to recommend Mashup services to users based on user interest and social network of services. This approach firstly extracts users' interests from their Mashup service usage history and builds a social network based on social relationships information among Mashup services, Web APIs and their tags. The approach then leverages the target user's interest and the social network to perform Mashup service recommendation. Large-scale experiments based on a real-world Mashup service dataset show that our proposed approach can effectively recommend Mashup services to users with excellent performance. Moreover, a Mashup service recommendation prototype system is developed."
51,"Service oriented computing (SOC) allows resources on a network to be made available as services. For a business service, differentiated services can be provided based on the usage context, i.e., location, age, purpose and user profiles. In differentiated services, service outcomes depend on context. Currently there is no efficient technical solution for supporting differentiated service development. In this paper we present an approach to deliver differentiated services realized by configurable business processes so that service flexibility, manageability and reusability can be achieved."
52,"QoS-aware service composition intends to integrate services from different providers and maximize the global QoS in order to increase the user's satisfaction degree while subjecting to dynamic context constraints. Current composition approaches only focus on optimizing a single process to maximize the satisfaction degree for one party. When multiple processes are performed concurrently by their selfish users in a dynamic resource-constrained environment, new issues will arise, i.e., undesirable competition for service resources, extra waiting and frequent change of contexts. To address these issues, this paper aims to optimize QoS-aware services composition for multiple selfish users if the communication among users is allowed. Firstly, we propose an extensional QoS-aware service selection model for each process. Then based on this model, we present fault handling mechanisms before and during the execution of concurrent composite services for concurrent processes based on a multi-issue negotiation protocol among agents, and an adaptive context-aware service re-selection mechanism for adjusting the service execution plan for each running composite service in the dynamic resource-constrained environment. Comparative experiments reveal our approach facilitates to increase the average satisfaction degree, reduce the average waiting time of multiple users, and make the satisfaction degrees among multiple users more evenly distributed in the dynamic resource-constrained environment."
53,"Web services have many important advantages. But their great drawback, the invocation overhead, has not been a research focus. So far, only invocations of simple Web services were considered. But Web service composition may create additional performance problems. Our measurements demonstrate that Web service composition may reduce the maximal load of a system drastically. The reduction quotient increases quasi-exponentially with the number of service compositions. We call that phenomenon ""service congestion"" since it is not due to the combined payload of the composed services. Such detrimental performance effects can not be tolerated in many areas. For that reason, we propose an optimized service composition architecture as a solution. This service component architecture uses service connectors on top of standard Web service middleware. It optimizes automatically the local invocation of services with a Lookup&amp;Service bus. The result is that no service congestion occurs since local service invocations have the cost of local calls"
54,"The vision of the Semantic Web is to reduce manual discovery and usage of Web resources (documents and services) and to allow software agents to automatically identify these Web resources, integrate them and execute them for achieving the intended goals of the user. Such a composed Web service may be represented as a workflow, called service flow. Current studies of Web services are not sufficient for automatic composition. This paper presents different types of compositional knowledge required for Web service discovery and composition: syntactic, semantic and pragmatic knowledge. As a proof of concept, we have implemented our framework in a cardiovascular domain which requires advanced service discovery and composition across heterogeneous platforms of multiple organizations. Within this framework, we describe (1) How to represent the compositional knowledge, which plays a role in service discovery and composition, in DAML-S; (2) How heterogeneous medical services intemperate in a composed medical service flow. (3) To solve this knowledge level integration, we build on an ontology integration method called SEMIO (Semantic Interoperability)."
55,"Monitoring resource utilization is an essential task in utility computing (UC). Typically, a UC manager with adaptors is used to orchestrate and collect the monitoring results. However, large IT systems, such as those in data centers are required to monitor dynamic resources running in distributed platforms, thus demanding a complex UC manager structure. This paper introduces a novel approach that utilizes a platform specific aspect-oriented programming (AOP) tool to dynamically weave a monitoring Web service into a running resource to enable communication with a UC manager exposed with standard monitoring Web service interfaces. This AO-Web service approach provides a simple yet powerful and effective means for the dynamic monitoring of distributed resources running in heterogeneous platforms."
56,"Organizing Web services into functionally similar clusters, is an efficient approach to discovering Web services efficiently. An important aspect of the clustering process is calculating the semantic similarity of Web services. Most current clustering approaches are based on similarity-distance measurement, including keyword, ontology and information-retrieval-based methods. Problems with these approaches include a shortage of high quality ontologies and a loss of semantic information. In addition, there has been little fine-grained improvement in existing approaches to service clustering. In this paper, we present a new approach to grouping Web services into functionally similar clusters by mining Web service documents and generating an ontology via hidden semantic patterns present within the complex terms used in service features to measure similarity. If calculating the similarity using the generated ontology fails, the similarity is calculated by using an information-retrieval-based term-similarity method that adopts term-similarity measuring techniques used by thesaurus and search engines. Another important aspect of high performance in clustering is identifying the most suitable cluster center. To improve the utility of clusters, we propose an approach to identifying the cluster center that combines service similarity with the term frequency-inverse document frequency values of service names. Experimental results show that our clustering approach performs better than existing approaches."
57,"Nowadays, Web services are widely used because of their interoperability and reusability. Multiple Web services can be composed following some business logic specified by BPEL (Business Process Execution Language) scripts. Since BPEL scripts allow specifying concurrent workflow, typical concurrency problems, such as data race, atomicity violation and order violation, also commonly occur in BPEL scripts. These issues are hard to detect and reproduce due to their non-determinism and the special language features of BPEL. In this paper, we implement a tool to detect data races for WS-BPEL based on static analysis approach and constraints solver. Our system is based on three key concepts: (1) a preprocess model to record necessary information, (2) a thorough Happens-Before model of WS-BPEL concurrency, (3) constraint encoding to transfer Happens-Before relationship to constraints and check if there is a feasible solution (namely data races) by Z3-Str solver. We evaluate the usability and performance of our tool on 10 benchmark programs with effective results."
58,"Nowadays vast amounts of data are being produced in continuous ways. They may come from sensors, smart meters, application logs, monitoring software etc. The data need to be processed in realtime to gain actionable insights. Services like smart grid load balancing, cloud platform maintenance, can be carried out in an efficient way. Stream processing is the programming paradigm that answers such demand. When talking about stream processing, we can easily recall several famous open-source software frameworks such as Spark Streaming, Samza, Flink and Storm. Although they provide distributed, robust, low-latency stream processing engines, it's still difficult for an end user to set up a usable stream processing application from scratch. Firstly, users are required to write code to define their business related stream processing logic. Secondly, the submission and update of the stream processing logic require service restart, therefore it may lead to service unavailability for minutes. Thirdly, extra operation effort are required for handling scaling and failover issues. In this paper, we present RTA, a released research service on realtime data processing. The RTA service fills the gap between the stream processing requester and the existing software stacks. It offers a SQL-like stream query language for defining stream processing logic definition over streaming data. It allows users easily define their stream processing logic without programming. In RTA service, stream processing logic is also treated as a type of input, which enables online logic update without service downtime. The RTA service also provides scalability, high availability and resource isolation for serving multiple tenants. In this paper, we also provide a comprehensive evaluation of our service through a case study."
59,"With the increased number of web services advertised on the internet, it is becoming vital to resolve typical problems of service recommendation. Although service recommendation has been studied by researchers in recent years, existing methods have remarkable achievements on offering single service recommendation, not only considering functional features of web services but also non-functional features. However, the customers usually adopted composite services to satisfy complex and coarse-grained requirements. The traditional service recommendation does not have much concern about composite services. Through a long period of usage, the dependencies among composite services are hidden in historical usage records. In reality, these dependencies have great influence on the quality of service recommendation. To improve the effectiveness of service recommendation, this paper proposes a novel service recommendation approach based on service usage patterns. Firstly, the similar customer group of target customer is identified through the personal attribute based clustering and similarity of rating preference, Secondly, service usage patterns of the similar customer group are mined based on the variant of Generalized Sequential Patterns (GSP) algorithm, Thirdly, promising services are recommended for the target customer according to the matching degree between previously used services and service usage patterns, Finally, experimental results verify the efficiency and effectiveness of our approach."
60,"Today, multimedia system are still widely realized as monolithic systems. But building such applications using Service-Oriented Architectures - especially for the Processing and Delivery of continuous Multimedia data streams - has been a controversial topic since years. As a result, applications in the multimedia domain cannot yet benefit from Web service architectures. Thus, building and maintaining large-scale multimedia applications remains a difficult, costly, time-consuming and challenging problem. In this paper we present our approach for building large scale multimedia systems and compare it with the current state of the art, concentrating on the selection and validation of multimedia service composition."
61,"Although a data processing system often works as a batch processing system, many enterprises deploy such a system as a service, which we call the service-oriented data processing system. It has been shown that in-memory data processing systems suffer from serious memory pressure. The situation becomes even worse for the service-oriented data processing systems due to various reasons. For example, in a service-oriented system, multiple submitted tasks are launched at the same time and executed in the same context in the resources, compared with the batch processing mode where the tasks are processed one by one. Therefore, the memory pressure will affect all submitted tasks, including the tasks that only incur the light memory pressure when they are run alone. In this paper, we find that the reason why memory pressure arises is because the running tasks produce massive long-living data objects in the limited memory space. Our studies further reveal that the long-living data objects are generated by the API functions that are invoked by the in-memory processing frameworks. Based on these findings, we propose a method to classify the API functions based on the memory usage rate. Further, we design a scheduler called MURS to mitigate the memory pressure. We implement MURS in Spark and conduct the experiments to evaluate the performance of MURS. The results show that when comparing to Spark, MURS can 1) decrease the execution time of the submitted jobs by up to 65.8%, 2) mitigate the memory pressure in the server by decreasing the garbage collection time by up to 81%, and 3) reduce the data spilling, and hence disk I/O, by approximately 90%."
62,"Web service orchestration is becoming widely spread for the creation of composite Web services using standard specifications such as BPEL4WS. The myriad of specifications and aspects that should be considered in orchestrated Web services are resulting in increasing complexity. This complexity leads to software infrastructures difficult to maintain with interwoven code involving different aspects such as security, fault tolerance, distribution, etc. In this paper, we present ZenFlow a reflective BPEL engine that enables to separate the implementation of different aspects among them and from the implementation of the regular orchestration functionality of the BPEL engine. We illustrate its capabilities and performance exercising the reflective interface through a decentralized orchestration use case."
63,Provides a listing of current committee members and society officers.
64,"Websites increasingly embed semantic data for search engine optimization. The most common ontology for semantic data, schema.org, is supported by all major search engines and describes over 500 data types, including calendar events, recipes, products, and TV shows. As of today, users wishing to pass this data to their favorite applications, e.g., their calendars, cookbooks, price comparison applications or even smart devices such as TV receivers, rely on cumbersome and error-prone workarounds such as reentering the data or a series of copy and paste operations. In this paper, we present Semantic Data Mediator (SDM), an approach that allows the easy transfer of semantic data to a multitude of services, ranging from web services to applications installed on different devices. SDM extracts semantic data from the currently displayed web page on the client-side, offers suitable services to the user, and by the press of a button, forwards this data to the desired service while doing all the necessary data conversion and service interface adaptation in between. To realize this, we built a reusable repository of service descriptions, data converters, and service adapters, which can be extended by the crowd. Our approach for linking services to websites relies solely on semantic data and does not require any additional support by either website or service developers. We have fully implemented our approach and present a real-world case study demonstrating its feasibility and usefulness."
65,"In this paper, we present a REST Chart based approach to design and implement RESTful northbound API in SDN. In our approach, the RESTful API is modeled with Petri Net, and REST constraints, such as hypermedia driven, are enforced naturally. The RESTful structure of the designed northbound API can be checked almost automatically based on the described REST Chart model. The proposed approach has been applied to design and implement the northbound API of SDN in data center networks with OpenStack Neutron. To improve the protocol efficiency, we developed a structured caching mechanism for data network applications with SDN, that reduces the response time overheads by more than 65%, while it fully maintains the desired flexibility and scalability of the RESTful northbound API."
66,"Many real-world applications are complex, involving many user ""choices"", such as different functionalities, different ways to achieve a goal, etc. Conventional automated service composition models do not consider such potential choices, or simply consider them independently. Also, existing service composition models do not model exceptions and automated composition approaches require that after an exception, the original system goal should still be achieved. This may not be feasible for some exceptions. Thus, the service composition model should also consider alternate goals after exceptions occur. In this paper, we first define the concept of multi-functionality and develop a holistic service composition model. Since most existing composition reasoning techniques can only handle a single functionality, we extend them and develop new algorithms for automated holistic service composition. A case study system is used to illustrate how our approach automatically generates a holistic workflow for a system with multiple functionalities."
67,"The success of deep neural networks (DNN) in solving general machine vision problems has agitated a wave of its adoption in automated visual inspection solutions. Especially, DNN is able to learn by itself those relevant image features to reach a model that is robust to image quality variation, which promises very scalable solutions. The correlation between image acquisition hardware and image processing software, which is typical in traditional solutions, is alleviated. On this basis, we propose a novel visual inspection service architecture that is scalable, economic and reliable. The realization challenges of the visual inspection service are analyzed and the corresponding designs in model composition and model scheduling are presented. Special focus is placed on the runtime performance of inspection models and the efficient use of the computing resources of contemporary commodity servers."
68,"The Web Services Choreography Description Language (WS-CDL) is a specification developed by the W3C that can be viewed as a blueprint for the development of end-point services. Considering that it is the W3C candidate recommendation for web service choreography, it is worth providing a systematic approach for its modeling, analysis and verification. The Unified Modeling Language (UML) is the de facto industry standard for modeling. Applying UML to model WS-CDL is obviously a promising solution to bring together academics and practitioners in through a unique standard language. This paper proposes to use different UML diagrams to model WS-CDL. Given the UML specification of WS-CDL, we then provide a systematic way of formally analyzing and verifying WS-CDL."
69,"Context-aware service is a new service mode that can provide appropriate service automatically to improve service level based on context information. In context-aware service, service is provided based on the current scene of customer. The scene is identified by context, and scene transition is caused by the change of context. Current research focuses on modeling service with context directly. However, the fact is that the vast majority of context change does not cause the change of scene, and monitoring context to determine the scene directly is inefficient, especially of the multi-context application. We use event which is defined by context as the motivation of scene change and propose an event driven model of context-aware service: EDM. A case of smart home service is discussed to show how to use EDM to do the requirement analysis, design, and implementation of context-aware service. Experimental results show that this approach can effectively reduce the number of scene determination in multi-context application."
71,"A service-oriented application is composed of several web services to provide complex functionality that a single web service cannot provide. A set of services along with their control flows can be frequently used in multiple applications. Such services form a service composition pattern which is well tested in the numerous adoptions. Reusing service composition patterns in service composition provides an efficient way to improve the quality of new applications. To facilitate the documentation of service composition patterns, we propose an approach to automatically recognize service composition patterns from various applications. We identify service composition patterns by locating a set of associated services commonly used by different applications and recovering the control flows among the set of associated services."
72,"Handwritten Chinese character recognition (HCCR) is an important research field of pattern recognition, which has attracted extensive studies during the past decades. Recently convolutional neural network (CNN) based methods have achieved the state-of-the-art performance for handwritten Chinese character recognition. Nevertheless, handwritten Chinese character recognition is still limited to be effectively used in the actual environment due to the large-scale vocabulary and great diversity of handwriting style. In this paper, we constructed a handwritten Chinese character recognition service based on convolutional neural network, which tries to make effective use of handwritten based printed fonts and existing handwritten database. At the same time, the service can effectively collect more handwritten data to expand the training dataset, which makes it easy to adapt to the new handwriting styles. Meanwhile, We propose a multi-level recognition theory applied to online handwritten Chinese character recognition, which may improve the accuracy of handwritten Chinese character recognition and break the limitations of handwritten Chinese character recognition by identifying the structure of Chinese characters and possible stroke orders firstly. Furthermore, we try to apply the method of online character recognition to the offline character recognition based on the basic writing rules."
73,"The ability to build new (complex) services by composing existing services is one of the key benefits of the Service Oriented Architecture paradigm. Existing approaches to automate composition requires pre-planning or prediction of the number of required services, making them unsuitable in dynamic composition scenarios. To address this gap, we present a consistency-based service composition approach, where composition problems are modeled in a generative constraint-based formalism. We illustrate how the configuration of service processes differs from established constraint-based configuration techniques and develop an algorithm to synthesis valid service process compositions. We also show that our technique scales well to non-trivial problems."
74,"When moving from monolithic applications towards service-oriented multimedia frameworks, the composition of Web services to form complex multimedia workflows becomes a demanding problem. Especially mobile devices require a flexible composition strategy as they often have to move computationally complex or power-demanding tasks to powerful servers. Such a strategy also has to consider the changing environment due to movements of the device and it has to adapt to device-specific characteristics, e.g., the current battery level. Hence, mobile devices experience problems beyond the mere question of services' availability or successful execution. We propose the E
<sup>2</sup>
Mon algorithm that monitors the execution chain of Web services and gracefully recovers from failures of individual services and network-specific or device-specific alarms. The sophisticated control flow dynamically chooses the quality-optimal and cost-optimal composition of available services handling both successive and parallel service execution"
75,"Service-oriented environments facilitate dynamic processes whose properties can be altered during runtime. The transactional support of such processes holds specific requirements that are not completely covered by existing specifications. In this paper, we introduce a life cycle model for transactional dynamic processes and analyze existing specifications with respect to their potentials to support such a model. Subsequently, we propose a framework resolving the weaknesses of existing specifications and allowing comprehensive transactional coordination of dynamic processes."
76,"A major drawback of using SOAP for application integration is its enormous demand for network bandwidth. Compared to classical approaches like Java-RMI and CORBA, SOAP messages typically cause more than three times more network traffic. In this paper we will give a detailed survey of state of the art binary encoding strategies for SOAP and introduce a new experimental concept for SOAP compression, which makes use of the commonly available WSDL description of a SOAP Web service."
77,"A Web service is defined as an autonomous unit of application logic that provides either some business functionality or information to other applications through an Internet connection. Web services are based on a set of XML standards such as universal description, discovery and integration (UDDI), Web services description language (WSDL), and simple object access protocol (SOAP). Recently there are increasing demands and discussions about Web services privacy technologies in the industry and research community. In general, privacy policies describe an organization's data practices what information they collect from individuals (e.g., consumers) and what (e.g., purposes) they do with it. To enable privacy protection for Web service consumers across multiple domains and services, the World Wide Web Consortium (W3C) published a document called ""Web services architecture (WSA) requirements"" that defines some specific privacy requirements for Web services as a future research topic. At this moment, there is still no standardized Web services privacy technology. This paper briefly overviews the research issues of Web services privacy technologies."
78,"Web services gain momentum for developing flexible service-oriented architectures. Quality of service (QoS) issues are not part of the Web service standard stack, although non-functional attributes like performance, dependability or cost and payment play an important role for service discovery, selection, and composition. A lot of research is dedicated to different QoS models, at the same time omitting a way to specify how QoS parameters (esp. the performance related aspects) are assessed, evaluated and constantly monitored. Our contribution in this paper comprises: a) an evaluation approach for QoS attributes of Web services, which works completely service-and provider independent, b) a method to analyze Web service interactions by using our evaluation tool and extract important QoS information without any knowledge about the service implementation. Furthermore, our implementation allows assessing performance specific values (such as latency or service processing time) that usually require access to the server which hosts the service. The result of the evaluation process can be used to enrich existing Web service descriptions with a set of up-to-date QoS attributes, therefore, making it a valuable instrument for Web service selection"
79,"Recently, the Web Services Interoperability Organization(WS-I) has announced to have completed its interoperability standards work. The latest deliverables include the so-called ""Basic Security Profile"" and the ""Reliable SecureProfile"". This gives rise to the question whether or not Web Services adopters can rely on interoperability of Web Services stacks, in particular in terms of security and reliability features. To answer this question, we thoroughly analyze two important Web Services stacks for interoperability of WS-Security and WS-Reliable Messaging features. Our analysis shows that security and reliability features are far from being implemented in an interoperable manner. Additionally, we reveal that some of those interoperability problems are not even covered by WS-I profiles and therefore conclude that WS-I's work has not yet resulted in Web Services interoperability."
80,"Outlier detection has been shown to be a promising machine learning technique for a diverse array of felds and problem areas. However, traditional, supervised outlier detection is not well suited for problems such as network intrusion detection, where proper labelled data is scarce. This has created a focus on extending these approaches to be unsupervised, removing the need for explicit labels, but at a cost of poorer performance compared to their supervised counterparts. Recent work has explored ways of making up for this, such as creating ensembles of diverse models, or even diverse learning algorithms, to jointly classify data. While using unsupervised, heterogeneous ensembles of learning algorithms has been proposed as a viable next step for research, the implications of how these ensembles are built and used has not been explored."
81,"Telecommunication system providers move their IP multimedia subsystems to virtualized services in the cloud. For such systems, dedicated hardware solutions provided a reliability of 99.999% in the past. Although virtualization offers more cost efficient usage of such services, it comes with higher complexity for providing reliable running software components due to the fragile computation stack. In order to hide the impact of such problematic behaviors, automatic mechanisms may help to detect degraded state anomalies in order to execute remediation actions. This work introduces IFTM as a framework for unsupervised anomaly detection in a distributed environment based on real-time monitoring data. The proposed approach consists of two key concepts using an automatic identity function and threshold learning to distinguish between normal and abnormal system behaviors. The evaluation is performed on a testbed running an open source implementation of the IP multimedia subsystem (Clearwater) executed on a replicated Openstack cloud environment. Results show the applicability of IFTM with high detection rates (98%) and low number of false alarms."
82,"This paper studies the scenario where data in business documents is aggregated by different entities via the use of Web services in streamlined business processes. The documents are transported within the Simple Object Access Protocol (SOAP) messages and travel through multiple intermediary entities, each potentially makes changes to the data in the documents. The WS-security provides integrity protection by allowing portions of a SOAP message to be signed using eXtensible Markup Language (XML) signature scheme. This method however, has not considered the situation where a portion of data may be modified by another entity, therefore a need to allow the originating system to control which intermediary entity is authorized to change which portion of the data. The XML signature scheme also does not provide the final recipient the trust for the intermediary entity that makes the changes. In our paper, we study the security requirements for a streamlined business process, and proposes a novel scheme using sanitizable signature on SOAP messages to complement the XML signature to address not only integrity protection but also control of change as well as establishment of trust for intermediary entities. We show how the proposed scheme can be incorporated into the existing standards and be customizable to achieve flexible use of both the vanilla and sanitizable signatures as required in a business scenario. With the proposed technique, IT systems can be more loosely coupled and reap the benefits of distributed systems, such as delegation of work and encapsulation of business logic."
83,"This paper analyzes the execution behavior of web services on devices with limited resources. The experiments compare web services in the Axis2 and CXF frameworks analyzing performance and power consumption. To determine which framework is better suited for service provision, a testing environment and a performance and energy evaluation between them are presented. We show that the Raspberry Pi can be useful in service-oriented applications for different types of tasks. Bringing together the best features of small devices and SoC, it is possible to provide diverse, mobile and green applications."
84,"Online Social Networks (OSNs) have been used to enhance service provision and service selection, where trust is one of the most important factors for the decision making of service consumers. Thus, it is significant to evaluate the trustworthiness of the service providers along the social trust paths from a service consumer to a service provider. However, there are usually many social trust paths between an unknown service consumer and service provider. Thus, a challenging problem is how to effectively and effciently find those social trust paths that can yield trustworthy trust evaluation results based on the requirements of a service consumer particularly in the real-time OSN environments. In this paper, we first present a contextual trust-oriented social network structure and a concept of Quality of Trust (QoT). We then model the multiple social trust paths finding with end-to-end QoT constraints as the Multiple Constrained K Optimal Paths (MCOP-K) selection problem, which is NP-Complete. To deal with this challenging problem, based on the Monte Carlo method and our optimization search strategies, we propose a new efficient and effective approximation algorithm D-MCBA. The results of our experiments conducted on a real-world dataset of OSNs illustrate that D-MCBA can efficiently identify the social trust paths with better quality than our previously proposed MONTE K algorithm that is the most promising algorithm for the social trust path finding in OSNs."
85,"Today's Web applications and their respective business processes reside under the control of different organizations. Establishing federations between these organizations, i.e. bringing these business processes together by transcending organizational and security borders, raises a new class of security questions concerning the management of trust relationships between the autonomous bodies that wish to work together. Based on the Webcomposition architecture model we provide a modeling approach for federated Web applications. In this paper we present a methodology for formalizing these models using the ambient calculus for use in further computation. Based on the results we help the users to identify and detect security related aspects in Web-based federations."
86,"Wireless Web services, also called mobile services or M-services, provide access to Web services through wireless networks. In this paper, we propose novel access methods and multi-channel organization for mobile users to effectively access composite M-services in wireless broadcast networks. We define a few semantics for accessing broadcast based M-services and study their impact on access efficiency."
87,"Aligning different ontologies from similar (or same) domains is an active field of current research. There are various solutions which process and analyze lexical, structural or semantic information to align ontologies. However, there are few solutions that focus on interpreting the concepts that entities are presented with and using them in relation to the semantics implied in an ontology. In this paper, the prototype (OACLAI) is presented to tackle this by combining lexical analysis with consequences from reasoners which reflect the semantics implied in an ontology. We evaluate OACLAI over the four real ontologies and compare it against the seven solutions. The experiments show that the accuracy of OACLAI is higher than those of others on average."
88,"The design of maintenance mechanisms of distributed hash tables (DHTs) is usually specific to their initial graphs, and thus it is complicated and error-prone. Zhang and Liu propose in [4j the “distributed line graphs” (DLG) mechanism, a universal technique for designing DHTs based on arbitrary regular graphs while preserving the main features of the initial graphs. However, two important properties of DLG, the expandability and fidelity, have not been studied with detailed explanations or analysis. In this paper, we study the above properties of DLG transformations, and prove that (i) the DLG transformations are incrementally expandable, and (ii) DLG transformations from G
<sub>i</sub>
 to G
<sub>i+1</sub>
 keep fidelity."
89,"QoS has become an important measure for web service selection. In this paper, we present an approach which can provide the approximate QoS value for users, and support finding the optimal web service. Firstly, it clusters the users based on location and network condition, then according to the QoS historical statistics of users in the same cluster, uses the linear regression algorithm to predict the QoS value based on invocation time and workload."
90,"Web service directories are shared resources that have to accommodate a high number of concurrent read requests, whereas updates are relatively infrequent. To allow for the automatic composition of complex web services based on those contained in a directory, read requests may involve a series of queries which require a consistent view of the data. We have developed an efficient web service directory that is based on the Multiversion Generalised Search Tree (MVGiST), an integration of a multidimensional index structure with multiversion concurrency control. The MVGiST is able to index web services according to their input and output parameters, supports a high level of concurrent read requests, and guarantees consistency across multiple subsequent read queries. In this paper we evaluate the performance and scalability of the MVGiST and compare it with a traditional, locking-based concurrency control mechanism."
91,"Regression testing (RT), testing software with previously used test cases, is a mainstream practice in software maintenance. Regression test selection (RTS) is to reduce the number of tests which need to be retested. Safe RTS techniques add the assurance that no modification-revealing test case will be left unselected. Several effective safe RTS techniques were developed for traditional applications, but none of them can be directly applied to Web services, even though there have been RT tools and techniques for Web services test-case generation, and ranking competing services. We have developed an approach to adapt Rothermel and Harrold's safe RTS technique to Web services. This approach was designed to be automated. In doing so, we have recognized a set of challenging issues that arise as a result of multiple concurrent modifications in distributed, autonomous, but still interconnected services. We believe not only these issues are common to any automated RTS approach, the needs for the solutions to these issues will also become more and more keen as composite Web services are getting more and more ubiquitous."
92,Recent advances in Web services have made it practical to provide communication Web services to enable communication through SOA and package communication capability as services. This paper provides an appropriate implementation to deliver the multimedia conferencing communication components as Web services in order to be used simply by Web service clients in converged applications.
93,"During the design procedure of BPEL (Business Process Execution Language) processes, there may be control dependency deadlocks among the activities in BPEL. We design a novel Agree/Refuse Matrix (ARM) to real-timely detect control dependency deadlocks. Online detection approach can be integrated into Active BPEL Designer to extend their functions and improve the accuracy of BPEL processes. Most importantly, this method's real-time property can help avoiding detecting the whole process from scratch caused by process's partial modifications."
94,Today's mobile users have access to a wide range of Web-based services. This paper presents our m-Tableaux algorithm for enabling cost-efficient and optimised semantic reasoning to support pervasive service discovery. We present performance evaluation of the m-Tableaux optimisation strategies which clearly demonstrate its operational feasibility on a mobile device.
95,"Web services provide a standard means of interoperating between different software applications, running on a variety of platforms and/or frameworks. While the concepts of Web services are aimed at providing a standard means to support interoperable machine-to-machine interaction over a network, they do not solve the problem of trust between service requesters and providers. A trusted component is defined as a reusable software element possessing specified and guaranteed property qualities. The highly reusable nature of a Web service emphasizes the need for a ""trust ensuring"" mechanism between the requester and the provider of the service. The focus of this paper is to suggest a fortified Web services architecture introducing the concept of contracts to increase the level of trust between the requester and the provider of the requested service. In order to achieve this goal a new language is introduced into the fortified Web services architecture: WS-Contract. WS-Contract is a machine-processable specification of the Web service semantics, formally supporting the different levels of contract information. It defines the pre-and post conditions of the Web services interface, the synchronization policy of the service and the QoS parameters that should be maintained between the requester agent and the provider agent. WS-Contract relies, wherever possible, on existing Web services standards to build the necessary ""trust ensuring"" mechanism for Web services"
96,"With the number of smartphone applications (apps) growing explosively, it has a practical significance to provide personalized app recommendations. In this paper, we propose the GTRM, a new recommendation model which builds the top-N app list by optimizing the metric Group-oriented Mean Average Precision (GMAP). GMAP is an extension of the traditional metric Mean Average Precision (MAP) and it measures the precisions of top-N list in terms of the collective positions of related items rather than the position of individual item. Therefore, GTRM can recommend a more reasonable top-N app list by avoiding overfitting problem. The details of GMAP and GTRM are described. Extensive experiments on a real-world app dataset demonstrate the effectiveness of GTRM, and show that GTRM significantly outperforms the compared methods."
97,"In this paper we propose a meta-model for nonfunctional property descriptions targeted to support the selection of Web Services. The approach is based on the explicit distinction between NFP offered by providers and requested by users, on the concept of policy that aggregates NFP descriptions into single entities with an applicability condition, and finally on a set of constraint operators, which is particularly relevant for NFP requests. The semantic meta-model embracing the above perspective is defined by a BNF syntax whose semantics is formalized by an ontology. The ontology has been formalized in OWL-DL and WSML to provide for logical syntax. The logic upon which the meta-model supports NFP-based selection is discussed in the paper."
99,"In recent years, the issues in web service security have been widely investigated and various security standards have been proposed. But most of these studies and standards focus on the access control policies for individual web services and do not consider the access issues in composed services. Consider a simple service chain where service s
<sub>1</sub>
 accesses s
<sub>2</sub>
, and s
<sub>2</sub>
, in turn, accesses service s
<sub>3</sub>
. The information returned from s
<sub>3</sub>
 to s
<sub>2</sub>
 may be used to compute some results that are further returned to s
<sub>1</sub>
. The current web service security framework does not provide any mechanisms to control such an information flow, and hence, sensitive information may be leaked to s
<sub>1</sub>
 without the consensus of s
<sub>3</sub>
. In this paper, we propose an enhanced security model to facilitate the control of information flow through service chains. It extends the basic security models by introducing the concepts of delegation and pass-on. Based on these concepts, new certificates, certificate chain, delegation and pass-on policies, and how they are used to control the information flow are discussed."
100,"With the increasing demand for express delivery, a courier needs to deliver many tasks in one day and it's necessary to deliver punctually as the customers expect. At the same time, they want to schedule the delivery tasks to minimize the total time of a courier's one-day delivery, considering the total travel time. However, most of scheduling researches on express delivery focus on inter-city transportation, and they are not suitable for the express delivery to customers in the “last mile”. To solve the issue above, this paper proposes a personalized service for scheduling express delivery, which not only satisfies all the customers' appointment time but also makes the total time minimized. In this service, personalized and accurate travel time estimation is important to guarantee delivery punctuality when delivering shipments. Therefore, the personalized scheduling service is designed to consist of two basic services: (1) personalized travel time estimation service for any path in express delivery using courier trajectories, (2) an express delivery scheduling service considering multiple factors, including customers' appointments, one-day delivery costs, etc., which is based on the accurate travel time estimation provided by the first service. We evaluate our proposed service based on extensive experiments, using GPS trajectories generated by more than 1000 couriers over a period of two months in Beijing. The results demonstrate the effectiveness and efficiency of our method."
101,"Emerging grid applications desire not only high bandwidth but also the ability to control the topology and traffic engineering of the underlying networks, through Web service interfaces. To achieve that goal, we present an advanced user controlled lightpath provisioning (UCLP) system, where network resources and grid resources are both modeled as Web services and are seamlessly integrated into workflows."
102,"In this paper we present a SOAP extension for protecting the privacy of users of a Web service. This extension allows a user to prove to a remote SOAP server to be member of a trusted group without revealing his/her identity. Our extension has been designed as to ensure interoperability among SOAP applications written in different programming languages. We developed also some implementations of our extension using different programming languages. Moreover, we conducted an extensive experimentation of our implementation to prove its feasibility in a real-world context. In sums, our work suggests that privacy can be added to Web services with very little impact on the application developer and without compromising the performance of Web services."
103,"User interface (UI) design is an integral part of the software design process. The UI design not only outlines the look and feel of the system, but also helps in flushing out the requirements - by identifying what data is visible to and processed by different users. However, in any SOA methodology, UI design is typically considered out of scope. In this paper, we highlight the importance of UI design specification in the SOA landscape, from a service- identification perspective. Service identification, which is a key activity in any SOA-based development, involves specification of business requirements as a set of granular service definitions. We propose an approach for harvesting the UI design specification to define service requirements for the intended system; more specifically in terms of information and business service requirements. Our approach consists of the following steps: (1) capture user interface design in a format amenable to automated analysis, with appropriate references to data and process models, (2) identify requirements for information services from data that is displayed in the user interface, and (3) identify business service requirements from the UI navigation flow and the links between the UI and the business process model. To illustrate our approach, we present a case study using the Amazon associate Web services. The study demonstrates how the use of UI designs can lead to better service identification. The proposed approach can complement any existing SOA methodology that follows a top-down approach to identify services."
104,"Many web service providers use commercial cloud computing infrastructures like Amazon for flexible and reliable service deployment. For these web service providers, the cost of cloud computing usage becomes a big part of their IT department cost. Facing the diverse pricing models including on-demand, reserved, and spot instance, it is difficult for web service providers to optimize their cost. This paper introduces a new cloud brokerage service to help web service providers to minimize their cloud computing cost for deadline-constrained batch jobs, which have been a significant workload in web services. Our cloud brokerage service associates each batch job with deadline, and always tries to use cheaper reserved instances for computation to maintain a minimum cost. We achieve this with the following two steps: (1) given a set of jobs' specifications, determine the scheduling of jobs, (2) given the scheduling and pricing options, find an optimal instance renting strategy. We prove that both problems in two steps are computation intractable, and propose approximation algorithms for them. Trace-based evaluation shows that our cloud brokerage service can reduce up to 57% of the cloud computing cost."
105,"Web service provides a means to integrate functional components and for business IT solutions in an enterprise or cross-enterprise environment. Enterprise resources are consumed by Web services to provide desired capabilities expected by the users. However, beyond service provisioning, an enterprise needs to manage their Web services and related resources in an optimal manner in order to maximize the resource usage. This paper presents a framework of Web service management aimed for solving the resource allocation issues based on system dynamics models. Our approach combines the business operational consideration and IT infrastructure configuration. Dependency that is derived from impact analysis of different perspectives is an essential part of the model and can be built into control system. The resultant system embodies the capability of autonomic Web service management to certain degree if not fully. The reference architecture of this framework is accounted in this paper. We also demonstrate dynamical behavior of the system through simulation."
106,"Interoperability promised by Web service makes it a most promising technology for the development of next generation distributed heterogeneous software systems. Services should be compliant at signature, behavioral and semantic level to make the interoperation successful and correct. Service adaptation provides an effective approach to bridge the incompatibility of services to make them interoperate as well as possible. In this paper, we aim to contribute to the definition of a methodology to develop adaptors that are capable of making two incompatible services interoperate not only successfully but also correctly at semantic level. To achieve this goal, we proposed service specifications for both atomic and composite services with semantic dependency between outputs and inputs specified; then we proposed adaptor specification consisting of three parts, which are message mapping, action mapping and treatment for non-mapping messages. Based on service and adaptor specifications, an incremental derivation approach of a concrete adaptor is given."
107,"Business Process driven Service Oriented Architecture (SOA) allows for designing services that execute (or realize) atomic tasks of business processes. When developing and designing SOA applications using Model Driven Development (MDD), business processes and services are represented using specifications such as Unified Modeling Language (UML). UML based models help in specifying structural properties of services and the behavioral properties of a service composition, executing a business process. In a typical design scenario, architects develop process models (comprising of tasks) and service design independently and establish associations between tasks and the designed services. Service compositions representing business processes are verified with the designed services. Verification at design time is a manual activity and relies on the static information captured in these UML models. In this paper we describe a model-based approach that enables executing service compositions at design time. The process model and service model is transformed to an executable UML model and UML Action language (UAL) code fragments are automatically generated. UAL enables unambiguous specification of the behavior of service operations and their compositions. The generated executable model enables a precise behavior analysis of the process realization. We demonstrate the use of this approach by reporting on a reference model with 17 business processes and 127 business tasks realized using 13 Service interfaces."
108,"Service composition based on state dependent services is a challenge if it is done in a decentralized way that is without a centralized coordinating partner knowing all involved parties. In particular, the challenge is the combination of services to a composite service, such that every party involved considers its view of the composite service to be acceptable. In the paper a protocol is proposed which incrementally derives proposals for composite services. These proposals are then evaluated by the involved parties locally and a consensus on the local decisions on accepting or rejecting the proposed composite service is derived."
109,"Complex services are composed of simple services which typically need to be processed in a particular order. Two complex services only match if they agree on both, their simple services and their processing order. This matching semantics can be formalized by means of modelling complex services as finite state automata (FSAs), and analysing the intersection of the FSAs. However, computing the intersection of FSAs is computationally expensive, and thus, does not scale for large service repositories. This paper presents an approach for indexing and matching complex services using an abstraction that transforms the underlying FSA via its grammar into a form that can be indexed using available index mechanisms. Evaluation of this approach shows a performance gain of several orders of magnitude as compared to sequential matching."
110,"Web service filtering is an efficient approach to address some big challenges in service computing, such as discovery, clustering and recommendation. The key operation of the filtering process is measuring the similarity of services. Several methods are used in current similarity calculation approaches such as string-based, corpus-based, knowledge-based and hybrid methods. These approaches do not consider domain-specific contexts in measuring similarity because they have failed to capture the semantic similarity of Web services in a given domain and this has affected their filtering performance. In this paper, we propose a context-aware similarity method that uses a support vector machine and a domain dataset from a context-specific search engine query. Our filtering approach uses a spherical associated keyword space algorithm that projects filtering results from a three-dimensional sphere to a two-dimensional (2D) spherical surface for 2D visualization. Experimental results show that our filtering approach works efficiently."
111,"Service chaining, the act of stringing a sequence of services together to form a new service, is a key element of Web services. However, for Web services to reach its full potential the issue of testing service-chaining at the network level must be resolved. How can one map the microlevel service-service interactions to the macrolevel system performance? For instance, as service chains become longer and more complex, how do they affect end-user quality of service? Focusing on aggregate service chains $situations in which the user invokes a service that carries out the chain, without the user being aware of the individual services - we tackle these questions using a Java simulation tool to model service chaining, visualize network traffic and quantify service chain complexity. We demonstrate that one can orchestrate very complex service chains in a simple distributed manner and quantify how service chain complexity affects end-user quality of service and network loading."
112,"This paper presents an event based functionality integration framework to approach the issue of service personalization and service mashups. In contrast to existing data integration approaches, the proposed framework addresses the mashup issue from a new perspective by extracting and reasoning the context through user generated event, while recommending and aggregating the contextual services dynamically in response to the user's functional requirements. An event hierarchy is proposed to retrieve contextual information and analyze underlying functionalities. The three layer system framework, service recommendation logic, and the functionality integration are also presented."
113,"Sensors are pervasively deployed on mobile devices with the development of Internet of Things technology. Value-added services are innovated and developed by analyzing data streams from massive number of mobile sensors in online mode. Due to dynamic working condition of mobile sensors and the high data rate, back end analytic services confront incoming streams with large rate fluctuation and out-of-order time series. This puts forward special challenges in service implementation for commercial applications, where good reliability/scalability performance is a must. In this paper, a data ingestion and scheduling framework is proposed to enable large-scale tempo-spatial streams analysis in a reliable and cost-effective way. A case study on a real world application adopting this framework is introduced and its pilot result is presented."
114,"Advances in server, network, and storage virtualization are enabling the creation of resource pools of servers that permit multiple application workloads to share each server in the pool. This paper proposes and evaluates aspects of a capacity management process for automating the efficient use of such pools when hosting large numbers of services. We use a trace based approach to capacity management that relies on i) a definition for required capacity, ii) the characterization of workload demand patterns, iii) the generation of synthetic workloads that predict future demands based on the patterns, and iv) a workload placement recommendation service. A case study with 6 months of data representing the resource usage of 139 workloads in an enterprise data center demonstrates the effectiveness of the proposed capacity management process. Our results show that when consolidating to 8 processor systems, we predicted future per-server required capacity to within one processor 95% of the time. The approach enabled a 35% reduction in processor usage as compared to today's current best practice for workload placement."
115,"WS-Eventing is a W3C specification to enable publish-subscribe Web Services. This paper proposes a WS-Eventing extension for disseminating notifications by using a UDP multicast binding. To achieve this, some specification modifications will be done which do not affect legacy client implementations. We show that it is possible to extend WS-Eventing almost without losing backward compatibility. An exemplary proxy application illustrates that the extension can be embedded into open source and proprietary Web Service frameworks."
116,"This article proposes a unified methodology for designing asynchronous SOA (Service-Oriented Architecture) based on the asynchronous messaging models and patterns. Conventional SOA focuses on synchronous messaging. Although asynchronous messaging provides much efficient and productive way to coordinate services, design of aSOA (asynchronous SOA) is far more complicated due to the variety of messaging and architecture while assuring behavioral consistency of architecture. This paper proposes a model-driven design methodology for aSOA. The methodology is based on aMEPs (Asynchronous Message Exchange Patterns) identified by classifying the messaging in terms of behavioral concerns. Based on the meta-model of aSOA, a set of aSOA patterns is generated by composing aMEPs. Then, an aSOA pattern is selected and transformed to a platform specific aSOA on top of Web services standards. We successfully implemented an aSOA on Apache Axis, which enables to asynchronous messaging of SOAP over SMTP. We demonstrated that conventional methods are subsets of the proposed methodology, which is the major contribution of this work."
117,"In this paper, we propose a novel way of modeling Web services using semantic graph transformations. Each operation supported by a Web service is associated with a semantic annotation that describes the input and output messages using RDF graph patterns. The terms used in these patterns are defined in OWL ontologies that describe the application domain. A key difference between our model and existing semantic Web service models like OWLS is that it describes the inputs and outputs in terms of instance-based graph patterns, rather than in terms of concepts. This allows associating a rich set of constraints on the input and output data in terms of relations between instances. We also propose a composition model for Web service operations, that describes the conditions for composing services into workflows. The composition model includes the notion of semantic propagation, i.e. the semantic description of the output message of an operation depends on the semantics of the input message. We have developed a planner that uses this model to compose services, automatically. The planner uses DLP reasoning to aid plan search. We present performance results for the planner."
118,"The growing synergy between Web services and grid-based technologies is enabling profound, dynamic interactions between applications dispersed in geographic, institutional, and conceptual space. Such deep interoperability requires the simplicity, robustness, and extensibility for which XML has been conceived, making it a natural lingua franca for the network. Along with these advantages, there is a degree of inefficiency that may limit the applicability of XML. Firstly, we investigate the limitations of XML for high-performance and high-interactive distributed computing. Our experimental results clearly show that focusing on parsers, that are routinely used to desterilize XML messages exchanged in these system, we can improve the performance of a generic end to end Web services based solution. Secondly we present a new parser, the cache parser, which uses a cache to reduce the parsing time sender and receiver side, by reusing information related to previously parsed documents/messages similar to the one under examination. Finally, we show how our fast parser can improve the global throughput of any application based on Web or grid services, or also JAXP-RPC. Experimental results demonstrate that our algorithm is 25 times faster than the fastest algorithm in the market and, if used in a WS scenario, can dramatically increase the number of requests per second handled by a server (up to 150% of improvement) bringing it close to a system that does not use XML at all"
119,"Making your enterprise data and traditional Web services consumable through the World Wide Web is becoming increasingly important for a organization. REST (Representational State Transfer), a collection architecture principle, plays a key role in making your services easily integrated in both your enterprise applications and others via Web. This tutorial will teach you the basic of REST and the related application architecture, the Atom Publishing Protocol (APP) and how it stitches each resource in the service architecture, the best practices in constructing the REST services, and how to utilize and integrate other REST services with your existing Web service and application. Furthermore, it will also show you how to effectively use and consume the REST services from the IBM social software (Lotus Connections) in order to build your own social Web applications. You will not only learn the industrial products and trends in utilizing and making the consumable services using REST and APP, but also learn the details on how to make the most out of your own consumable services."
120,"With a growing number of web services, discovering services that can match with a user's query becomes a challenging task. It's very tedious for a service consumer to select the appropriate one according to her/his needs. In this paper, we propose a non-logic-based matchmaking approach that uses the Correlated Topic Model (CTM) to extract topic from semantic service descriptions and model the correlation between the extracted topics. Based on the topic correlation, service descriptions can be grouped into hierarchical clusters. In our approach, we use the Formal Concept Analysis (FCA) formalism to organize the constructed hierarchical clusters into concept lattices according to their topics. Thus, service discovery may be achieved more easily using the concept lattice. In our approach, topic models are used as efficient dimension reduction techniques, which are able to capture semantic relationships between word-topic and topic-service interpreted in terms of probability distributions. In our experiment, we compared the accuracy of the our hierarchical clustering algorithm with that of a classical hierarchical agglomerative clustering. The comparisons of Precision@n and Normalised Discounted Cumulative Gain (NDCGn) values for our approach, Apache lucene and SAWSDL-MX2 Matchmaker indicate that the method based on CTM presented in this paper outperform all the others matchmakers in terms of ranking of the most relevant services."
121,"Improving service quality has long been recognized as a key business strategy in consumer market. However, little is written in the literature about service among business organizations. The shortage of knowledge on business service (BS) poses a significant challenge in tracking and improving BS quality in today's e-commerce environment. To address this, this paper takes a first step to identify main BS dimensions in an e-commerce environment. It first provides a review on BS. Then the BS gap model and main BS dimension are identified. The paper concludes with a summary and future research suggestions."
122,"Advances in mobile Internet technology have enabled the clients of Web services to be able to adjust to context changes, which need to rely on monitoring to QoS of Web services. Most contemporary QoS prediction methods exploit the QoS characteristics for one specific dimension, e.g., time or location, and do not exploit the complicated relations among multi-dimension of context. This paper proposes a learning approach to quality-of-service (QoS) prediction of web services via multi-dimensional context derived from the past invocation history. To validate our approach, large-scale experiments are conducted based on a real-world Web service dataset, WSDream. The results show that our proposed approach achieves higher prediction accuracy than other approaches."
123,"Soft resources, which are system software components that use hardware or synchronize the use of hardware, are playing a critical role in the performance of multi-tier web systems, and thus it is quite important to tune the soft resource allocation for using the limited hardware resources to obtain maximum effectiveness. In this paper, we integrate both theoretical and experimental studies to the soft resource allocation problem. Specifically, we apply the queueing network model for formulating multi-tier web systems, and conduct experimental measurements based on the RUBiS benchmark system to obtain precise model parameters. Quantitative analysis is carried out, based on which an optimization model as well as an algorithm are put forward for soft resource allocation. The efficacy of our approach is validated by both theoretical analyses and experimental results."
124,"This paper presents our design and development of a context-driven content adaptation planner, which dynamically transforms requested Web content into a proper format conforming to receiving contexts (e.g., access condition, network connection, and receiving device). Aiming to establish a semantic foundation for content adaptation, we apply description logics (DLs) to formally define context profiles and requirements and automate content adaptation decision. In addition, the computational overhead caused by content adaptation can be moderately decreased through the reduction of the size of adapted content."
125,Service optimization and energy conservation requires a thorough understanding of the performance impact of different hardware configurations. In this paper we focus on the configuration of memory and investigate the impact of memory dynamic voltage and frequency scaling (DVFS) on the performance of services/applications. We propose a quantitative metric called frequency sensitivity (FS) and study memory FS of various benchmarks. Our experiments yield several insights for memory DVFS based performance tuning.
126,"Data-flow requirements is an important aspect of service composition. Although several approaches have been proposed to specify data-flow requirements, they cannot be efficiently exploited in dynamic setting, for example, where the composition participants (i.e., component services) need to be dynamically replaced. In this paper, we propose a new modeling methodology for data-flow requirements, in which we explicitly distinguish a data model of a composite service from data models of component services. We show that following our methodology, a composition can be adjusted to various dynamic changes with significantly less effort. We implement our solution using planning techniques and make its basic evaluation on a scenario from the travel domain."
127,"Ticketing system is an example of a Service System (SS) which is responsible for handling huge volumes of tickets generated by large enterprise IT (Information Technology) infrastructure components, and ensuring smooth operation. An issue is captured as summary on the ticket and once a ticket is resolved, the solution is also noted down on the ticket as resolution. Further the system maintains the provision of recording the time when a ticket is opened, acknowledged to user, resolved and/or closed, from which different QoS parameters could be obtained. For example, Resolution Time can be computed as the difference of resolution date and opening date of the ticket. QoS parameters are used to measure the performance of different aspects of a service. In case of impreciseness of observations of these parameters fuzzy sets seems to be an optimal tool to model them. To ensure better operation for services based on these QoS values we propose a two-stage analysis framework for QoS prediction of incoming tickets which includes fuzzy clustering of incident tickets based on QoS values and building a fuzzy regression model using this categorization and the textual contents of tickets. Further we carry out a fuzzy correlation analysis of different categories (clusters) of QoS parameters. Lastly we report on our experimental results."
128,"In systems that require several services to collaborate, specifying coordination protocols is vital, but costly. Additionally, several properties, which are derived from laws, regulations, requirements, etc., must be satisfied. Coordination protocol composition approaches construct specific protocols in a cost effective manner in accordance with the composition intentions. However, existing composition approaches are insufficient in terms of satisfying properties. Existing approaches use concrete specifications to identify composition intentions and do not consider interference between compositions. Herein we propose a new composition approach in which a developer directly expresses his or her intentions as constraints via metadata, and then the system searches for optimal composition methods based on the constraints."
129,"In this paper we address the problem of qualitative and quantitative analysis of timing aspects of Web service compositions defined as a set of BPEL4WS processes. We introduce a formalism, called Web service timed state transition systems (WSTTS), to capture the timed behavior of the composite Web services. We also exploit an interval temporal logic to express complex timed assumptions and requirements on the system's behavior. Building on top of this formalization, we provide techniques and tools for model-checking BPEL4WS compositions against time-related requirements. We also present a symbolic algorithm that can be used to compute duration bounds of behavioral intervals that satisfy such requirements. We perform a preliminary experimental evaluation of our approach and tools with the help of an e-Government case study"
130,"In a recommender system, items can be rated across multiple fields by users with varying degrees of familiarity. Hence, the ratings in a recommender system should have different recommended weights. Ratings in fields where in the user has high or low familiarity should be given high or low recommended weights, respectively. However, current recommendation algorithms ignore this problem and use the ratings indiscriminately, thus affecting the accuracy of the recommendation system. In this paper, we provide a focused study of user-familiarity degree-aware recommendation and develop a user-familiarity degree-aware latent factor model for recommendations that considers both user familiarity and item features reflected by the tagging information. We also design a user-familiarity degree-aware probability matrix factorization model, which computes the degree of familiarity of a user with the items he/she has rated. By using the user-familiarity degree, different recommended weights are given to every rating to obtain precise recommendations. The experiment results on real-world datasets show that our algorithm significantly outperforms state-of-the-art latent factor models and effectively improves the accuracy of the recommendation results."
131,"Data Mashup is a special class of mashup application that combines information on the fly from multiple data sources to respond to transient business needs. Data mashup is a difficult task that would require an important programming skill on the side of mashups' creators, and involves handling many challenging privacy and security concerns raised by data providers. This situation prevents non-expert users from mashing up data at large. In this paper, we present a declarative approach for mashing-up data. The approach allows data mashup creators to build data mashups without any programming involved. The approach builds the mashups automatically and takes into account the data's privacy concerns. We evaluate the efficiency of the approach via a thorough set of experiments. The results show that handling data privacy introduces only a negligible cost in the mashup building time."
132,Transactions are a fundamental technology for building efficient and reliable web service based applications. Various models and protocols have been developed by academic and industrial research community in order to effectively manage web services transactions. We propose a novel abstract model for dynamically modeling distinct web services transaction protocols. Model-based testing techniques can be used on the abstract model in order to automatically generate test scenarios.
133,"Web services are gaining momentum as a major vehicle to deliver business functionalities on the Web. More and more business organizations have begun to use Web services to facilitate user interactions and the collaboration among themselves. This essentially forms a large service space, which still keeps growing. Meanwhile, there may be functionality overlaps among different service providers. The concept of Quality of Web Service (QoWS) is emerging as a key feature in distinguishing between competing service providers. We present in this paper a systematic approach for efficient service selection by using QoWS as the major criterion. In particular, we adopt a relational approach that enables to store QoWS information in a relational DBMS and leverage standard relational operators for efficient service selection. We perform a preliminary set of experiments to evaluate the proposed service selection algorithms."
134,"The ability to dynamically discover and invoke a Web service is a critical aspect of service oriented architectures. An important component of the discovery process is the matchmaking algorithm itself. In order to overcome the limitations of a syntax-based search, matchmaking algorithms based on semantic techniques have been proposed. Most of them are based on an algorithm originally proposed by M. Paolucci, et al. [19]. In this paper, we analyze this original algorithm and identify some correctness issues with it. We illustrate how these issues are an outcome of the greedy approach adopted by the algorithm. We propose a more exhaustive matchmaking algorithm, based on the concept of matching bipartite graphs, to overcome the problems faced with the original algorithm. We analyze the complexity of both the algorithms and present performance results based on our implementation of both these algorithms. We show that the complexity of our algorithm is equivalent to that of the original algorithm in spite of the improvements we have made to address the correctness issues."
135,"Trust and reputation for web services emerges as an important research issue in web service selection. Current web service trust models either do not integrate different important sources of trust (subjective and objective for example), or do not focus on satisfying different user's requirements about different quality of service (QoS) attributes such as performance, availability etc. In this paper, we propose a Bayesian network trust and reputation model for web services that can overcome such limitations by considering several factors when assessing web services' trust: direct opinion from the truster, user rating (subjective view) and QoS monitoring information (objective view). Our comprehensive approach also addresses the problems of users' preferences and multiple QoS-based trust by specifying different conditions for the Bayesian network and targets at building a reasonable credibility model for the raters of web services."
136,"Distributed software systems are the basis for innovative applications. The key for achieving survivable and maintainable distributed systems is agility because the non-deterministic nature of distribution would otherwise leave the system uncontrollable, especially in emerging mobile ad-hoc networks. A mobile ad-hoc network (MANET) is based on a self-organizing and rapidly deployed network of mobile services to collaborate without using any pre-existing fixed network infrastructure. Survivability is defined as the capability of a service to fulfil its mission in a timely manner, even in the presence of attacks, failures, or accidents. There are four key survivability properties: resistance, recognition, recovery and adaptation. Recovery, a hallmark of survivability, is the capability to maintain critical components and resource during attack, limit the extent of damage, and restore full services following attack. Exception handling is a way to deal with the recovery aspect of survivability. Resistance can be viewed as the process of limiting access to critical and vulnerable resources only to authorized users, programs, processes, or other systems. This paper bridges the analysis of secure business process and its recovery aspect in terms of exception handling in the context of access control requirements. We propose an integrated approach to engineer a survivable distributed system through dynamic regeneration of workflow specifications in the context of Business Process Execution Language for Web Services (BPEL) and eXtensible Access Control Markup Language (XACML)"
137,"In the era of big data, data intensive applications have posed new challenges to the filed of service composition, i.e. composition efficiency and scalability. How to compose massive and evolving services in such dynamic scenarios is a vital problem demanding prompt solutions. As a consequence, we propose a new model for large-scale adaptive service composition in this paper. This model integrates the knowledge of reinforcement learning aiming at the problem of adaptability in a highly-dynamic environment and game theory used to coordinate agents' behavior for a common task. In particular, a multi-agent Q-learning algorithm for service composition based on this model is also proposed. The experimental results demonstrate the effectiveness and efficiency of our approach, and show a better performance compared with the single-agent Q-learning method."
138,"We present a technique called Dynamic Distributed Service Coordination Protocol (DDSCP) that enables dynamic and distributed coordination for composed services and applications in telecommunication networks. Individual service components are modeled as Web services and DDSCP facilitates coordination among these components by dispatching executable processes to the service components that specify different steps that the service component must follow in response to (receipt of) specific messages and events. The collective and concurrent execution of these processes at different service components achieves overall goals of the service. The planning and creation of these processes is not our focus in this paper. We describe the structure and processing of different messages DDSCP, and describe how this protocol can work. Our model has several advantages over the existing service platforms for 3rd Generation Mobile Networks, such as Parlay/OSA, and the Web-service composition models. These advantages include introduction of flexibility among network components at finest level, ease of creation of highly customized services, easy integration with foreign components, reduced application complexity, increased reuse of application components, and possibility of increased user participation in managing her services, and thus reducing load on the network."
139,"Dependability is one of the most important challenges for service-oriented architectures if their success shall continue in critical settings such as air traffic control or finance and banking. Replication of services and the underlying resources is one of the primary fault tolerance techniques for achieving dependability. While replication is well known in traditional fields (e.g. databases), it is rather in its infancy in service-oriented environments. Thus, in order to reduce the dependability gap we are currently facing in service-oriented environments, we contribute with a replication middleware for Web services which is built upon the Java-based Axis2 SOAP engine and provides a variant of primary-backup replication. Performance evaluations of our middleware implementation show the relatively low overhead of replication if the number of replicas is small."
140,"Due to differences in consumer requirements, a Web service usually has multiple service variants for use in different business contexts. In such situations, delivering customizable services helps increase efficiency not only in service description and publication but also in service consumption. However, existing approaches for providing customizable services enforce the tight coupling between providers and consumers. Nor do they take into account recursive nature of service customization. Consequently, the approaches hamper the widespread use of customizable services in SOA. In this paper, we propose a language, namely Web Service Variability Description Language (WSVL), which formalizes the customization interface between providers and consumers using the XML technology to address these problems. We also describe a reference architecture for service deployment and a service engineering technique which together support the provisioning of WSVL-based customizable services. A proof-of-concept prototype system is introduced to demonstrate the feasibility of our approach."
141,"With the rapid growth of the web services technologies, users often leverage various web services to perform their daily activities, such as on-line shopping. Due to the massive amount of web services available, a user faces numerous choices to meet their personal preferences when selecting the desired services from the web services with the similar functionality. Therefore, it becomes tedious and cumbersome tasks for users to discover and compose services. To reduce user's cognitive burden, it is critical to support automated service composition and make efficient recommendation of personalized services to achieve user's overall goals. However, existing approaches only offer users with limited options designed for the interest of a group of users without considering individual users' interests. To allow users to compose personalized services without much manual specification, we propose a machine learning approach that applies a learning-to-rank algorithm, RankBoost, to automatically learn user preferences and the prioritization of the preferences from users' historical data. Moreover, our approach uses the multi-objective reinforcement learning (MORL) algorithm to make trade-offs among user preferences and recommends a collection of services to achieve the highest objective. We conduct an empirical study to evaluate our approach by collecting the historical data from 12 subjects. The results demonstrate that our approach outperforms the two well-established baseline approaches by 100%-200% in terms of precision on recommending services."
142,"One of the main benefits of web services is the dynamic composability, however how to achieve this is one of the current research challenges. Web service composition has been studied and, amongst other methods, the use of natural computing methods has been proposed previously. In this paper, we address the need for a fast response when computing the most suitable sequence of services. In particular, we propose a novel heuristic immune algorithm with an efficient encoding and mutation method. The algorithm involves two steps: an immune selection operation, which is maintaining antibody population diversity and a clonal selection. The use of a vaccine during the evolution provides heuristic information that accelerates the convergence. Our experimental results illustrate that the proposed heuristic immune algorithm is very effective in improving the convergence speed."
143,"The efforts and findings of the last decades of research on the formalization and the verification of Web services have given a certain level of assurance on Web services. However new challenges such as high availability and security issues are not fully addressed. In fact, Web services are exposed to attacks that appear continuously. These issues have naturally paved the way to a new research topic that aims at providing new techniques for making Web services attack tolerant. In this paper, we present a state of the art of attack tolerance, especially for Web services. We also present our approach to address such issues through a combination of techniques leveraging in particular diversification. The paper ends with our promising results and a discussion to highlight the perspectives and research direction."
145,"A common architecture in today's development of distributed systems is the service-oriented architecture (SOA) implemented using Web services. Until recently, it was difficult to build a SOA based grid/distributed system using Web services due to the inability to learn the state of services. The state of a Web service could only be accessed through specialized clients and/or services. Should the specialized client or service fail, the state can't be accessed. This paper shows the innovative resources via Web instances (RVWI) framework. RVWI grants to web services the ability to show the state of dynamic resources in their WSDL. This was achieved via software components called connectors which watch for any changes in a resource and updates the web service. The significance of this report is the support for resources which can change state between requests and the innovation is the improvement of state updates between the service and discovery services."
146,"Security is today a relevant requirement for any distributed application, and in particular for these enabled by the Web such as e-health, e-commerce, and e-learning. It is thus crucial that the use of Web services, stand-alone or composed, provide strong security guarantees. Web services security encompasses several requirements that can be described along the well known security dimensions, that is: integrity, whereby a message must remain unaltered during transmission; confidentiality, whereby the contents of a message cannot be viewed while in transit, except by authorized services; availability, whereby a message is promptly delivered to the intended recipient, thus ensuring that legitimate users receive the services they are entitled to. Moreover, each Web service must protect its own resources against unauthorized access. This in turn requires suitable means for: identification, whereby the recipient of a message must be able to identify the sender; authentication, whereby the recipient of a message needs to verify the claimed identity of the sender; authorization, whereby the recipient of a message needs to apply access control policies to determine whether the sender has the right to use the required resources. In the tutorial we will first discuss the main security requirements underlying the interactions between clients and Web services and among the Web services themselves. Then we will describe how such security requirements are addressed by standards for Web services security recently developed or under development by various standardizations bodies. Standards that are covered include: WSS, that encompasses a large number of components addressing various security aspects; XACML, that is related to access control and has been recently extended with a profile for Web services access control; WS-Federation, Liberty Alliance and Shibboleth, that address the important problem of identity management in federated organizations. Issues related to the use of th..."
147,"In SOA, services may become volatile and fail to deliver the quality of service as requested by users. In this paper, we present an approach for repairing failed services by replacing them with new services and ensuring the new service process still meets the user specified end-to-end QoS constraints. An iterative structural inspection algorithm is designed to produce reconfiguration regions that include one or more failed service. By reconfiguring only services in the selected regions, the business process will not be affected significantly. The algorithm may also utilize those available QoS constraints to relax the original constraints of a reconfiguration region and to provide more effective reconfiguration solutions. We also present the middleware components to support the service reconfiguration in the LLAMA framework."
148,"Nowadays, the exploding number of functionally similar Web services has led to a new challenge of selecting the most relevant services using quality of service (QoS) aspects. Traditionally, the relevance of a service is determined by computing an overall score that aggregates individual QoS values. Users are required to assign weights to QoS attributes. This is a rather demanding task and an imprecise specification of the weights could result in missing some user desired services. Recent approaches focus on computing service skyline over a set of QoS aspects. This can completely free users from assigning weights to QoS attributes. However, two main drawbacks characterize such approaches. First, the service skyline often privileges services with a bad compromise between different QoS attributes. Second, as the size of the service skyline may be quite large, users will be overwhelmed during the service selection process. In this paper, we introduce a new concept, called alpha-dominant service skyline, to address the above issues and we develop a suitable algorithm for computing it efficiently. Experimental evaluation conducted on synthetically generated datasets, demonstrates both the effectiveness of the introduced concept and the efficiency of the proposed algorithm."
149,"Actually Internet applications can provide not only information, but also, another way of getting distributed computing. Cooperative information systems are autonomous and heterogeneous systems, distributed geographically, but interconnected. Web Services provides a set of interoperable standards that can be used to connect distributed applications. On this environment, security is a critical issue, and an attack can expose systems services without authentication. An end-to-end connection, like the ones involved in such systems, usually requires that an authentication can be shared between different information systems. Web Services security model is not yet fully defined and a lot of proposals are emerging, delaying the adoption of this technology in many situations. In this paper we present multiplatform authentication control system based on an extension of SRP protocol, using SAML. Within this solution, authentication control can be leveraged, even with weak passwords and an authentication assertion can be exchanged with different cooperative information systems."
150,"Today, there is a huge amount of security services that can be used to implement different security requirements in Web Service based systems. For example, identity management services are required for authentication and authorization whereas message logging services are necessary to achieve non-repudiation. However, the deployment and configuration of these security services usually requires expert knowledge about the systems and expert knowledge about security requirements and implementations which a person can only learn by experience. Furthermore, today's Web Service based systems become increasingly complex. Thus, implementing security requirements is a complex and error prone task, even for experts. For this paper, we analysed several service-based implementations for identity management and their differences in the service orchestration. We present an approach to derive the needed security services, their configuration, and their connections to the functional services, based on defined security requirements for a Web Service based system. Therefore, we evaluate the UML use case model of the system and apply service security pattern derived during the analysis of the identity management implementations."
151,"This paper presents a novel Web-based service-oriented framework to support collaborative product commerce. The framework coordinates processes to design and manufacture a product in a loosely coupled environment using Web services. We have developed a service model using OWL. It provides facilities for companies to discover services that other companies provide. Choreography of Web services is achieved using the process grammar. Usage of process grammar helps to configure processes dynamically, facilitating collaboration among companies. The framework, therefore, provides a truly distributed architecture for management of service composition and interoperation, lending a unique functionality to the framework."
152,"Optimizating semantic Web service compositions is known to be NP-hard, so most approaches restrict the number of services and offer poor scalability. We address the scalability issue by selecting compositions which satisfy a set of constraints rather than attempting to produce an optimal composition. Firstly, we define constraints within an innovative and extensible quality model designed to balance semantic fit (or functional quality) with quality of service (QoS) metrics. The semantic fit criterion evaluates the quality of semantic links between the semantic description of Web services parameters, whilst QoS focuses on non-functional criteria of services. Coupling these criteria allows us to further constrain and select valid compositions. To allow the use of this model in the context of millions of services as foreseen by the strategic EC-funded project SOA4All, we i) formulate the selection problem as a constraint satisfaction problem and ii) test the use of a stochastic search method. Finally we compare the latter with state-of-the-art approaches."
154,"Interest in service oriented architecture (SOA) is rapidly increasing in the business world due to the many benefits it offers such as reliability, manageability, reusability, flexibility, efficiency, and interoperability. There are many security technologies and models being developed for SOA. They implement or encode specific aspects of authentication, authorization, encryption, trust, and access control respectively but none of them was entirely devoted to integrity. In this paper we propose service Clark-Wilson integrity model (SCWIM), a top down integrity model for SOA capable of describing sufficient conditions to protect data integrity in any SOA implementation. Based on the original Clark-Wilson integrity model, our model can form the basis for system security audits and assist SOA architects in developing systems that protect data integrity, as well as providing guidance for evaluating existing SOA systems."
155,"The development of the Internet and Web 2.0 eased communication among people and triggered the flourish of Web based communities. Among them, there are blog, social networks, music networks, and recommendation network, etc. There are also communities of massively multiplayer online games. In particular, 3D based virtual worlds, like Second Life, are highly welcomed by people. Users contribute to these communities while at the same time obtain value from the communities instead of passive browsing. The relationships among the community members and community services could be represented by different types of networks. The topology of networks influence the way how people interact and information spread within the community. In this paper, we propose a mechanism to facilitate the communities to grow into scale-free networks. Scale-free communities have good properties that could potentially speed up its accumulation of value for both the members and the community itself. We extend WSDL into scale-free Web services using Web services resource framework."
156,"In this paper we report on our work towards a novel system that aims at the combination of classical failure handling methods for composite service execution and support for automatic handling of semantic application level failures. In particular, we formulate the notion of Control Flow Intervention; a generalization for handling those two types of failure categories. When combined with automatic service composition it constitutes an optimistic forward recovery method. The method adopts expressive semantic service annotations provided by OWL resp. OWL-S."
157,"A telecom operator (""service provider"", SP) offers various services to subscribed customers by partnering with various third party providers (""content provider"", CP). The SP acts as a liaison between subscribers and partners. One of the main functions of the SP, therefore, is to match the ""demand"" of the subscribers with the ""supply"" of the CPs. Such a matching is a prerequisite for efficient service selection while ensuring customer satisfaction, and is useful for optimisation, such as resource allocation and load balancing. The ""demand"" requirements and ""supply"" guarantees can be concretized using service level agreements (SLAs). SLAs can be expressed formally using standards such as WSLA or WS-Agreement. We present a system that automates the task of finding a matching between these two sets, subscriber-SP and SP-CP, of SLAs. First, the SLAs are normalised to a common denominator, then composed if required, and finally the matching engine computes and outputs the map. The matching algorithm, which is of central importance, compares logical expressions involving predicates. The logical expressions are first converted into CNF-form, and instead of naive O(m times n) comparisons, we develop a more efficient approach to solve the problem. As a proof-of-concept, we have implemented a prototype as an Eclipse plugin"
158,"Unlicensed taxis are widely considered as major obstacles to city traffic regulation and public safety. Thus, many governments have issued restrictions for car-hailing services and alleged that the use of unlicensed vehicles was illegal. However, it is very challenging that traffic administrative enforcements face limited manpower to prohibit unlicensed taxis, due to costly and time-consuming procedure of on-site evidence collection. In this paper, we propose an effective service to incorporate human mobility mechanism into unlicensed taxis detection from massive city-wide vehicles. We first extract 276 spatio-temporal features, which are grouped into two categories, including daily behaviors and sustainable behaviors to capture the mobility characteristics of unlicensed taxis. Second, we investigate the detection accuracy of three machine learning techniques, viz. support vector machines, decision tree, and logical regression. We illustrate our approach using real-world vehicle license plate recognition dataset in Xiamen, China, which contains 336 million passing records for 6.2 million vehicles filmed by 439 devices in August 2016. Experimental results reveal that LR outperforms SVM and DT in prediction accuracy and F-score measurement, while SVM is capable of identifying the largest number of unlicensed taxis."
159,"The Sloan Digital Sky Survey (SDSS) science database describes over 230 million objects and is over 1.6 TB in size. The SDSS Catalog Archive Server (CAS) provides several levels of query interface to the SDSS data via the SkyServer website. Most queries execute in seconds or minutes. However, some queries can take hours or days, either because they require non-index scans of the largest tables, or because they request very large result sets, or because they represent very complex aggregations of the data. These ""monster queries"" not only take a long time, they also affect response times for everyone else - one or more of them can clog the entire system. To ameliorate this problem, we developed a multiserver multiqueue batch job submission, execution, and tracking system for the CAS called CasJobs. The transfer of very large result sets from queries over the network is another serious problem. Statistics suggested that much of this data transfer is unnecessary; users would prefer to store results locally in order to allow further joins and filtering. To allow local analysis, a system was developed that gives users their own personal databases (MyDB) at the server side. Users may transfer data to their MyDB, and then perform further analysis before extracting it to their own machine. MyDB tables also provide a convenient way to share results of queries with collaborators without downloading them. CasJobs is built using SOAP XML Web services and has been in operation since May 2004."
160,"Channel passing mechanisms enable dynamically determining destinations of message transferring. WS-CDL, a language developed by W3C for the specification of Web services choreographies, adopts channel passing to support dynamic Web services composition. A choreography can be projected into individual services or orchestration skeletons. It is a challenge to ensure the services generated from a choreography always have sufficient and correct channels to complete their collaboration. In fact, WS-CDL is not ready for rigorous validation and implementation with respect to channel passing, since it provides no structure for specifying explicitly which role should firstly initialize which channel variable. Here we propose an algorithm to uncover these implicit assumptions, that is implemented as an extension to Pi4SOA. With the help of the algorithm, some existing methods for verification and implementation can be applied on choreographies written in WS-CDL. In addition, we propose an approach to detect design defects in choreographies, and show how a defect is found from the main sample choreography in WS-CDL Primer. It seems that choreographies with channel passing are error prone. Methods and tools are necessary to support designers in this field. Also, we suggest improving the situation by adding a syntactical construct to WS-CDL."
161,"Survivability is crucial for computer systems that support critical infrastructures of our society. For those in paradigm of SOA where the computing settings are intrinsically open, traditional survivability model and safeguard mechanisms are no longer applicable. To rise to the challenge, we propose a formal definition and a corresponding framework to evaluate the survivability of SOA systems. We treat survivability as a multidimensional QoS property, and then give a holistic evaluation method based on the Hidden Markov Model."
162,"Reliability is an essential quality requirement for web services. Existing techniques for measuring reliability of web services mainly focus on failures caused by code-based defects. For data-centric web services, the reliability of services can be significantly affected by the quality of data used to provide services. However, the impact of data quality on the reliability of web services has rarely been explored. We present an approach to estimate data quality and to incorporate data quality with the reliability of software components in the reliability estimation of web services. To demonstrate the proposed approach, we present a case study on a government web service. In this study we observed that more than 60% of service failures reported over a three-month period were caused by invalid data. The results show that by taking into account data quality, the reliability estimate of the web service is more accurate than the traditional reliability measurement."
163,"The message-based communication among services in Service Oriented Architecture (SOA) is vulnerable to various security attacks, and has to be well protected by security mechanisms, which may sacrifice service performance due to limited system resources. In this paper, an adaptive tradeoff model for service performance and security in service- based systems is presented. This model can be used to adjust security configurations of services to provide sufficient protection and satisfy service performance requirements for SOA-based systems simultaneously. The construction of this model includes the development of a set of metrics to quantitatively measure the performance and security of services, the development of a tradeoff objective function incorporating service performance and security, and the parameter estimation through experiments. An example of service-based secure voice communication system is used to illustrate the construction of this model."
164,"Many popular web service networks are content-rich in terms of heterogeneous types of entities and links, associated with incomplete attributes. Clustering such heterogeneous service networks demands new clustering techniques that can handle two heterogeneity challenges: (1) multiple types of entities co-exist in the same service network with multiple attributes, and (2) links between entities have diverse types and carry different semantics. Existing heterogeneous graph clustering techniques tend to pick initial centroids uniformly at random, specify the number k of clusters in advance, and fix k during the clustering process. In this paper, we propose Service Cluster, a novel heterogeneous service network clustering algorithm with four unique features. First, we incorporate various types of entity, attribute and link information into a unified distance measure. Second, we design a Discrete Steepest Descent method to naturally produce initial k and initial centroids simultaneously. Third, we propose a dynamic learning method to automatically adjust the link weights towards clustering convergence. Fourth, we develop an effective optimization strategy to identify new suitable k and k well-chosen centroids at each clustering iteration. Extensive evaluation on real datasets demonstrates that Service Cluster outperforms existing representative methods in terms of both effectiveness and efficiency."
165,"For users' convenient access to Web services, this paper presents a method for generating XForms based user interfaces from WSDL files. The proposed method uses an XML schema in a WSDL file, and consists of three steps: processing a WSDL file, generating XForms codes, and embedding them into a host language document. Specifically, to generate XForms codes, rules for generating XForms codes, which support complex types and map simple types into appropriate input controls, are proposed. Experimental results with a large volume of WSDL files show that the proposed method generates XForms-based user interfaces from most of WSDL files successfully."
166,"Collaborative Filtering (CF) has been increasingly employed as an effective vehicle for providing personalized service recommendations in service computing. CF exploits historical user-service interaction information to predict the preference of service users. A key challenge faced by CF is to handle new users with no previous interaction information. We present a novel strategy that integrates Matrix Factorization (MF) with decision tree learning to bootstrap service recommendation systems. The proposed strategy first employs MF to partition existing users into a set of user groups. In practice, only a small amount of user-service interaction information is observed. The MF based user partitioning scheme also provides a way to estimate the missing interaction information based on the group structure. The tree learning algorithm then leverages these estimated information and exploits user groups as class labels to learn a decision tree. Few highly discriminative services are identified as tree nodes to adaptively query a new user based on the interaction results with the prior services in the tree. Through a short and intuitive bootstrapping process, the new user is classified into one of the user groups, via which the user's preference is predicted. We conduct a set of experiments on real-world service data to demonstrate the effectiveness of the proposed bootstrapping strategy."
167,"Today's business process orchestration languages such as WS-BPEL and BPML have high-level constructs for specifying flow of control and data, but facilities for allocating tasks to humans are largely missing. This paper presents SoftAlloc, a work allocation language with soft constraints, and explains the requirements and trade-offs that led to its design, in particular, what soft constraints are, and how they enable business process definitions to capture allocation rules, best practices, and organizational goals without rendering the business processes too strict. SoftAlloc combines with virtually any business process language and any conceivable legacy system, while guaranteeing polynomial performance. We present the design, the formal definition, and an evaluation of SoftAlloc."
168,"Recently, Web services have become a new technology trend for Enterprise Application Integration (EAI) and more and more applications based on Web services are emerging. One of the problems in using Web services in business applications such as logistics is services composition automatically and efficiently. In this paper, we present a Dynamic, Demand-Driven Web services Engine called D3D-Serv to implement composite service functionality that is used to dynamically build composite services from existing services according to different business logics and requirements. In this D3D-Serv framework, the most challenging function to implement is dynamic selection of service providers at run time. The highly dynamic and distributed nature of Web services often makes some service providers overloaded at certain times while others idle. To solve this problem, we propose an efficient services selection and execution strategy that is based on the queuing theory and can provide guarantees for the QOS (Quality of Service) under provider's limited resources. Preliminary experimental results have shown that this algorithm is effective."
170,"Traditional methods for implementing highly resilient systems are costly; and their methods using specialized hardware and high-speed interconnects do not extend to cloud.&nbsp;&nbsp;We demonstrate an architecture that rethinks the traditional approach and uses cloud's agility to provision additional capability as failures occur.&nbsp;&nbsp;This architecture continues serving requests with the best possible response even as the primary service deteriorates and even fails completely.&nbsp;&nbsp;It offloads requests according to their need to access the primary service; first redirecting requests with less need to lower-grade secondary capability that is provisioned on-demand on the cloud in order to reserve primary capacity for requests that need it (e.g., first redirect web-browsing requests to a slightly out-of-date cached dataset, and reserve the primary, most-consistent data service for check-out requests).&nbsp;&nbsp;Our approach 1) triages access to the primary service based on request needs in times when it is overwhelmed, and 2) provisions on-demand cloud capability to offload and continue serving other requests as needed.&nbsp;&nbsp;The result is a highly resilient system architecture that scales via cloud."
171,"Logs, which record valuable system runtime information, have been widely employed in Web service management by service providers and users. A typical log analysis based Web service management procedure is to first parse raw log messages because of their unstructured format; and then apply data mining models to extract critical system behavior information, which can assist Web service management. Most of the existing log parsing methods focus on offline, batch processing of logs. However, as the volume of logs increases rapidly, model training of offline log parsing methods, which employs all existing logs after log collection, becomes time consuming. To address this problem, we propose an online log parsing method, namely Drain, that can parse logs in a streaming and timely manner. To accelerate the parsing process, Drain uses a fixed depth parse tree, which encodes specially designed rules for parsing. We evaluate Drain on five real-world log data sets with more than 10 million raw log messages. The experimental results show that Drain has the highest accuracy on four data sets, and comparable accuracy on the remaining one. Besides, Drain obtains 51.85%~81.47% improvement in running time compared with the state-of-the-art online parser. We also conduct a case study on an anomaly detection task using Drain in the parsing step, which determines the effectiveness of Drain in log analysis."
172,"In this paper, we study Web services methods and approaches to enable real-time communication services over IP. This approach extends Web services methodologies from service integration to a service-oriented approach for communication. In particular, we describe the generic Web service based application session management, WS-Session, the two-way full duplex Web service interaction framework, and most importantly, the development of Web service initiation protocol (WIP) which is a full featured Web service and SOA based communication framework for multimedia and voice communication over IP. We show that WIP provides a service oriented architecture which can be extended seamlessly from a P2P endpoint to an endpoint with advanced call control and switching capabilities typically requiring the assistance of a dedicated PBX. A prototype of WIP system is fully developed. Architectural design and system implementation of Web service based communication endpoints are studied and applied to realize service-oriented communication over IP. Advances of WIP indicate the beginning of a full Web service and SOA based communication paradigm that can reshape the converged communication over IP."
173,"Web services fail to deliver on the promise of ubiquitous deployment and seamless interoperability due to the lack of a uniform, standards-based approach to all aspects of security. In particular, the enforcement of access policies in a service oriented architecture is not addressed adequately. We present a novel approach to the distribution and enforcement of credentials-based access policies for Web services (3PAC) which scales well and can be implemented in existing deployments."
174,"In this paper, we describe our work in progress on Web services recommendation for services composition in a Mashup environment, by proposing a new approach to assist end-users based social interactions capture and analysis. This approach uses an implicit social graph inferred from the common composition interests of users. We describe the transformation of users-services interactions into a social graph and a possible means to leverage that graph to derive service recommendation. As this work is in progress, this proposal was implemented within a platform called SoCo where preliminary experiments show interesting results."
175,"Application-caching occurs when applications assume control for data caching rather then relying on middleware to manage their data. This paradigm is a good fit for""main-memory caching"" in which data storage and retrieval issues are relatively straight-forward. However, the caching requirements of certain workloads exceed even modern main-memory capacity. Although disk-offload offers the potential to overcome this capacity constraint, it poses a problem for the application-caching paradigm since application data may now be resident in either main-memory or on disk. In this paper, we therefore examine how disk-offload middleware can best support the application-caching paradigm for data-centric web-services. We present two disk-offload caching architectures, evaluate their performance, and draw conclusions as to their relative effectiveness."
176,"The concept of trust in web services mainly deals with the degree of belief that a client or a group of clients have over services functioning satisfactorily and providing the expected results. With services being invoked in composition with each other, computing the trust of the composition and selecting services that deliver the highest trust for it becomes a desired goal. In this paper we demonstrate how using Bayesian networks and its supporting queries, we can select the set of services among all candidates that would provide highest global trust."
177,"To keep up with the trend of globalization and informatization, an increasing number of enterprises decide to run their business process in a service-based manner with the help of Web Service technology. In order to manage such service-based business process (SBP), it is vital that the dependencies among the internal process and the exposed external services are correctly developed and maintained. SBP is dynamic by nature, therefore it is necessary to develop a practical and robust method to verify the correctness of SBP. In an SBP, complex dependencies exist not only between internal process and involved services but also within their components (activities, data, operations, etc.). The complex dependencies make the correctness verification for SBP a challenging task. In this work, we develop a correctness verification approach to handle this task. A Petri net based model is proposed with a hierarchical structure to cover the characteristics of SBPs. This model can support the control flow patterns that are necessary for SBPs. A set of correctness properties for SBP are identified which any SBP developers shall consider, and the respective verification methods are developed."
178,"With the development of information technology, more and more web services have emerged, thereby making it difficult for customers to find their favorite services quickly and accurately. To overcome this difficulty, recently the collaborative filtering (CF) technique has been widely employed for personalized service recommendation, meanwhile improving the profits of service providers. Although the CF-based web service recommender systems have shown their potential, they appear to be vulnerable to shilling attack problems. Therefore, in this paper we analyze a general form of web service shilling attacks and four kinds of classical attack models, e.g., average attack, bandwagon attack, random attack, and segment attack are thoroughly investigated. Furthermore, we also study the impact of distribution-aware Pareto attack models. To demonstrate how shilling attacks alter the recommendation results, this paper analyzes 1) the variation of Quality-of-Service (QoS) prediction values of target services, 2) the QoS value prediction shifts of services with short response time which are more likely recommended, and 3) the comparison of prediction shift caused by classical attack models and Pareto attack models. The experimental results on WS-DREAM dataset revealed several interesting findings about the predictions of QoS values of target service correlated to different attack models. It is expected that this work can provide some insight for future vulnerability analysis of CF-based web service recommender systems."
179,"In our earlier work, competitive Web services market was proposed to address one of the major business concerns, namely ""competitiveness"", of the current Web services research. This paper aims to attack the issues of how to model and analyze competitive Web services market, which ranges from the service node at the micro level to the competitive composite service providers at the macro level. Moreover, in order to tackle and contain the randomness embedded in a Web services system, we apply the Markov chain theory to analyze the availability of Web services market. Moreover, we analyze the stochastic performance of the Web services market using queuing theory and propose the adaptive control methodology to improve the performance."
180,"Due to the inherent stochastic nature of services execution environment within service oriented systems, a runtime adaptation of the given composition may be required. We address a runtime service adaptation mechanism based on conditional retries for the orchestrated web services. The conditional retry may be issued while a concrete service within composition is executed. The retry could either invoke the same concrete service or a functionally equivalent web service that implements the same task. We use dynamic programming to determine the optimal time instances at which the current request should be terminated before request replication. The calculation takes into account different QoS parameters like services' response -- time distributions and cost - relating parameters, and the solution optimizes the expected revenue of composite service provider. We illustrate the benefits of our approach by numerical calculations, and discuss the impact of considered QoS parameters to the solution at hand."
181,This paper presents a solution to performance issues in the quality of service aware selection of Web services using techniques of parallelism and mechanisms of inference provided by Semantic Web. The results point to a significant improvement in the speed of searching Web services and thus makes the use of semantic resources viable in distributed systems to provide better quality of service to the clients.
183,"On a crowdsourcing platform, in order to cheat for rewards or sabotage the crowdsourcing processes, spam workers submit numerous random and erroneous answers to the tasks published by honest requesters. This type of behaviours extremely reduces the enthusiasm of honest users, which may lead a crowdsourcing platform to a failure. To defend the threats from spam workers, reputation-based defense mechanisms and verification-based defense mechanisms have been proposed in crowdsourcing environments. However, reputation-based defense fails to indicate the trust level of a worker who boosts its reputation by transacting with his/her accomplices. In addition, verification-based defense is costly and ineffective when facing a large number of spam workers with ""good"" reputations. Thus, it is a challenging problem to effectively defend the threats from spam workers. In this paper, we propose a new trust vector-based threat defense model CrowdDefense. Firstly, we build a Crowdsourcing Trust Network (CTN) consisting of requesters, workers and their transaction relations. Then, we analyze three threat patterns of spam workers. Based on the analysis, we infer the trust relation between a worker and a requester who are indirectly linked with each other. Moreover, we compute a worker's trust relations with different requesters, and present them in a new Worker Trust Vector (WTV) that indicates the worker's global trust. As spam workers always succeed in the transactions with their accomplices, they cannot obtain comprehensively good trust scores in their WTVs and thus are prevented from participating in tasks. The experiments demonstrate that CrowdsDefense significantly outperforms the state-of-the-art approaches in terms of preventing spam workers from participating in the tasks published by honest requesters."
184,"The growing number of Additive Manufacturing Web (AMW) services, offered by different providers over the Internet, makes it challenging for consumers to compare these AMW services to select a service of their choice. In addition, it is even more challenging for consumers to compare these AMW services against their personal preferences. This is because, consumers personal preferences on multiple non-functional attributes such as price, material, accuracy and schedule, should be considered for AMW service selection. The decentralized nature of AMW services coupled by the need to consider consumers personal preferences during AMW service selection, requires a system that will serve as a broker between AMW services and consumers. In this paper, we propose a service broker system for AMW services that provides consumers with a single point of access to a large number of AMW services from many additive manufacturing service providers. This broker system also incorporates the first real application of service selection with fuzzy logic based personalized preferences and trade-off. We develop a method to generate fuzzy membership functions for each non-functional attribute. This makes it easy for consumers to specify their fuzzy membership functions. Finally, we present an application case study to demonstrate the feasibility of brokerage in AMW services and also evaluate our method in terms of performance."
185,"Web services proxy is an extendable intermediate that can monitor and control XML communication with good performance. It adopts a plug-in architecture enhanced by characteristic techniques named ""MFI4P"" and ""XPath centric architecture"". MFI4P efficiently controls plug-in libraries over various XML operations by optimizing transformation cost of XML between the libraries. The XPath centric architecture performs XML processing better and makes it easy to develop plug-in by using streaming XPath and ""two phase rule evaluation"". Experimental results show many types of management functions can be built as plug-ins and XML processing overhead is minimized in many cases."
186,"Transaction concept plays a key role in business success. However, existing transaction models are not applicable in service grid environment. Though many new models have been suggested, they still have some deficiencies, e.g., major extension to services are required, and they are hard to implement, to name a few. In this paper, a new model is proposed based on the analysis of existing work and the characteristics of service grid environment. Besides, model implementation issues are also covered"
187,"Push notification is an important approach to distribute interesting information to users timely. With the fast development of mobile devices and mobile applications, push notification is getting more and more popular. The convergence of mobile and IoT also bring new challenges on how the system can handle the mixed push channels designed for M2M communication and human interaction, and enable the effective interaction with both human and IoT devices involved. IoT devices may push notifications of sensor data in a high frequency. To enable push notification for both of mobile devices and IoT devices, the push notification system is required to achieve high throughput to handle the frequent notifications, and support content matching to filter out the undesired notifications. To enable effective push notification with both human and IoT devices involved, the system is required to understand the users' interests for notifications with the IoT devices providing the users' contexts. That is to say users need an intelligent push notification system. In this paper we propose a high performance context-aware push notification system for the converged mobile and IoT messaging. We designed high performance content matching engine as the core to enable efficient message dispatching for push notification according to highly personalized interest to ensure IoT messages to be involved in push notification. A user's interest of notifications is highly related with his context. Therefore, based on the content-matching engine, a framework for efficient context information fusion is built to support various types of context-aware push notification, towards intelligent push notification. Also we designed shared connection scheme to reduce the resource cost. Based on the content-match engine and context-aware features, the proposed push notification service can support both of group push notification and bi-directional push notification. Tests are conducted for performance evaluation."
188,"Although service composition has received a great deal of attention and there are many solutions to the problem of composition, finding potential service compositions efficiently is still challenging because of the fact that the number of candidate services can be very large. In this paper, a tree-based method of Web service composition is proposed, by which all potential candidate composite services that meet the user's request can be created. Moreover, these composite services are ordered according to the QoS attribute concerned by the user, so the user can find the best and arbitrary top-n composite services."
189,"Current Infrastructure-as-a-Service (IaaS) clouds offer both on-demand and reservation instance purchasing options. Users can combine these two options dynamically to serve time-varying demands while minimizing their instance acquisition costs. However, when future demands are unknown, it is far from trivial for cloud users to make optimal instance purchasing decisions. To deal with this problem, a carefully designed online algorithm can be employed to guide users in acquiring instances without any prior knowledge of future demands while guaranteeing a competitive ratio. In this paper, we propose an instance reselling model, in which a cloud user can temporarily rent out its idle reserved instances to other users through pay-as-you-go model. We also design online instance acquisition strategies which achieve a better competitive ratio than previous methods. Through extensive simulations based on both synthetic data and real-world traces, we show that our online algorithm under the proposed reselling model can outperform previous models and achieve significant cost savings."
190,"Existing Web service access control models focus on individual Web services, and do not consider service composition. In composite services, a major issue is information flow control. Critical information may flow from one service to another in a service chain through requests and responses and there is no mechanism for verifying that the flow complies with the access control policies. In this paper, we propose an innovative access control model to empower the services in a service chain to control the flow of their sensitive information. Our model supports information flow control through a back-check procedure and pass-on certificates. We also introduce additional factors such as the carry-along policy, security class, and transformation factor, to improve the protocol efficiency. A formal analysis is also presented to show the power and complexity of our protocol."
191,"One of the critical challenges for service oriented computing systems is the capability to guarantee scalable and reliable service provision. This paper presents Reliable GeoGrid, a decentralized service computing architecture based on geographical location aware overlay network for supporting reliable and scalable mobile information delivery services. The reliable GeoGrid approach offers two distinct features. First, we develop a distributed replication scheme, aiming at providing scalable and reliable processing of location service requests in decentralized pervasive computing environments. Our replica management operates on a network of heterogeneous nodes and utilizes a shortcut-based optimization to increase the resilience of the system against node failures and network failures. Second, we devise a dynamic load balancing technique that exploits the service processing capabilities of replicas to scale the system in anticipation of unexpected workload changes and node failures by taking into account of node heterogeneity, network proximity, and changing workload at each node. Our experimental evaluation shows that the reliable GeoGrid architecture is highly scalable under changing service workloads with moving hotspots and highly reliable in the presence of both individual node failures and massive node failures."
192,"Recent years have witnessed a growing interest in context-aware recommender system (CARS), which explores the impact of context factors on personalized Web services recommendation. Basically, the general idea of CARS methods is to mine historical service invocation records through the process of context-aware similarity computation. It is observed that traditional similarity mining process would very likely generate relatively big deviations of QoS values, due to the dynamic change of contexts. As a consequence, including a considerable amount of deviated QoS values in the similarity calculation would probably result in a poor accuracy for predicting unknown QoS values. In allusion to this problem, this paper first distinguishes two definitions of Abnormal Data and True Abnormal Data, the latter of which should be eliminated. Second, we propose a novel CASR-TADE method by incorporating the effectiveness of True Abnormal Data Elimination into context-aware Web services recommendation. Finally, the experimental evaluations on a real-world Web services dataset show that the proposed CASR-TADE method significantly outperforms other existing approaches."
193,"In this paper, we focus on the link predication problem in social networks. Our approach is based on the observation that there is a large amount of social behavior taking place every day which contains substantial information about user intrinsic characteristics that influence the dynamics of social networks. In order to obtain a deeper understanding of user behavior, we introduce the concept of latent factor to capture the motivation behind social activities. Since user relationships are often asymmetric, we also take into account bilateral user wishes with respect to friend as preferences, which is beyond traditional approaches or overall measurements. Two combination modes are proposed, independent fusion and interdependent fusion, to integrate these hybrid metrics with traditional measurements for link inference. In order to quantify the sensitivity of each element in metrics we use information theory. Experimental results on several real datasets show that our approach has better performance than previous methods."
194,"We address the problem of large-scale data integration, where the data sources are unknown at design time, are from autonomous organisations, and may evolve. Experiments are described involving a demonstrator system in the field of health services data integration within the UK. Current Web services technology has been used extensively and largely successfully in these distributed prototype systems. The work shows that Web services provide a good infrastructure layer, but integration demands a higher level ""broker"" architectural layer; the paper identifies eight specific requirements for such an architecture that have emerged from the experiments, derived from an analysis of shortcomings which are collectively due to the static nature of the initial prototype. The way in which these are being met in the current version in order to achieve a more dynamic integration is described."
195,"Quality of service (QoS) management in compositions of services requires careful consideration of QoS characteristics of the services and effective QoS management in their execution. A Web service is a software system that supports interoperable application-to-application interaction over the Internet. Web services are based on a set of XML standards such as simple object access protocol (SOAP). The interactions of SOAP messages between Web services form the theoretical model of SOAP message exchange patterns (MEP). Web Services Business Process Execution Language (WSBPEL) defines an interoperable integration model that facilitates automated process integration in intra- and inter-corporate environments. A service-level agreement (SLA) is a formal contract between a Web services requestor and provider guaranteeing quantifiable issues at defined levels only through mutual concessions. Based on a prior research work on message detail record (MDR), this paper further proposes a SOAP message tracking model for supporting QoS end-to-end management in the context of WSBPEL and SLA. This paper motivates the study of QoS management in a Web service composition framework with the evolution of a distributed toolkit in an industrial setting."
196,"In recent years, the evolving of IoT (Internet of Things) has resulted in the deployment of massive numbers of sensors in various fields, such as the Energy and Utility (E&amp;U) industry. These sensors are continuously producing a huge amount of time series data, which creates a correspondingly huge demand for time series data analysis, such as pattern searching. However, analysis on time series data is currently implemented as custom applications, a strategy that suffers from low efficiency and high maintenance costs. Hence there is a need to provide a service for analysis on time series data that reduces maintenance costs and enhances query efficiency. Existing time series data management services lack the capability to perform pattern searches on the massive amount of data from sensors. This paper presents Time Series analytics as a Service (TSaaaS), a scalable analytic service for time series data in IoT scenarios. We designed pattern searching in TSaaaS that can support efficient and effective searching on truly massive amounts of time series data with very little overhead on the IoT system. To simplify access to the TSaaaS, we created a group of RESTful web interfaces. TSaaaS is implemented as an extension to the Time Series Database service in the IBM cloud platform offering (Codename: BlueMix), which is a new product to accelerate IoT application development. TSaaaS targets a future release of the Time Series Database service. We have conducted proofs of concept (PoC) of TSaaaS with real-world customers from power meter management and bridge monitoring in China. The pilot results and other experiments show that for a selection of pattern cases provided by customers, pattern searches via our service are 10-100 times faster than the existing techniques, while the additional storage cost for the service provider accounts for only about 0.4% of original time series data."
197,"Large-scale scientific data management and analysis usually relies on many distributed scientists with diverse expertise. In recent years, such a collaborative effort is often composed and automated into a dataflow-oriented process, a so-called scientific workflow. However, existing scientific workflow tools are single user-oriented and do not support collaborative scientific workflow composition, execution, and management among multiple distributed scientists. In this paper, we report our study of collaboration protocols towards building a tool supporting collaborative scientific workflow composition. Based on a scientific collaboration ontology, we propose a collaboration model supported by a set of collaboration primitives and patterns. The collaboration protocols are then applied to support effective concurrency control in the process of collaborative workflow composition."
198,"Web services (WS) technology is becoming pervasive in the development of distributed systems and is an appealing vehicle for service presentation and horizontal integration. On the other hand, model integrated computing (MIC) offers a means of system integration in the vertical direction by using domain-specific modeling, and then synthesizing the software system from the high-level model using a model-specific generator. This paper presents a meta-modeling approach to WS to explore the application of MIC in WS development and its contribution."
199,"The growth of the Internet has been accompanied by the growth of Web services (e.g. e-commerce, e-health) leading to the need to protect the personal privacy of Web service users. However, it is also important to be able to measure a Web service in terms of how well it protects personal privacy. Such a capability would benefit both users and developers. Users would benefit from being able to choose (assuming that such measures were made public) the service that has the greatest ability to protect user privacy (this would in turn encourage Web service providers to pay more attention to privacy). Developers would benefit by being able to incrementally measure and modify their services during development until certain target levels of privacy protection are reached. This paper presents an approach for measuring how well a Web service protects personal privacy and illustrates the approach with an example"
200,"Accurate and lightweight evaluation of web service security properties is a key problem, especially when business processes are dynamically built by composing atomic services provided by different suppliers at runtime. In this paper, we tackle this problem by proposing a security certification approach that virtually certifies a composite service for a set of security properties, starting from certificates awarded to the component services."
201,"This work is motivated by the increasing importance and business value of data in the fields of business process management, scientific workflows as a field in eScience, and Internet of Things, all of which profiting from the recent advances in data science and Big data. We introduce a management life cycle that renders data as first-class citizen in service choreographies and defines the functions and artifacts necessary for enabling transparent and efficient data exchange among choreography participants. The inherent goal of the life cycle, functions and artifacts is to help decouple the data flow, data exchange and management from the control flow in service compositions and choreographies. This decoupling enables peer-to-peer data exchange in choreographies and provides the means for more sophisticated data management and exchange, as well as data exchange and provisioning optimization."
202,"IT services need an automatic and flexible ability to react to dynamic changes in their environment. Managing change effectively and reducing the negative effects of day-today operations has become one of the most important tasks in IT service management, which require hiring highly skilled IT professionals with correspondingly high labor costs. There is a challenge to select, implement and integrate the right resources quickly and effectively. Although there is a growing body of research into IT management, many techniques are either too narrow (focusing on a single component rather than the entire system), or they address only configuration data collection and integration. Instead, one needs to scale or respond to special domain knowledge, collaboration and the right data for helping IT professionals to improve their work. In this paper, we propose a knowledge-sharing based collaborating management system for IT service management. It aims to bridge the gap between domain experts' knowledge and manageable systems. We developed a proof-of-concept of an impact analysis service based on knowledge-sharing; it establishes an IT service management collaboration paradigm and ecosystem, leveraging the expert's rich knowledge and experience to improve the management quality and reduce the cost. A case study driven by customers demonstrates that collaboration with knowledge sharing is effective both at constructing useful system analysis services and in using those services to improve system management."
203,"Copyright and Reprint Permissions: Abstracting is permitted with credit to the source. Libraries may photocopy beyond the limits of US copyright law, for private use of patrons, those articles in this volume that carry a code at the bottom of the first page, provided that the per-copy fee indicated in the code is paid through the Copyright Clearance Center. The papers in this book comprise the proceedings of the meeting mentioned on the cover and title page. They reflect the authors' opinions and, in the interests of timely dissemination, are published as presented and without change. Their inclusion in this publication does not necessarily constitute endorsement by the editors or the Institute of Electrical and Electronics Engineers, Inc."
204,"With the development of mobile network and corresponding techniques, more and more works focus on providing efficient services based on mobile devices. Furthermore, motivated by IoT, studies of local distributed mobile devices attract attentions of both industry and academia in recent years. However, existing storage systems cannot manage data and support the QoS of mobile services well. This paper presents LKSM, a light weight key-value storage system, which can be deployed on either one node or multiple nodes. To the best of our knowledge, it is the first attempt to propose key-value store in this scenario. We carefully analyze the challenges when designing the system on mobile cluster, and further propose RDS for addressing. With the help of RDS, LKSM achieves the goal of lower latency, better scalability, and higher availability. We organize LKSM using a log-structured merge-tree, and implement it based on LevelDB, an open source key-value storage system proposed by Google. Experiments on physical smartphones demonstrate that LKSM presents much higher performance compared with the ported LevelDB on mobile devices."
205,"The service-oriented architecture (SOA) makes application development easier, because applications can be built from existing services with a bottom-up methodology. However, it is difficult to determine if a desired new service can be built from existing services. Not only the functional consistency of the existing services, but also the consistency of their non-functional (such as security) aspects must be verified. Message protection is an aspect of security. Every service needs an appropriate security policy defining the protection of messages exchanged between the parties to the service. Because of the intricacy of the Web services security policy language, it is difficult to verify the consistency of the security policies. We are developing a method to verify the consistency of security policies by abstracting them. Each security policy is abstracted, and then attached as a security type to the corresponding service in the application model. The security type denotes a security level for message protection. The security developer defines the possible abstraction methods. In this paper, we define the constraint of abstraction methods based on the semantics of the policy language. And also we state verifying the consistency of security types by using information flow analysis."
206,"Web sites serve content both through Web services as well as through user-viewable Web pages. While the consumers of Web-services are typically 'machines', Web pages are meant for human users. It is highly desirable (for reasons of security, revenue, ownership, availability etc.) for service providers that content that will undergo further processing be fetched in a prescribed fashion, preferably through a supplied Web services. In fact, monetization of partnerships within a services ecosystem normally means that Web site data translate into valuable revenue. Unfortunately, it is quite commonplace for arbitrary developers to extract or leverage information from websites without asking for permission and or negotiating a revenue sharing agreement. This may translate to significant lost income for content providers. Even in cases where Web site owners are happy to share the data, they may want users to adopt dedicated Web service APIs (and associated API-servers) rather than putting a load on their revenue-generating websites. In this paper, we introduce a mechanism that disables automated Web scraping agents, thus forcing clients to conform to the provided Web Services."
207,"Service composition is a widely used method in ubiquitous computing that enables accomplishing complex tasks required by users based on elementary (hardware and software) services available in ubiquitous environments. To ensure that users experience the best Quality of Service (QoS) with respect to their quality needs, service composition has to be QoS-aware. Establishing QoS-aware service compositions entails efficient service selection taking into account the QoS requirements of users. A challenging issue towards this purpose is to consider service selection under global QoS requirements (i.e., Requirements imposed by the user on the whole task), which is of high computational cost. This challenge is even more relevant when we consider the dynamics, limited computational resources and timeliness constraints of ubiquitous environments. To cope with the above challenge, in this paper we present QASSA, an efficient service selection algorithm that provides the appropriate ground for QoS-aware service composition in ubiquitous environments. The contribution of QASSA is three-fold. First, it formulates service selection under global QoS requirements as a set-based optimisation problem, benefiting from recent proposals in the domain of multi-objective optimisation. Second, QASSA resolves this problem in an efficient way using clustering techniques, namely the K-Means algorithm. Third, QASSA is devised in two versions, viz., centralised and distributed versions, so that it can be executed on top of centralised and decentralised infrastructures in ubiquitous environments. Results of experimental studies are presented to illustrate the timeliness and optimality of QASSA."
208,"One of the major goals of Web services is to make easier their composition to form more complex services, modeled as workflows. A key role in the Web services composition is the selection of a proper service for each activity in the workflow. In general, this requires the exchange of sensitive information of users, requiring the composition, as well as of involved service providers. So far this problem has been investigated in the setting of orchestrated service composition, under the assumption of the presence of a broker coordinating the composition. However, a promising alternative approach is the one of choreography, where each service involved in the service composition has to locally manage service selection and invocation. In this paper, we propose a framework to enforce user and provider requirements in the scenario of service choreography in a privacy-preserving way, that is, without the releasing of any private information of users and providers. To achieve this result we make use of different privacy-preserving protocols. As it will be shown in the paper, the proposed solution does not implies relevant overhead."
209,"WSDL provides the potential for Web services to enrich consumers' lives. However, it has had only limited success in enterprise environments and even less in the mass market. Apart from the difficulties and high costs involved in current approaches, another reason is the low precision for Web Services discovery using the most widely known tool: search engines. Seeking for effective ways to change the situation, we conducted an experiment to examine better approaches of using general-purpose search engines to discover Web Services. We used nine different approaches for publishing Web Services and two groups of total 18 queries for retrieving them using Yahoo and Google search engines. The queries were fired to each search engine daily over a week and the top 100 search results returned from every search are collected and analyzed. The results show that for both search engines, embedding a WSDL specification in a Web page that provides semantic description of the service yield the best results."
210,"Web services composition design, verification and monitoring are active and widely studied research directions. Little work however has been done in integrating these related dimensions using a unified formalism. In this paper we propose a declarative event-oriented framework, called DISC, that serves as a unified framework to bridge the gap between the process design, verification and monitoring. Proposed framework allows for a composition design to accommodate various aspects such as data relationships and constraints, Web services dynamic binding, compliance regulations, security or temporal requirements and others. Then, it allows for instantiating, verifying and executing the composition design and for monitoring the process while in execution. The effect of run-time violations can also be calculated and a set of recovery actions can be taken, allowing for the self-healing Web services composition."
211,Presents the welcome message from the conference proceedings.
212,"Since there are many Web services on the Internet, personalized Web service selection and recommendation is very important. In this paper, we present a new similarity measure for Web service similarity computation and propose a normal recovery collaborative filtering (NRCF) method for personalized Web service recommendation."
213,"SWSD practices cannot be defined independently of the situation in which they are applied. To address this challenge, we propose an approach for guiding the construction and execution of situational SWSD methods by reusing and integrating different existing SWSD method components suited to the specific situation on hand."
215,"Today Web services have grown in context of both business to business (B2B) and business to customer (B2C) applications. Web services are the most popular mode of implementing service oriented architecture (SOA). With this growth and acceptance in the industry, the role of security is crucial. Most of the existing security mechanisms in Web services like XML encryption, digital signatures, user tokens etc. provide security on one basic assumption that source of the request is legitimate. But a typical denial of service attacker can use these sources as reflectors and play around with the contents of a Web service body to create an attack scenario. In this paper, we propose PreSODoS - a framework to detect and prevent XML based denial of service (XDoS) attacks on Web services based applications. The framework relies on content introspection to detect any XDoS possibility. We use a Patricia trie based representation so that the schemas and the request messages can be compared and validated in a performance efficient manner. PreSODoS is capable of detecting any repetitive request message and sense an attack scenario and trigger corresponding prevention mechanisms"
216,"In recent years, a number of scientific workflow management systems (SWFMSs) have been developed to help domain scientists synergistically integrate distributed computations, datasets, and analysis tools to enable and accelerate scientific discoveries. As more scientific research projects become collaborative in nature, there is a compelling need of dedicated services to support collaborative scientific workflows on the Internet. This paper reviews the state of the art of the field of scientific workflows towards the support of collaborative scientific workflows, identifies critical research challenges, and presents our ongoing research work aiming to study how to create services supporting collaborative scientific workflows."
217,"This paper investigates the problem of reputation management in composite services. Our focus is on developing a method of distribution of reputation received by a composite service to its component services. The proposed method enables the composite service to provide a fair distribution of reputation values so that a component service is neither penalized nor awarded for the bad and good performances respectively, of other component services. Experiment results show that the proposed technique propagates the ""fair share"" of reputation from the composite service to its component services."
218,"Services computing is playing a critical role in recent years in many fields and we observe a rapidly increasing number of web accessible services and their compositions nowadays. However, our earlier empirical study reveals that, overall the public available services are under-utilized, and when they are used, they are used mostly in an isolated manner. This phenomenon inspires us to further explore a methodology to help consumers understand the usage pattern of the service ecosystem, including interactions among services, and the evolution of these interactions. Based on the derived usage pattern, this methodology also introduces a service recommendation method that suggests both services and their compositions, in a time-sensitive manner. We firstly construct an evolution network model from the historical usage of the services in the ecosystem. Then a rank-aggregation-based link prediction method is proposed to predict the evolution of the ecosystem. Based on this link prediction method, we can recommend services and compositions of interest to service developers. Through an experiment on the real-world mashup-service ecosystem, i.e., Programmable Web, we demonstrated that our approach can effectively recommend services and compositions with better precision than the methods we compared."
219,"Web services allow devices running on different platforms to communicate with one another using standardized definitions and access ways. Due to recent developments in mobile networks and devices, many researches are on going to apply Web services to mobile network environments. In this paper, we propose an efficient method that discovers services based on a proposed mobility-based clustering in mobile ad-hoc networks. In order to maintain stable clusters and select a proper service discovery architecture, the proposed method uses the mobility of nodes including direction. Experimental results under different mobility models show that the proposed method outperforms a conventional clustering and service discovery method."
220,"This paper analyses the well known web services representations based on web services descriptions and more generally on the content of WSDL files to evaluate their interest for discovery, clustering and recommendation tasks.Unfortunately, this analysis shows that these representations are very basic and do not lead to good results. Therefore,we introduce a new representation called symbolic reputation which is computed from relationships between web services.Different implementation issues are discussed and the results considering real world web services are analysed to determine the usefulness of the introduced representations for web services discovery, clustering and recommendation."
221,"Due to complex environment of the coal mine, it's necessary to monitor the information of underground environment, device and miner instantly in order to ensure the safety of coal mine production. However, the exiting coal mine can not meet the requirements of coverage without blind spots as it is developed by the wired network. This paper proposes a RESTful Web services mashup augmented coal mine safety monitoring and control automation using ZigBee wireless sensor network, which can collect the underground temperature, humidity methane values and personal position through sensor nodes in the coal mine, and also collects the personnel position information inside the mine, and then implement a RESTful Application Programming Interface (API) on sensor nodes to provide access to sensors and actuators, allowing for them to be easily combined with other enterprise information resources based on the success of mashup applications. We also illustrated three different of scenarios for RESTful Web service mashups representing for coal mine safety monitoring and control automation. Finally, we give the conclusions."
222,"With the overload of information on the Web, Recommender Systems (RSs) are becoming increasingly popular and have been employed to provide suggestions to meet different requirements. RSs are utilized in a variety of areas including movies, music, social tags, user group and products as Web services evoked on the Internet either as mobile Apps or PC-based applications. However, it is challenging to achieve personalized recommendations instead of offering up too many lowest common denominator recommendations. Understanding how products relate to each other is important because it has great impact on the performance. Furthermore, the personalized sequential behavior, which is closely related to a particular product, is essential for recommender systems. Most models simply integrate features from users and items without considering potential product bundle relationships between products exposed by users' personalized sequential behaviors. In this paper, a novel method based on Factorizing Personalized Markov Chain (FPMC) is proposed to comprehensively explore the latent bundle relations from users perspective, along with the hidden correlative semantics between products obtained from logic regression method, which provides a unified view to describe the user preferences, product/item features, and the user sequential patterns in timely manner. The involved semantic features are extracted using deep learning models. We evaluate our method on real-world Amazon datasets and our framework significantly outperforms other baseline models, especially on sparse datasets. The experimental results show that our approach qualitatively captures personalized behaviors with superior recommendation performance."
223,"Software agents controlling production devices must maintain an up-to-date view of the physical world state in order to efficiently reason and plan their actions. Especially in a factory automation system, the world state undergoes rapid evolution, and the world view must remain synchronized with the changes. This paper discusses two approaches to updating the world view based on event notifications sent by web services representing production devices in a manufacturing system. One of the approaches is based on separately specified update rules, and one automatically uses the semantic web service descriptions formulated in OWL-S. While this paper specifically focuses on the factory automation domain, the approaches presented are applicable to other domains as well."
224,"Web service technologies are playing an increasingly important role in service-oriented architecture design and application convergence over Mobile ad hoc networks (MANET). Due to the decentralized administration and dynamic wireless connectivity problems, accomplishing reliable service discovery in MANET faces a large number of challenges. In order to relieve the communication inefficiency among service providers and clients caused mainly by the unpredictable node mobility, this paper proposes a cross-layer service discovery scheme which enables improved network efficiency and reduced resource consumption. Firstly a network-layer based underlay framework is presented. It specifically establishes a reliability-oriented source routing mechanism which is equipped with a novel reliability-maximized path selection metric and a backup path support fast route recovery strategy. The cross-layer design is prudentially realized by piggybacking service discovery procedures on the reliability enhanced underlay routing mechanism. Simulation analysis verifies that the proposed scheme improves service discovery reliability by achieving low rediscovery frequency, and guarantees high network efficiency by providing reduced service discovery delay and control overhead."
225,"In the past a few years, the Web has undergone a tremendous change towards a highly user-centric environment. Millions of users can participate and collaborate for their own interests and benefits. Service oriented computing and Web services have created great potential opportunities for the users to build their own applications. Then, it is a pressing issue that, the users can compose services without too complex tasks and efforts. In this paper, we introduce a user-oriented approach which aims to simplify service composition. We leverage the plentiful information residing in service tags, both from service descriptions (such as WSDL) and the annotations tagged by users. Employing some mining algorithms, a direct acyclic graph is built up to represent potential composition opportunities. With a simple and intuitive search, it allows users to explore the space of potentially composable services and achieve service composition in a heuristic manner. We have developed a composition advisor to provide recommendations guiding and assisting the users. It also lets the users discover and make use of services without having to understand too many details of individual candidate services. To enable the users to accomplish service composition in a more interactive access channel, we finally provide a user-friendly prototype based on Web browsers. It undoubtedly reduces the complexity and lowers the entry barrier for the users, and makes them better play their role in the service-oriented Web environment."
226,"Availability of a wide variety of Web services over the Internet offers opportunities of providing new value added services built by composing them out of existing ones. By integrating individual existing Web services the technology enables the provision of advanced and sophisticated services, such as allowing users to use different types of resources and services simultaneously in a simple procedure. However the management and maintenance of a large number of Web services is not easy and, in particular, needs appropriate authorization policies to be defined so as to realize reliable and secure Web Services. The required authorization policies can be quite complex, resulting in unintended conflicts, which could result in information leaks or prevent access to information needed. This paper proposes a logic based approach using for specifying authorization policies and detecting conflicts resulting from the combination of various kinds of authorization and constraint policies used in Web services environments. The method not only enables static detection of policy conflicts but also yields information that is helpful for correcting the policies. An automated induction-based theorem prover SPIKE is used as verification back-end."
227,"Service compositions inherently require multiple services each with its domain-specific functionality. Therefore, how to mine matching patterns between services in relevant domains and compositions becomes crucial to service recommendation for composition. Existing methods usually overlook domain relevance and domain-specific matching patterns, which restrict the quality of recommendations. In this paper, a novel approach is proposed to offer domain-aware service recommendation. First, a K Nearest Neighbor variant (vKNN) based on topic model Latent Dirichlet Allocation (LDA) is introduced to cluster services into semantically coherent domains. On top of service domain clustering results by vKNN, a probabilistic matching model Domain Router (DR) based on Extreme Learning Machine (ELM) is developed for decomposing a requirement to relevant domains. Finally, a comprehensive Domain Topic Matching (DTM) model is built to mine relevant domain-specific matching patterns to facilitate service recommendation. Experiments on a large-scale real-world dataset show that DTM not only gains significant improvement at precision rate but also enhances the diversity of results."
228,"The emergence of Web services has changed the Internet a lot, and greatly facilitated the development of service based software systems. How to select appropriate services and compose them according to given context to satisfy a user's requirement is a big challenge. This paper proposes a novel Genetic Algorithm (GA) method to synthesis web services in a context-aware environment. We first present a context space model to illustrate both contexts and services in a formal way, we utilize GA to compose context-aware services according to users' preference. We transform the problem of service composition to a multi-objective optimization problem. To resolve the conflict and dependencies among services in GA process, we propose a service similarity tree (SST) model to measure the similarity between services. Finally, we design a simulation experiment to evaluate our method. The experiment result shows that our method is a promising one to solve service composition problem in a context-aware environment."
229,"Due to the rapid growth in both the number and diversity of Web services on the web, it becomes increasingly difficult for us to find the desired and appropriate Web services nowadays. Clustering Web services according to their functionalities becomes an efficient way to facilitate the Web services discovery as well as the services management. Existing methods for Web services clustering mostly focus on utilizing directly key features from WSDL documents, e.g., input/output parameters and keywords from description text. Probabilistic topic model Latent Dirichlet Allocation (LDA) is also adopted, which extracts latent topic features of WSDL documents to represent Web services, to improve the accuracy of Web services clustering. However, the power of the basic LDA model for clustering is limited to some extent. Some auxiliary features can be exploited to enhance the ability of LDA. Since the word vectors obtained by Word2vec is with higher quality than those obtained by LDA model, we propose, in this paper, an augmented LDA model (named WE-LDA) which leverages the high-quality word vectors to improve the performance of Web services clustering. In WE-LDA, the word vectors obtained by Word2vec are clustered into word clusters by K-means++ algorithm and these word clusters are incorporated to semi-supervise the LDA training process, which can elicit better distributed representations of Web services. A comprehensive experiment is conducted to validate the performance of the proposed method based on a ground truth dataset crawled from ProgrammableWeb. Compared with the state-of-the-art, our approach has an average improvement of 5.3% of the clustering accuracy with various metrics."
230,"We present the Web service description framework (WSDF), which provides both a representation mechanism and a runtime system architecture for semantically enriched Web Services. We analyze existing languages such as BPEL4WS and OWL-S before addressing their deficiencies in our proposal. Our approach allows a client to invoke a service based solely on a shared ontology, i.e. without prior knowledge on the API, providing an important building block towards a global, flexible information infrastructure. Another main point is that WSDF can be applied to clients and services written in a conventional object oriented programming language. This is achieved by lifting data structures to an ontology level in which rich logical statements about services can be formalized. We also present a detailed system architecture that covers planning, invocation, and the automatic processing of the service results, which is accomplished using the observer design pattern and by asserting the result in the respective model. Furthermore, the required annotations can be specified conveniently by placing comments in the source code."
231,"The main issues for the fulfillment service level agreements (SLA) are concerned with problem of variability of QoS properties (vQoS). Indeed, the QoS properties may evolve frequently either because of internal changes or because of workload fluctuations. To solve the vQoS problem, we first introduced three variability operators: replicate, delete and replace. These operators will be used to reconfigure CWS when the SLA contract is violated. The first two operators are used to add and remove Web service instances, while the last one is used to substitute some faulty Web services. Then, we proposed an incremental approach for modeling and verifying the composites services (CWSs) reconfiguration using Event-B. We start by abstractly specifying the main requirements and then we refine them through several steps to model CWSs. The consistency of each model and the relationship between an abstract model and its refinements are obtained by formal proofs. Finally, we used ProB model-checker to trace possible design errors. We have exploited the LTL for dynamic reconfigurations to characterize the correct behavior of CWSs reconfiguration."
232,"In this paper we describe tool support for a model-based approach to verifying compositions of Web service implementations. The tool supports verification of properties created from design specifications and implementation models to confirm expected results from the viewpoints of both the designer and implementer. Scenarios are modeled in UML, in the form of message sequence charts (MSCs), and then compiled into the finite state process (FSP) algebra to concisely model the required behavior. BPEL4WS implementations are mechanically translated to FSP to allow an equivalence trace verification process to be performed. By providing early design verification and validation, the implementation, testing and deployment of Web service compositions can be eased through the understanding of the behavior exhibited by the composition. The tool is implemented as a plug-in for the Eclipse development environment providing cooperating tools for specification, formal modeling and trace animation of the composition process."
233,"Reliability is an essential quality requirement for service-oriented systems. A number of models have been developed for predicting reliability of traditional software, in which code-based defects are the main concern for the causes of failures. Service-oriented software, however, shares many common characteristics with distributed systems and web applications. In addition to residual defects, the reliabilities of these types of systems can be affected by their execution context, message transmission media, and their usages. We present a case study to demonstrate that the reliability of a service varies on an hourly basis, and reliability forecasts should be recalibrated accordingly. In this study, the failure behavior of a required external service, used by a provided service, was monitored for two months to compute the initial estimates, which then continuously re-computed based on the learning of the new failure patterns. These reliabilities are integrated with the reliability of the component in the provided service. The results show that with this progressive re-calibration we provide more accurate reliability forecasts for the service."
234,"Communications among Web services are asynchronous. Asynchronous models of service compositions face the problem that the performance of verification is bring down with states explosion. We describe an approach, which utilize the interaction-independence of component services in the composition and forms all component services into different groups. Preliminary experiment results show that the verification of service compositions upon these groups can decrease the size of exploration states and hence improve the performance."
235,"Service-oriented Architectures support the provision, discovery, and usage of services in different application contexts. The Web Service specifications provide a technical foundation to implement this paradigm. Moreover, mechanisms are provided to face the new security challenges raised by SOA. To enable the seamless usage of services, security requirements can be expressed as security policies (e.g. WS-Policy and WS-SecurityPolicy) that enable the negotiation of these requirements between clients and services. However, the codification of security policies is a difficult and error-prone task due to the complexity of the Web Service specifications. In this paper, we introduce our model-driven approach that facilitates the transformation of architecture models annotated with simple security intentions to security policies. This transformation is driven by security configuration patterns that provide expert knowledge on Web Service security. Therefore, we will introduce a formalised pattern structure and a domain-specific language to specify these patterns."
236,"The behavioral analysis for Web services provides a priori detection of errors to ensure successful interactions in services invocation and composition, and the behavioral substitution of Web services is one of the most important issues in such analysis. In this paper, we propose to formalize the behavior of a Web service by π-calculus. Based on the formalization, we introduce two notions of behavioral substitution of Web services namely strong and weak simulation. Furthermore, we propose a derivative approach to analyzing the behavioral substitution of services according to the given notions, which is implemented based on an existing tool of π-calculus. The proposed approach takes advantage of formalization and theory of π-calculus, so that the formalized services can be naturally analyzed and the behavioral substitution of them can be easily determined."
237,"Configuration management is a complex task, even for experienced system administrators, which makes self-managing systems a desirable solution. Self-management implies the need for a model based on which configuration changes may be decided. In previous work, we described a method for constructing a state-transition model of application behavior, by observing the application in simulation. This method relied on an expert to manage the (simulated) application in order to collect the necessary observations for constructing the model. However, that method was agnostic about (a) the size of the system space space as implied by the granularity of the observations, and (b) the sufficiency of the actual observations collected for understanding the application in a variety of configurations and environments. In this paper, we replace the (expensive) expert domain knowledge with automatic approaches to ensuring coverage of the application, and demonstrate the superiority of this approach. We present empirical data regarding state space and granularity to explore the use of state models for understanding applications."
238,"QoS-aware service composition intends to maximize the QoS of a composite service when selecting service providers. This paper proposes a service composition scheme that uses a combination of Integer Programming, case-based reasoning, and, genetic algorithms techniques. The scheme reduces the service composition costs by reusing existing compositions. Experiments show that, compared with solutions purely based on Integer Programming, the proposed scheme is effective in reducing the time for carrying out service composition."
239,"Modern scientific computations are usually data intensive, involving large-scale, heterogeneous and structured scientific datasets. Modeling, organizing, and processing scientific data have become key challenges for scientific workflow management systems (SWFMSs). In contrast to business data, which is usually relational and stored in databases, scientific data is often hierarchically organized and collection oriented. Although several data models have been proposed for SWFMSs, none of them provides a formal data model with a set of well-defined operators. In this paper, we take a first step towards formalizing a collection-oriented data model, called collectional data model, to model hierarchical collection oriented scientific data, and a set of well-defined operators to manipulate and query such data. We then apply the collectional data model to VIEW, a dataflow-based scientific workflow composition framework, whose workflow constructs are extended to support collections. We implement our techniques and validate them by a case study in a biological simulation project."
240,"We present an automated approach to generate functional conformance tests for semantic Web services. The semantics of the Web services are defined using the inputs, outputs, preconditions, effects (IOPEs) paradigm. For each Web service, our approach produces testing goals which are refinements of the Web service preconditions using a set of fault models. A novel planner component accepts these testing goals, along with an initial state of the world and the Web service definitions to generate a sequence of Web service invocations as a test case. Another salient feature of our approach is generation of verification sequences to ensure that the changes to the world produced by an effect are implemented correctly. Lastly, a given application incorporating a set of semantic Web services may be accessible through several interfaces such as 1) direct invocation of the Web services, or 2) a graphical user interface (GUI). Our technique allows generation of executable test cases which can be applied through both interfaces. We describe the techniques used in our test generation approach. We also present results which compare two approaches: an existing manual approach without the formal IOPEs information and the IOPEs-based approach reported in this paper. These results indicate that the approach described here leads to substantial savings in effort with comparable results for requirements coverage and fault detection effectiveness."
241,"The past decade has witnessed a fast growth of web-based services, making discovery of user desired services from a large and diverse service space a fundamental challenge. Service clustering has been demonstrated as a promising solution by automatically detecting functionally similar services so that they can be searched and discovered together. In this way, both the efficiency and accuracy of service discovery can be improved. However, the autonomous nature of service providers leads to highly diverse usage of terms in their respective service descriptions. Furthermore, a typical service description is comprised of very limited terms due to the small number of (and focused) functionalities offered by the service. These unique characteristics make service descriptions different from regular text documents, which poses additional challenges when clustering large-scale services. Recent works show that service clustering can benefit from discovery and use of functionality-related latent factors to represent services as opposed to a large and diverse set of terms. Nonetheless, how to determine the total number of latent functional factors and sparsely assign them to each service description arises as a central challenge, especially for a large service space where there is no easy way to enumerate the types of different functionalities. In this paper, we propose a machine learning method that automatically learns the number of latent functional factors in a service space. It also enforces the sparsity constraint, which allows each service to be represented by a small number of latent functional factors. The sparsity constraint is in line with the fact that most real-world services only provide limited functionalities. We conduct extensive experiments on two sets of real-world service data to demonstrate the effectiveness of the proposed service clustering approach."
242,"Differential deserialization (DDS) is an optimization technique that exploits similarities between incoming SOAP messages to reduce deserialization time. DDS works by checkpointing the state of the SOAP deserializer at various points while deserializing a message, and using those checkpoints to avoid full deserialization of similar messages. DDS can improve performance in many cases, but its benefit is limited by the potentially significant memory and processing overhead associated with its checkpointing mechanism. Differential checkpointing (DCP) substantially reduces memory use, but still requires significant processing overhead. In this paper, we introduce lightweight checkpointing (LCP), a checkpointing approach that significantly reduces the cost of both DDS and DCP, in terms of both memory use and processing time. LCP statically determines locations in the incoming message where it would be most efficient to create checkpoints. LCP creates checkpoints much faster than both our original DDS checkpointing mechanism and our DCP approach. LCP also has significantly smaller memory requirements. For example, in some of our test cases, LCP requires only 10% of the memory that DCP requires, and only 3% of the memory that our original approach required. In terms of processing time, deserialization with LCP is approximately 50% to 60% faster than without differential deserialization, when approximately half the message is unchanged from the previous message"
243,"In this paper, we develop an XML document retrieval system supporting a multimedia Web service for a digital museum. It can support unified retrieval on XML documents based on both document structure and image content. To achieve it, we perform the indexing of XML documents describing Korean porcelains used for a digital museum, based on not only their basic unit of element but also their image color and shape features. In addition, we provide a similarity measure for a unified retrieval to a composite query, based on both document structure and image content. Finally, we implement our XML document retrieval system designed for a digital museum Web service and analyze its performance in terms of retrieval time, insertion time, storage overhead, as well as recall and precision measure."
244,"Web Services communicate through XML-encoded messages and suffer from substantial overhead due to verbose encoding of transferred messages and extensive (de)serialization at the end-points. We demonstrate that response caching is an effective approach to reduce Internet latency and server load. Our Tantivy middleware layer reduces the volume of data transmitted without semantic interpretation of service requests or responses and thus improves the service response time. Tantivy achieves this reduction through the combined use of caching of recent responses and data compression techniques to decrease the data representation size. These benefits do not compromise the strict consistency semantics. Tantivy also decreases the overhead of message parsing via storage of application-level data objects rather than XML-representations. Furthermore, we demonstrate how the use of aspect-oriented programming techniques provides modularity and transparency in the implementation. Experimental evaluations based on the WSTest benchmark suite demonstrate that our Tantivy system gives significant performance improvements compared to non-caching techniques."
245,"The procedure of picking services bound to abstract tasks is usually called service selection in Service Oriented Architecture. In recent years, most studies focus on improving the Quality of Service (QoS) of the composed service. These techniques, however, are facing a new challenge, brought by the big data era, namely, the time and money wasted in data transmission, called transmission cost, cannot be optimized locally like QoS. To address this challenge, in this paper, we study and formalize the problem of transmission cost aware service selection, named TcSS. Owing to the insufficient service transfer rates, we propose a framework on a relaxation problem by making use of the service network ontology structure. The entire framework comprises two stages, an off-line stage to arrange the service network information from logs and an online stage to satisfy the service selection requirement efficiently. The solution of the relaxation problem is an approximation of the original TcSS with the approximate ratio guarantees. Finally, extensive experiments on real data establish the effectiveness and efficiency of our approach."
247,"What Is Innovation? It’s not always about inventing something entirely new. Innovation occurs at the intersection of invention and insight. It’s about the application of invention - the fusion of new developments and new approaches to solve problems. (Sam Palmisano, Delivered at the Council on Competitiveness Annual Meeting, Washington, D.C., October 30, 2003). In the 21st Century, we are approaching fundamental limits of technology that will drive new paradigms for software and systems. Software complexity is driving a rethinking of software development. There are external forces including government and societal, that are significantly influencing the technology agenda. In addition, the changing business environment demands new approaches to use of IT - transformation must be fueled by innovation. Can we manage innovation? Can we create a culture of innovation? Can we work with customers and partners to drive innovation? Various innovation approaches are being deployed to enhance IBM’s innovation ecosystem. Lessons learned and future innovation drivers will be presented."
248,"In Service-Oriented Computing (SOC) environments, the trustworthiness of each service provider is critical for a service client when selecting one from a large pool of service providers. The trust value of a service provider is usually in the range of [0, 1] and is evaluated from the ratings given by service clients, which represent the subjective belief of service clients on the satisfaction of delivered services. So a trust value can be taken as a subjective probability, by which one party believes that another party can perform an action in a certain situation. Hence, subjective probability theory should be adopted in trust evaluation. In addition, in SOC environments, a service provider usually can invoke the services from other service providers forming a composite service. Thus, the global trust of a composite service should be evaluated based on both the subjective probability property of trust and complex invocation structures. In this paper, we first interpret the trust dependency caused by direct service invocations as conditional probability. Then, on the basis of trust dependency, we propose a Subjective probability based deductive (SELECTIVE) approach to evaluate the subjective global trustworthiness of a composite service. All these processes follow subjective probability theory and keep the subjective probability property of trust in evaluations. Our experimental results demonstrate that when compared with existing approaches our proposed SELECTIVE approach can yield more reasonable results."
249,"Since current heterogeneous Intrusion Detection Systems (IDSs) have not been designed to work in a cooperative manner, sharing security information among them poses a serious challenge especially in large-scale High Speed Networks (HSN) environment. The integration become more difficult when we should reduce computing and memory costs incurred by the high speed IDSs communication. Fortunately Web Services technology represents a good choice for IDSs integration thanks to its characteristics such as platform transparency and loose coupling. In this context, this paper presents a lightweight RESTful Communication model for coordinating different high speed distributed IDSs. Experimental results show an important gain in terms of data exchanged size and transmission time."
250,"Accurate estimation of quality of online services is both an important and difficult problem, since a service has many interdependent quality attributes influenced by several contextual factors. It is even more challenging as quality ratings come from sources with unknown reliability, each source may rate a service on different quality aspects. Although several solutions have been proposed, there is little work addressing all these issues thoroughly. In this paper, we show that domain knowledge on service structure and related constraints, such as causal dependencies among quality attributes and contextual factors, while widely available, can be exploited to effectively address the above issues in a theoretically-sound framework. Theoretical analysis shows that computational cost of the approach is acceptable, and accurate evaluation of service quality requires a reasonable number of user feedback, provided services have a small number of quality attributes and contextual factors."
251,"Process knowledge, such as tasks involved in a process and the control flow and data flow among tasks, is critical for designing business processes. Such process knowledge enables service composition which integrates different services to implement business processes. In the current state of practice, business processes are primarily designed by experienced business analysts who have extensive process knowledge. It is challenging for novice business analysts and non-professional end-users to identify a complete set of services to orchestrate a well-defined business process due to the lack of process knowledge. In this paper, we propose an approach to extract process knowledge from existing commercial applications on the Web. Our approach uses a Web search engine to find websites containing process knowledge on the Internet. By analyzing the content and the structure of relevant websites, we extract the process knowledge from various websites and merge the process knowledge to generate an integrated ontology with rich process knowledge. We conduct a case study to compare our approach with a tool that extracts ontologies from textual sources. The result of the case study shows that our approach can extract process knowledge from online applications with higher precision and recall comparing to the ontology learning tool."
252,"Service-Oriented Computing is a paradigm that uses services as building blocks for building distributed applications. The primary motivation for orchestrating services in the cloud used to be distributed business processes, which drove the standardization of the Business Process Execution Language (BPEL) and its central notion that a service is a business process. In recent years, there has been a transition towards other motivations for orchestrating services in the cloud, e.g., XaaS, RMAD. Although it is theoretically possible to make all of those services into WSDL/SOAP services, it would be too complicated and costly for industry adoption. Therefore, the central notion that a service is a business process is too restrictive. Instead, we view a service as a technology neutral, loosely coupled, location transparent procedure. With these ideas in mind, we introduce a new approach to services orchestration: Ozy, a general orchestration container. We define this new approach in terms of existing technology, and we show that the Ozy container relaxes many traditional constraints and allows for simpler, more feature-rich applications."
253,"A novel benchmark, WSBen, for testing Web services discovery and composition is presented. WSBen includes: (1) a collection of synthetic Web services (WSDL) files with diverse characteristics and sizes; (2) test discovery and composition queries and solutions; and (3) external files for statistical analysis and AI planners. Users can fine-tune the generated WSDL files using various parameters such as skewness or matching type. It is our hope that WSBen provides useful insights for researchers evaluating the performance of Web services discovery and composition algorithms and software"
254,"OAuth is an open security standard that enables users to provide specific and time bound rights to an application to access protected user resources, stored on some external resource server, without needing them to share their credentials, with the application. Using OAuth, a client application gets one access token for further use through an HTTP redirect response from the resource server once the user authenticates the resource access. Unlike websites, for locally installed packaged web applications the main security challenge is to handle the redirect response appropriately. This paper proposes a novel method to execute OAuth flow from such applications with the help of web runtime framework that manages the life cycle of these applications. We compare our approach with other two approaches for OAuth flow handling proposed in the literature. Experimenting with different categories of packaged web applications, we found our approach blocking all illegal OAuth flow executions. Our approach also gives better OAuth response handling time and power consumption performance."
255,"Substantial savings from asset reuse result when the right assets are identified in the very early stages of a client engagement. Unfortunately, advanced identification approaches (known by having high precision and recall, such as behavior-based approaches) cannot be adopted in these early stages, because at these early stages, there is no many details nor much understanding about the client functional requirements. On the other hand, unstructured keyword-based identification approaches are known of having low precision and recall. To overcome this problem, we argue that assets descriptions should have explicit information about the business activities realized by the assets. To be able to capture this information in a machine understandable format, this paper proposes a model for describing reusable assets functional scopes using component business maps (CBMs), in which the asset scope is represented as a hierarchy of CBM elements. Adopting this scope model, the paper proposes a rapid identification approach for reusable assets that retrieves assets based on their CBM projections. We believe the proposed approach provides better precision and recall when compared to unstructured keyword-based approaches."
256,"As the World Wide Web continues to grow unbounded, users expect intelligent processing and accurate coverage of all its domains. To allow for the same, we present a novel approach to identify and extract key information from web pages with commendable accuracy. We extract important information such as the Title, Main Image, Description, Keywords and FavIcon from a webpage where available, using only the HTML responses without any explicit webpage rendering. The algorithm was modelled to be fast without compromising on its accuracy, is fully automatic, language independent and runs without any human supervision or training. We test our algorithm extensively on over one hundred thousand webpages and successfully extract the key information for 97% of them with an impressive average extraction time of less than 500 milliseconds per webpage."
258,"A service ecosystem consists of services and their compositions (i.e., mashups) and evolves as a complex network system. It is driven by continuously emerged new services and the mashups of old services and new ones. Complex network analysis can be a powerful tool to study the static structure as well as the evolution of a service ecosystem. This paper presents a methodology to study such a system and an empirical study of Programmable Web. To the best of our knowledge, Programmable Web is the largest and most active Web APIs and mashups collection and consists of 4337 services and 6092 service compositions by Nov-2011. We conduct a comprehensive network analysis to quantitatively characterize the static structure and dynamic evolution of the ecosystem. The findings of this paper not only can help understand the current usage pattern and the evolution trace of the ecosystem, but also are applicable to other Web service systems."
259,"For automatically composing Web services in a correct manner, information about their behaviors (an abstract model) has to be published in a repository. This abstract model must be sufficient to decide whether two, or more, services are compatible (the composition is possible) is possible without including any additional information that can be used to disclose the privacy of these services. The compatibility property is defined by different variants of the well known soundness property on open workflow nets. These properties guarantee the absence of livelocks, deadlocks and other anomalies that can be formulated without domain knowledge. In this paper we address the automatic abstraction of Web services and the checking of their compatibility using their abstract models only. To abstract Web services, we use the symbolic observation graph (SOG) approach that preserves necessary information for service composition and hides private information. We show how the SOG can be adapted and used so that the verification of different variants of compatibility can be performed on the composition of the abstract models (SOGs) of Web services instead of the original composite service."
260,"One of the main ideas of Service-Oriented Computing (SOC) is the delivery of flexibly composable services provided on world-wide markets. For a successful service discovery, service requests have to be matched with the available service offers. However, in a situation in which no service that completely matches the request can be discovered, the customer may tolerate slight discrepancies between request and offer. Some existing fuzzy matching approaches are able to detect such service variants, but they do not allow to explicitly specify which parts of a request are not mandatory. In this paper, we improve an existing service matching approach based on Visual Contracts leveraging our preliminary work of design pattern detection. Thereby, we support explicit specifications of service variants and realize gradual matching results that can be ranked in order to discover the service offer that matches a customer's request best."
261,"We describe a complete and largely automated method for the development of systems from Web services, which comprises the encapsulation of services, as well as their composition, verification and subsequent implementation in a model-driven manner. The paper follows the steps of the method: In a first phase, we import WSDL descriptions automatically as UML~2.x activities and provide them as building blocks, with some optional, manual adaptations. In a second phase, these building blocks can be used to compose an application that orchestrates Web services. The building blocks have behavioral contracts that enable automated, incremental verification based on compositional model checking. We demonstrate the approach by a subscription-based service to receive SMS messages."
262,"This paper presents a methodology and a set of tools for the modelling, validation and testing of Web service composition, conceived and developed within the French national project WebMov. This methodology includes several modelling techniques, based mainly on some variations of Timed Extended Finite State Machines (TEFSM) formalism, which provide a formal model of the BPEL description of Web services composition. These models are used as a reference for the application of different test generation and passive testing techniques for conformance and robustness checking. The whole WebMov methodology is integrated within a dedicated framework, composed by a set of tools that implement the model representation, the test generation and passive testing algorithms. This framework also permits the interaction of these tools to achieve specific modelling and testing activities in a complementary way. A case study based on a real service, a Travel Reservation Web Service, is presented as well as the results of the application of the proposed WebMov methodology and tools."
263,"Automation of web service composition is one of the most interesting challenges facing the semantic web today. Despite approaches which are able to infer partial order on services, data flow (i.e., the way data is exchanged among services) remains implicit and difficult to be inferred and automatically generated. Since web services have been enhanced with formal semantic descriptions, it becomes conceivable to exploit and reason on their semantic links (i.e., semantic matching between their functional output and input parameters) to infer data flow. Our approach has been directed to meet the main challenges facing the latter problem i.e., how to effectively i) guarantee whether a data flow is well-formed and ii) infer data flow between services based on their Description Logics (DL) descriptions. To this end, we apply constructive DL reasoning abduction, contraction and introduce the non standard DL reasoning join to model and infer data flow in compositions. The preliminary evaluation results showed high efficiency and effectiveness of the proposed approach."
264,"Many researchers propose that, not only functional but also non-functional properties, also known as quality of service (QoS), should be taken into consideration when consumers select services. Consumers need to make prediction on quality of unused web services before selecting. Usually, this prediction is based on other consumers' experiences. Being aware of different QoS experiences of consumers, this paper proposes a collaborative filtering based approach to making similarity mining and prediction from consumers' experiences. Experimental results demonstrate that this approach can make significant improvement on the effectiveness of QoS prediction for web services."
265,"Electronic Contract (eContract) has been recognized as a good combination of technical specification and legal documentation for establishing and regulating virtual organizations built for dynamic collaborations. This paper presents a design and implementation of an eContract service with an aim of providing a trusted collaboration platform for collaborators. The implemented service uses Web services technologies to facilitate its collaborators not only to contribute resources in an eContract, but also to negotiate and instantiate them through eContract. This paper describes the interface and protocols for the eContract service. The architecture, interface and protocols designed for the service are demonstrated using an example of providing universal connectivity service for a telepresence application in the context of eResearch domain."
266,"A central element of emerging service oriented architectures (SOA) is the ability to develop new applications by composing enterprise functionality encapsulated in the form of services - whether within a given organization or across multiple ones. Semantic service annotations, including annotations of both functional and non-functional attributes, offer the prospect of facilitating this process and of producing higher quality solutions. A significant body of work in this area has aimed to fully automate this process, while assuming that all services already have rich and accurate annotations. In this article, we argue that this assumption is often unrealistic. Instead, we describe a mixed initiative framework for semantic Web service discovery and composition that aims at flexibly interleaving human decision making and automated functionality in environments where annotations may be incomplete and even inconsistent. An initial version of this framework has been implemented in SAP's guided procedures, a key element of SAP's enterperise service architecture (ESA)"
268,"The similarity-based aggregation of XML documents is a proven method for reducing network traffic. However, when used in conjunction with XML security standards, a lot of pitfalls, but also optimization potentials exist. In this paper, we investigate these issues, showing how to exploit similarity-based aggregation for rapid distribution of digitally signed XML data. Using our own implementation in two different experimental settings, we provide both a thorough evaluation and a security proof for our approach. By this we prove both feasibility and security, and we illustrate how to achieve a network traffic reduction of up to 82% in total."
269,"Modeling and deployment of e-contracts is a challenging task because of the involvement of both technological and business aspects. There are several frameworks and systems available in the literature. Some works mainly deal with the automatic handling of paper contracts and others provide monitoring and enactment of contracts. Because contracts evolve, it is useful to have a system that models and enacts the evolution of e-contracts."
270,"Data Providing Services (DPSs) have the sole purpose of retrieving data from existing sources according to their input parameters while also providing a semantic description of the data they provide using a parametrized view over a domain ontology. A layered model of viewing DPSs is proposed consisting of the data acquisition, syntactic and semantic layers. It is shown that by defining all three layers, a DPS may be generated and managed exclusively by its declarative definition. This will increase the agility and efficiency with which DPSs may be deployed and managed. As a development model, a set of reusable messages are created, these messages are to be semantically annotated using a view over the domain ontology and are syntactically represented such that they may be exported to XML Schema. These messages are used within the DPS definition where their views over the domain ontology are parametrized and the data acquisition layer is defined to acquire data from the source."
271,Geospatial processing often involves complex and complicated geospatial data types. It is extremely inefficient if not infeasible to require scientist users maintain type compatibility of the ports of Web services that are connecting to each other. This study proposes to extend the type checking system of Kepler scientific workflow system to help scientist users composite geospatial Web services more effectively.
272,"A growing number of domains are adopting semantic models as a centralized gateway to heterogeneous data sources, or directly for modeling and managing relevant information. In such contexts, it is crucial to grant access to the semantic model and its data only to the authorized users. In this paper, we present a fine-grained access control model specifically tailored to semantic models. One of the relevant features of the model is the granularity of the resources that can be protected. Access control can be enforced at the level of both the model's concepts and the concepts' instances by means of a query rewriting strategy. The proposed model has been implemented adopting the XACML standard and the SeRQL query language; services exposed by the implementation can be used to trans- paretly integrate authorization into existing systems."
274,"The idea of the future internet of services is to combine several services of numerous service providers to new value-added services or applications. To sell these services on so-called service marketplaces the providers have to ensure a high quality of service execution. But how could a provider of a composed service ensure the quality of the whole application, if single services of other providers fail or do not reach the required level of quality? This article describes an approach for modeling adaptation in web service compositions to ensure a guaranteed quality of service for the whole composite service. A special adaptation mechanism is the rebinding of single services while the process is executed if the services fail or could not reach the needed QoS level. We will present a solution for modeling these rebinding concepts in BPEL processes and the infrastructure to support this adaptation mechanism at run-time."
275,"Negotiation of service level agreements (SLAs) is very important for maintaining quality of service (QoS) of composite Web services-based business processes. The process of negotiation involves specification of negotiation parameters, exchanging offers to conduct the actual negotiation process, and then finally generating the formal SLA if the negotiating parties come to a consensus. We propose a negotiation broker (NB) middleware framework to facilitate automated negotiations of SLAs for Web services in a service oriented architecture (SOA). High level business goals, contexts, preferences, constraints, and values of the negotiation issues are expressed as a policy specification by each of the negotiating parties. The NB maps the policy specifications to low level negotiation strategy models and parameters in order to conduct the negotiation locally as a trusted broker. We present a model and an example of the high level negotiation policy specification. We also present our NB framework including a prototype implementation to illustrate the mapping of the policy to a time-dependent negotiation strategy model."
276,"Nowadays, many engineering studies are conducted to securely exploit depleted oil fields for CO2 Storage.These studies follow complex workflows of data processing services described by geologists. If no explicit semantics is applied to describe these workflows, it is not possible to share them between geologists by reusing existing ones or for composing new ones. The focus of our work is to make the semantics explicit in order to facilitate the geologists daily work. In this article, we first explain how geologists operate today. Then, we enrich such workflows with semantic indexes through ontology based characterizations."
277,"Cloud computing is evolving as a key computing platform for sharing resources that include infrastructures, software, applications, and business processes. Virtualization is a core technology for enabling cloud resource sharing. However, most existing cloud computing platforms have not formally adopted the service-oriented architecture (SOA) that would make them more flexible, extensible, and reusable. By bridging the power of SOA and virtualization in the context of cloud computing ecosystem, this paper presents seven architectural principles and derives ten interconnected architectural modules to form a reusable and customizable cloud computing open architecture (CCOA). Two case studies on infrastructure and business cloud are used to deliver business and practical value of infrastructure and business process provisioning services over the Internet. We also present some potential value-added services of the proposed CCOA to guide strategic planning and other consulting practices of cloud computing."
278,"This article proposes a new architectural principle for LISA (LInked Service Architecture) based on the Linked Data, ROA (Resource-Oriented Architecture), and meta-level service broker. LISA employs a two-layer architecture separating the concerns into meta-level of linking and base level of provisioning of services. A meta-level broker can dynamically coordinate services, and make the provisioning at the base level truly decentralized."
279,"By employing Web services technology, the grid system is evolving to be more manageable service infrastructure, including lifetime management, discovery of characteristics, and notifications. Reliable messaging is one of the key issues addressed for quality of services in Web services. In this paper, we propose a reliable message scheme designed for mobile environments in the context of a Web services architecture; Web services-Wireless Reliable Messaging (WS-WRM). We also consider the federation issues with emerging specifications proposed by leading Web services standard groups. We eventually intend to extend the reliability to mobile end-nodes in a more efficient way. In this paper, we address the design issues and describe the detailed scheme of messaging architecture."
280,"Service computing is a popular development paradigm in information technology. The functional properties of Web services assure correct functionality of cloud applications, while the nonfunctional properties such as reliability might significantly influence the user-perceived availability evaluation. Reliability rankings provide valuable information for making optimal cloud service selection from a set of functionally-equivalent candidate services. There existed several approaches that can conduct reliability ranking prediction for Web services. Those approaches acquire different rankings with different preference functions. It is arduous to determine whether there exists the best one in them, and what is the best one if not. This paper proposes a learning approach to reliability ranking prediction for Web services which utilizes past service invocation logs to train preference function. To validate the proposed approach, large-scale experiments are conducted based on a real-world Web service dataset, WSDream. The results show that our proposed approach achieves higher prediction accuracy than the existing approaches."
281,"To automate the analysis of service descriptions, [?] proposed a simple and expressive business protocol model (the specification of possible message exchange sequences) based on state machines, supporting rich timing constraints. Furthermore, developers of client applications need to be aware not only of functional aspects but also of non-functional aspects including privacy. In fact, the major concerns of a client are the disclosure of its personnel data conveyed during the message exchange. The aim of the paper is to study the ability of business protocol to handle the privacy and its time-related properties."
282,"The tutorial aims at providing a deep comprehension of the Web service composition problem and automated techniques to tackle it. Web service composition is currently one the most hyped and addressed issue in the service oriented computing. Starting from an analysis of current technologies and standards for Web service composition, the tutorial will lead the attendees to consider formal models at the base of current proposals, and techniques that can be fruitfully considered to address automatic composition synthesis in each of them. More in detail, attendees will consider: (i) basic technologies and standards for Web service invocation and description (SOAP, UDDI, WSDL,...); (ii) advanced technologies and standards for orchestration and inter-organizational process enactment, in particular WS-BPEL and WS-CDL; (iii) models for Web service composition; (iv) formal tools for both data-centric and process-centric synthesis, including query reformulation a'la data integration, transition-systems based formalisms, trace-based formalisms, logics of programs and processes. In particular, we will show how these formal tools can be applied for automatic Web service composition; (v) current state-of-the-art research results in automatic service composition, drawing a comparison and defining a unifying framework"
283,"With REST becoming a popular paradigm for Web services, more and more use cases are applied to it, including some that require transactional guarantees. We propose a RESTful transaction model that satisfies both the constraints of transactions as well as those of the REST architectural style. We provide formal proof of consistency and recoverability in the proposed framework and show the robustness of its properties in the presence of concurrent transactions."
284,"As software consumption is shifting to mobile platforms, enterprises are looking for efficient ways to reuse their existing legacy systems by exposing their functionalities as services. Mining services from legacy code is therefore an important problem for the enterprises. In this paper we present a technique for mining service candidates from the database applications. Central to our mining technique is the specification and identification of data-access patterns which specify how a program interacts with the databases. In addition to finding service candidates which are internal functions in the source code, we also provide an algorithm to expose the function as a stateless service by generating a wrapper function around the internal function. We demonstrate the effectiveness of our technique on two open source applications and twelve industrial applications."
285,"Selecting the optimal service from a set of functionally equivalent services is non-trivial. Previous research has addressed this issue making use of Quality of Service (QoS) attributes of the candidate services. In doing this, researchers have however assumed that the customers' preference of the various QoS attributes varies linearly with the actual attribute values. In this work, we put forward a technique that overcomes this restriction and compares functionally equivalent services on the basis of the customers' perception of the QoS attributes rather than the actual attribute values. We utilize the 'mid-level splitting' method to track the customer's preference vis-a-vis the actual attribute values. Further, we utilize the 'Hypothetical Equivalents and In equivalents Method' to assign weights, reflecting the importance, to the attributes on the basis of the customer preference. The whole procedure is demonstrated using a simple running example."
286,"As in linguistics the semantics co-exist with pragmatics to provide complete and unambiguous meaning to utterances, in the same way the pragmatics should augment the semantics in intelligent applications. Service-oriented computing is an area where such intelligent applications are greatly facilitated. In this paper we propose a pragmatic methodology to Web service discovery which utilizes both the pragmatics and the semantics. This methodology aims to solve a very basic problem of existing semantic discovery approaches: the inability of selecting the most appropriate service among many semantically equivalent Web services."
287,"Service-oriented computing has emerged as a new component-based software development paradigm in a network-centric environment. By using a standard description language and protocol, services can be used to wrap legacy software systems to be integrated beyond the enterprise boundary across heterogeneous platforms. Nevertheless, the challenges come in tandem with the opportunities because of the inherent dynamic characteristics within a distributed environment. In particular, there is a need for dynamic adaptation for provisioned services to accommodate the ever-changing business requirements externally as well as the computing resource status internally, while maintaining the continuousness of service provisioning. We present a dynamic Web service provisioning approach based on .NET common language runtime, one of the two primary Web services platforms, exploring the runtime code manipulation at the intermediate language (IL) level rather than at the source code level. Meanwhile, we show how the service provisioning can be adapted in a modularized way by complementing the conventional service-oriented architecture (SOA) with a repository of adaptation aspects. Moreover, we demonstrate how dynamic service provisioning can be used for nonfunctional property assurance."
288,"There is a growing demand for provisioning of different levels of quality of service (QoS) on scalable Web servers to meet changing resource availability and satisfy different client requirements. The proportional differentiation model is getting momentum because of its fairness and differentiation predictability. It states that QoS of different traffic classes should be kept proportional to their pre-specified differentiation parameters, independent of the class loads. In this paper, we present a processing rate allocation scheme for providing proportional response time differentiation on Web servers. A challenging issue is how to achieve processing rates for different request classes in the implementation. We propose a process allocation strategy, which dynamically and adaptively changes the number of processes allocated for handling different request classes while ensuring the ratios of process allocation specified by the processing rate allocation scheme. We implement the process allocation strategy at application level on Apache Web servers. Experimental results show that the processing rate can be achieved by the adaptive process allocation strategy and the Web servers can provide predictable and controllable proportional response time differentiation."
289,"Personalization or privacy control? is a question one frequently asks himself/herself nowadays with the wide spread of service provision for users on the move. The right to protect intimacy or the right of privacy is a permanent and genuine right of any person. Context-awareness, offering to users services that react proactively to user environment and service conditions, although desirable, breaks into the private sphere. In the Internet world Web Services are usually employed as building blocks in such context-aware applications. In this paper, the reflection of user privacy preferences in the provision of context-aware Web Services is addressed. A user privacy preferences language for context-aware Web Service environments, namely Consumer Privacy Language (CPL), is proposed along with an adaptation mechanism for SOAP messages. The adaptation remains transparent to the end-user, who can profit from Web Services offering both context- and privacy-awareness."
290,"In today's service computing environments, user needs and expectations are constantly changing. New services emerge while old ones become obsolete and need to be replaced. In such settings, composite services need to be adaptive to changes in user requirements and the environment. This paper proposes a conceptual framework for modeling compositional adaptation for services founded on a requirements monitoring facility. This facility helps maintain adherence between user requirements changes and the dynamics of service composition structure and quality attributes. Specifically, user requirements are represented as goals and soft goals, service composition structure is represented with a CSP-like grammar, and the adaptation mechanism is based on AI planning. The proposed approach is evaluated in a service simulation environment of real-world supply-chain adaptation scenarios."
291,"In the process of web service composition, the check and prevention of semantic incompatibility is one of the most important issues. In this paper, a controlled Petri net (CtlPN)-based model for web service composition is proposed. Meanwhile, the optimal controller is constructed, such that the appropriate vectors of controllable place and arc are appended in the key transition which can lead to deadlock states. In addition, for the semantic incompatibility case, a policy based on appending optimal controller is presented. It is proved that our policy can be a good solution. Finally, the proposed controller is transformed as the activity of BPEL."
292,"Much effort is being made by the IT industry towards the establishment of a Web services infrastructure and the refinement of its component technologies to enable the sharing of heterogeneous application resources. Traditional roles of the service provider, service requestor and service broker and their interactions are now being improved upon to enable more effective services. The implementation of the Web service broker is currently limited to being an interface to the service repository for service registration, browsing and/or programmatic access. In this work, we have extended the functionality of the Web services broker to include constraint specification and processing, which enables the broker to find a good match between a service provider's capabilities and a service requestor's requirements. This paper presents the extension made to the Web Services Description Language to include constraint specifications in service descriptions and requests, the architecture of a constraint-based broker, the constraint matching technique, some implementation details, and preliminary evaluation results."
294,"In this paper, we explore the use of domain-independent and domain-specific ontologies to find matching service descriptions. The domain-independent relationships are derived using an English thesaurus after tokenization and part-of-speech tagging. The domain-specific ontological similarity is derived by an inference on the semantic annotations associated with Web service descriptions. Matches due to the two cues are combined to determine an overall semantic similarity score. By combining multiple cues, we show that better relevancy results can be obtained for service matches from a large repository, than could be obtained using any one cue alone."
295,"The way Internet is used changes: demand grows for critical services that cross several provider networks; guaranteeing a required end-to-end quality of service (QoS) across several networks becomes a challenge. Some critical services (e.g. video-conference, VPN etc.) can not be satisfied in a best effort fashion. The use of QoS contracts (service level agreements, SLAs) is effective for management of such services. However, the problem of meeting end-to-end QoS requirement remains: no centralized entity can compute overall QoS for chains of contracts, and a fortiori, such contract chains can not be optimized centrally. Thus, the following problem has to be solved: given an end- to-end QoS request and collections of available SLAs on each participating domain, establish an end-to-end contract committing a chain of providers and giving optimal service under most reliable guarantees available."
297,"As one of the key characteristics of software as a Service (SaaS), multi-tenancy aims to support massive customers by sharing application instances and databases. To achieve the high economies of scale, one of the most issues needing to be solved in the real industry is that, given a fixed number of nodes, how to optimally place on-boarding tenants to maximize the total supported number of tenants without violating their SLA requirements. This paper focuses on this problem, which is called On-line Tenant Placement Problem (OTPP). In order to calculate the resource consumption of on-boarding tenants, a novel resource consumption estimation model for multi-tenant pattern is proposed in this paper. Based on this model, we explore the complexity of OTPP. A robust heuristic is proposed for the OTPP. The simulation experimental results show the high effectiveness and the good efficiency of our algorithm."
298,"On a crowdsourcing platform consisting of requestersand workers, it is a challenge to recommend suitableworkers for a human intelligence task (HIT) published by a requester. A suitable worker is the one who has a high probability of submitting a correct answer for the published HIT. However, there are four problems that make the existing methods be less effective in recommending suitable workers. First, on most of crowdsourcing platforms, the great majority of workers have good reputations and thus are regarded as homogenous workers who have equal opportunities to be recommended. Secondly, dishonest workers may gain recommendations by counterfeiting good reputations and overstating personal skills. Moreover, the classical data sparsity and cold start problems co-exist in crowdsourcing environments. To effectively differentiate homogenous workers, we firstlycalculate a worker's performance in different types of HITspublished by different requesters. Aiming to improve the accuracy of predicting a worker's performance, we propose a metric which separately considers two requesters' similarities in transacting with the common workers they trust and in transacting with the common workers they distrust. Afterwards, targeting dishonest behaviours, we propose a transaction-based trust model. Targeting the data sparsity problem, we propose a new trust sub-network extraction algorithm (TSE) to discover more requesters who can provide trustworthy opinions for generating recommendations. Furthermore, we propose two strategiesfor solving the cold start problem. Finally, by incorporatingthe similarity metric, the new trust model, the new trustsub-network extraction algorithm and the new strategies, wepropose a novel trust-aware worker recommendation methodCrowdRec. The experimental results illustrate that CrowdRecsignificantly outperforms CF and three state-of-the-art trustbased recommendation methods both in terms of accuracy and coverage."
299,"The next generation intelligent devices need to grow and evolve with the user. Mobile user framework solution allows us to move towards this goal. In this paper, we describe a service framework that captures user's interest and intent which is mined through latent analysis of content. Applications using such a framework can provide adaptive and customized services to the user. We provide details of four demonstrator applications that use this framework, utilizing user interest and intent information to provision targeted advertisements, recommend products and songs. We provide details of our evaluation with subjective and objective analysis of the system."
300,"In composite web services one can only either hide the identities of the participants or provide end-to-end confidentiality via encryption. For a designer of inter-organizational business processes this implies that she either needs to reveal her suppliers or force her customers to reveal their information. In this paper we present a solution to the encrypted data modification problem and reconciliate this apparent conflict. Using a generic sender-transformer-recipient example scenario, we illustrate the steps required for applying XML transformations to encrypted data, present the cryptographic building blocks, and give an outlook on advantages and weaknesses of the proposed encryption scheme. The transformer is then able to offer composite services without itself learning the content of the messages."
301,"We present a new approach for checking the compatibility of policy descriptions. At present, policies are widely used for explicitly expressing non-functional properties, capabilities, constraints and requirements of Web services. Policies are crucial in the negotiation phase of service discovery and selection. Typically, a potential service consumer has its own policy that specifies the conditions the service has to fulfill. Ideally, an automatic negotiation process identifies a mutually agreeable policy for both the Web service consumer and provider. WS-Policy defines policy intersection as a ""first approximation"" for determining the compatibility of policies. However, policy intersection has a major weakness: It is a purely syntactic approach neglecting the semantics of the involved policy assertions. In addition, it is unspecific on how to include domain-specific processing. In this paper we present a new solution that overcomes these deficits by introducing an entailment relation that reflects the semantics of assertions and policies. This paper not only discusses the formal foundations but also introduces the required algorithms such as ""semantic policy differencing""."
302,"Sustainable success of service oriented applications relies on capabilities to manage possible service failures. To substitute a failed service with some other equivalent service is unavoidable in recovering a suspended application due to failure of a constituent service. In this paper, we report a rule based approach to Web service substitution in order to secure availability of services. Availability provides delivery assurance for each Web service so that Simple Object Access Protocol (SOAP) messages cannot be lost undetectably, especially in a Web service composition. The rules are written in Semantic Web Rule Language. The rules are a formal representation of a categorization-based scheme to identify exchangeable Web services. This scheme not only tackles the issue of heterogeneity of domain ontology in describing the Web services, it also adapts itself by learning newly discovered ontology instances. A technical framework of Web service substitution using rule based deduction is demonstrated. Experiments on service substitution based on the proposed framework achieve a best precision of 85%."
303,"Web service selection based on quality of service (QoS) has been a research focus in an environment where many similar web services exist. Current methods of service selection usually focus on a single service request at a time and the selection of a service with the best QoS at the user's own discretion. The selection does not consider multiple requests for the same functional web services. Usually, there are multiple service requests for the same functional web service in practice. In such situations, conflicts occur when too many requesters select the same best web service. This paper aims at solving these conflicts and developing a global optimal service selection method for multiple related service requesters, thereby optimizing service resources and improving performance of the system. It uses Euclidean distance with weights to measure degree of matching of services based on QoS. A 0-1 integral programming model for maximizing the sum of matching degree is created and consequently, a global optimal service selection algorithm is developed. The model, together with a universal and feasible optimal service selection algorithm, is implemented for global optimal service selection for multiple requesters (GOSSMR). Furthermore, to enhance its efficiency, Skyline GOSSMR is proposed. Time complexity of the algorithms is analyzed. We evaluate performance of the algorithms and the system through simulations. The simulation results demonstrate that they are more effective than existing ones."
305,"The growing need for an integrated view of scientific information from different sources has led to the need for scientific information integration, and on the other hand, SOA is one most prevailing technology for its advantages on solving integration problems. In this paper, we argue that the deployment of SOA in an organization should be business domain-specific, and propose an approach called CAFISE-S, which introduces SOA into scientific information integration from a business view-aspect. Business service is put forward as basic elements in CAFISE-S to model business context and IT services coherently in a semantic way. Based on business service, CAFISE-S provides a business domain-specific modeling method for specification of information services, and then supports business-oriented publication, management and usage of information services. The implementation of CAFISE-S platform and an application of CAFISE-S in a real-world project of scientific information integration are also presented in this paper."
306,Provides an abstract of the keynote presentation and may include a brief professional biography of the presenter. The complete presentation was not made available for publication as part of the conference proceedings.
307,"With the popularity of cloud computing and micro service architectures, various service ecosystems including services, venders, and service-based processes continuously emerge on Internet or in an enterprise. Semantics of services from different venders may be described by distributed domain ontologies. Distributed knowledge brings difficulty to competition and cooperation among services, and hampers the evolution of a service ecosystem. In this paper, we propose a distributed knowledge based evolution model (DKEM) to promote competition and cooperation among services from different venders. DKEM considers stability as key factor in competition, and a stability evaluation model is designed to compute stability of services, venders, and service-based processes according to service invocation histories. Based on the evaluation model, two evolution patterns are given, and they can automatically explore new and more stable cooperation among services by means of runtime self-adaption mechanism. A prototype system for DKEM is implemented and a series of experiments show that DKEM is effective for competition and cooperation among services with distributed knowledge, and, evolved processes have higher stability and response efficiency."
308,This paper reports on our experiences with combining Heart of Gold and Language Grid technology to provide more language resources available on Web. Heart of Gold is known as middleware architecture for integrating deep and shallow natural language processing components. The language grid is an infrastructure built on top of the Internet to provide distributed language services. Having Heart of Gold available as Web services in the language grid environment would contribute to interoperability among language services.
309,"Though 20 years have passed since the birth of CSCW, the original goal of it is not reached as well as people expected. This situation is mostly due to the supporting technology especially the infrastructure. Today, great changes have taken place in technology, including grid computing and Web services. These technologies, we think, significantly affect the application of CSCW. In this paper, a framework called CoFrame is proposed to answer the challenges faced by CSCW. Based on the emerging grid and Web service technologies, CoFrame provides some general yet flexible cooperation related services and organizes them into different layers. The elaborately designed services and architecture make CoFrame adaptive to diverse requirements of different domains. The paper details the framework and demonstrates its application with a case study in e-learning."
310,"Pervasive computing environments augment physical spaces with a large number of devices and services that help users perform different kinds of tasks. Users in these environments interact with one or more Web services using various devices to achieve their goals. One of the problems in these environments is discovering and coordinating different Web services for achieving the user's goals. Users may not be aware of which services and devices are available in an unfamiliar environment and how to interact with them in order to achieve their goals. In order to simplify a user's interaction with the environment, we present a novel approach of modeling and managing a user's interaction with the environment based on workflows. We have built a prototype, using the popular business workflow language (BPEL) that models various processes in pervasive environments as workflows. We found that this approach improves the usability of these environments. It also increases flexibility in changing the model of interaction without having to touch individual services and applications. This approach is particularly useful in helping visitors in public spaces like malls, museums, supermarkets and hospitals."
311,"Recently, there has been a growing interest in web service composition and the related security issues. In this paper, we propose a framework for the decentralized execution of composite web services capable to ensure the correctness as well as the security of the execution. Our framework relies on a data structure, called container, which is passed among the web services participating in the composition. The container is encrypted and authenticated in such a way to ensure the correctness of the execution flow as well as a set of relevant security requirements."
312,"In this work, we present a framework for the semantic composition of web services based on Statecharts and uniform community service descriptions. Our model is a two step process. In the first step, we derive the execution model of the user's query. The execution model is specified in Statecharts formalism; whereas the user's query is described in OWL-S. Therefore, a mapping from Statecharts formalism to OWL-S is developed. In the second step, we instantiate the developed execution model through invocation of available e-services instances. Hence, and a result, we obtain an execution plan (said also strategy) satisfying user constraints. The key features of the proposed framework could be summarized as follows. First, unlike other existing languages, using OWL-S enables the semantic description of e-services. These semantics are taken into consideration in our composition strategy. Second, the user constraints (or preferences) are taken into account during composition and are expressed as a finite set of logical formulas with the Knowledge Interchange Format (KIF) language."
313,"The specification, design and implementation of web service applications need to address three major aspects: Orchestration of Services, Conversation and Choreography. In distributed computing, abstractions such as scripts have been used to abstract patterns of communication hiding low level details. In this paper, we demonstrate an approach of integrating orchestration with scripting to depict a pattern of communication or conversations among various agents."
314,"caGrid has accumulated a repository of biomedical services, however, how a cancer researcher can find proper services in the caGrid when needed remains a big challenge. This research aims to enhance the cyber infrastructure of caGrid, by developing a mechanism that turns caGrid services into semantic-aware interoperable services. We proposed a service semantics model, and developed a technique that automatically extracts semantic metadata from static WSDL service descriptions. Such semantic information is stored as loosely coupled annotations that can be queried using semantic Web techniques, to enhance services discovery and composition. We also proposed a two-phase discovery technique that helps users quickly identify interested service operations. This paper also reports our examinations over available techniques and recommends a feasible infrastructure for biomedical service reuse. A prototyping system is developed as a proof of concept."
315,"Pervasive Information System (PIS) represents a new generation of Information Systems (IS) available anytime, anywhere in a pervasive environment. In this paper, we propose to enhance PIS transparency and efficiency through a context-aware intentional service prediction approach. This approach allows anticipating user's future needs, offering and recommending him the most suitable service in a transparent and discrete way. We detail in this paper our service prediction mechanism and present encouraging experimental results demonstrating our proposition."
316,"Quality-aware service composition starts from an abstract workflow. The tasks of the workflow are associated with functional types for which concrete services can be retrieved from a registry. Abstract tasks have to be mapped to concrete services before the workflow is executed. The goal is to maximize the workflow quality by choosing the right combination of services. Spending more time in discovery and composition will increase the quality of the resulting workflow. Restricted resources motivate however the question about the optimal tradeoff between composition effort and solution quality. In this paper, we aggregate the three phases discovery, composition, and execution into a common cost metric. We motivate why this cost metric may dynamically change depending on the system state and the properties of the workflow at hand. We present and analyze an iterative algorithm that automatically balances the effort spent in different phases. We are able to prove a near-optimal number of iterations. Additionally, we provide extensive experimental evaluations showing that our algorithm significantly outperforms static approaches in dynamic scenarios."
317,"This paper proposes a conceptual model for modeling static and dynamic event sources by reusing the information in WSDL. A set-theoretic semantics for event subscription is introduced, based on which two metrics, recall and precision, are proposed to measure the accuracy of event subscriptions. We discuss the accuracy of several event subscription strategies under the framework of Web service event subscription standard proposals (WS-Eventing and WS-Notification). Four major types of event broker design patterns are discussed based on two visibility/reachability factors: if sink knows the source and if source can deliver events directly to the sink. The implication on the broker state and message routing is studied in this analysis. A prototype implementation indicated that these design patterns are feasible"
318,"Service cloud provides added value to customers by allowing them to compose services from multiple providers. Most existing web service security models focus on the protection of individual web services. When multiple services from different domains are composed together, it is critical to ensure the proper information flow on the chain of services. In a service chain, each service needs to determine whether the sensitive information can be directly or indirectly disseminated to the subsequent services. Also, each service in the chain needs to decide whether to accept the data passed to it directly or indirectly from prior services. Moreover, the input data that service si receives from si-1, si. InF, may cause certain side effects inside si, such as updating si's backend database using data computed from si. InF. Service si may wish to allow such side effects in one situation while reject some side effects in another situation. All these decisions should be made based on the service's information flow control policies. To achieve fine-grained information flow control, it is also necessary to analyze the flow and processing of the data and derive the dependencies between the data dynamically generated or used in a service chain. In this paper, we develop a run-time information flow control model for service cloud. First, we develop a run-time dependency analysis mechanism which enables each service in the service chain to determine the correlation between the locally accessed data and the data dynamically generated by the services in the service chain. Then, we develop a model to enable each service in a service chain to specify policies on how its sensitive information can be released to its subsequent services and what types of input data from prior services can be accepted and how they can flow within the services. Finally, we design a run-time protocol to enforce these policies in a service chain."
319,"This paper proposes a hybrid concurrency control scheme for transactional composite services. The scheme uses the information gathered from the workflow specifications of the composite services to reduce the overhead in detecting cycles in the serialization graph. The scheme carries out runtime analysis of the SQL statements used by the composite services to determine whether the clients that execute composite services depend on each other more accurately. As a result, it reduces the response time to some users. The proposed scheme also tackles the repeated rollback problem facing many concurrency control schemes."
321,"Competitive pressures on organizations force a need to rapidly build relationships with newer businesses. As desperate organizations begin to share business processes and solve each others problems, there is a need to share sensitive information. Though standards like WS- * attempt to offer security standards, they do not offer a mechanism to automatically negotiate trust levels in a federation of systems. This inability adversely affects the time to market. Automatic trust negotiation is a process oriented approach to classify information, assign trust levels to cooperating businesses, upgrading or downgrading trust levels based on the qualities of the relationship. This paper details a case study delivered to a large airline business. This case study demonstrates a service oriented competitive decision making system which uses intelligent agents not only to negotiate based on business strategies but also the appropriate trust levels."
322,"With the popularity of social network and the increasing number of Web Services, making individual service recommendation has been a hot research spot nowadays. In this paper, we present a service recommendation algorithm named as URPC-Rec (User Relationships &amp; Preferences Clustering and Recommendation), which first clusters users based on their history behaviors such as the services they ever invoked, and then makes personalized recommendations for users considering both the clustering results and user basic information and relationships, such as gender, age, occupation, preference tags, etc. The case study indicates that URPC-Rec can effectively reduce the dimensionality of sparse matrix, and partially solve the cold-start problem of recommendation systems. The comprehensive experiment shows that URPC-Rec algorithm with user relationships and references has better recommending result than the one without user information and the collaborative filtering approach."
323,"Service-oriented computing is gaining wider acceptance. For Web services to become practical, an infrastructure needs to be supported that allows users and applications to discover, deploy, compose and synthesize services automatically. For this automation to be effective, formal semantic descriptions of Web services should be available. In this paper we formally define the Web service discovery and composition problem and present an approach for automatic service discovery and composition based on semantic description of Web services. We also report on an implementation of a semantics-based automated service discovery and composition engine that we have developed. This engine employs a multi-step narrowing algorithm and is efficiently implemented using the constraint logic programming technology. The salient features of our engine are its scalability, i.e., its ability to handle very large service repositories, and its extremely efficient processing times for discovery and composition queries. We evaluate our engine for automated discovery and composition on repositories of different sizes and present the results."
325,"In recent years, due to the growth of information on the internet, the number of available Web services has increased. Clustering Web services based on their functional features to different domains have started to play a major role in several service management tasks such as efficient Web service discovery and recommendations. In this paper, we propose a novel ontology-based approach for Web service clustering. Instead of using traditional methods, we focus on the similarity and specificity of terms for ontology generation. The amount of domain-specific information included in a term is used to define the specificity of that term. Specific terms are more powerful than general terms for describing a large amount of domain information. Taking advantage of this, we generate a new ontology, which is then used to calculate similarity by defining new logic-based filters. When the similarity calculation fails, we apply information retrieval-based methods. Based on a comprehensive evaluation that we conducted to measure the performance of our method, our novel clustering approach was shown to be more effective in terms of precision, recall, Fmeasure, purity and entropy than other existing clustering approaches."
326,"Air pollution is becoming a serious issue that threatens everyone's daily life. Accordingly how to measure the local air quality easily and quickly becomes an urgent problem. Sensor-based methods are relatively expensive, and image based air quality measurement is a promising direction as it is often less cost. Meanwhile web service of precise air quality measurement is of great importance as it allows to timely monitor the air pollution and can provide recommendations for decision makers. This paper devises an effective web service to address this challenging problem. Specifically we offer a service letting mobile device users to upload photos taken outdoor with meta information. Once the background web service computation is finished, we return the air quality level at their location to the users. In our service system, it includes three basic modules: 1) using GPS location information to get the basic air quality value from the nearest official air quality station, 2) using photo by our air quality assessment based on dictionary learning for image representation to compute air quality. In this method we add ℓ
<sub>21</sub>
 norm to the target function to promote the nonzero coefficients of the words aggregate within similar quality level, 3) using photo uploaded by user by our CNN based photo air pollution estimation method to obtain the air quality estimation. Finally we combine the above three estimations to reach the final estimation to the end users. Empirical experiments are conducted on the real-world dataset that collaborates the efficacy of our method."
327,"A key impediment to the widespread adoption of Web services is the relatively limited set of tools available to deal with quality-of-service (QoS) factors (Ran, 2003). QoS factors pose several difficult challenges in how they may be articulated. While the functional requirements of a service can be represented as predicates to be satisfied by the target system, QoS factors are effectively statements of objectives to be maximized or minimized. QoS requirements occur naturally as local specifications of preference. Dealing with QoS factors is therefore a multi-objective optimization problem. In effect, these objectives are never fully satisfied, but satisfied to varying degrees. In evaluating alternative design decisions, we need to trade-off varying degrees of satisfaction of potentially mutually contradictory non-functional requirements. One key contribution of this paper is the use of the constraint hierarchies framework from hierarchical constraint logic programming framework in dealing with quality of service (QoS) factors . We show how QoS factors can be formulated as soft constraints and how the machinery associated with constraint hierarchies can be used to evaluate the alternative trade-offs involved in seeking to satisfy a set of QoS factors that might pull in different directions. We apply also this approach to the problem of reasoning about Web service selection and composition, and establish that significant value can be derived from such an exercise"
328,"Web services are software components that can be discovered and employed at runtime using the Internet. Conflicting requirements towards the nature of these services can be identified. From a business perspective, Web services promise to enable the formation of ad-hoc cooperations on a global scale. From a technical perspective, a high degree of standardization and rigorous specifications are required to enable the automated integration of Web services. A suitable technology for Web services has to mediate these needs for flexibility and stability. In this paper a new approach to the description of Web service semantics is introduced. It is a visual approach based on the use of software models and graph transformations and allows for the description of innovative services while providing a precise matching concept. An implementation using current standards and tools is available."
329,"The service technology and crowdsourcing movement have spawned a host of successful efforts that promote the rapid development of the human service ecosystem. In this ecosystem, a large number of globally-distributed freelancers are organized to tackle a range of tasks over the web. These crowdsourcing services provide convenience for civilians with lower price and shorter response time. However, the convenience cannot whitewash many unstable factors that are caused by human involvement, such as undefinable reputation, unstable quality, crowdturfing, and etc. In this paper, we present a comprehensive data-driven investigation of one prominent supply-driven human services marketplace-Fiverr-wherein we analyze freelancers' marketing behaviors and their offering services (called ""gigs""). As part of this investigation, we identify the key features that can be used to evaluate freelancers' influence and develop a GSRC (Gig service property + Seller Impact + Customer Review + Semantic Content) model to predict gig service quality. As far as we know, this is the first attempt that involves the service semantic info in the prediction model and integrates all these four aspect factors."
330,"Many modern enterprises are employing serviceoriented systems to execute their transactions. These systems generate an abundance of events, which can be analyzed to diagnose and improve business processes. However, the events are distributed over different data sources, as service-oriented systems are often implemented asWeb services involving different departments in an organization. They need to be consolidated in a process view since it makes no sense to analyze individual events. Existing approaches depend on an explicit case notion to correlate events. They work well on data from process-aware systems (e.g., BPM/WFM systems) which record events explicitly and each event is attached with a case id (a global identifier) to indicate its related process instance. However, most serviceoriented systems (e.g., ERP and CRM) produce object-centric data (e.g., database tables), which record events implicitly (e.g., through redo logs) and separately without a common case id. In this paper, we propose a novel approach to correlate events by the data perspective (e.g., by a so-called ""object path""). Besides, we define the concepts of correlation patterns and evaluation metrics, which enable users to evaluate and select the best way to correlate events based on their needs."
331,"Reputation is useful for establishing trust between Web service (WS) providers and WS consumers. In the context of WS composition, a challenging issue of reputation management is to propagate a user's impression of a composite WS (i.e., the user's feedback rating) to its component WSs. In this paper, we propose a Shapley value based approach which can achieve fair impression propagation, that is, the reputation of a component WS is never awarded (or penalized) for the good (or bad) performances of the other peer component WSs in the same composite WS. The fairness of the proposed approach is validated through theoretical analysis and experimental results."
332,"In Database-as-a-Service (DBaaS), a large number of tenants share DBaaS resources (CPU, I/O and Memory). While the DBaaS provider runs DBaaS to ""share"" resources across the entire tenant population to maximize resource utilization and minimize cost, the tenants subscribe to DBaaS at a low price point while still having resources conceptually ""isolated"" according to service level agreements (SLAs). To optimize this dichotomy of goals, we propose a dynamic resource allocation framework that periodically re-allocates resources to tenants to maximize resource utilization while tolerating a low risk of SLA violations. We model the resource allocation problem as a modified unbounded knapsack problem. The model introduces an additional fairness constraint to assign residual resources to active tenants, while avoiding that few tenants consume all residual resources. Performed experiments demonstrate the effectiveness and efficiency of the proposed allocation algorithm for a synthetic workload with burstiness and predicted tenant behavior."
333,"In the era of Big Data, data analysis gives strong competition power to enterprises. As services for Big Data Analysis (BDA) become prevalent, analysis services with intelligence and autonomy using automatic service composition show very bright prospects in the BDA market. Service composition consists of four stages: workflow generation, discovery, selection, and execution. In this paper, we propose a novel service discovery approach that considers two key concerns in the discovery domain towards better quality as well as effective service composition. BDA services are fine grained according to the domain and functional behaviors. The services need a domain context-aware and precision-guided discovery approach. Therefore, we propose domain ontology-based service discovery. It is mainly focused on the BDA domain for precise service discovery considering all behavioral signatures between queries and services. As for the second concern, components in composed services depend greatly on each other in situations such as workflow for data analysis. We show that linking services together considering sociability or user preference gives better discovery performance. We propose a Linked Social Service Network (LSSN) with multiple feature attribute-based service discovery for BDA. Our approach combines two advantages, the precision and sociability of Web services. The experimental results show that both of these methods perform well based on their perspectives, better than previous approaches."
334,"The rapid advance of technologies like Web services enables solutions to exploit more and more of the potential of the Web in many ways, recently especially in the field of federating distributed systems within or between businesses. To sustain maintainability and cope with the evolving application life cycles, a global uniform view on all involved sub systems and underlying infrastructure is required. This can be provided by a model that serves as a map for the landscape of the overall system to act as a guide to evolution. In this paper, we introduce the i2Map as an approach to these modeling demands focused on federable Web-service-based applications."
335,"Like traditional local APIs, web service APIs (web APIs for short) evolve, bringing new and improved functionality as well as incompatibilities. Client programs have to be modified according to these changes in order to use the new APIs. Unlike client programs of a local API, which could continue to use the old API, clients of a web API often do not have the option not to upgrade, since the old version of the API may not be provided as a service anymore. Therefore, migrating clients of web APIs is a more critical task. Research has been done in the evolution of local APIs and different approaches have been proposed to support the migration of client applications. However, in practice, we seldom observe that web API providers release automated tools or services to assist the migration of client applications. In this paper, we report an empirical study on web API evolution to address this issue. We analyzed the evolution of five popular web APIs, in total 256 hanged API elements, and carefully compared our results with existing empirical study on API evolution. Our findings are threefold: 1) We summarize the API changes into 16 change patterns, which provide grounded supports for future research, 2) We identify 6 completely new challenges in migrating web API clients, which do not exist in the migration of local API clients, 3) We also identify several unique characteristics in web API evolution."
336,"In this paper we present a Web services-based platform that facilitates and speeds up the development and deployment of context-aware, integrated mobile speech and data applications. The platform is capable of handling different types of context and offers sophisticated personalization mechanisms. To illustrate the usefulness of the platform and to validate the claim that cross-platform application development, in particular mobile, context-aware applications is easier and faster with Web services technologies, we present a demonstration application. It serves tourists with interesting information and services in their specific context, and contributes to the achievement of their current goals. Finally, we present a number of problems that we experienced in the implementation process as well as the feedback that we received from real users who tested our application."
337,"This paper studies the problem of designing practical web service recommender systems that make quality predictions based on web service invocation histories. We formalize a generic architecture for such systems, that include both an online training module and an off-line training module. In addition, we develop a general online and off-line training algorithms to demonstrate the advantages of such an architecture and the natural fit of stochastic gradient descent algorithms for the architecture. The advantages of our proposed architecture has been confirmed by the comparisons with existing web service recommendation algorithms on a real-life dataset."
338,"This paper presents an architecture to facilitate efficient evaluation and selection of 3rd party Web services for service providers. Most service provider architectures have primarily focused on providing Web service front ends to legacy systems, aggregating and delivering services via workflows. These architectures primarily considered static business contracts between the service provider and its (Web-service enabled) business partners. This approach makes these architectures inflexible to variations in business requirement, partners' performance and customer requirements. Our architecture provides a flexible means for service providers to optimize business performance. Based on the historical performance, extant context, and optimising business rules, the appropriate service is selected and invoked to serve a customer request. We have developed a prototype system ODSS-in-ENDS, that demonstrates this capability."
339,"Service discovery is one of the key problems that have been widely researched in the area of Service Oriented Architecture (SOA) based systems. Web Service clustering is a technique for efficiently facilitating service discovery. Most Web Service clustering approaches are based on suitable semantic similarity distance measure and a threshold. Threshold selection is essentially difficult and often leads to unsatisfactory accuracy. In this paper, we have proposed a self-organizing based clustering algorithm called Taxonomic clustering for taxonomically organizing semantic Web Service advertisements. We have tested the algorithm on both simulation based randomly generated test data and the standard OWL-S TC test data set. We have observed promising results both in terms of accuracy and performance."
340,"Service-oriented architecture (SOA) is a loosely-coupled architecture designed to meet business needs of an organization. It is becoming a trend for system development and integration where systems group functionality around business processes. Although SOA does not require Web services, Web services are based on accepted standards and drive SOA to the mainstream. There are at least two challenges with quality management of SOA based Web service systems. One of them is how to link explicitly its technical capabilities with customerspsila needs to satisfy customerspsila functional and nonfunctional requirements. The second is how to determine targets of Web service technical attributes. The first issue is not addressed at all and the second issue is dealt with subjectively in the current practice of development of SOA based Web service systems. Quality function deployment (QFD) is a major quality management system used to determine product development characteristics from customer requirements. It has found its success in improving quality of complex products, such as automobiles, aircrafts, and consumer electronics, although it has not been used in the development of SOA based Web service systems. In this paper, we analyze a number of quality of Web service requirements and their related technical attributes, and apply the QFD for developing SOA based Web service systems by linking quality of service requirements to Web service design attributes. An impact based linear regression method is used to determine technical targets of design features of SOA based Web service systems for the satisfaction of quality of service requirements."
341,"We present an approach for converged communication services over IP, based on the concept of WSIP (Web service SIP). In our approach, each WSIP node is both a SIP endpoint that communicates in the SIP world through SIP signaling, and a Web service SOAP node that provides a native and generic service integration environment for binding SIP based communication in Web services. The unification of SIP with Web service provides a new converged communication paradigm. It allows dynamic service discovery and binding that integrate SIP as a component in the business transaction. The clear separation of service integration environment and SIP signaling in our approach maintains the simplicity and efficiency of the existing SIP protocol, while leverages the power of both methods in IP based communication services. The ubiquitous service integration nature of Web service provides a disciplined solution to maintain and update large number of SIP endpoints from different installation bases. It enables dynamic monitoring, repairing, and updating SIP UA endpoints, which is essential to the success of VoIP using SIP. The proposed approach was implemented in a prototype research system, and several advantages of WSIP over the existing approaches are observed."
342,"J2EE components and APIs have remained largely unmined, and their potential as performance barometers therefore largely unrealized, because the tools businesses have traditionally wielded to service transactions - database servers, directory services, application servers themselves - have been unable to provide businesses and software developers the access they need to capture, measure, and optimize discrete transactions. Businesses that lack insight into J2EE components and APIs lack the ability to manage some of e-business's most vital performance assets, assets that, when performing poorly or not at all, can down whole distributed applications. This paper discusses the requirements of J2EE application performance management systems and five commercial tools."
343,"Semantic descriptions are able to provide more accurate information on the characteristics of Web services, allowing these to be dynamically discovered without human intervention. Semantic web services can also be automatically composed through the use of discovery mechanisms able to identify inter-related services that can be combined together. This paper presents an approach for automatic discovery and composition of semantic Web services. The proposed approach allows services to be combined when a single service does not satisfy the requirements specified in a discovery request. In order to identify a composition, a mechanism that implements the proposed approach builds a graph of semantically matching services, based on the information annotated on service descriptions using the SAWSDL (Semantic Annotations for WSDL) standard."
344,"With the booming number of web services, it is a challenge for inexperienced developers to select suitable services and make service compositions. Therefore, recommending services based on user queries becomes a necessity. For modeling the queries and services' descriptions, many recent studies are based on LDA (Latent Dirichlet Allocation). However, some previous empirical works indicate that LDA model doesn't gain high accuracy in generating latent presentation which is subject to the restrictive assumption of the Dirichlet-Multinomial distribution. In this paper, we propose a Time-aware Collaborative Poisson Factorization (TCPF) to tackle the problem. TCPF takes Poisson Factorization as the foundation to model mashup queries and service descriptions separately, and incorporate them with the historical usage data together using collective matrix factorization. Experiments on the real-world ProgrammableWeb dataset show that our model outperforms the state-of-the-art methods (e.g., Time-aware collaborative domain regression) by 7.7% in terms of mean average precision, and costs much less time on the sparse, massive and long-tailed data set."
345,Provides a listing of current society officers.
346,"Prediction of Quality of Service (QoS) values plays an important role in service selection, discovery, and recommendation. Previous works show that the QoS values would be influenced by the location information. However, these researches do not consider the fact that some users may provide untrustworthy QoS values even though they are in the same location region. QoS values from these unreliable users could significantly affect the QoS prediction accuracy. To address this issue, this paper proposes an alternative and efficient approach to predict the missing QoS values, referred as the Location and Reputation aware Matrix Factorization based Location Information (LRMF). LRMF combines both the user's reputation and location information to achieve more accurate prediction results. Experiments are conducted on a real-world Web service QoS dataset, and results show that the proposed method outperforms many other existing QoS prediction methods."
347,"The growing success of WS-related technologies has resulted in a large number of providers, which implement services of varying degree of sophistication and complexity. While on the one hand the availability of a wide array of services has created a competitive and flexible market that suits well the needs of different type of users, on the other hand, it requires them to select among possibly hundreds of similar services. As such, Web service selection plays a crucial role in Web service life-cycle. Here, several application-dependent requirements might constrain the selection of the best service. In this paper, we study the privacy implications caused by the exchange of large amount of potentially sensitive data required by optimized strategies for service-selection. In particular, we propose a comprehensive framework to uniformly protect users' and service providers' privacy needs, at the time of service selection. We define a solution that allows matching of the search criteria against the Web services attributes in a private fashion such that both criteria and service attributes are kept private during the matching. Further, we propose an approach to protect service provisioning rules from unwanted disclosure, both from the user and the service provider's perspective. Our experimental evaluation and complexity analysis demonstrate that our algorithms are efficient."
348,"In this tutorial, we present the results of abstracting a reference architecture for SOA based on multiple projects during the past 6 years. As an example SOA Solution Reference Architecture, the SOA Solution Stack (S3) defines the layers, architectural building blocks, design decisions, patterns, options and architectural decisions and the separation of concerns needed to model, architect, assemble, deploy and manage an end-to-end solution in the context of a service-oriented approach. The SOA Solution Stack (a.k.a. Service-oriented Solution Stack) provides a blue print for an enterprise or application architecture scope. The SOA Solution Stack is based on establishing the building blocks of SOA: services, components and flows that collectively support business processes and goals. The meta-data underlying each layer and relationship between layers can further facilitate SOA in bridging the gap between business and IT from solution modeling to solution realization."
349,"Two main goals of our e-mail client system are filtering out junk e-mails and blocking e-mail sent out by malicious code. The famous Ling-Spam Corpus was experimented to show the good performance of our system. Moreover, our system can prevent malicious code impersonating SMTP to send out infected e-mail or secrecy of the computer system."
350,"In this paper, we propose an alternative approach for verifying a conformance between choreography and the black box implementation of stateful Web service whose only external behaviors can be observed. Our framework uses an adapted version of Angluin's algorithm to infer a Mealy machine model that represents the observable behaviors of the implemented Web service. By transforming the Mealy machine to the modeling formalism LTS, the model checker LTSA can be used for checking a trace equivalence relation which is the conformance criterion in this work."
351,"Web services technologies are undergoing some significant changes at several levels, ranging from architectural styles (REST vs.. SOAP) to message formats.To cope with these changes, there is a need to design services in an abstract fashion that is less sensitive to these changes so that our technological and economic investments are protected. To address this issue, this paper proposes an abstract modeling framework, called Infoset, both as a design and analysis tool and a message processing architecture. The benefit of Infoset is demonstrated first on some dual Web services that provide both REST and SOAP access. The Infoset analysis modeling reveals the commonality and differences between dual Web services, making it feasible to design abstract web services. Infoset also leads to a rule based message processing architecture that avoids the problems in current static Object/XML SOAP engines. By dynamically computing the context of Infoset rules in message workflows, dependences between integrated Web services are reduced and the reusability of services logic is increased. A prototype of this architecture is implemented and the preliminary experimental results indicate that the performance is satisfactory."
352,"Web service discovery is a main challenge despite the enhanced proposed methods based on information retrieval techniques (word sense disambiguation, stemming, etc.), domain knowledge and ontology. Unfortunately, the proposed approaches are, however, complex in practice. Despite the addition of extra information to WSDL documents, discovering a required web service looks like finding a needle in a haystack. Thus, we propose in this paper a seamless way to discover a more appropriate web service independently of its type (simple, composed or semantic). In fact, we consider the usage of the service and more particularly, users' requirements, and we invoke it using a generated GUI. This approach concerning a Web service search engine based on popularity is implemented and validated by performing an experiment."
353,"The technology for web services has facilitated composition of a new service by combining existing services. The resulting composite service is required to guarantee Quality of Service (QoS), such as price, in addition to the demanded function. Each composite service faces computationally-expensive service selection by exploring enormous service combinations for optimization of a utility function and satisfaction of global constraints. In addition, each concrete service has difficulties in determining QoS that is profitable while attractive for customers, or composite services. Specifically, information on rival services is generally secret and changeable, which is required for the optimal decision. In response to these problems, this paper proposes an algorithm for service selection using Vickrey auction. In the first phase of the proposed algorithm, the composite service selects some best concrete services through the reverse Vickrey auction, only considering the utility function. In the second phase, the global constraints are satisfied through adjustments based on the Vickrey auction. The proposed algorithm avoids full exploration of services combinations for efficient computation, while encouraging providers to declare their best QoS without caring about extra profit."
354,"Web services have been widely used to support context-aware applications for context information retrieval in a ubiquitous environment. However, most of the existing research efforts in this field only focus on using SOAP web service as an enabling technology. In this paper, we compare REST and SOAP web services for supporting ubiquitous environments. We describe our approaches to deploy an open ubiquitous computing environment using REST style services along with semantic web and mobile technologies. We also discuss an example context-aware application developed for the environment and show how REST style services can contribute in moving ubiquitous computing technologies into the real world."
355,"The Internet has been encouraging and enabling business collaborations via online transactions over the Web. However, managing transactions in a long-running business process across domains still remains a challenge. In this paper, we propose a novel financial-compensation-based transaction management model, fcBTxM, to address this challenge. Unlike classical transaction management, fcBTxM does not attempt to recover data consistency via rollback when a failure occurs. Instead, our model always tries to forward-roll a business process via financial compensation. We use a state machine to capture and describe the states of our transaction model and their relationships. We also develop a set of technologies and protocols for enabling the new transaction management. A real business collaboration example is used to demonstrate the concept, and preliminary testing results are provided to evaluate our technologies."
357,The conference offers a note of thanks and lists its reviewers.
358,"Service selection, where some of the services are accessed indirectly as constituents of composite services, is difficult for the following reasons: (1) the interpretation of service qualities is subjective; (2) evidence must be combined from multiple sources; (3) service profiles change dynamically; and (4) constituent services may be only partially observable behind composite services. We propose an approach where we map service qualities to a common probabilistic trust metric. Whereas current trust approaches estimate the trustworthiness of a composite service based on a fully observable and static setting, we propose a statistical approach built on expectation maximized over a finite mixture model. Our experiments show that our approach can dynamically punish or reward the constituents of composite services while making only partial observations."
359,"Most service composition approaches rely on top down decomposition of a problem and AI-style planning to assemble services into a meaningful whole, impeding reuse and flexibility. In contrast, our approach starts from declarative knowledge about the semantics of individual services and constructs a full-blown orchestration process that supports sequence, choice and parallelism. The approach, which is able to generate OWL-S and WS-BPEL based description, is unique in producing complex compositions out of semantic links between services in a flexible way. It also allows reusing knowledge about semantic dependencies in the network to generate new compositions through new requests and modification of services at run-time."
360,"Sending malicious content to users for obtaining personnel, financial, or intellectual property has become a multi-billion dollar criminal enterprise. This content is primarily presented in the form of emails, social media posts, and phishing websites. User training initiatives seek to minimize the impact of malicious content through improved vigilance. Training works best when tailored to specific user deficiencies. However, tailoring training requires understanding how malicious content victimizes users. In this paper, we link a set of malicious content design factors, in the form of degradations and sophistications, to their potential to form a victimization prediction metric. The design factors examined are developed from an analysis of over 100 pieces of content from email, social media and websites. We conducted an experiment using a sample of the content and a game-based simulation platform to evaluate the efficacy of our victimization prediction metric. The experimental results and their analysis are presented as part of the evaluation."
361,"QoS-aware service composition intends to maximize the global QoS of a composite service while selecting candidate services from different providers with local and global QoS constraints. With more and more candidate services emerging from all over the world, the network delays often greatly impact the performance of the composite service, which are usually not easy to be collected before the composition. One remedy is to predict them for the composition. However, new issues occur in predicting network delay for the composition, including prediction accuracy and on-demand measures to new services, which affect the performance of network-aware composite services. To solve these critical challenges, in this paper, we take advantage of the geographic location information of candidate services. We propose a network-aware QoS (NQoS) model for the composite service. Based on that, we present a novel geographic location-based NQoS prediction approach before composition, and a NQoS re-prediction approach during the execution of the composite service. Extensive experiments are conducted on the real-world dataset collected from PlanetLab. Comparative experiment results reveal our approach facilitates to improve the prediction accuracy and predictability of the NQoS values, and increase global NQoS of the composite service while ensuring its reliability constraints."
362,"We introduce Grid Resources for Industrial Applications (GRIA), a project that aims to enable commercial use of the Grid. GRIA enables service providers to rent out spare CPU cycles, and clients to hire those CPU cycles. Web services play a key role in the architecture of GRIA, chiefly through their interoperability and security features - they provide a well-defined means of limiting clients' access to discrete operations, thus allowing clients to do ""work"" on a remote application server without giving them full shell access to the server. In this paper, we focus on requirements, business processes and security, and interoperability and standardisation issues raised by this work. We also describe some of the lessons learned through experience of designing, implementing and deploying a prototype system, GRIA vI."
363,"Most current practical methodologies and workflow systems for service composition and workflow creation in e-science pursue a semi-automatic way to allow users to discover and select appropriate services to include in a workflow based on semantic and conceptual service definitions. However, few of these approaches consider the potential for reuse: to share the knowledge gained during the service composition process and to reuse complete or partial of existing workflows. We believe that providing a capability for reuse of this knowledge and workflows could be an important component in a workflow system. In this paper, we present a methodology and an enhanced system design to facilitate the reuse of knowledge and workflows. It contains a hierarchical workflow structure representation, knowledge management and knowledge discovery components to capture and manage the reusable information in a system, and an approach for using a graph matching algorithm to discover similar workflows."
364,"Service composition has the capability of constructing coarse-grained solutions by dynamically aggregating a set of services to satisfy complex requirements, but it suffers from dramatic decrease on the efficiency of determining the best composition solution when large scale candidate services are available. Most current approaches look for the optimal composition solution by real-time computation, and the composition efficiency greatly depends on the adopted algorithms. To eliminate such deficiency, this paper proposes a semi-empirical composition approach which incorporates two stages, i.e., periodical clustering and real-time composition. The former partitions the candidate services and historical requirements into clusters based on similarity measurement, and then the probabilistic correspondences between service clusters and requirement clusters are identified by statistical analysis. The latter deals with a new requirement by firstly finding its most similar requirement cluster and the corresponding service clusters by leveraging Bayesian inference, then a set of concrete services are optimally selected from such reduced solution space and constitute the final composition solution. Instead of relying on solely historical data exploration or on pure real-time computation, our approach distinguishes from traditional methods by combining the two perspectives together. Experiments demonstrate the advantages of this approach."
365,"The popularity of Web-based transactions and the need for more sophisticated content distribution methods has helped to fuel the rapid growth of Web Service adoption, specifically, HTTP-bound Web services. Secure and efficient content delivery has long been a requirement of traditional Web-based distribution schemes, and existing Web infrastructure provides numerous options for securing and optimizing HTTP. Two exemplary technologies are SSL/TLS and HTTP compression. While efforts to solidify the more granular WS-Security standards are ongoing, and methods for XML message compression schemes continue to be investigated, HTTP provides an interim solution, supporting transactional security and message compression. The SSL/TLS and HTTP compression technologies have become commoditized and pervasive. And with the trend in content delivery toward hardware offload for these functions, modern data centers have begun to raise the bar for performance. In this paper, we examine three different paradigms for implementing SSL/TLS and HTTP compression: software-based functionality, server-resident hardware accelerators, and centralized network-resident hardware accelerators. We discuss the trade-offs between the two different offload techniques (i.e., PCI accelerator vs. network proxy) and explore their relationship to the current performance activities, in the field of Web services. In analyzing the results for SSL/TLS offload, and the effects of compression, in conjunction with SSL/TLS, we draw parallels with the efforts of WS-Security and XML message compression. Although optimizations for software-based cryptography continues to advance, the potential for hardware-based acceleration should not be overlooked. We discuss our results and address deployment scenarios for optimizing Web-based transactions, and the future optimization of Web Service transactions"
366,"Run-time service discovery and late-binding constitute some of the most challenging issues of service-oriented software engineering. For late-binding to be effective in the case of composite services, a QoS-aware composition mechanism is needed. This means determining the set of services that, once composed, not only will perform the required functionality, but also will best contribute to achieve the level of QoS promised in service level agreements (SLAs). However, QoS-aware composition relies on estimated QoS values and workflow execution paths previously obtained using a monitoring mechanism. At run-time, the actual QoS values may deviate from the estimations, or the execution path may not be the one foreseen. These changes could increase the risk of breaking SLAs and obtaining a poor QoS. Such a risk could be avoided by replanning the service bindings of the workflow slice still to be executed. This paper proposes an approach to trigger and perform composite service replanning during execution. An evaluation has been performed simulating execution and replanning on a set of composite service workflows."
367,"Current Web services standards do not well support automatically discovering and selecting services that can be safely integrated with existing components. Semantic matching of service specifications based on the behaviors of Web services, i.e., the ways a client may interact with the service, can be used to solve the problem. Therefore, the transformation of the OWL-S process model specifying the behaviors of Web services into an extended deterministic finite state automaton (EDFA) is presented in this paper. The automata describe Web services in a more accurate way: the nodes represent states maintained by services; the state transitions labelled by binary-tuples (input, output) rather than letters represent communication activities of services; moreover, the automaton structures describe the temporal sequences of communication activities that represent the behaviors of Web services. The compatibility of Web services can be evaluated with testing the emptiness of the languages accepted by EDFAs"
368,"Web Service composition languages promise a cheap and effective means for application integration over the Internet as in typical B2B interaction scenarios. BPEL is the upcoming standard for Web Service composition and several implementations of it are already available. However, for Web Service composition languages to keep their promises it is essential to provide more support for security. Companies will embrace Web Service composition languages only if their requirements of confidentiality, integrity, authentication, etc. are fulfilled. In this paper, we look at security in Web Services compositions and present a framework for securing BPEL compositions using WS-Security and WS-Policy. The main components of our framework are the process container implemented by a set of aspects in AO4BPEL, an aspect-oriented extension to BPEL, the security service and the deployment descriptor. We also introduce the notion of policy-based process deployment to check the compatibility of the security policies of the composition and its partners at deployment time."
369,"Presence technology conveys the willingness and ability of an entity to communicate and is very useful in developing innovative applications. Protocols and APIs have been proposed for presence, but these APIs are at a low level of abstraction. Web services provide the flexibility to develop loosely coupled applications with coarse-grained interfaces. This paper proposes a novel Web service for presence application development. This Web service is presence protocol independent and its use is illustrated in the paper by a concrete application. We have implemented it as a gateway towards an IETF SIMPLE based presence server, but other implementations are possible. The mapping is discussed and the prototype described. Performance measurements are also made. The results indicate that the additional overhead introduced because of XML based SOAP messages is not prohibitive, although significant."
370,"This paper provides an artifact-pattern-matching framework and mathematical model to analyze the dynamic behaviors of the SOA solution design in model driven fashion and provide recommendations for optimal solution pattern enablement for solution artifacts. The artifact-pattern-matching system can be dynamically tuned based on the practitionerspsila final selections of there commendations. Specifically, we propose a set of solution patterns to guide SOA solution architects through the process of consuming and configuring SOA artifacts for composing SOA solutions. The resulting multi-dimensional cascading flagging method is also presented in this paper. As an example, impact analysis patterns are used as solution patterns to support traceability enablement. We present some future directions of leveraging reinforcement learning algorithms to enrich the design quality analytics of SOA solution."
371,"Immersive Web (IW for short) applications such as Second Life and SimCity are increasingly popular among individual users and companies. However, it is difficult to build a hybrid Immersive Web application, because of the different architectures used for building such IW applications. In this paper, we propose a service-oriented framework to support the mashup of heterogeneous IW applications for hybrid IW applications. We first model each object in IW (e.g., a virtual album or a virtual room) as an IW Object Service (IWOS) located by a unique URL with standardized service interfaces. As such, one IW application can use an IWOS instance in another IW application. Moreover, non-IW applications (e.g., an online album system) can also access an IWOS instance via its service interface. We further propose a service-oriented framework to manipulate these IWOS instances, to support the transition of these objects mainly between different IW applications, and to enable the transition between IW applications and non-IW applications. Our framework also includes the fundamental services to build hybrid IW applications. Finally, we provide examples to demonstrate the advantages of our framework, and conduct a qualitative analysis to show the effectiveness."
372,"As there are various risks of failure when Web Services are deployed in unreliable environment, the execution of a composite service requires the assurance of the transaction mechanism. However, existing QoS-aware composition approaches do not consider the transactional constraints during service selection. We address this issue by considering the combination of transactional and QoS requirements. Firstly, the novel construction and processing rules are proposed to guarantee the atomic consistency of the composite service and the correctness of these rules is proved subsequently. Then, on basis of these rules, an Ant Colony System based service selection algorithm is presented to guarantee the end-to-end QoS constraints on the premise of ensuring the atomic consistency during service selection. Therein, an optimization strategy is suggested to shrink the searching space of the algorithm tremendously. Finally, experimental results show the efficiency and effectiveness of the algorithm and demonstrate further the correctness of the construction and processing rules through simulations."
373,"With the increasing popularity of web services on the Internet, service selection has become an important issue in large-scale web service systems. In recent years, with the growing energy consumption associated with IT systems and services, energy efficiency has drawn extensive attention in service allocation and selection. However, existing approaches for energy efficient web service selection face great challenges, because of the high dynamics and unpredictability of task arrivals, and large scale of current web service systems. In this paper, we present a distributed online approach for web service selection that jointly considers the conflicting performance metrics including response time, queue congestion and energy consumption. Targeting at optimizing the long-term average system reward, our approach does not require any priori knowledge of the statistics or prediction on task arrivals. Mathematical analysis as well as simulation experiments demonstrate its effectiveness in optimizing energy efficiency while stabilizing the system."
374,"As a traditional algorithm, the string match meets a challenge with the development of the massive volume of data because of gene sequencing. Surveys show that there will be a huge amount of short read segments during the process of gene sequencing and the need for a highly efficient is urgent. The BWA is an effective algorithm to deal with the short read mapping. Compared with other short read mapping algorithms, the BWA algorithm has a smaller size, and this does not influence its effect. However, there is still not a system is used to accelerate the BWA algorithm especially. Thus we decide to build a system to expedite the algorithm and make it satisfied with the application of gene sequencing. In this paper, we present genome sequencing services on scalable energy-efficient accelerators. Especially, we first introduce the BWA algorithm and claim the reason for the choice of the algorithm. Then, we implement an accelerator based on FPGA to improve the performance of the algorithm. Compared to the other major platforms in accelerating the algorithm, we discuss the advantages of the FPGA platform and the limit of the other platform. Last, we build our hardware platform with a Xilinx ZYNQ FPGA development board, and the result shows that our accelerator can achieve a promising speedup and resource utilization and make it balanced between power and cost."
375,"Service monitoring is an important research problem in service-based systems (SBSs), which aims to monitor the failure of services in a timely manner while using resources as few as possible. Most of the existing service monitoring approaches are centralised which suffer the potential of single point of failure and are not suitable in distributed large scale SBSs. Moreover, these centralised monitoring approaches are designed only in single-tenant SBSs. Nowadays, the scale of SBSs is extremely large, i.e., including a large number of services and clients. Thus, it is essential for service monitoring approaches to work well in distributed large scale SBSs and support multi-tenancy. Towards this end, in this paper, an agent-based decentralised service monitoring approach is developed in multi-tenant SBSs. Compared to the centralised approaches, the proposed decentralised approach can avoid the single point of failure and can balance the computation over the monitoring agents. Also, unlike existing approaches, the proposed approach is developed in multi-tenant SBSs. Experimental results demonstrate that the proposed approach can respond as quickly as centralised approaches but has much less computation overhead than centralised approaches."
376,"Composition of Web Services (WS) into business processes (BP) often results in occurrence of defects in a process: implicit dependencies, incorrect contexts, non-optimal or bottlenecked workflow, and deadlocks. To deal with these problems, WS Mining (process mining in SOA) research provides methods and tools to discover, evaluate and enhance real world processes basing on a process model discovered from a log. Unfortunately, current research in this field only concerns SOAP-WS which are not as well-suited as RESTful-WS in the context of current research trends like Internet of Things or Web 2.0. WS Mining methods and tools should consider RESTful-WS where functionality of the system is expressed in the form of resources and relationships among them. In this paper we show the idea of discovering process models for interacting RESTful-WS with respect to both workflow and resources perspectives. We introduce extended version of the α algorithm (AA), RESTful-WS mining algorithm (RMA), which discovers hierarchical process models and resource-oriented perspective of a process including local and global behavior. Finally, we present a brief discussion on how RMA decomposes a problem into smaller ones, significantly reducing the execution time in real time scenarios."
377,"This paper addresses the challenge of Web Services performance in eBay Item Visibility Framework (IVF). There are two types of performance challenges when Web Services are introduced in IVF. First, if IVF is a caller to other eBay internal Web Services for Cross Border Trade business, what can we do to keep IVF machines healthy and fully functional in the instance other Web Services take more than three seconds to return the response? Second, as a Web Services provider to serve item visibility information to other applications, how can IVF return response of hundred items to clients in milliseconds? Both questions are discussed in this paper. Solution and results are presented with conclusion."
379,"Service-oriented computing enables organizations to package their offerings as modular, self-contained services. As these organizational capabilities are more commonly distributed, perhaps redundantly, across cloud environments, there is an opportunity to provision services in locations that maximize energy-efficiency. In this paper, we introduce a process, model, and power estimation techniques to facilitate a priori decision support with respect to provisioning software services in context of the server on which they reside. Favorable results from our experiments demonstrate the ability to estimate the power consumption of web services while differentiating energy-efficiency of redundant instances."
380,"eScience consists of computation-intensive workflows executing on highly distributed networks. Service compositions aggregate web services to automate scientific and enterprise business processes. Along with the increased demand for data quality and Quality of Service (QoS) for an accurate outcome in a shorter completion time, execution of the eScience workflows and service compositions are also required to be distributed efficiently across various geo-distributed nodes. This paper presents Mayan,
<sup>1</sup>
 a Software-Defined Networking (SDN) based approach for service composition. Mayan i) facilitates an adaptive execution of scientific workflows, ii) offers a more efficient service composition by leveraging distributed execution frameworks, in addition to the traditional web service engines, and iii) enables a very large-scale reliable service composition by finding and consuming the current best-fit among the multiple implementations or deployments of the same service."
381,"The dependability of composite services is largely affected by their constituent Web services. Composite services have to operate in an open and dynamically changing environment in order to leverage the best performing services available at the moment. Hence, there is the need for an efficient mechanism to provide reliable service rankings. In this paper we present a novel, generic, and customizable reputation infrastructure to automatically and transparently monitor the execution of composite services, taking both functional and non-functional properties into account. The experienced Web service quality-of-service is communicated to a configurable reputation mechanism that publishes service rankings. Our reputation infrastructure supports notifications upon changes in service reputation, enabling self-tuning and self-healing properties in the execution of composite services. We implemented our architecture using standard technologies, such as BPEL and JavaEE. Performance measurements show that our infrastructure causes only moderate overhead."
382,"Scientific (e.g. genetic) data and relative processing tools are usually located at different Web sites; scientists have been strongly expecting a universal solution to compare, integrate, and analyze these heterogeneous and usually large size data with processing tools from distant sites for years. This paper presents a pipelined Web services workflow framework: SPL (service pipeline logic) that can reduce frequent data interaction between sites and mitigate network traffic incurred by huge scientific data transmission during the course of processing a bioinformatics workflow. The paper shows that the processing time of a complex bioinformatics workflow can be saved remarkably with the application of SPL."
383,"End-users repetitively perform various on-line tasks and invoke multiple web services for their re-occurring activities, such as planning a trip. Usually, end-users have to complete different tasks in order to achieve a goal, and look through large volumes of services to find the best ones that satisfy their constraints, such as a budget limit. Current approaches on service composition require programming skills and domain knowledge to accomplish goals. Moreover, existing approaches lack an automatic way to analyze end-users' goals and extract relevant tasks for achieving goals. In this paper, we provide a lightweight service composition framework for end-users with limited technical background. Our framework analyzes endusers' goals expressed in natural languages to mine tasks (e.g., plan a trip) and non-functional constraints (e.g., budget &lt;; 500). Our framework extracts task models from textual descriptions of tasks (e.g., eHow, a How-to instruction website) to guide the selection of services and recommend web services that can finish tasks and satisfy constraints. We have designed and developed a prototype as a proof of concept. We conduct case studies to evaluate the effectiveness of our framework. Our framework can identify tasks with a precision of 93% and a recall of 77%, and extract non-functional constraints with a precision of 89% and a recall of 76%. A user study shows that our framework is helpful for end-users to compose services."
384,"The paper addresses the problem of maintaining the quality of service (QoS) of an orchestration of Web services (WS), which can be affected by exogenous events (i.e., faults). The main challenge in dealing with this problem is that typically the service where a failure is detected is not the one where a fault has occurred: faults have cascade effects on the whole orchestration of services. The paper presents a novel methodology to treat the problem that is not based on Web service (re)composition, but on an adaptive re-execution of the original orchestration. Specifically, an orchestrator Manager exploits an abstract representation of the whole orchestration and a diagnostic module to localize the source of the detected failure. Then, the Manager drives the re-execution of the orchestration by deciding which service activities can be skipped, and which others must be re-executed."
386,"Attracted by the advantages of cloud computing, more and more services and applications are migrated to this new paradigm. As promising as it is, cloud computing also brings new challenges to many research issues, such as service scheduling. Most existing scheduling methods are offline and could not deal with the uncertainties and dynamics during the execution, especially in the dynamic cloud environment. In view of this challenge, in this paper, we propose an uncertainty-aware evolutionary scheduling method for cloud service provisioning. It aims at dealing with uncertainties during execution and updating the scheduling so as to meet the deadline and optimize the execution cost of cloud applications. Our method consists of two phases, baseline scheduling and evolutionary scheduling during execution. In baseline scheduling, we suggest a reverse-auction-based pricing mechanism for service provisioning. In evolutionary scheduling, an uncertain model with three types of uncertainties is considered and four uncertain events are discussed. Accordingly, the evolutionary scheduling policy is presented based on intermediate workflow to get a global optimal schedule, so as to improve the success rate for the execution of the cloud applications. Finally, experiments are designed and performed to demonstrate the effectiveness of our method."
387,"We propose an integrated framework that manages changes in long term composed services. The main procedure of change reaction is presented. One of the most challenging research issues of change management is how to automate the process of change reaction. To address this issue, we propose a semantic support, which centers around a tree-structured Web service ontology. The ontology is expected to provide sufficient semantic for change reaction. We propose a set of algorithms for efficiently querying semantics from the ontology. We conduct a set of experiments to evaluate the performance of the proposed algorithms."
388,"Providing non interrupted Web Services from resource limited mobile devices needs to be done in a rather light-weight manner. Processing and communication will drain the battery rapidly, hence, both should be kept at a minimum. This paper describes the outcomes of an investigation into simple offloading mechanisms that facilitate provision of adaptive and distributed Restful mobile web services from resource constrained mobile devices. Offloading considers the distributed hosts processing as well as communication capabilities. Using queuing theory, the performance gained from distributing mobile web service tasks is explored. In addition, the theoretical boundaries of different flavours of offloading mechanisms are presented. The analytical, as well as the experimental results show the differences in performance between these mechanisms."
389,"Nowadays personalised service provisioning becomes more feasible due to the increasing availability of smart devices, such as smart phones, tablet computers, Personal Digital Assistants and Play stations. These smart devices can dynamically detect the context data and upload them to support other interesting software applications, such as Facebook and Google maps. Context can become richer and more retrievable if links are established for semantically related context data sets. Taking advantage of the recent digital and Web technologies, this paper proposes a novel Linked Context model that applies the Linked Data principles to model and obtain context data from both users and services in one unified framework to support personalised service provisioning at runtime."
390,"Nowadays, the development of services that span over both the Internet and telephony networks is driving significant efforts towards the integration of services offered by IT providers with telecom operators ones. Web services have often been recommended for providing, composing and realizing telecom services but introducing them means facing up with several challenges. This work sharpens benefits and drawbacks of Web service applications within a telecom environment focusing in particular on JAIN SLEE architecture, which defines a standard environment targeted at communication-based applications"
391,"The main task of microservice extraction is to find which software entities (e.g., methods, classes) should be grouped together from existing monolithic software as candidate microservices, responsible for specific functionalities and evolving independently. Current methods extract microservices by analyzing source code and following the assumption that ""classes with strong relation should be in the same service"", which originates from software structure analysis. We find that 1) many program behaviors cannot be explicitly reflected in the source code, and 2) the relation at code-level is not equivalent to the same functionality. Thus, we propose a functionality-oriented microservice extraction (FoME) method in this study by monitoring program dynamic behavior and clustering execution traces. Instead of source code analysis, the execution traces of a program are applied to group source code entities that are dedicated to the same functionality. We also construct a systematic measurement of microservice by integrating five complementary metrics of service cohesion and coupling. These metrics measure Functional Independence of microservices. That is, it qualifies whether a microservices can have its own responsibilities independently. In the experiment, our method is compared with three state-of-the-art methods on four open-source projects. The microservice candidates generated using our method present similar functional cohesion to the services produced using the other methods, but have considerably looser coupling measurements (dramatically reducing measurements of IRN and OPN)."
392,"We propose a semantic framework for automatically identifying events as a step towards developing an adaptive middleware for Service Oriented Architecture (SOA). Current related research focuses on adapting to events that violate certain non-functional objectives of the service requestor. Given the large of number of events that can happen during the execution of a service, identifying events that can impact the non-functional objectives of a service request is a key challenge. To address this problem we propose an approach that allows service requestors to create semantically rich service requirement descriptions, called semantic templates. We propose a formal model for expressing semantic templates and for measuring the relevance of an event to both the action being performed and the nonfunctional objectives. This model is extended to adjust the relevance of the events based on feedback from the underlying adaptation framework. We present an algorithm that utilizes multiple ontologies for identifying relevant events and present our evaluations that measure the efficiency of both the event identification and the subsequent adaptation scheme."
393,"Service-based software system (SBS) is a software system based on service-oriented architecture (SOA). Although often treated as a composite service, an SBS is proposed from a more practical point of view based on restricted service provisions. In the highly competitive market, just meeting such requirements seems not enough to get more customers for service providers, and they usually provide additional preferential policies, such as a special order ""buy-two-get-one-free"". However, most of current adaptation approaches focus on single transaction, which makes it hard to take full advantage of such preferential policies in reselecting substitutable services. In this paper, we try to make the adaptation decision and reselect services from a broader view, i.e. expand the computation domain from single transaction to the whole lifecycle of an SBS by considering all of the past, current and predicable future executions. We call it ""long-term benefit"" to distinguish benefit in current approaches and propose a long-term benefit driven adaptation approach in this paper. In our approach, services that would bring the max expected long-term benefit would be selected and substituted into current instance in once adaptation. As the long-term benefit is accumulated in several executions, i.e. it depends on a decision sequence, we model the decision making problem as a sequential decision problem, and describe a realization based on partially observable Markov decision process (POMDP) for maximizing the real income in providing an SBS as an example."
394,"Deciding on Web service equivalence in process-aware service compositions is a crucial challenge throughout the composition life cycle. Restricting such decisions to (activity) label equivalence constitutes a simplification for many practical applications: if two Web services have equivalent labels, does this necessarily mean they are equivalent as well? In many scenarios other factors play an important role. Examples include context information (e.g., input and output messages) and information on the position of Web services within compositions. In this paper, we introduce the composition life cycle and discuss specific requirements for Web service equivalence along its different phases. We define adequate equivalence notions for design, execution, analysis, and evolution of service compositions. Main focus is put on attribute and position equivalence. Altogether this paper is a first step towards a new understanding and treatment of equivalence notions in service compositions."
395,"This paper presents an extension to OWL-Q, a prominent semantic quality-based service description language, called Q-SLA, enabling to specify SLAs. This extension advances the state-of-the-art by covering all possible information aspects needed to enable proper and automatic support to all service management activities. A particular use-case is also provided highlighting Q-SLA's main benefits."
396,"Web service recommendation is of great importance when users face a large number of functionally-equivalent candidate services. To recommend Web services that best fit a user's need, QoS values which characterize the non-functional properties of those candidate services are in demand. But in reality, the QoS information of Web service is not easy to obtain, because only limited historical invocation records exist. To tackle this challenge, in recent literature, a number of QoS prediction methods are proposed, but they still demonstrate disadvantages on prediction accuracy. In this paper, we design a location-based hierarchical matrix factorization (HMF) method to perform personalized QoS prediction, whereby effective service recommendation can be made. We cluster users and services into several user-service groups based on their location information, each of which contains a small set of users and services. To better characterize the QoS data, our HMF model is trained in a hierarchical way by using the global QoS matrix as well as several location-based local QoS matrices generated from user-service clusters. Then the missing QoS values can be predicted by compactly combining the results from local matrix factorization and global matrix factorization. Comprehensive experiments are conducted on a real-world Web service QoS dataset with 1,974,675 real Web service invocation records. The experimental results show that our HMF method achieves higher prediction accuracy than the state-of-the-art methods."
397,"Engineering volunteer services calls for novel self-adaptive approaches for dynamically managing the process of selecting volunteer services. As these services tend to be published and withdrawn without restrictions, uncertainties, dynamisms and 'dilution of control' related to the decisions of selection and composition are complex problems. These services tend to exhibit periodic performance patterns, which are often repeated over a certain time period. Consequently, the awareness of such periodic patterns enables the prediction of the services performance leading to better adaptation. In this paper, we contribute to a self-adaptive approach, namely time-awareness, which combines self-aware principles with dynamic histograms to dynamically manage the periodic trends of services performance and their evolution trends. Such knowledge can inform the adaptation decisions, leading to increase in the precision of selecting and composing services. We evaluate the approach using a volunteer storage composition scenario. The evaluation results show the advantages of dynamic knowledge management in self-adaptive volunteer computing in selecting dependable services and satisfying higher number of requests."
398,"Multi-tenant service-based systems (SBSs) have gained unprecedented prominence in recent years. Network-accessible Web services are composed in the form of business process to simultaneously fulfill multiple tenants' functional and multi-dimensional quality-of-service (QoS) requirements. Those services often operate in a distributed and volatile environment. It is of tremendous importance to monitor the services to detect or predict the anomalies timely to avoid QoS violations and Service Level Agreement (SLA) breaches. However, monitoring Web services consumes resources and incurs system overhead. It is impractical to constantly monitor all the services in an SBS. Thus, it is a significant challenge to monitor the services of a multi-tenant SBS in a cost-effective manner. In this paper, we propose CM4MTS (Criticality-based Monitoring for Multi-Tenant SBSs) for formulating cost-effective monitoring strategy. The criticality of a service is evaluated based on its impacts on the quality of the SBS upon runtime anomalies and the tenants sharing the service. The services with higher criticalities in an SBS are given higher priorities in monitoring resource allocation. Extensive experiments show that CM4MTS outperforms representative approaches in ensuring the quality of multi-tenant SBSs."
400,"Summary form only given. We address the question, ""in the brave new world of Web services and service-oriented architectures (SOA), how does data fit in?"" We bring data modeling concepts to bear on the world of services, yielding an approach in which enterprise data access is handled by a collection of interrelated data services. We show how the approach can be realized on a foundation of XML standards, namely XML Schema, Web services, and XQuery. We show that this approach provides a uniform and declarative framework for integrating enterprise data assets that are drawn from disparate underlying sources, including both queryable and non-queryable data sources as well as data that is encapsulated by Web services. We also show that the approach yields data services that are easily and efficiently reusable."
401,"A primary benefit of Web services is that they provide a uniform implementation-independent mechanism for accessing distributed services. Building and deploying such services do not benefit from the same advantages, however. Different Web services containers are implemented in different programming languages, with different constraints and requirements placed on the programmer. Moreover, client side programmers must use the Web service interface specified by the service developer. Therefore, the kinds of applications and uses for a Web service are unnecessarily restrictive, constrained by the granularity of access defined by the interface and by the characteristics of the service functions. This paper describes an approach that addresses both of these drawbacks by enabling Web service containers with the ability to accept new mobile code on the fly, and to run it within the containers, providing direct local access to the containers' other services. The code can be specified in a small simple language (a subset of C), and translated and passed to the container in a common XML-based intermediate language called X#. This approach effectively removes the dependence on any single implementation environment. Our prototype implementation for two different containers demonstrates the feasibility of the approach, which represents a first step toward write-once deploy-anywhere Web services."
402,"With the unprecedented and dramatic development of Web services in recent years, designing novel approaches for efficient Web service prediction has become of paramount importance. Quality of Service (QoS) plays a critical role in Web service recommendation. However determining QoS values of Web services is still a challenging task. For example, some QoS properties (e.g., response time, throughput) may hold different values for different users. In this paper, we describe how to develop a novel approach, PLMwsp, based on a probabilistic latent model, to predict effectively the QoS values of Web services. A Web service prediction has been developed, and experiments have been conducted to show the efficacy of our approach."
403,"Service-oriented communication (SOC) is a new trend in the industry to enable communication through a service-oriented architecture (SOA) and thereby encapsulate communication capabilities as services. In this paper, we design the session initiation protocol (SIP) based multimedia conferencing communication services model, And mainly focus on formal analysis for BPEL based multimedia conferencing communication services orchestration and to guarantee the process correctness for such applications, and also providing an automated support for the formal analysis model of their behavior. Finally, we give the conclusions."
404,"This paper presents a prediction model for software services availability measured by the mean-time-to-repair (MTTR) and mean-time-to-failure (MTTF) of a service. The prediction model is based on the experimental identification of probabilistic prediction for variables that affect MTTR/MTTF, based on monitoring service data collected at runtime."
405,"In QoS-based Web service recommendation, predicting QoS(Quality of Service) for service users will greatly aid service selection and discovery. In order to improve the prediction accuracy of Collaborative filtering algorithms, various factors are taken into account (e.g., location factor, environment, etc.). But seldom do investigators take the factor of time into account. Actually, QoS performance of Web services is highly related to the service status and network environments which are variable against time. Thus, this paper proposes a time-aware collaborative filtering algorithm to predict the missing QoS values. To validate our algorithm, this paper conducts series of large-scale experiments based on a real-world Web service QoS dataset. Experimental results show that the time-aware collaborative filtering algorithm significantly improves prediction accuracy."
406,"Probabilistic duration representations can be used to forecast the response time of (composite) Web services, based on empirical data of past executions or calculated from control flow structures. The capability to choose the fastest among similar services or to optimize services based on probable process execution times are only two possible application areas."
407,"We show how a standard grid service architecture can be improved by interposing a policy enforcement engine between a calling application and the relative client stubs. Therefore, with our solution selection and invocations are not hard-coded into client applications but (declaratively) defined and enforced outside the clients; therefore they can be (de)activated and modified online. Our policies are specified using the PDL language which supports specification of preferences and prohibitions in the routing of remote invocations to Web services."
408,"Generally, in crowdsourcing, providers advertise their task offerings (i.e. the open call model) largely to crowdworkers who subscribe their interest in working (i.e. subscription model). The combined open call and subscription model represent significant bottlenecks for recruitment in the paradigm of crowdsourcing. Consequently, attracting and retaining a crowd are the major challenges to the success of a crowdsourcing platform and forming a labor market. To address this problem, we introduce a worker-job matching model for crowdsourcing supported by a service-oriented architecture. The service-oriented architecture implements a push-pull mechanism and an underlying algorithm based on collaborative filtering techniques. Preliminary studies show that the infrastructure can effectively infer the levels of expertise of potential crowdworkers based on their profile and past performance history."
409,"As growing number of real-world activities are performed through Internet connected services, there are increasing needs to make the behaviors of both service consumer and provider accountable. Many efforts attempting to regulate services and to guarantee service qualities lack sufficient accountability support. This paper treats accountability as a service and proposes a novel contract-based accountability service model to tackle this problem. The model uses federated accountability services to audit interactions between service consumers and service providers so that misbehaviors can be detected with undeniable evidences. We show how Internet data management services can be made accountable using this service model. We implemented the data management service and characterized its performance."
411,"Service Level Agreements (SLAs) have been proposed in the context of web services to maintain acceptable quality of service (QoS) performance. This is specially crucial for composite service orchestrations that invoke many atomic services to render functionality. A consequence of SLA management entails efficient negotiation protocols among orchestrations and invoked services. In composite services where data and QoS (modeled in a probabilistic setting) interact, it is difficult to select an atomic service for negotiation, in order to improve end-to-end QoS performance. A superior improvement in one negotiated domain (eg. latency) might mean deterioration in another domain (eg. cost); improvement in one of the invoked services may be annulled by another due to the control flow specified in the orchestration. In this paper, we propose a integer programming formulation based on first order stochastic dominance as a strategy for re-negotiation over multiple services. A consequence of this is better end-to-end performance of the orchestration compared to random selection of services for re-negotiation. We also demonstrate this optimal strategy can be applied to negotiation protocols specified in languages such as Orc. Such strategies are necessary for composite services where QoS contributions from individual atomic services vary significantly."
412,"High energy physics (HEP) and other scientific communities have adopted service oriented architectures (SOA) as part of a larger grid computing effort. This effort involves the integration of many legacy applications and programming libraries into a SOA framework. The grid analysis environment (GAE) (Lingen et al., 2004) is such a service oriented architecture based on the Clarens grid services framework (Steenberg et al., 2004) and is being developed as part of the compact muon solenoid (CMS) experiment at the large hadron collider (LHC) at European Laboratory for Particle Physics (CERN). Clarens provides a set of authorization, access control, and discovery services, as well as XMLRPC and SOAP access to all deployed services. Two implementations of the Clarens Web services framework (Python and Java) offer integration possibilities for a wide range of programming languages. This paper describes the Java implementation of the Clarens Web services framework called 'JClarens' and several Web services of interest to the scientific and grid community that have been deployed using JClarens."
413,"Insider attacks in which misbehaving Virtual Machines (VMs) take part of the cloud system and learn about its internal vulnerabilities constitute a major threat against cloud resources and infrastructure. This demands setting up continuous and comprehensive security arrangements to restrict the effects of such attacks. However, limited security resources prohibit full detection coverage on all VMs at all times, which can be exploited by attackers to examine the selective detection strategies and adjust their own attack plans accordingly. Motivated by the absence of any approach that accounts for such a challenge in the domain of cloud computing, we propose in this work an adaptive detection strategy that formulates a Stackelberg security game to enable the cloud system to optimally exploit its available amount of security resources to maximize the detection of distributed attacks, knowing that attackers have the ability to monitor the cloud system's strategies and adjust their own attack plans. Experiments carried out on the CloudSim framework reveal that the proposed solution maximizes the detection of distributed attacks and minimizes false negatives and positives compared to a maximin-based detection strategy, while being scalable to the increase in both the number of co-hosted VMs and percentage of co-resident attackers."
415,"With the prevalence of the online to offline (O2O) commerce, the short-distance instant logistics service becomes increasingly popular in China, which is a new logistics model emerged recently and provides timely logistics for O2O local life service. Being different from traditional logistics, extra waiting time generates when order's pick-up location arrival time is early than its pick-up ready time, and dynamic orders dispatch strategy is also a key factor affecting overall logistics service quality. However, existing industry practices are dispatching orders to nearest couriers and logistics sequences are arranged by couriers themselves. To improve overall dispatch solutions, we present a smart dynamic order dispatch system (SmartDODS), and a personalized logistics sequence recommendation (PLSR) service to optimize couriers logistics sequence in real-time. At last, we use historical dispatch data set of SHBJ to evaluate our system performance. The experimental results demonstrate our system can effectively optimize logistics dispatch solutions."
416,"Widget aggregators such as iGoogle and Netvibes are broadly adopted by the mass market. They enable end-users to personalize their environment with their preferred services (Widgets). However, the usage in an enterprise context is not yet investigated. In this paper, we firstly show that in addition to personalization capability, the integration of business processes should be considered. Secondly, we propose a new Widget aggregator that enables the end-user to personalize a business process by chaining Widgets according to his/her needs and habits. Thirdly, we introduce a new approach for specifying an end-user process; an approach which enables even ordinary end-users, without computing skills, to define their processes. Finally, we validate these concepts by implementing and testing a prototype. As a consequence, this work does not only impact Widget aggregators, but it also innovates in end-user service creation research by proposing an intuitive tool, understandable even by ordinary end-users, for specifying their processes (composite services)."
417,"Semantic annotation plays an important role for semantic-aware web service discovery, recommendation and composition. In recent years, many approaches and tools have emerged to assist in semantic annotation creation and analysis. However, the Quality of Semantic Annotation (QoSA) is largely overlooked despite of its significant impact on the effectiveness of semantic-aware solutions. Moreover, improving the QoSA is time-consuming and requires significant domain knowledge. Therefore, how to verify and improve the QoSA has become a critical issue for semantic web services. In order to facilitate this process, this paper presents a novel lifecycle framework aiming at QoSA assessment and optimization. The QoSA is formally defined as the success rate of web service invocations, associated with a verification framework. Based on a local instance repository constructed from the execution information of the invocations, a two-layer optimization method including a local-feedback strategy and a global-feedback one is proposed to improve the QoSA. Experiments on real-world web services show that our framework can gain 65.95%~148.16% improvement in QoSA, compared with the original annotation without optimization."
418,"Despite the benefits those communities FM radio broadcasting services provide to their listeners, more than half of the community stations in Japan are struggling financially. This situation has arisen because the Internet has attracted advertising revenue away from radio broadcasting channels. One way to revive local FM radio broadcasting is by adding new representation function to radio signals. It is designed for many listeners to share the same information simultaneously. On the other hand, while the Web provides a variety of content, its ability to broadcast is inferior to that of TV or radio broadcasting. Combining radio broadcasting and the WWW provides a new broadcasting media. The control signals used for accessing the Web are put onto radio sound by using Acoustic OFDM technology. We developed and evaluated such a broadcasting system."
419,"The web services technology has been created to support communication between heterogeneous platforms. Despite its maturity, built upon more than a decade of experience, research and practice show that the technology still fails to connect web service client applications to servers, even when the programming languages involved are the same. This is especially troubling for service providers, as a failure in the inter-operation of web services may lead to disastrous consequences for the services involved, which frequently support businesses. In this paper, we present INTENSE, a service deployed as an on-line web application, designed to test the interoperability of a web service against specific client-side platforms. The tool is able to test the pre-runtime steps involving code generation and the end-to-end runtime communication, present in a web service interaction with a client. We used INTENSE to test a set of web services deployed on Glassfish and WildFly against the well-known Metro JAX-WS, JBossWS, and Axis2 client platforms, which disclosed severe interoperability issues."
420,"To solve the problem of low information integration for heating industry, a framework of information service platform is proposed. The framework realizes information sharing and integration control of central heating. Four core components are designed to implement process development, service collaboration, publish/subscribe and event rule. A heating management system has been designed based on these components, which realizes intelligent and security of the production management. And three application subsystems have been developed to realize heating maintenance service, multi-level alarm service and heating charging service. The information service platform effectively achieves information integration and rapid service development."
421,"A critical step in the process of reusing existing WSDL-specified components is the discovery of potentially relevant Web services. Traditional category based Web service retrieval usually can achieve good recall but worse precision because some semantically relevant Web services are not actually relevant as they cannot provide suitable interfaces. In this paper, we present an interactive Web services retrieval mechanism to refine the coarse retrieval results set in category based retrieval. In the refinement, the signature matching of Web services that concerning the structure of operation specifications is investigated from a multi-instances view. In detail, each Web service is represented as a bag in multiple instance learning, while each operation in this Web service is regarded as an instance. This representation lies in that a user regards a service as useful if at least one operation provided by this Web service is useful. Experimental results show that our approach can improve the retrieval performance significantly: It can gain 83% precision in average after two rounds of user relevance feedback"
422,"An enterprise service-oriented architecture is typically realized on a messaging infrastructure called an enterprise service bus (ESB). An ESB is a bus which delivers messages from service requesters to service providers. Since it sits between the service requesters and providers, it is not appropriate to use any existing capacity planning methodology for servers, such as modeling to estimate an ESB's capacity. There are programs which run on an ESB called mediation modules. Their functionalities vary and depend on how people use the ESB. This creates difficulties for capacity planning and performance evaluation. This paper proposes a performance evaluation methodology and techniques for ESBs. We actually run the ESB on a real machine while providing a pseudo-environment around it. In order to ease setting up the environment we provide ultra-light service requestors and service providers for the ESB under test. We show that the proposed mock environment can be set up with practical hardware resources available at the time of hardware resource assessment. Our experimental results showed that the testing results with our mock environment are equivalent to the results in the real environment"
423,"Support from development tools and infrastructure frameworks is crucial to increase the development of Web APIs that follow the REST architectural principle, leaving the software developer free to focus on the implementation of the business core of the application. This paper introduces a framework for semantic description of RESTful Web APIs, which is based on annotations added to the application code that associate resources, properties and operations with terms semantically described by vocabularies and ontologies. The proposed framework enforces the adoption of design principles for modelingWeb APIs, focusing on resource representations and targeting important features for high concurrency services, such as low coupling and high flexibility among layers."
424,"A web-based service consists of layers of programs (components) in the technology stack. Analyzing program executions of these components separately allows service vendors to acquire insights into specific program behaviors or problems in these components, thereby pinpointing areas of improvement in their offering services. Many existing approaches for testing as a service take an orchestration approach that splits components under test and the analysis services into a set of distributed modules communicating through message-based approaches. In this paper, we present the first work in providing dynamic analysis as a service using a virtual machine (VM)-based approach on dynamic data race detection. Such a detection needs to track a huge number of events performed by each thread of a program execution of a service component, making such an analysis unsuitable to use message passing to transit huge numbers of events individually. In our model, we instruct VMs to perform holistic dynamic race detections on service components and only transfer the detection results to our service selection component. With such result data as the guidance, the service selection component accordingly selects VM instances to fulfill subsequent analysis requests. The experimental results show that our model is feasible."
425,"In federated networks, trust management is critical to information sharing and online collaboration. Security tokens provide a way to convey and exchange trust-related information for security and privacy purposes. Common users encounter difficulties when they have to handle security tokens across heterogeneous domains. Semantic gaps and incompatibilities are major barriers for trust-related information exchange in federated trust management. This paper uses intermediary-based, query-based and hybrid approaches to resolve these issues for different types of information in security tokens, and proposes three exchange models accordingly. This paper also provides a comprehensive framework using Web services to exchange security tokens across security domains with suitable approaches and exchange models."
426,"Reputation systems have become an important means to help users build trust, reduce information asymmetry and filter information in the context of online services provision. Different users cannot rate services under the same criteria due to the scale and dynamism of these systems. Thus, aggregating cardinal ratings into reputation will potentially lead to unreliable and misleading result, which makes reputation systems necessarily consider the impossibility of interpersonal utility comparisons. In this paper, we exploit the ordinal user preferences between services to compute reputation of services. A distance metric is defined to measure the discrepancy between two rating vectors and the reputation computation problem was formalized as an optimization problem. Then, genetic algorithm is used to solve the optimization problem to find a reputation vector that minimizes the total number of disagreements with the rating matrix. We conduct a comprehensive experimental study and performance analysis to evaluate the effectiveness and efficiency of the proposed method."
427,"As primarily modular and distributed architectures, service-oriented architectures may impose new challenges in software evolution. Since web services evolve independently, this may cause disruptions to the proper function of consuming software. In this paper, we present \wsd, an Eclipse plug-in to support the evolution of service clients, including (a) identifying the differences between two service versions, (b) automatically adapting the client application to the new version, and (c) testing the client to confirm it functions properly."
428,"A composite service is typically specified using a language such as BPEL4WS and orchestrated by a single coordinator node in a centralized manner. The coordinator receives the client request, makes the required data transformations and invokes the component Web services as per the specification. However, in certain scenarios businesses might want to impose restrictions on access to the data they provide or the source from which they can accept data. Centralized orchestration can lead to violation of these data flow constraints as the central coordinator has access to the input and output data of all the component Web services. In many cases existing methods of data encryption and authentication are not sufficient to handle such constraints. These data flow constraints, thus, present obstacles for composite Web service orchestration. In this paper we propose a solution for orchestrating composite Web services under data flow constraints. The solution is based on decentralized orchestration, in which a composite Web service is broken into a set of partitions, one partition per component Web service. To overcome data flow constraints, each partition is executed within the same domain as the corresponding component Web service and hence, has the same access rights. However, there are, in general, many ways to decentralize a composite Web service. We apply a rule based filtering mechanism to choose a set of partitions that does not violate the specified dataflow constraints."
429,"In the composite service which runs for a long time under the heterogeneous and loose-coupled circumstance, the failure of service tends to occur. The transaction and recovery mechanism is urgently needed in order to guarantee the end-to-end QoS of the workflow and satisfy the user requirement. In this paper we address the composite service recovery issue in the way of substitution with the consideration of QoS constraint, based on our previous research work of the transactional construction and processing rules and the global optimization service selection algorithm, TSSA. Firstly, the service execution graph (SEG) is introduced and a service execution solution selection algorithm is proposed to choose one from the solution set of TSSA which has the highest success rate of recovery. Then, the concepts of execution backup path and switch cost are introduced and a search algorithm is presented to search for the optimal backup path when current service failure occurs. Meanwhile, a local induction algorithm based on positive feedback is described which could rapidly construct an transactional execution path when no backup execution path can be found in SEG and also guarantees the transactional constraint of composite service. Finally, experimental results show the recovery algorithm proposed in this paper is efficient and has high reliability."
430,"The subject of business-IT alignment has increasingly attracted the attention of executives in the past decade. The purpose is to leverage the potential of IT to improve business performance. This goal can be achieved by developing mechanisms to enhance the communication between business and IT. In the same track, its essential to apply methods to assure that IT investments are driven by the strategic goals and priorities of the company. Service-Oriented Architecture (SOA)is widely recognized as a technology that promotes IT alignment and business agility. However, strategic goals of firms have been frequently neglected when dealing with technical subjects such as web service selection and deployment. In this work, we propose a multiple criteria decision method based on the Analytic Hierarchy Process (AHP) that is able to accommodate in a single framework both the technical and the strategic requirements of web service deployment and selection. Our framework help IT managers to find the configuration that best translates the preferences of the customer and the priorities of the company. A numeric case study compares our approach to other traditional uses of AHP."
431,"The service-oriented architecture paradigm is influencing modern software systems remarkably and Web services are a common technology to implement such systems. However, the numerous Web service standard specifications and especially their ambiguity result in a high complexity which opens the door for security-critical mistakes.This paper aims on raising awareness of this issue while discussing a vulnerability in Amazonpsilas Elastic Compute Cloud (EC2) services to XML wrapping attacks, which has since been resolved as a result of our findings and disclosure. More importantly, this paper discusses the verification steps required to effectively validate an incoming SOAP request. It reviews the available work in the light of the discovered Amazon EC2 vulnerability and provides a practical guideline for achieving a robust and effective SOAP message security validation mechanism."
432,"Service clustering is the foundation of service discovery, recommendation and composition. Most of the existing methods mainly use service attribute information and ignore the semantic-based invocation relationships among service users. In fact, mutual invocation relationships between services occur on operations of the corresponding services, while service attributes are the whole service description. Our main challenge may be to effectively combine these two kinds of data for service clustering. To address this issue, we propose a new probabilistic generative model which contains two closely connected parts, one characterizing operation community memberships by using operation invocation relationships, and the other characterizing service cluster memberships by utilizing service attributes. The correlations between these two parts are characterized by the relationships between operation communities and service clusters. To train this model, we provide a nested expectation-maximization algorithm. Experimental results show its superior performance over the existing methods for service clustering."
433,"There are numerous existing notations and standards in the Web service community. These may be grouped broadly into three competing families, namely; Web services, semantic Web, and electronic business. Although the families are competing, we expect that applications will cut across them and there is a need to map from one to another and to analyze compatibility and other properties. Therefore we survey how they deal with different aspects. We then illustrate with examples, the aspects of contracts captured by one language from each of the three competing families in addition to WSDL, the core standard for Web services description. The result is a classification based on the aspects of computations: functionality, protocol, and for instance performance covered by the languages. The classification is used to identify similarities between semantic models and thus find potential mappings between the families. Furthermore, this gives a handle on analysis techniques that may apply to the aspects in a particular family."
434,"Service oriented computing is being increasingly exploited in the architecture of current web applications. The interactions among the deployed web services are becoming vital to accomplish heterogeneous and compound business goals. Service selection and composition are two common tasks which are highly influenced by the quality of these interactions. Thus, assigning web services QoS-based trust scores that are incrementally updated, provides a means to assist both tasks. Then, services with higher trust scores are more likely to be selected than those with smaller ones. They are, additionally, more prone to be incorporated as part of composite services. In this paper, we propose a probabilistic approach based on Bayesian networks (BN) to learn the composition structure of composite services and compute QoS-based trust scores in an online setting. The learning of the BN structure and parameters is based on modeling the QoS, which is represented by the BN's variables, using a multinomial generalized Dirichlet distribution (MGDD). The effectiveness of our approaches is empirically assessed using real and synthetic data. Our experimental results show that MGDD provides a flexible and accurate representation of the QoS. They also prove the capability of the BN approach to learn the composition structure, and further the responsibility of the constituent services in the quality of the composite service even when their QoS is partially observed."
435,"Tag recommendation has gained significant popularity for annotating various web-based resources including web services. Compared with other approaches, tag recommendation based on supervised learning models usually lead to good accuracy. However, a high-quality training data set is needed, which demands manual tagging efforts from domain experts. While we could leverage the tags of existing web services assigned by their developers, the quality of these tags may not be good enough to build accurate classifiers for tag recommendation. In this paper, a novel multi-label active learning approach is proposed for web service tag recommendation. The proposed approach is able to identify a small number of most informative web services to be tagged by domain experts. We further minimize the domain expert efforts by learning and leveraging the correlations among tags to improve the active learning process. We conduct a comprehensive experimental study on a real-world data set and results demonstrate the effectiveness of our approach."
436,"As more web services are offered on the Web, it is becoming increasingly difficult for users to manage and search for online content, using only flat keyword searching. Users often forget how they tagged their data but may remember generic information such as the location they were in when they took the picture. We describe a framework for personalized context-aware search of ontology-based tagged data. The tag ontology leverages additional information on data coming from a user and a resource, besides the tagged keyword, in order to augment search information. The framework uses general concepts taken from PeCMan, a personal content manager and GloServ, an ontology-based global service discovery system to implement the front-end and back-end of the overall system."
437,"Mobile agent technology has been evolving since late 1990s and its development is essentially independent of developments in distributed computing technology such as SOA, Semantic web and Web services. Incorporating mobile agents bring undeniable benefits to a distributed application. Present mobile agent technology fails to leverage the interoperable web infrastructure developed in a standard compliant manner. Here we fuse Workflow, Web 2.0, SOA and WS-BPEL and create a distributed computing environment (ACtive E-commerce Framework called ACEF) that permit creation of inter operable, infrastructure leveraging migratable code for design of active internet application."
438,"Vulnerability detection tools are frequently considered the silver-bullet for detecting vulnerabilities in web services. However, research shows that the effectiveness of most of those tools is very low and that using the wrong tool may lead to the deployment of services with undetected vulnerabilities. In this paper we propose a benchmarking approach to assess and compare the effectiveness of vulnerability detection tools in web services environments. This approach was used to define a concrete benchmark for SQL Injection vulnerability detection tools. This benchmark is demonstrated by a real example of benchmarking several widely used tools, including four penetration-testers, three static code analyzers, and one anomaly detector. Results show that the benchmark accurately portrays the effectiveness of vulnerability detection tools and suggest that the proposed approach can be applied in the field."
441,"As part of web services life-cycle, providers frequently face decision about changes without a clear understanding of the impact on their clients. The identification of clients' consumption patterns constitute invaluable information to support more effective decisions. In this paper, we present a framework that supports the discovery of service usage profiles, to bring awareness on the distinct groups of consumers, and their usage characterization in terms of detailed service functionality. The framework encompasses monitoring of clients requests, constituting a general purpose Usage Database, and a process to cluster client applications and derive usage profiles. The paper details the framework and presents experiments."
442,"This paper presents a methodology to perform passive testing of behavioural conformance for the web services based on the security rule. The proposed methodology can be used either to check a trace (offline checking) or to runtime verification (online checking) with timing constraints, including future and past time. In order to perform this: firstly, we use the Nomad language to define the security rules. Secondly, we propose an algorithm that can check simultaneously multi instances. Afterwards, with each security rule, we propose a graphical statistics, with some fixed properties, that helps the tester to easy assess about the service. In addition to the theoretical framework we have developed a software tool, called RV4WS (Runtime Verification engine for Web Service), that helps in the automation of our passive testing approach. In particular the algorithm presented in this paper is fully implemented in the tool. We also present a mechanism to collect the observable trace in this paper."
444,"The population of mobile devices capable of participating in the Internet has increased dramatically in the last few years. To include this population into the Web service world requires support for the most important features, in particular security at the message level. This paper covers our approach to implement XML security specifications on mobile devices that allows efficient single-pass processing of XML encryption and signatures. Furthermore, we propose extensions to security specifications to better take into account the needs of mobile devices. We demonstrate the performance of our implementation, as well as our proposed extensions, through experiments, carried out in a real mobile environment."
445,"Web services are the building blocks of the emerging computing paradigm based on service-oriented architectures. A Web service is a self-describing, open component that supports rapid composition of distributed applications. Web service definitions are used to describe the service capabilities in terms of the operations of the service and the input and output messages for each operation. Such definitions are expressed in XML by use of the Web Service Definition Language (WSDL). Unfortunately, a WSDL description only addresses the functional aspects of a Web service without containing any useful description of non-functional or quality of service (QoS) characteristics. This paper introduces a lightweight WSDL extension for the description of QoS characteristics of a Web service. The extension is carried out as a metamodel transformation, according to principles and standards recommended by the model driven architecture (MDA). The WSDL metamodel is introduced and then transformed into the Q-WSDL (QoS-enabled WSDL) metamodel. The proposed Q-WSDL extension can effectively be used to specify QoS requirements, to establish service level agreements (SLA), to add QoS-oriented characteristics when querying registries of Web services and to support the automated mapping from WSDL documents to Q-WSDL ones and from UML models to Q-WSDL Web services"
446,"The business process execution language for Web services (BPEL4WS) offers a new standards-based approach to building flexible business processes by orchestrating multiple Web services. Unit testing is critical to guarantee the creation of correct and reliable BPEL4WS business processes. Yet this is a topic that is still to be explored, both in research and industry. Lack of unit test tooling support has affected BPEL4WS process development in terms of quality and efficiency. This paper proposes a BPEL4WS unit test framework, which includes a BPEL4WS process composition model, a test architecture, a lifecycle management schema and a test design outline. An example implementation of the framework illustrates the whole approach. By adopting this test framework, the experience of developing, testing and debugging a BPEL4WS processes will be greatly improved."
447,"Use of web services is one of the most rapidly developing technologies. Since web services are defined by XML- based standards to overcome platform dependency, they are very eligible to integrate with each other in order to establish new services. This composition enables us to reuse existing services, which results in less cost and time consumption. One of the recent problems with web service composition is to maximize the overall Quality of Service (QoS) of the composed service. Most common elements of QoS are response time, availability, reliability, throughput and cost (price). Since the selection of the optimal execution plan that maximizes the composition's overall QoS is a NP hard problem, applying optimization techniques is very popular. In this work, we propose an improved Genetic Algorithm based approach to optimize the overall QoS of the composed service. Experimental results indicate improvement for QoS of the composition built by the proposed methods."
448,"In order to naturally incorporate the information provided by Web services into the data integration systems, we propose a novel model, uniform query, to uniformly describe the data query/view and the semantic of information-providing Web services. With the proper matching algorithm we have implemented a prototype in which Web services act as normal data sources (e.g. RDF datasets), and data queries could be answered through dynamic invocation of matched services."
449,"The number of Web services has substantially increased in response to the needs of business activities. Moreover, toolsets and APIs ensuring the easy development and deployment of these services have emerged. As a result, the number of registries holding the great deal of Web services has also increased. Therefore, binding these services, whenever the user needs to reuse them, is time and effort consuming. To cope with this problem and in response to the limits of current approaches, we propose a local repository-based approach to optimize the binding of the frequently used Web services. Furthermore, an experimental study is presented to situate the proposed approach to other ones dealing with the binding feature of Web services."
450,"Cloud computing is an emerging computing paradigm that users can request on-demand computing services through networks and cloud computing platforms anytime and anywhere. Some distinguishing characteristics of cloud computing are elasticity, scalability, hardware virtualization, fast service configuration, etc. In cloud computing environments, three kinds of services can be provided, including Infrastructure as a Service (IaaS), Platform as a Service (PaaS) and Software as a Service (SaaS). These cloud services can be composed into a value-added service to satisfy the dynamic needs of Internet users. This paper proposes a self-healing framework for service composition in cloud environments."
451,"The emergence of Web 2.0 and its related technologies such as HTML5 has empowered the end users and made it possible for them to compose their own Web applications. Yet, most of the current development has mainly concentrated on the support of the composition of enterprise-oriented services and scientific workflows, with not enough effort made to support the composition of end user-oriented services. This paper presents HyperMash, a service composition approach for the end users. The salient feature of this approach is its support of on demand heterogeneous service composition, which allows the end users to create their own composite services by combining RESTful services with SOAP-based services at runtime. In addition, to make it easier for the end users to compose RESTful services, HyperMash provides a full set of RESTful interface features. The paper describes the conceptual framework and working principles of HyperMash and illustrates the HyperMash approach through two examples."
453,"This paper presents a Configurable Cloud Service Discovery and Selection System (C2SDS2) that aims to guide Cloud users in retrieving configurations of IaaS Cloud resources over the Internet. The C2SDS2 takes into account both user functional and non-functional requirements in the retrieval and selection process. In this work, configurable services are designed as directed Cloud extended feature graphs inspired by graph structures and feature models. The discovery-based matching is performed in two steps. In the first step, the structural matching is performed by adapting two heuristics: (1) Hungarian and (2) VG (Volgenant-Jonker) which is an improved Hungarian algorithm. In the second step, the QoS matching and ranking are achieved using three different methods of directed weighted graph matching namely the Eigen-decomposition, the Symmetric polynomial transform and the Linear programming methods. We show the efficiency and effectiveness of our system through an experimental study conducted on a configurable IaaS services collection. The experiment results show the performance and the efficiency of the algorithms combinations."
454,"The selection of Web services is typically based on both functional and nonfunctional attributes of the service, such as the quality of service (QoS) levels. Reputation, a widely acknowledged nonfunctional QoS attribute is currently expressed as the average of user ratings given to the service. However, this expression confines reputation to the subjective perception of the end user and is limited by the lack of an objective representation of performance history. In this paper, we address the need for a reputation mechanism that couples the subjective perception of the end user with the objective view of performance history. To represent performance history, we propose a novel QoS metric termed verity. Verity measures the degree of consistency exhibited by the service provider in delivering the quality levels laid out in the service contract, over a range of previous transactions. We express reputation as a composition of user rating, the compliance levels exhibited by the provider and the verity value. We contend that this reputation expression is a more viable attribute of quality than user rating alone."
455,"With the rapid development of service-oriented computing (SOC) and service-oriented architecture (SOA), the number of services is rapidly increasing. How to organize and manage services effectively in repositories to improve the efficiency of service discovery and composition is important. This paper proposes three categorization rules to classify services for a large scale repository to form a relational taxonomy. The service retrieve scope can be drastically narrowed by this taxonomy. Therefore, the efficiency of service discovery and service composition can be greatly improved. We evaluate and compare the performance of the proposed method and other related ones via a publicly available test set, ICEBE05. The experimental results validate the effectiveness and high efficiency of the proposed one."
456,"In order to satisfy incremental business demands, it is often required to combine functionalities of several services together. A number of approaches for service composition have been therefore proposed in both academic and industrial communities. These approaches, such as BPEL, WSCI, etc. can be categorized into static, manual service composition methods. In addition, by applying Semantic Web technologies, many research works have been investigated to support automatic service composition. Despite of the significant results being achieved, the task of service composition is still a challenging and complex issue. The main reason is that the former approaches require too much detail and technical interventions for defining business processes while the latter approaches are not much scalable for sophisticated applications. In this paper, we will introduce our approach for developing an ontology/language based on the OWL, called OWL-T (T stands for task), which can be used for users describing and specifying formally and semantically their needs at a high-level abstraction, which can be then transformed into executable business processes by underlying systems. The OWL-T aims at facilitating the modeling of complex demands or systems without regarding details of low-level and technical aspects of underlying infrastructure."
457,"Currently, business requirements for rapid operational efficiency, customer responsiveness as well as rapid adaptability are driving the need for ever increasing communication and integration capabilities of the software assets. Service Oriented Architecture (SOA) is generally acknowledged as being a potential solution to expose finely grained pieces of software components on a network that are reusable and composable. Provisioning of business services for different business purposes may require the rapid assembly of their core functionality with different infrastructure capabilities and policies in different contexts. In this paper, the authors propose a SOA based governance model that permits to handle non functional requirements in a dynamic way."
458,"Existing scientific workflow tools, created by computer scientists, require that domain scientists meticulously design their multi-step experiments before analyzing data. However, this is oftentimes contradictory to a domain scientist's routine of conducting research and exploration. This paper presents a novel way to resolve this dispute, in the context of service-oriented science. After scrutinizing how Earth scientists conduct data analytics research in their daily work, a provenance model is developed to record their activities. Reverse-engineering the provenance, a technology is developed to automatically generate workflows for scientists to review and revise, supported by a Petri nets-based workflow verification instrument. In addition, dataset is proposed to be treated as first-class citizen to drive the knowledge sharing and recommendation. A data-centric repository infrastructure is established to catch richer provenance to further facilitate collaboration in the science community. In this way, we aim to revolutionize computer-supported Earth science."
459,"Nowadays, systems are more and more open, distributed and collaborative. In this context, access control is an important issue that should be studied, specified and well enforced. This work proposes a new access control model for collaborative systems: ""PolyOrBAC"". On the one hand, we extend OrBAC (organization-based access control model) to specify local as well as collaboration access control rules; on the other hand, we enforce these security policies by applying Web services mechanisms (XML, SOAP, UDDI and WSDL). Then, we present a representative scenario of secure collaborative applications. Furthermore, we propose a XACML-based implementation of PolyOrBAC and we discuss the most important approaches that emphasize access control in collaborative environments."
460,"Cloud computing environment requires a more open and loosely-coupled service and resource management model. Web Services for Management specification (WS-Management), as an initiative of DMTF organization, can help to manage IT resources cross multiple domains in cloud environment. In this paper, we propose a novel WS-Management-based Cloud-oriented resource management model. We describe the main components of this model. And then, we discuss our management model verification experimental scheme focusing on DASH resource. Finally, we present conclusion and future work."
461,"Service orientation is one of the most popular paradigms for developing modular distributed software systems. In spite of the substantial research effort dedicated to the development of methods and tools to support SOAP-based service-oriented application development, in practice, RESTful services have surpassed SOAP-based services in popularity and adoption, primarily due to the simplicity of their invocation. However, poor adoption of REST specification standards and lack of systematic development tools have given rise to many, more or less compliant, variants of the Restful style constraints, which undermine the evolvability and interoperability of these systems. In this paper, we describe a tool that supports the systematization of RESTful application development, through the use of semi-automatically constructed WADL interface specifications, without compromising the ease of the overall practice. We illustrate the use and advantages of our tool on real-world REST APIs. Additionally, we comment on how REST APIs are documented, especially in comparison to the auto-generated WADLs."
462,"In a composite service for multiple users, users that locate in the different network position are related to network parameters that change dynamically. Therefore, we need a service composition method that can not only handle many user requests, but also adapt to the change of the current network parameters. We use queuing theory and reliability theory to model services, and propose a runtime service composition method. The method obtains multiple service execution paths for each kind of user requests, and chooses the proper candidate service in runtime according to the current network state. The results show that our method is effective and can adapt to the changes of the network parameters."
463,"With the increase of the energy consumption associated with IT systems and services, energy efficiency is becoming a critical concern in the design, development and management of web service systems. In this paper, both the web service selection and server dynamic speed scaling are optimized by maximizing the quality of service (QoS) revenue and minimizing energy costs. Stochastic models of web service systems are proposed, and quantitative analysis of the performance and energy consumption is carried out. In addition, the service selection and speed scaling problem is formulated as a Markov Decision Process (MDP) problem, and algorithms to solve it are introduced. Furthermore, we propose an agent-based optimization framework and design related algorithms to solve the service selection and speed scaling problem in large-scale web service systems. Finally, their effectiveness is validated by simulation results."
464,"Service-oriented computing (SOC) has emerged as the eminent market environment for sharing and reusing service-centric capabilities. The underpinning for an organization's use of SOC techniques is the ability to discover and compose Web services. Although industry approaches to composition have a strong notion of business processes, these approaches largely use syntactic descriptions. As such composition is limited since the true functionality of ambiguous service operations cannot be inferred. Alternatively, academia uses semantic approaches to disambiguate services, but, at the same time, most of these approaches neglect the process rigor needed for complex compositions. In this paper we present a generalized semantics-based technique for automatic service composition that combines the rigor of process-oriented composition with the descriptiveness of semantics. Our generalized approach extends the common practice of linearly linked services by introducing the use of a conditional directed acyclic graph (DAG) where complex interactions, containing control flow, information flow and pre/post conditions, are effectively represented. Furthermore, the composition can be represented semantically as OWL-S documents. Our contributions are applied for automatic workflow generation in context of the currently important bioinformatics domain."
465,"This paper introduces a method for automatic composition of semantic Web services using Linear Logic (LL) theorem proving. The method uses semantic Web service language (DAML-S) for external presentation of Web services, while, internally, the services are presented by extralogical axioms and proofs in LL. We use a process calculus to present the composite service formally. The process calculus is attached to the LL inference rules in the style of type theory. Thus the process model for a composite service can be generated directly from the proof. The subtyping rules that are used for semantic reasoning are presented with LL inference figures. We propose a system architecture where the DAML-S translator, the LL theorem prover and the semantic reasoner can operate together to fulfill the task. This architecture has been implemented in Java."
466,"This paper presents an analytical model of the 3-tiered e-learning system on power line network based on Web services. Power line communication has high influence and is high speed medium that can be utilized as platform for Web based applications such as e-learning. To achieve this idea, we designed a 3-tiered e-learning system on power line network based on Web services and SOAP technology. Data tier is a distributed learning object repository to provide learning contents. Business tier is implemented in MV/MV (63 kV/20 kV) electrical power substations and Presentation tier is implemented in MV/LV (20 kV/400 V) electrical power substations. In this paper, we present analytical model of 3-tiered e-learning on symmetric and asymmetric power line network based on LMS and LCMS services. For this purpose, we use network queuing model and mean value analysis to predict response time of system. We show that to improve the response time of e-learning, the Web and application servers should be selected based on topology and requests of customers to e-learning services."
467,"This paper gives a first insight in an ongoing investigation. Our generic aim is to describe the current situation of available Web services in context with their quality of service (short QoS) behavior. The current quality behavior of offered Web Services should be evaluated in particular as an important aspect of an effective service level management. After providing an introduction to Web service technology, this contribution primarily aims to describe the topic of service level agreements (short SLA) management in such environments. In this context, we will highlight the general contents of SLA agreements, describe the interaction chain for service provision that arises in Web-service-based solutions, and provide a brief explanation of the opportunities offered by the WSLA framework that IBM has developed especially for Web services. Finally we describe the conception, prototypical development and the possible application of a measurement service for monitoring the quality behavior of a specific Web service. This measurement service can be used to support service level agreements."
468,"In the past decade, multi-robot simultaneous localization and mapping (SLAM) has been widely studied. However, the problem of collaborative SLAM with a large number of robots, such as dozens of robots, is far from being well solved. The challenges stem from not only the computation complexity in large-scale map merging but also the inefficiency to enable the parallel computing in this process, which is indispensable for us to make avail of the frontier of computing technology such as powerful cloud infrastructure. To effectively address these challenges, especially the latter one, we propose a scalable and real-time multi-robot visual SLAM framework based on the cloud robotic paradigm. The prominent feature of our framework is that it can distribute the SLAM process to multiple computing hosts in a cluster, which enables map building in parallel. To eliminate the bottleneck from data sharing between different sub-tasks, we also introduce diversified messaging pattern for various messaging scenarios, as well as the consistency policies for map data. The evaluations on the prototype of our framework, have shown that our method can do support as many as 256 robot entities simultaneously, without any compromising on the precision of poses estimation and map building."
469,"This article proposes to identify and recommend scientific workflows to promote their reuse and repurposing. Specifically, a scientific workflow is converted into a layer hierarchy, which specifies hierarchical relations between this workflow, its sub-workflows, and activities. Semantic similarity is calculated between layer hierarchies of workflows in order to construct a scientific workflow network model. A graph-skeleton based clustering method is adopted for grouping layer hierarchies into clusters. Barycenters in clusters are identified for facilitating cluster identification and workflow ranking and recommendation. Experimental result shows that this technique is efficient and accurate on ranking and recommending appropriate clusters and scientific workflows."
470,"Accurate capacity measurement of Internet services is critical to ensure high-performing production computing environments. In this work, we present our solution of performing accurate capacity measurement. Referred to as ""Redliner"", it uses live traffic in production environments to drive the measurement, hence avoiding many pitfalls that prevent capacity measurement from obtaining accurate values in synthetic lab environment. Redliner works by intelligently redirecting a portion of production traffic to the SUT (Service Under Test) and realtime analyzing the performance. It has been adopted by hundreds of services inside LinkedIn and is executed for various types of capacity analysis on a daily basis."
471,"Businesses are changing rapidly and organizations tend to act worldwide and are increasingly becoming distributed over the continents. As a consequence, distributed software systems have to keep track with rapidly changing markets. Business rules provide support for capturing some knowledge that changes frequently. Current business rule systems manage and execute business rules, however, typically lack support for increasingly distributed software systems, in particular, with respect to flexibility and reuse of business rules across distributed rule engines. In this paper we propose a service-oriented distributed business rules system that manages and deploys business rules to various business rule engines. Furthermore, we present the design and some implementation aspects of a service-oriented business rules system based on WS-coordination. The system supports management and deployment of business rules to various business rules engines. Furthermore, we present a framework that unifies the access to several heterogeneous business rules engines, and we propose a solution that automatically generates and provisions Web services for executing business rules managed by a business rule engine."
472,"In this paper, we propose to use communities as a mean to organize Web services registries in a multi-registry environment. First, we propose a semantic model for Web services registry description (WSRD). A WSRD description depicts the functionalities offered by services advertised by a given registry. Thereafter, we propose an implicit approach for building communities based on the WSRD descriptions using a fuzzy clustering technique. Eventually, this clustering will be helpful for selecting an adequate registry for service requesters. Provided experiments in this paper show the feasibility of our approach."
473,"Selection of existing services for agile Web application design offers many advantages, namely, the availability of widespread solutions in the form of data and services shared over the Web, and reduced development costs. In this paper, we propose a framework to support service selection, where a Web application developer takes into account choices made by other developers to learn from them in selecting suitable services. To this aim, we weight as more important the choices made by developers who present a larger number of followers in a network of social relationships. Such a network is used to compute developers' rank to support service selection."
474,"In the literature, there exist several approaches for monitoring the execution of BPEL processes. They concentrate on different properties, adopt different languages, work at different levels of abstraction, and assume different perspectives. Even if the field is rather new, we do not think that this diversity is a limitation of current solutions; rather it is intrinsic in the problem itself. We claim that, instead of working on the definition of the ultimate approach for BPEL monitoring, we should push a cooperative approach based on the integration of different solutions.In this paper, we present a first step in this direction, and describe a monitoring framework which is obtained by integrating two well-known approaches, namely Dynamo and Astro. This integration, which happens both for the language used for expressing the properties to be monitored, and for the architecture of the monitoring framework, allows to combine the advantages of the two approaches and to obtain a general, comprehensive solutions for BPEL monitoring."
475,"The business process execution language (BPEL) is a language to orchestrate web services into a single business process. In a choreography view, several processes are interconnected and their interaction behavior is described from a global perspective. This paper shows how BPEL can be extended for defining choreographies. The proposed extensions (BPEL4Chor) distinguish between three aspects: (i) participant behavior descriptions, i.e. control flow dependencies in each participant, (ii) the participant topology, i.e. the existing participants and their interconnection using message links and (iii) participant groundings, i.e. concrete configurations for data formats and port types. As BPEL itself is used unchanged, the extensions facilitate a seamless integration between service choreographies and orchestrations. The suitability of the extensions is validated by assessing their support for the Service Interaction Patterns."
476,"The existing distributed ontology evolution approaches are not scaled to dynamic environments like Semantic service architecture (SSOA). As the SSOA-based system grows in size, the complexity of ontology change management increases, especially if the services ontologies are heterogeneous. In this paper, a novel agent-based ontology evolution framework is developed for services which consume ontologies in semantic SOA-based applications. A prototype is built by using the JADE agent platform for evaluation."
478,"With the increase of published Web services, it has become a great challenge to recommend service consumers the best services with regard to the quality of services (QoS). Collaborative filtering is often employed to predict the QoS of a specific service to a certain consumer. However, in existing collaborative filtering based service recommendation approaches, the context under which consumers submit a recommendation request is seldom taken into account when filtering similar recommenders and their corresponding experience. In this paper, we propose a new method dubbed CASR (Context-Aware Services Recommendation) by referring to previous service invocation experiences under similar context with the current consumer, which is of great importance in the personalized service recommendation system. First, the proposed algorithm clusters the service invocation records according to the similarity on context properties and selects the cluster that is most similar to the context of current consumer. Then it predicts the QoS of an unused service for current consumer based on the filtered recommendation records by Bayesian inference. Experimental results demonstrate that the proposed approach can significantly improve the accuracy of QoS prediction and service recommendation."
479,"This paper presents a framework that adapts the conventional home electric appliances with the infrared remote controls (legacy appliances) to the emerging home network system (HNS). The proposed method extensively uses the concept of service-oriented architecture to improve programmable interoperability among multi-vendor appliances. We first prepare APIs that assist a PC to send infrared signals to the appliances. We then aggregate the APIs within self-contained service components, so that each of the component achieves a logical feature independent of device(or vendor)-specific operations. The service components are finally exported to the HNS as Web services. Thus, the legacy appliances can be used as distributed components with open interfaces. To demonstrate the effectiveness, we also implement an actual HNS and integrated services with multi-vendor legacy appliances"
481,"With the realisation of the potential benefits of business process automation, many standards, best practices and technologies have evolved to model and execute business processes. The emerging Web services technology provides great flexibility for the development of cross platform, service oriented applications. This paper addresses the utilisation of the Web services technology for business process enactment and discusses a standardised architecture for Web service based business process execution. As a key objective it highlights the need for standardisation in Web service based business processes execution. In elaborating on the above need, this paper defines the development of a standard architecture to address key concerns such as service invocation, integration, transaction management, security and resource management in using the Web services technology for business process execution."
482,"Web services help in achieving increased automation across organizational boundaries. In this paper, we present an approach for annotating WSDL documents with semantically rich descriptions. We also present an algorithm that considers such annotations in addition to just the types of input and output parameters. Our matchmaking algorithm not only returns match/no-match answers but in case of a match a set of conditions under which a Web service offers the desired functionality"
483,"This article describes a systematic approach to testing behavioural aspects of Web Services that communicate using the JSON data format. As a key component, the Quviq QuickCheck property-based testing tool is used to automatically generate a large number of test cases from an abstract description of the service behaviour in the form of a finite state machine. The same behavioural description is also used to decide whether the execution of a test case is successful or not. To generate random JSON data for populating tests we have developed a new library, jsongen, which given a characterisation of the JSON data as a JSON schema, automatically derives a QuickCheck generator which is capable of generating an infinite number of JSON values that validate against the schema."
484,"With the emergence of Service Component Architecture (SCA), all interests were focused on representing this architecture in a formal way in order to be able to prevent the specifications failures. In this context, our recent works were interested in formalizing structural properties of the SCA specifications, particularly in defining structural compatibility between connected services. In fact, verifying structural compatibility is necessary but not sufficient. In this paper we intend to represent, in a first step, the SCA behavioral properties by means of Event-B invariants and events. In a second step, we established behavioral compatibility between services interacting together which is considered as a delicate task and has a great importance in guaranteeing reliable communication between services. The consistency and the validity of the obtained model have been proved by the Event-B dedicated tools."
485,"Context awareness in Web services is gaining momentum. Since it is not a trivial task, it suffers from lack of a general solution. In this paper, we introduce a novel approach for context-aware semantic Web services which is applicable for any environment. It is established based on composition of context provider Web services and other context-aware semantic Web services. In addition, an extended version of the semantic Web service ontology language for semantic Web services is introduced, in order to make it possible to find appropriate context-aware semantic Web services based on available context information. So as to make it applicable for any environment, the solution does not hold any assumption about the user's context source. Rather, the property of context level is defined."
487,"With the IoT technology increasing and aging social have coming, personalized service assisted elder and patient living is a critical application in IoT-Based Healthcare application. However, the scale and complexity of personalized service is increasing with wildly applied to our life, which cause response time decrease and resource waste in large-scale IoT-Based Healthcare application. Therefore, it is necessary of studying on dealing with the large-scale and complexity of personalized services in large-scale IoT-Based Healthcare application. In this paper, we propose proactive personalized service leveraging Complex Event Processing (CEP) to deal with a large number and complexity of personalized services. Firstly, personalized service defined as complex event pattern that expresses in the form of Directed Acyclic Graph (DAG). Secondly, we propose a complex event pattern partitioning and clustering algorithms to optimize the processing of dealing with personalized services. Finally, we realize a prototype system based on proposed our approach named BCEPCare. Experiment result shows that BCEPCare is superior to the traditional ESPER in large-scale IoT-Based healthcare application."
488,This paper proposes a document based framework for the modeling of web-based choreographies involving a tight combination of workflow and data management. Our starting point is Active XML proposed by S. Abiteboul - AXML documents are XML documents with embedded service calls. We enhance Active XML with a rich notion of interface and we propose an effective technique to decide if provided services and needs of callers (defined as interfaces) are compatible. We also explicitly take distribution into account and allow for the composition of distributed AXML systems.
489,"Despite the great momentum gained about the testing, analysis and verification of BPEL process, little attention has paid to the debugging issues, especially about the building of ad hoc debuggers. In this paper, we propose and implement RBDB (reversible BPEL debugger), a specially made reversible debugger for BPEL process. RBDB is built on the abstract debugging APIs to fulfill its functionality. A reversible concurrent debugging model and three strategies to handle different type of external Web services are introduced later. Finally, a comprehensive analysis of experiment data are presented. Evaluation results demonstrate that RBDB can improve users' efficiency significantly and decrease the invoking times of external services substantially."
490,"The Web is moving toward a collection of interoperating Web services. Achieving this interoperability requires dynamic discovery of Web services on the basis of their capabilities. The capability of a service can be properly determined by using not only its functional description (or service interface), but also its quality attributes as judged by previous users of the service. We develop a service repository that extends UDDI registries. This repository combines an ontology of attributes with evaluation data. We base our repository on a new query and manipulation language based on DAML. Our language includes support for a rich set of operations, which are needed to maintain an attribute ontology, publish services, rate services, and select services based on their functional attributes as well as evaluations by others. We have implemented our approach and evaluated its practical completeness via a number of key query and manipulation templates."
491,"Service directories are a key component of distributed systems where shared information must be managed efficiently. For a directory with a large numbers of entries, the result set of a query may be large, too. In this case, it is important to order the results according to heuristics and to retrieve them incrementally. Our contribution is an integrated directory system specially adapted to large-scale service discovery and composition. We introduce DirQL, a flexible query language for the matching and ranking of service descriptions. As results are incrementally retrieved, our system is able to lazily compute the result set based on: 1) the organization of the directory as a special balanced search tree that has an extra ""intersection"" discriminator, 2) a scheme for transforming the original query into one taking into account the tree structure of the directory, and 3) the organization of partial results in a heap structure sorted according to the transformed query. We also report on experimental results regarding the usage of the directory by a composition engine solving randomly generated problems."
492,"A combination of factors: expanding user bases, the ubiquity of mobile communications, and newer technologies such as cloud computing and multi-core computing, are pushing todays systems to grow larger and larger. With their loosely coupled nature, distributed messaging systems often play a key role in such architectures. However, just like other parts of the architecture, those messaging systems also need to scale up, and they need to do so in three dimensions: quantity of messages, number of users, and size of messages. Although most current systems handle the first two dimensions, few of them efficiently support the third dimension. This paper proposes a novel method to implement a scalable and persistent broker that supports a publish/subscribe model and distributed queues using a NoSQL database and a coordination framework. We will discuss the design that uses recent advances in scalable database management and distributed coordination middleware, and we will compare the proposed models with other distributed message brokers."
493,This paper describes a framework supporting the runtime monitoring of requirements for systems implemented as compositions of Web-services specified in BPEL. The requirements that can be monitored are specified in event calculus. The paper presents an overview of the framework and describes the architecture and implementation of a tool that we have developed to operationalise it. It also presents the results of a preliminary experimental evaluation of the framework.
494,"Protocol-level mismatch is one of the most important problems in service composition. The commonly used reachability exploration method focuses on verifying deadlock-freeness. When this property is violated, the states and traces in the reachability graph only give clues to re-design the composition. The process must then repeat itself until no deadlock is found. In this paper, multiple Web service interaction is modeled with a Petri net called composition net (C-net). The protocol-level mismatch problem is transformed into the deadlock structure problem of a C-net. If mismatches are found, a solution based on Petri net siphons is proposed. The proposed method is shown to achieve higher efficiency for resolving protocol-level mismatching issues than traditional ones do."
495,"Web services are a popular way of implementing a Service-Oriented Architecture (SOA), which has gained rapid adoption and support from leading companies in industry. Testing can be used to help assure both the corectness and robustness of a web service. Because manual testing is tedious, tools are needed to automate test generation and execution for web services. This paper presents a framework and its supporting tool for automaically generating and executing web-service requests and analyzing the subsequent request-response pairs. Given a service provider's Web Service Description Language (WSDL) specification, we first automatically generate neessary Java code to implement a client (service requestor). We then leverage automated unit test generation tools for Java to generate unit tests (including extreme, special, and random input values), and execute the generated unit tests, which in turn invoke the service under test. Finally we an alyze the large number of request-response pairs from the web service invocation and identify robustness problems. We have applied our framework to freely available web services and our experiences show that we can quickly gen erate and execute web-service requests that may reveal robustness problems with no knowledge of the underlying web service implementation."
496,"Key-word based researches of service discovery focus on direct match of userpsilas requirements and often neglect relations between services. While techniques based on conventional semantic offer many kinds of relations, considerable time is spent on reasoning. In this paper, we utilizes E-FCM (Element Fuzzy Cognitive Map) to describe services for the reason that E-FCM can keep the semantic information as much as possible and E-FCMs can be automatically created for web services. Furthermore, instead of reasoning, the semantic relations among E-FCM are built based on computation, therefore semantic relations among services can be found out quickly. We focus on the associated semantic relations among services because complex applications always comprise of services with associated functions. The associated link network (ALN) is constructed upon associated relations to generate associated web service flows, which can be used to create complex applications, thus to facilitate discovery efficiency and improve utilization of services."
497,"The modern data economy, which has been described as ""Big Data"", has changed the status quo on digital content creation and storage. While data storage has followed the schema-dictated approach for decades, the recent nature of digital content, which is widely unstructured, creates the need to adopt different storage techniques. Thus, the NoSQL database systems have been proposed to accommodate most of the content being generated today. One of such NoSQL databases that have received significant enterprise adoption is the document-append style storage. The emerging concern and challenge however is that, research and tools that can aid data mining processes from such NoSQL databases is generally lacking. Even though document-append style storages allow data accessibility as Web services and over URL/I, building a corresponding data mining tool deviates from the underlying techniques governing web crawlers. Also, existing data mining tools that have been designed for schema-based storages (e.g., RDBMS) are misfits. Hence, our goal in this work is to design a unique data analytics tool that enables knowledge discovery through information retrieval from document-append style storage. The tool is algorithmically built on the inference-based Apriori, which aids us to achieve optimization of the search duration. Preliminary test results of the proposed tool also show high accuracy in comparison to other approaches that were previously proposed."
498,"Web service recommendation in an efficient and accurate manner has become a significant tool with information overload and an increasingly urgent demand to provide appropriate recommendations to users. Among the service recommendation algorithms, Collaborative Filtering (CF) gives credence to user inputs by comparing user's correlations. Performance of the service recommendation approaches becomes deficient due to the data sparsity and cold-start issues, which make the incomplete and inadequate information to analyze a user predicament on Web services. This paper proposes a CF-based recommendation approach that first alleviates the sparsity problem using a novel ontology-based clustering approach that used domain specificity and service similarity for the ontology generation. Then, we propose a trustbased user rating prediction by determining the trust value between users by calculating the correlation of users. The experimental results indicate that the proposed approach can effectively alleviate the sparsity and cold-start problems by lower prediction error compared with existing sparsity managing mechanisms in service recommendations."
499,"The idea to build systems based on services, by reusing and combining software made available independently via different technologies and channels and dynamically organizing them into coherent processes, has a very well recognized potential. Achieving this potential stands crucially on the ability to recognize and exploit the context in which such applications operate, in terms of the available services, of the actual setup at run-time, and of the interaction with human actors. Changing contexts require run-time reaction, by adapting the ongoing overall process enacted by the service-based system to unexpected deviations. Continuous context-aware adaptation is hencefore a strong requirement in this setting. In this paper, we propose a service delivery platform designed for the continuous context-aware adaptation of service-based systems, based on the idea of monitoring the underlying context via a hierarchical model, and using the context to drive the choice and execution of adaptive pervasive flows. We describe the approach through an example from the smart cities e-mobility domain."
500,"In order to meet the dynamic business environment, the alignment of business services and IT services has long been a challenge to both business and IT people. In this paper, a graph centric, two-phase analysis and design method for service system is proposed which is based on the recent evolutions in these fields like service oriented architecture (SOA). The method proposed in this paper views a service system comprising of business services and IT services as a layered complex system and uses matrices to represent the interactions between different layers. The method could help decision makers to analyze the impact of service changes during service design period. This could also facilitate the design of a more flexible IT service system reacting to business changes. The aim of this paper is to seek balance among different parts of a service system, and to provide a holistic view of the system for different role players. Results from this paper could give service designers some insights on how to design an enterprise service bus (ESB) in the SOA architecture of an enterprise"
501,"Software as a Service (SaaS) is an increasingly important service delivery model in cloud computing, and multitenancy makes it possible to support large scale customized tenants with only one code base. However, the complexity of multi-tenant architecture may lead to poor performance and low resource utilization. The customized demands may also lead to high operating cost. It is very important to develop an accurate model to predict the performance of the multi-tenant SaaS. To this end, a multi-tenant queueing network model is developed. Based on the model, a balanced SLA-aware tenant placement algorithm is proposed considering that customized tenants may need more resources to be placed together. The algorithm is effective in nearly 90% of the simulations comparing with other heuristic algorithms. Furthermore, the optimization problem on dynamic resource provision to minimize the operating cost is studied. As the original optimization problem is NPhard, a continuous upper bound is used to convert the original optimization problem into a convex optimization which can be solved efficiently in polynomial time. Finally, it is demonstrated that the approximate ratio of the proposed approach is no greater than 1.2 in more than 90% of the simulations."
502,"Many enterprises have a growing interest in service composition to construct their business applications. With the increase of alternative services, Quality of Service (QoS) becomes an important indicator of obtaining optimal composite services. Due to the dynamic nature of the service environment, a composite service may not guarantee to deliver an overall optimal QoS. Re-optimization approaches have been developed to handle a dynamic environment. However, these approaches do not consider the diversity of alternative solutions, which may lead to better solutions. In this work, we introduce an adaptive approach, called estimation of distribution algorithm based on Restricted Boltzmann Machine (rEDA). rEDA effectively maintains the diversity of alternative solutions, by leveraging the inference ability of Restricted Boltzmann Machine to capture the potential solutions. It also provides a predictive guidance for the exploration of solution space, by considering the degree of how well a service contributes to the global QoS. The experimental evaluation shows that rEDA has a significant improvement on effectiveness and efficiency over existing approaches."
503,Mapping XML document schemas and Web service interfaces to programming languages has an important role in effective creation of quality Web service implementations. This paper presents a novel way to map XML data to the C++ programming language with emphasis on use in mobile and embedded systems. The proposed solution offers more flexibility and more compact code that is critical in embedded environments. The paper describes the concept and the architecture of the solution and compares it with existing solutions
504,"Web services following distributed object computing technology like DCOM, CORBA provides remote procedure call mechanism based on XML-based open standard such as SOAP, WSDL, and UDDI, and it is spotlighted as means of integration and collaboration at e-business. Especially, UDDI is the Web services registry enabling to register and search Web services that takes charge of providing infrastructure for Web services. However, the existing UDDI has a few problems that searching process is very simple and it cannot provide information of Web services quality and quality-based retrieval. Therefore, this study suggests improved UDDI model that evaluates the Web services quality and use this information for searching."
505,"In this paper, we examine the online authentication method based on Merkle (hash) tree focusing on its reliability. Coming from side channels in online authentication, the effectiveness runs into danger in the long run. With consideration of effectiveness, we present a Merkle tree based online authentication resilient against side channels by obfuscating authentication proofs. Security and efficiency are analyzed to demonstrate the practicality of the proposed approach."
506,"There is a growing need for a new type of WS-*/SOA standards that could facilitate hierarchical, object-oriented composition of client-side executable code. This is especially true for the sorts of client-side logic embedded in AJAX and rich Internet applications, virtual worlds and MMORPGs; code that deals with issuing requests to servers, processing their responses, rendering UI, interacting with users, and processing asynchronous events from other client nodes. The paper offers an analysis of client-side composition patterns, a brief explanation why they lack adequate support from the existing web technologies, and design guidelines for client-side component integration environments to follow. The proposed guidelines have been successfully implemented in a prototype system. Our analysis is thus strongly rooted in reality; it is based on real experiences with concrete application scenarios. The paper concludes by highlighting the key architectural aspects of our implementation with respect to the principles listed earlier."
507,"Time constrained service composition raises several problems. Researches on QoS-driven service composition provide some preliminary solutions, but there are still some unsolved issues, which can be attributed to the following reasons: (1) the huge time consumption of inter-domain validation, (2) the dynamic execution time of services and (3) the difficulty in defining time constraint due to the opaque feature of composite services. In this paper, we propose a novel service composition algorithm, which models the service composition as multi-domain scheduling problem with minimal service resources and time constraint. Each service is modeled as an exclusive resource during its execution period. By computing the inter-domain communications and available services in each domain, the domain with optimal utilization rate is obtained to arrange services. Meanwhile, loop parallelization is adopted when a service cannot be executed on schedule. Moreover, redundant services of the initial composition are further optimized. Our experiment results show that our approach can effectively achieve service composition with time constraint."
508,"With the popularity of SOA, SOA policy becomes one of the core technical enablers for SOA governance and management. Different in several aspects from traditional policy for distributed system management, policy applied in SOA solutions needs to take into account various policy types and enforcement points in different layers of SOA solution stack and different phases of SOA lifecycle. As well, it has the unique requirements on compliance to match existing SOA technologies and characteristics, as simplicity, standardization, high performance, etc. In this paper, a novel context model based SOA policy management framework by innovatively extending W3C Service Modeling Language (SML) and ISO Schematron is introduced. Firstly, a common context model distilled from SOA policy types typically including service policy, service governance policy, application policy and business policy, is presented. Then, the core components - context model based policy engine and definition tool are described. Finally, it is illustrated how the unified definition tools and policy engine are manipulated within the context model based SOA policy framework to manage and enforce policies in typical scenarios as service meta-data management, service match making and business process management. Based on the project, we participated in the works for defining W3C SML V1.1 working draft and proposed the works introduced in this paper to W3C SML Working Group. This paper demonstrates how these technologies and architectures significantly enhance the capability of SOA governance and management throughout whole SOA lifecycle and spanning the layers of SOA solution stack."
510,"In this paper we describe the implementation of multiversion concurrency control on the generalized search tree (GiST), an index structure introduced by Hellerstein. For large-scale service directories, the need arises for a data storage system capable of handling substantial amounts of multidimensional data efficiently, as well as being able to support queries which are natural to the type of data stored in the directory. The GiST is an indexing structure that lends itself particularly well to this type of application. However, the solutions that have been proposed to address concurrency control on the GiST do not meet the requirements of large-scale service directories. The solution proposed here optimizes towards highly concurrent read accesses that are far more frequent than updates to the stored data."
511,"The advent of the Smart City domain has led to the creation of massive amounts of diverse data. Stakeholders in this domain need to be able to analyze this data in order to make informed planning decisions. To address this complex task, Distributed Analytical Environments (DAEs) have emerged. These environments consist of different distributed analytical and data services, which are composed in a dynamic way to deliver insights that are crucial for stakeholders. Since these environments deal with business critical and sensitive information, strict compliance constraints apply. These constraints lead to situations where certain concrete services are not allowed to exchange data, even though their interaction is necessary to produce the desired results. Finding a valid solution in the space of possible instantiations is a non-trivial problem. In this paper we introduce Nomads, a framework that enables service mobility in such constrained dynamic composition environments to overcome aforementioned restrictions. The framework improves the overall satisfiability and therefore also the quality of constrained DAEs. We outline the requirements of a representative DAE scenario, provide a detailed problem formulation, and then discuss the service mobility framework along with our solution finding algorithm. The evaluation demonstrates that the Nomads framework considerably increases the number of successfully performed compositions even in highly constrained environments."
512,"We envision users discovering suitable Web objects and configuring them on-the-fly with their desired high-level application logic, with the programming and deployment carried out entirely on the Web. Easy configurability and interplay of Web entities implies evolution of a few common sense, yet powerful set of core primitives for effective coordination, akin in simplicity and strength to the HTTP protocol. Current Web services technology lacks Infrastructure support, theoretical sound fundamental framework for Web services coordination and composition, and easy use tools for Web application development. Our Web coordination bond system gears towards finding solutions to aforementioned research challenges."
513,"Automated Ranking is crucial in the process of automated Web Services execution. Often adaptation and ranking (used interchangeably) of discovered Web services is carried out using functional and non-functional information of Web Services. Existing approaches are either found to be only focusing on semantic modeling and representation only, or using data mining and machine learning based approaches on unstructured and raw data to perform discovery and ranking. We propose an approach to allow semantically formalized representation of logs during Web Service execution and then use such logs to perform ranking and adaptation of discovered Web Services. We have built Semantic FP-Tree based technique to perform association rule learning on functional and non-functional characteristics of Web Services. The process of automated execution of Web Services is improved in two steps, i.e., (1) we provide semantically formalized logs that maintain well-structured and formalized information about past interactions of Services Consumers and Web Services, (2) we perform an extended association rule mining on semantically formalized logs to find out any possible correlation in functional and non-functional characteristics of Web Services during past execution which is then used in automated ranking and adaptation of Web Services."
515,"Existing ridesharing services have focused on on-demand trip matching, which resembles traditional taxi dispatching. This may encourage more private vehicles on the road, which aggravate traffic congestions in peak hours rather than alleviating them. We propose CommuteShare, a novel ridesharing service for daily commuters that encourages long-term ridesharing among commuters with similar commuting patterns, to increase the traffic efficiency in peak hours. We first identify commuting private vehicles (CPVs) from traffic records and model their commuting patterns. We then design a dynamic model to formulate the intention level of a CPV driver to offer a ride based on the spatio-temporal convenience and dynamic traffic conditions. Based on the commuting patterns of the CPVs and the dynamic model of the CPV drivers, we propose a ridesharing algorithm to compute ridesharing matches among CPVs. We perform extensive experiments on three real-world cross-domain urban big datasets from a major city of China. Experimental results show that, using the proposed CommuteShare service, over 5,300 private vehicles can be reduced daily on average during morning peak hours, with a reduction of 7-minute average waiting time for the riders."
516,"Recent developments in daily business life indicate demand for facilitating process execution on various environments, including mobile devices with limited capabilities. This paper summarizes our current research on extending service oriented architecture to lightweight mobile devices. Status of necessary enabling technologies, programming interfaces, and the supporting device base was surveyed. Availability of both mobile consumption and provision of services are reviewed. Based on our research and concept demonstration, we conclude that full-fledged mobile participation in SOA is not far in the future, and that partial implementations can already be built."
517,"In the service-oriented paradigm web service interfaces are considered contracts between web service subscribers and providers. However, these interfaces are continuously evolving over time to satisfy changes in the requirements and to fix bugs. Changes in a web service interface typically affect the systems of its subscribers. Therefore, it is essential for subscribers to recognize which types of changes occur in a web service interface in order to analyze the impact on his/her systems. In this paper we propose a tool called WSDLDiff to extract fine-grained changes from subsequent versions of a web service interface defined in WSDL. In contrast to existing approaches, WSDLDiff takes into account the syntax of WSDL and extracts the WSDL elements affected by changes and the types of changes. With WSDLDiff we performed a study aimed at analyzing the evolution of web services using the fine-grained changes extracted from the subsequent versions of four real world WSDL interfaces. The results of our study show that the analysis of the fine-grained changes helps web service subscribers to highlight the most frequent types of changes affecting a WSDL interface. This information can be relevant for web service subscribers who want to assess the risk associated to the usage of web services and to subscribe to the most stable ones."
518,"Service selection has been a critical concern for constructing a distributed service-based system. So far, research efforts on service selection are mainly based on the criteria of functionality and QoS (Quality of Service). However, the credit of Web service represents the reputation and importance of a Web service, which also provides a key criterion for selecting service. Then the challenge is how to measure the credit of Web service. In this paper, we propose a method for measuring the credit of service through service network. First, we propose a model for constructing exact service network, which is effectively applied to different type of services like WSDL and Web API. Next, an approach is proposed to depict the credit of service from three aspects: popularity, influence and authority. We use about 15000 services from Seekda and ProgrammableWeb in the experiments, the results show that our approach can rank the credit of service effectively, and play a positive role in measuring the credit of a service."
519,"Increasing automation requires open, distributed, service-oriented systems capable of multicriteria-driven, dynamic adaptation for appropriate response to changing operating conditions. We combine a simple architecture with a novel algorithm to enable openness, distribution, and multi-criteria-driven service composition at runtime. The service-oriented architecture involves mediator Web services coordinating other Web services into compositions necessary to fulfil user requests. By basing mediator services' behavior on a novel multicriteria-driven (including quality of service, deadline, reputation, cost, and user preferences) reinforcement learning algorithm, which integrates the exploitation of acquired knowledge with optimal, undirected, continual exploration, we ensure that the system is responsive to changes in the availability of Web services. The reported experiments indicate the algorithm behaves as expected and outperforms two standard approaches."
520,"Workflows have been widely used to coordinate business activities, software components and services. By making use of cloud resources, workflow systems can be deployed as common services to process a large quantity of workflow execution requests from various users. A challenge for the workflow service is that it should be elastic, i.e., to scale up or scale down to meet the requirements of processing dynamic and changing workloads efficiently while not wasting resources. To address this issue, we implemented an Elastic Workflow Service (EWS). Specifically, we propose a pattern prediction-based strategy in EWS which can dynamically deploy or undeploy components onto cloud resources according to the workload levels. The experimental results show that our configuration strategy has better performance when compared against other strategies."
521,Provides a listing of current committee members.
522,"Service-Oriented Architectures (SOAs) successfully evolve over time to update existing exposed features to the users and fix possible bugs. This evolution process may have a negative impact on the design quality of Web services. Recent studies addressed the problem of Web service antipatterns detection (bad design practices). To the best of our knowledge, these studies focused only on the use of metrics extracted from the implementation details (source code) of the interface and the services. However, the quality of service (QoS) metrics, widely used to evaluate the overall performance, are never used in the context of Web service antipatterns detection. We start, in this work, from the hypothesis that these bad design practices may impact several QoS metrics such as the response time. Furthermore, the source code metrics of services may not be always available. Without the consideration of these QoS metrics, the current detection processes of antipatterns will still lack the integration of symptoms that could be extracted from the usage of services. In this paper, we propose an automated approach to generate Web service defect detection rules that consider not only the code/interface level metrics but also the quality of service attributes. Through multi-objective optimization, the proposed approach generates solutions (detection rules) that maximize the coverage of antipattern examples and minimize the coverage of well-designed service examples. An empirical validation is performed with eight different common types of Web design defects to evaluate our approach. We compared our results with three other state of the art techniques which are not using QoS metrics. The statistical analysis of the obtained results confirm that our approach outperforms other techniques and generates detection rules that are more meaningful from the services' user perspective."
523,"Detecting difference between process models is important for many business process management scenarios, such as process version control and process merging. However, it is far from trivial to detect the process difference. Existing work suffers from drawbacks like inappropriate data structure support or expensive computation. In this paper, we propose FB-Diff, a feature-based difference detection approach. Firstly, a semi-ordered tree model called task based process structure tree (TPST) is used to represent a process model, which can correctly describe the structure as well as the behavior (the execution sequence of task nodes). Then FB-Diff adopts a divide and conquer strategy to find the similar parts of two TPSTs. Specifically, we divide the TPST into fragments that are represented by feature vectors. A feature vector consists of six features, and each feature describes a key characteristic of the fragment. Based on the similar parts, the edit script that can transform one TPST into the other is generated. The extensive experimental evaluation shows that our method can meet the real requirements in terms of precision and efficiency."
524,"In a distributed Web service integration environment, the selection of Web services should be based on their reputation and quality-of-service (QoS). Various trust models for web services have been proposed to evaluate the reputation of Web services/service providers. Current mechanisms are based on tracing the feedbacks to the past behaviors of Web services. However, very few of them consider the robustness and attack-resiliency of the trust models. In this paper, we present an attack resilient distributed trust management system in a Web service management environment. The proposed attack resilient trust model uses two vectors to capture the behavior and the trustworthiness of a Web service/service provider based on our analysis on the possible attacks against the trust models. We also present a set of experiments that show the effectiveness of our trust model in detecting malicious behavior of service providers."
525,"The purpose of the XML Key Management Specification (XKMS) is to facilitate the use of a Public Key Infrastructure (PKI) by transferring the complexity associated with PKI to a trusted Web Service. Although this specification contains information on how compatibility with PKIs such as PGP and SPKI/SDSI can be reached, it is straight focused on X.509 PKI. This work uses XKMS to define a federated management model for SPKI/SDSI. In a web of SPKI Federations, the proposed model follows a peer-to-peer approach for discovering and establishing certificate chains. This model introduces a search algorithm and its effectiveness was verified through simulations."
526,"Based on real development experience, the paper presents a collection of design techniques for building enterprise web services. By applying the techniques to web services development, not only the development increases reusability and productivity, but also the web services improve agility and compatibility.Enterprise web services require high grade of competency in designing web service contracts. A contract of web service formalizes an agreement between web service provider and consumer, in the forms of WSDLs, service schemas and policies. Though contract-first method provides great potential of directly dealing with the contracts, and a number of articles have been published regarding designing WS and XML schemas, however it is still hard for developers to find cookbooks or guidelines concentrated on designing web service contracts with contract-first method. To fill the gap, a set of design techniques are introduced and deployed in practice, incorporating some best practices scattered over the web services community. These techniques cover most of the key aspects of web service, including consolidating service schemas in line with business entities, constructing coarse-grained namespaces, applying versioning over WSDLs and service schemas, and writing fine-grained filters with contracts."
527,"With the development of Service Computing and Big Data research, more and more heterogeneous data generated in the process of Service Computing attracts our attention. Combining correlated data sources may help improve the performance of a given task. For example, in service recommendation, one can combine (1) user profile data (e.g. Genders, age, etc.), (2) user log data (e.g., Click through data, service invocation records, etc.), (3) QoS data (e.g. Response time, cost, etc.), (4) service functional description (e.g., Service name, WSDL document, etc.) and (5) service tagging data (i.e., Tags annotated by users) to build a recommendation model. All these data sources provide informative but heterogeneous features. For instance, user profile and QoS data usually have nominal features reflecting users' background and services' qualities, log data provides term-based features about users' historical behaviors, and service functional description and tagging data have term-based features reflecting services' functionalities and users' collective opinions. Given multiple heterogeneous data sources, one important challenge is to find a unified feature subspace to capture the knowledge from all data sources. To handle this problem, in this paper, we propose a Heterogeneous Feature Selection framework, named as WS-HFS, in which the consensus and the weight of different sources are both considered. Moreover, we apply the proposed framework to Web service clustering as a case study, and compare it with the state of the art approaches. The comprehensive experiments based on real data demonstrate the effectiveness of WS-HFS."
528,"Runtime monitoring of Web service compositions with WS-BPEL has been widely acknowledged as a significant approach to understand and guarantee the quality of services. However, most existing monitoring technologies only track patterns related to the execution of an individual process. As a result, the possible inconsistency failure caused by implicit interactions among concurrent process instances cannot be detected. To address this issue, this paper proposes an approach to specify the behavior properties related to shared resources for web service compositions and verify their consistency with the aid of a parametric stateful aspect extension to WS-BPEL. Parameters are introduced in pattern specification, which allows monitoring not only events but also their values bound to the parameters at runtime to keep track of data flow among concurrent process instances. An efficient implementation is also provided to reduce the runtime overhead of monitoring and event observation. Our experiments show that the proposed approach is promising."
529,"Web service interactions lie in the core of SOA. Due to the autonomy, heterogeneity and continuous evolution of Web services, mediators are usually needed to support service interactions to overcome possible mismatches that may exist among business processes. In this paper, we introduce a space-based architecture for process mediator which considers both control-flow and data-flow, present possible mismatch patterns, and suggest how they can be automatically mediated. Our work can be used to perform runtime mediation and thus to facilitate service interactions."
530,"Hybrid cloud computing paradigm has recently be widely advocated, where Software-as-a-Service (SaaS) providers can extend their local services into the public clouds seamlessly. In this way, dynamic user request workload to a SaaS can be elegantly handled with the rented computing capacity in public cloud. However, although a hybrid cloud may save cost compared with the private cloud, it still introduces considerable renting cost and communication cost. How to optimize such an operational cost becomes one major concern for the SaaS providers to adopt such a hybrid cloud computing paradigm. However, this critical problem remains unanswered in the current state of the art. In this paper, we focus on optimizing the operational cost for the hybrid cloud model by theoretically analyzing the problem with a Lyapunov optimization framework, and accordingly providing an online dynamic provision algorithm. In this way, our approach can address the real-world challenges where no a priori information of public cloud renting prices is available and the future probability distribution of user requests is unknown. We then conduct experimental study based on a set of real-world data, and the results confirm that our algorithm can work well in reducing the cost."
531,"Application frameworks are widely used in software engineering to support reuse by capturing the shared architecture among a family of applications. Their role in Web service construction has, however, been mostly ignored. Reuse in general has rather been considered in the context of Web service composition than as a means to use existing implementations to build new services with related functionality. In this paper we discuss reuse in Web service development, focusing on families of Web services that share a common architecture and a set of functionalities. Techniques supporting reuse rely on identifying and managing variation points. We propose a categorization of possible variation points in service endpoints, WSDI descriptions, and business logic. A pattern-based approach for managing variation and specifying a Web service framework to an actual service application is introduced. The approach is applied to specify a sample Web service framework."
532,"The benefits of connecting businesses through Web-services-based trading exchanges are huge. These exchanges allow various companies to connect supply chains in various industries or across industries, introducing new efficiencies and ways of buying, selling, and brokering products and services. Connecting businesses through Web-services-based trading exchanges give companies the ability to transact in ways that were never before possible. These exchanges help connect buyers to sellers and provide a shorter procurement trading lifecycle. Having this account, in this work a business processes integration and monitoring system has been developed to automate, integrate and monitor many of the enterprise business processes described as Web services without recurring to large investments in software development and deployment. The contribution of this work consists in a service-oriented architecture that follows the SOA's principles of improving economical benefits of business collaborations."
533,This paper discusses preliminary stages and key research challenges tackled by TEOS (experiments) in the EC funded BonFIRE project. TEOS experiments aim to determine the conditions for achieving resilient and optimal service compositions for very large scale service composition scenarios relating to Future Internet applications. We deploy and test two large scale service optimization models under different scenarios and configurations on the multi-site cloud infrastructure established by the EC BonFIRE initiative. The outcome of TEOS will not only verify the feasibility of the optimization models in dealing with large number of services and future-internet scenarios but also the nature and design of our experiments will help in evaluating the underlying BonFIRE infrastructure.
534,"Web service compositions run in complex computing infrastructures where arising events may affect the quality of the system. However, crucial Web service compositions cannot be stopped to apply changes to deal with problematic events. Therefore, the trend is moving towards context-aware Web service compositions, which use context information as a basis for autonomic changes. Under the closed-world assumption, the context and possible adaptations are fully known at design time. Nevertheless, it is difficult to foresee all the possible situations arising in uncertain contexts. In this paper, we leverage models at runtime to guide the dynamic evolution of context-aware Web service compositions to deal with unexpected events in the open world. In order to manage uncertainty, a model that abstracts the Web service composition, self-evolves to preserve requirements. The evolved model guides changes in the underlying WS-BPEL composition schema. A prototype and an evaluation demonstrate the feasibility of our approach."
535,"With the convenient connection to network, more and more individual information including sensitive information, such as contact list in Mobile Phone or PDA, can be delegated to the professional third service provider to manage and maintain. The benefit of this paradigm is, on one hand to avoid the sensitive information leakage when individual devices failed or lost, on the other hand to make only the authorized users access and share the delegated information online anytime and anywhere. However, in this paradigm the critical problems to be resolved are to guarantee both the privacy of delegated individual information and the privacy of authorized users, and what is more important to afford the owners of communication devices to have high level of control and power to create their own particular access control policies. In this paper, we present an approach to implement the personalized access control at third service provider in a privacy preserving way. Our approach implements the critical problems above in this paradigm by using selective encryption, blind signature and the combination of role based access control and discretionary access control."
536,"Maintaining the Quality of Service (QoS) is important for self-healing web service-based distributed interactive applications. It requires the ability to deal with permanently changing constraints both at the communication and the execution levels. Preventing or repairing QoS degradation also requires the capacity of identifying its possible or actual sources and the capacity of reconfiguration decision and enforcement. Dealing with these issues is especially challenging for web services since the self-healing solution has to preserve the dynamic composition property and to be seamless for the service requesters, while being always usable under the different deployment constraints. In this paper, we present a self-healing middleware framework able to provide the self-healing properties for QoS management in web service-based distributed interactive applications. The framework implementation has been achieved in the context of the WS-DIAMOND project. It covers the whole cycle of adaptation management including monitoring and analysis of QoS values, and substitution-based reconfiguration."
537,"Web services technology has emerged as a key infrastructure that enables business entities to interact with each other without any human inventions. In order for the technology to be widely used, especially in any field where a large volume of transactions may be processed, it is highly desirable that the Web services engine should tolerate such environments. In this paper, we present a novel approach for improving Web services performance. We first focus on the fundamental characteristics of the Web services in that the SOAP messages on the wire are mostly generated by machines and have a lot of similarities among the processed messages. By making use of these features and eliminating the redundant processing, we propose a new deserialization mechanism that reuses matching regions from the previously deserialized application objects from earlier messages, and only performs deserialization for a new region that would not be processed before. Through our experiments in this paper, we observed that our approach obtained a 288% performance gain (maximum) by incorporating the differential deserialization into the Axis SOAP engine."
538,"Deep learning has been an emerging field of machine learning during past decades. However, the diversity and large scale data sizes have posed significant challenge to construct a flexible and high efficient implementations of deep learning neural networks. In order to improve the performance as well to maintain the scalability, in this paper we present SOLAR, a services-oriented deep learning architecture using various accelerators like GPU and FPGA based approaches. SOLAR provides a uniform programming model to users so that the hardware implementation and the scheduling is invisible to the programmers. At runtime, the services can be executed either on the software processors or the hardware accelerators. Experimental results on the real state-of-the-art FPGA board demonstrate that the SOLAR is able to provide a ubiquitous framework for diverse applications without increasing the burden of the programmers. Moreover, the speedup of the GPU and FPGA hardware accelerator in SOLAR can achieve significant speedup comparing to the conventional Intel i5 processors with great scalability."
539,"Web service technologies offer a successful way for interoperability among applications. Now it is important to face how to model systems based on service functionality and also how to add extra-functional properties to them. This is the reason why we propose first of all a versatile and simple UML profile based on the service component architecture specification for modeling services and, secondly, a new UML profile is proposed in order to model and reuse extra-functional properties in the named models. Besides, the property profile provides enough information to enable property code and description generation at a later stage"
540,"Web service evaluation is a phase of the Web service selection in which discovered Web services are evaluated with respect to user request, which means that the non functional properties (NFPs) offered by Web services are compared with the non functional properties requested by users. The fact that users and providers can express their NFPs in very flexible ways makes the management of NFPs a very complex task. In this paper we propose a computing-oriented description of NFPs and a novel approach to NFP-based service evaluation based on Hierarchical Constraint Logic Programming. This proposal extends our previous work on Policy Centered Meta-model (PCM)."
542,"Web services are becoming important in applications from electronic commerce to application interoperation. While numerous efforts have focused on service composition, service selection among similar services from multiple providers has not been addressed. Such issue is more serious when services are embraced in Grid platforms, which are usually resource-conscious. Experimental results show that our considerations are valid and our preliminary solution works well in our Globus grid network."
543,"The framework for automatic mediation of two process models composed of semantically annotated Web services is presented. Process mediation is hard because of many possible mismatches between process models. We introduce algorithms for the process models analysis to find possible mappings between provider's and requester's process models, or to identify incompatibilities that cannot be reconciled with given set of available data mediators and external services. Results of the analysis phase are used in the mediator runtime component. In particular, we show how the workflow and dataflow mismatches can be resolved."
544,"Web services aim to support efficient integration of applications over Web. Most Web services are stateful, such as services for business processes, and they converse with each other via properly ordered interactions, instead of individual unrelated invocations. In order to address efficient integration of conversational Web services, we create a unified specification model for both conversation protocol and composition; we propose methods to integrate a partner service with complex conversation protocol into a composition of Web services; assure the correctness of composition by formal verification. The mapping between our model and BPEL4WS is also discussed."
545,"Service ecosystem consists of all kinds of services, and some of them may be composed by developers to create new mashups. Existing work on service recommendation and composition mine either frequent patterns from mashup-service usage records, or latent topics from service metadata. In this paper, we propose Service Co-occurrence LDA (SeCo-LDA), a novel approach that mines latent topic models over service co-occurrence patterns. The key idea is to treat each service as a document, and its bag of co-occurring services as the bag of words in that document. Using this model, we can analyze such service co-occurrence documents with a probabilistic topic model. We show how to derive service co-occurrence topics, and then validate our model on the real-world ProgrammableWeb.com dataset. We illustrate that SeCo-LDA can discover meaningful latent service composition patterns including their temporal strength and services' impacts, which conventional Apriori can not reveal. Comparing with Apriori, content matching based on service description and LDA directly using mashup-service usage records, we have demonstrated that SeCo-LDA can recommend service composition more effectively, 5% better in terms of MAP than the baseline approach."
546,"User authentication is a crucial security component for most computing systems. But since the security needs of different systems vary widely, authentication mechanisms are similarly diverse. In particular, independently-managed Web and grid services vary with regard to the type of security token (credential) used to prove user identity (username/password, X.509 signing, Kerberos, etc.). Forcing users to manage and present credentials manually for each service is tedious, error-prone and potentially insecure. In contrast, we present CredEx, an open-source, standards-based Web service that facilitates the secure storage of credentials and enables the dynamic exchange of different credential types using the WS-Trust token exchange protocol. With CredEx, a user can achieve single sign-on by acquiring a single (default) credential then dynamically exchanging that credential as needed for services that authenticate a different way. We describe the design and implementation of CredEx by focusing on its use in bridging password-based Web services and PKI-based grid services, illustrating how interoperability between these realms can be based upon the WS-Security and WS-Trust specifications."
547,"This paper proposes a novel trust evaluation method for service choreography. Compared with current work towards this problem, it considers not only the trust for individual partner services and the explicit trust relation among partner services that have logical dependencies for each other, but also the implicit trust relation implied in data-dependencies among services. A serial of experiments, using the simulation tool Net Logo, are carried out to compare the evaluation results between the proposed method and the method without data-dependency consideration. The result shows that taking consideration of the data-dependency trust improves the accuracy of trust evaluation to a great extent."
548,"Due to the increasing number of available web services, discovering the best service that matches a user requirement is still a challenge. In most cases the discovery system returns a set of very similar services and sometimes it is unable to find results for some complex queries. Therefore, integrating web service discovery and composition, taking into account the diversity of discovered results, in a unified way is still a big issue for web services. In this paper, we propose a novel service ranking algorithm for diversifying web services discovery results in order to minimize the redundancy in the search results. This algorithm chooses a set of selected web services based on relevancy, service diversity and service density. We also propose a new method to generate service dependency network using the Formal Concept Analysis (FCA) framework. The generated graph is used to select the composition of discovered web services set. Experimental results show that our method performs better than others baseline approaches."
549,"Android system has been the crucial platform for the mobile service ecosystem. As a typical open source project, the release of the android system is a challenging issue because many developers are working on the related projects and it will affect millions of mobile service running on the platform. Therefore, investigating the release process of Android system is important for the mobile service ecosystem. Particularly, in this paper, we will focus on the release features prediction issue of what features should be included in the new publishing version. The valid changes and release notes are transformed into low-dimensional vectors and then the automatic labelling methodology is developed to detect the features. Combing with the time series forecasting model, an approach to predict the published features in the new version is presenting. Based on the data collected from the Android Open Source Project (AOSP), the experiments show that: comparing with the state-of-the-art, our approach achieves 13.83% to 17.69% precision improvement in releasing feature predictions and we can effectively detect the spike features for further compatibility management."
550,"Tagging technique is widely used to annotate objects in Web 2.0 applications. Tags can support web service understanding, categorizing and discovering, which are important tasks in a service-oriented software system. However, most of existing web services' tags are annotated manually. Manual tagging is time-consuming. In this paper, we propose a novel approach to tag web services automatically. Our approach consists of two tagging strategies, tag enriching and tag extraction. In the first strategy, we cluster web services using WSDL documents, and then we enrich tags for a service with the tags of other services in the same cluster. Considering our approach may not generate enough tags by tag enriching, we also extract tags from WSDL documents and related descriptions in the second step. To validate the effectiveness of our approach, a series of experiments are carried out based on web-scale web services. The experimental results show that our tagging method is effective, ensuring the number and quality of generated tags. We also show how to use tagging results to improve the performance of a web service search engine, which can prove that our work in this paper is useful and meaningful."
551,"Security certification schemes for Service-Oriented Architecture (SOA) extend service specifications with the evidence that a service supports a set of security properties and provides a given level of assurance. However, services are subject to continuous refinements, and uncontrolled changes can easily invalidate existing certification results and require re-certification from scratch, with high costs and overheads on service providers. In this paper, we present an approach to manage the impact of service evolution on security certification. Our approach aims to support the incremental certification of evolving services and re-use, as much as possible, the certification evidence available from older certificates in the release of a new certificate."
552,"The rapid adoption of model-driven design (MDD) methodology in SOA-based solution design requires an adaptive tooling environment that can systematically improve designers' productivity. Ideally, the environment should be flexible enough to both handle frequently changing requirements and support new features without intensive coding efforts. In this paper, we provide a coding-free enablement framework to realize such extensible tooling environments based on a mathematical abstraction of key models in SOA solution design using graph theory definition. This abstraction formalizes the SOA modeling logic and semantics, and also guides the implementation of an extensible and customizable tooling environment. As a case study, we illustrate how our framework is able to transform the development style from Java programming to text editing through our implementation of a UML 2.0 based SOA modeling environment using IBMpsilas Rational Software Architect (RSA) development platform."
553,"To fully utilize Web-services, users and applications should be able to discover, deploy, compose and synthesize services automatically. This automation can take place only if a formal semantic description of the Web-services is available. In this paper we present a markup language called USDL (Universal Service Description Language), for formally describing the semantics of Web-services."
556,"In this paper, we provide an analysis of the impacts of some reputation parameters that an agent-based Web service holds while being active in the environment. To this end, we deploy a reputation model that ranks the Web services with respect to their popularity in the network of users. We model and analyze the arrival of requests and study their impacts on the overall reputation. The Web services may be encouraged to handle the peak loads by gathering to a group. Besides theoretical discussions, we also provide significant results, which elaborate more on the details of the system parameters. We extend the details of these results to empirical results and link the observations of the implemented environment to the results that we theoretically obtain."
557,"With the increasing service-oriented applications, efficient service data management is becoming a practical and important issue. Currently, most exchange messages among different Web services is SOAP, which is represented by XML-formatted data. So, the main task of service data management is to manage XML format message files in order to provide efficient store and query for service requestors. Storing XML documents in relational databases has drawn much attention in recent years because it can leverage existing investments in relational database technologies. However, most former work defines relational schema for XML files based on heuristics without considering application characteristics, hence fails to produce efficient relational schema for various applications. In this paper, we propose an autonomic, workload-aware approach to generate relational schema according to dynamic application characteristics or user specified workload. Our approach adopts the genetic algorithm to find optimal mappings, and an elegant encoding method and related operations are proposed to manipulate mappings using bit strings. We implemented the proposed algorithm and our preliminary experiment results showed that our algorithm was more robust, more suitable for dynamic SOA applications and produced better relational schema than existing work"
558,Presents the welcome message from the conference proceedings.
559,"With the widespread of Service-oriented Computing (SOC) and the increasing number of available web services in several domains, service discovery has become one of the main challenges in SOC. Lack of rich service descriptions is one of the several factors that exacerbate this challenge. Therefore, additional information about web services is required. Several approaches and sources have been proposed in the community to gather this information, such as domain experts, service providers, service consumers, etc. However, the increasing number of web services requires automatic approaches and additional sources of information. In this paper, we introduce a novel approach to generate annotations for web services, e.g., tags, by sampling their automatic invocations. The generated annotations are integrated in web forms that are used for future calls of these web services. Providing correct values for input parameters of web services is one of the main challenges involved in this work. We use four sources to assign values for input parameters, namely, random values, outputs of other operations within the same web service, other web services, and external data sources, e.g., DBpedia, Word Net. We have implemented our approach in the context of our service registry and validated it through several experiments."
560,"QoS-based Web service recommendation has recently gained much attention for providing a promising way to help users find high-quality services. To facilitate such recommendations, existing studies suggest the use of collaborative filtering techniques for personalized QoS prediction. These approaches, by leveraging partially observed QoS values from users, can achieve high accuracy of QoS predictions on the unobserved ones. However, the requirement to collect users' QoS data likely puts user privacy at risk, thus making them unwilling to contribute their usage data to a Web service recommender system. As a result, privacy becomes a critical challenge in developing practical Web service recommender systems. In this paper, we make the first attempt to cope with the privacy concerns for Web service recommendation. Specifically, we propose a simple yet effective privacy-preserving framework by applying data obfuscation techniques, and further develop two representative privacy-preserving QoS prediction approaches under this framework. Evaluation results from a publicly-available QoS dataset of real-world Web services demonstrate the feasibility and effectiveness of our privacy-preserving QoS prediction approaches. We believe our work can serve as a good starting point to inspire more research efforts on privacy-preserving Web service recommendation."
561,"Service reuse aims at improving the efficiency of software development and providing common functionalities which are not linked to any particular business process. However, the existing service reuse methods are confined to the reuse of atomic services or processes encapsulated as stand-alone services. How to reuse arbitrary granularities of Service Process Fragment (SPF) is a challenging problem with great application value. This paper presents a novel Variable Granularities Index (VGI) based on SSM-Tree on service processes. VGI could realize the unified index on both atomic and composite services and maximize reuse of them. To verify the feasibility and effectiveness, we construct a sample dataset which contains 500 thousand processes and 127 million atomic services based on the Web Service Challenge Testset Generator (CTG). The experimental results show an effective and efficient approach for SPF query."
562,"New sensor technologies, powerful mobile devices, and wireless communication standards strongly proliferate ubiquitous and pervasive computing. This is particularly true for healthcare applications. Modern non-invasive unobtrusive sensors enable sophisticated monitoring of the human body. As a result of this monitoring a growing amount of elderly people suffering from chronic diseases will benefit from an enhanced quality of life and improved treatment. Healthcare applications process a vast amount of continuously generated data, this has become a serious challenge. Data stream management addresses this problem in general. However, healthcare applications have additional requirements, e.g., flexibility and reliability. As a consequence, we propose the combination of a sophisticated information infrastructure, called hyperdatabase, and data stream management. Hyperdatabases already allow for reliable process management in a peer-to-peer fashion. In this paper, we elaborate the close relation between distributed process management and data stream management. Further, we show that data stream processing can significantly benefit from an infrastructure for reliable process management. We present the architecture of our prototype infrastructure that supports processing of continuous data streams with a high degree of flexibility and reliability."
563,"Driven by the widespread application of Service-Oriented Architecture (SOA), the quantity of web services and their users keeps increasing in the service ecosystem. Since services are hosted by service providers, it will be very helpful to predict the tendency of services invocation for service providers, so that proper actions may be taken to ensure the quality of services. Two major challenges exist in predicting the tendency of services invocation, however. First, different service invocation sequences may bear different and complicated characteristics, which is hard to be modeled generally. Second, the intricate relations between service invocation sequences are valuable but hard to be discriminated and utilized. To address these issues, a deep neural network, named Piecewise Recurrent Neural Network (PRNN), is developed by taking both generality and pertinence into consideration. For generality, PRNN extracts complicated characteristics of all service invocation sequences through Long Short-Term Memory (LSTM) units. For pertinence, PRNN develops a piecewise mechanism, through which service invocation sequences can be clustered automatically and predicted discriminatingly. Extensive experiments in real-world dataset show that PRNN outperforms baseline methods in predicting the tendency of services invocation."
564,"The reliability is a major factor that limits the success of promising service oriented architecture in bioinformatics. To evaluate services reliability and availability and create indicators of quality of service, we have defined an XML format designed to exchange service status reports. We demonstrate the interest of sharing status reports between service consumers and producers by providing two example applications, a workflow management system plug-in and a Web report generator. Furthermore, we describe the framework that we use to distribute quality of service management features, QBIOS. QBIOS enables service test case creation, scheduled execution and monitoring. We illustrate the use of our open test strategy at the level of a group of scientific partners sharing bioinformatics services."
565,"Recent development in daily business life indicates demand for higher reliability of SOA software. After analyzing the traditional software reliability models (Markov chain based), it was found that many software with complicated relationships of components did not satisfy the Markov property. This paper analyzed various complex components' relationships (parallel, loops, backup, fault- tolerance, request and response, etc.) how to influence the reliability of the whole system, and solved these complicated relationships to satisfy the Markov property. This work extended the scope of the application of the Markov Model and improved the accuracy of reliability estimation."
566,"An increasing amount of Web services are being implemented using process management tools and languages (BPML, BPEL, etc.). The main advantage of processes is that designers can express complex business conversations at a high level of abstraction, even reusing standardized business protocols. The downside is that the infrastructure behind the Web service becomes more complex. This is particularly critical for Web services that may be subjected to high variability in demand and suffer from unpredictable peaks of heavy load. In this paper we present a flexible architecture for process execution that has been designed to support autonomic scalability. The system runs on a cluster of computers and reacts to workload variations by altering its configuration in order to optimally use the available resources. Such changes happen automatically and without any human intervention. This feature completely removes the need for the manual monitoring and reconfiguration of the system, which in practice is a difficult and time-consuming operation. In the paper we describe the architecture of the system and present an extensive performance evaluation of its autonomic capabilities."
567,"The cost and time of deploying HPC applications on clouds is a problem. Instead of conducting their research discipline specialists are forced to carry out activities for application deployment, publication and ease of access. In response, a new approach for HPC application deployment and access in clouds is proposed. The major innovations are a new approach to deploying and executing HPC applications on IaaS and PaaS clouds, and exposing HPC applications as services. Through three case studies this paper demonstrates the feasibility and effectiveness of the proposed approach that could lead to the building of a SaaS library of discipline-oriented services evocable through user friendly, discipline specific interfaces. The new approach will reduce the time and money needed to deploy and expose discipline HPC applications."
568,"In this paper, we present a novel approach for specification of invocable semantic Web resources in a unified manner. Our approach enables specification of Web services, composite Web services, Web pages and Web sites with one single process description language in a unified manner. The basic idea behind our work is that different types of invocable Web resources differ mainly in the communication protocol and message formats. The language abstracts from various communication protocols and message formats and hence enables modeling of business logic without worrying much about technical details. Our approach in contrast to existing annotation techniques does not enforce the providers to modify their Web resources."
569,"The interoperable and loosely-coupled web services architecture, while beneficial, can be resource-intensive, and is thus susceptible to denial of service (DoS) attacks in which an attacker can use a relatively insignificant amount of resources to exhaust the computational resources of a web service. We investigate the effectiveness of defending web services from DoS attacks using client puzzles, a cryptographic countermeasure which provides a form of gradual authentication by requiring the client to solve some computationally difficult problems before access is granted. In particular, we describe a mechanism for integrating a hash-based puzzle into existing web services frameworks and analyze the effectiveness of the countermeasure using a variety of scenarios on a network test bed. Client puzzles are an effective defence against flooding attacks. They can also mitigate certain types of semantic-based attacks, although they may not be the optimal solution."
570,"While most service-oriented solutions are developed for system integration or data integration, we propose a Web services-based solution, Eucalyptus, for provisioning applications and networks on demand. Eucalyptus is built on the power of the user-controlled lightpath provisioning (UCLP) tool, which pioneers a user-centric and service-oriented approach for creating and managing a private end-to-end optical network. As a first user of UCLP, Eucalyptus aims to construct an participatory environment for geographically distributed teams of architects and industrial designers with the with the support of configurable broadband switched networks, as well as the traditional routed IP networks. The contribution of Eucalyptus is to provide a proof-of-concept example on how Web service and service-oriented architecture (SoA) can effectively provide on-demand provisioning for heterogenous resources in hybrid networks. These resources can be provisioned, launched, monitored, terminated, and reserved through Web services. Eucalyptus is network and platform neutral. It offers a single point of entry for users to access resources ranging from video conference applications, rendering clusters, to the underlying networks. Each resource is configured through a resource-specific Web service. Eucalyptus also includes a set of generic management Web services to coordinate sessions, to manage resources and users, and to compose workflows, such that the network and the application are properly configured for the users engaging in a participatory design session."
571,"Identity management is a key technology to enable federation of web services for authorized users. It mainly focuses on exchanging identity related information properly among web based systems. However, little attention has been given to multimedia services. We propose a design and architecture of a federating proxy that relays user's authentication and session related information to existing systems that employ different communication protocols. It provides a framework for bridging gap in the schema and syntax of the protocols while it reuses deployed infrastructure."
572,"A generalized declarative constraint framework is proposed in this paper to address challenges of applying constraints to model-driven SOA deployment and configuration. By representing deployment metamodel and domain knowledge as patterns, constraints can be easily described, automatically validated and analyzed. This framework reduces the human-intensiveness and error-proneness in the activity of applying constraints to a SOA deployment topology. We implemented this framework over a model-driven SOA deployment platform leveraging OCL as the declarative constraint language, and verified it in industry deployment scenarios."
573,"This tutorial will take the audience through an ""aggregated"" SOA engagement and discuss key processes, activities, and deliverables through the entire life cycle, especially in service modeling, realization, integration through an enterprise service bus and governance. Best practices and some anti-patterns will also be presented and discussed. It is mostly based on some of the speakers' pioneering project practices and lessons learned since SOA and Web service's inception."
574,"This paper proposes an extensive reuse approach for SOM, which utilizes a multi-facets ontology system supporting reuse of various service-oriented assets, such as business processes, collaboration templates and services. The paper also presents an iterative service-oriented modeling process based on the ontology and assets repository, which reuses assets from the first to last modeling phase by matching between assets descriptions, and results in a service model that represents the specification of the required service-oriented application."
575,"The growing popularity of cloud computing has magnified the rise of software reuse by facilitating service provisioning over the Internet. At the same time, a new generation of mobile apps has emerged relying on backend services that expand the app functionally, while reducing the overhead on limited mobile resources. The Web service approach promises great flexibility in offering software functionality over the network, while maintaining interoperability between heterogeneous platforms. In addition, recent years have witnessed the rise of user-facing service developments that can be consumed on-the-go with a standard interface, such as Restful Web services. However, the discovery of such services does not match their growing popularity and remain challenging. Users cannot tolerate long latency in finding relevant services to their requests. In this paper, we propose a robust and efficient Web service discovery approach that uses statistical methods and indexing techniques to improve the precision and response time of the discovery process. Experimental results demonstrate that the proposed approach outperforms the state-of-the-art discovery mechanisms and significantly reduces the query response time by at least 77%, while maintaining comparable accuracy."
576,"The increasing availability of positioning data from mobile devices facilitates new opportunities for location analytics systems, which provide insights into the movement behavior of targets across various localities. Similar to web analytics systems, positioning data can be utilized to count, for example, returning visitors in venues, calculate visit frequencies for certain time intervals, or to identify typical movement paths for different groups of visitors inside and outside buildings. However, a major challenge for location analytics is still to deal with the heterogeneity of data from various positioning systems. In thispaper we present a platform that enables location analytics as a service and copes with the heterogeneous spatiotemporal data of diverse accuracy, frequency, and coverage. Furthermore, it allows to query large positioning datasets according to various data dimensions and metrics. In an additional four-month field trial the applicability of our approach was reviewed using the example of WLAN positioning data from an office environment."
577,"We propose a new framework for social-sensor cloud services selection based on spatio-textual correlation between user's query and service. The proposed research defines a formal social-sensor cloud service model that abstracts the functional and non-functional aspects of social-sensor data on the cloud in terms of spatio-temporal, textual and quality of service parameters. Proposed framework is a 4-stage filtering algorithm, to select social-sensor cloud services based on user query and quality of service demands. 4-stage filtering is based on spatial correlation, textual correlation, visual features and quality of service parameters. Analytical results are presented to show the performance of the proposed approach."
579,"We present in this paper a trust-based game theoretical model for Web services collaboration. The devised model is an application of the generic model about tasks allocation for agents, which is presented in. We consider that a collaboration between Web services captures a possible interaction for achieving a specific task. We consider such collaboration as a game in which several Web services compete to win a task by bidding a cost for achieving it. The game winner is the Web service which has the minimal trust-based cost, which is the product of the cost and the inverse of the trust value of the bidding Web service. Initial trust values are computed based on the distance between the actual and the announced quality attribute values of a Web service. These values are computed during a collaboration evaluation period. They are updated after each game round based on a bayesian model. We show in this paper how the use of trust allows a safer collaboration between Web services with respect to collaborations where there is no consideration of the trust as a criteria for allocating tasks."
580,"Web services are promoted as a new model for distributed systems, yet many skeptics see them as simply a poor implementation of traditional remote procedure calls (RPC) or distributed objects. Previous comparisons support the skeptics: Web services are shown to be significantly slower than RPC, and they lack features like automatic proxies. However, these studies are biased because they are based on an RPC communication style. Web services support a document-oriented style of communication that performs well even in the face of the high latency found in Internet or business transactions. We investigate these issues by comparing the design and implementation of a small file server application implemented using RMI and Web services. For this application, using the most straightforward implementation in both technologies, Web services outperform RMI when accessing multiple/deeply nested files, especially over high-latency channels. However, the default Web services interfaces are awkward to use, so we develop a technique for wrapping the Web service to make it as easy to use as the distributed object implementation. The same wrappers are then used to implement the document-oriented communication style in RMI, which improves performance but significantly complicates the design. This case study provides a more detailed comparison of the relationship between Web services and distributed objects"
581,"QoS prediction is critical to Web service selection and recommendation. This paper proposes a collaborative approach to quality-of-service (QoS) prediction of web services on unbalanced data distribution by utilizing the past usage history of service users. It avoids expensive and time-consuming web service invocations. There existed several methods which search top-k similar users or services in predicting QoS values of Web services, but they did not consider unbalanced data distribution. Then, we improve existed methods in similar neighbors' selection by sampling importance resampling. To validate our approach, large-scale experiments are conducted based on a real-world Web service dataset, WSDream. The results show that our proposed approach achieves higher prediction accuracy than other approaches."
582,"Data lakes have recently emerged as an alternative solution to costly traditional data warehouse solutions. To exploit data lakes, however, there is a need for means that assist users in combining and integrating data stored within a data lake. In this paper, we position ourselves in the recurrent context where a user has a local dataset that is not sufficient for processing the queries that are of interest to him/her. We show how data lakes, or more specifically the service lakes, since we are focusing on data providing services, can be leveraged to answer user queries, taking into account the quality of the services and respecting the (time and monetary) budget set by the user."
583,"Online Social Networks (OSNs) have been used as the means for a variety of applications, like employment system, e-Commerce and CRM system. In these applications, social influence acts as a significant role, affecting people's decision-making. However, the existing social influence evaluation methods do not fully consider the social contexts, like the social relationships and the social trust between participants, and the preferences of participants, which have significant impact on social influence evaluation in OSNs. Thus, these existing methods cannot deliver accurate social influence evaluation results. In our paper, we propose a Context-Aware Trust-Oriented Influencers Finding method, called CT-Influence, with social contexts taken into account. We conduct experiments onto two real social network datasets, i.e., Epinions and DBLP. The experimental results illustrate that our CT-Influence method greatly outperforms the state-of-the-art method So Cap in terms of effectiveness and efficiency."
584,"Although Service-Oriented principles have been widely adopted by High Throughput Computing infrastructure designers, the integration between SOA and HTC is made difficult by legacy. jGASW is a framework for wrapping legacy scientific applications as Web Services and integrating them into an intensive computing-aware SOA framework. It maps complex I/O data structures to command lines and enables dynamic allocation of computing resources; including execution on local hosts or on grid infrastructures; data transfer management and support of non-functional concerns."
585,"Due to the growing pervasiveness of the service paradigm, modern systems are now often built as software as a service, and tend to exploit underlying platforms and virtualized resources also provided as services. Managing such systems requires that we be aware of the behaviors of all the different layers, and of the strong dependencies that exist between them. In this paper we present the Multi-layer Collection and Constraint Language (mlCCL). It allows us to define how to collect, aggregate, and analyze runtime data in a multi-layered system. We also present ECoWare, a framework for event correlation and aggregation that supports mlCCL, and provides a dashboard for on-line and off-line drill-down analyses of collected data. Empirical assessment shows that the impact of the approach on runtime performance is negligible."
586,"This paper presents an emerging tool for security configuration of service-oriented architectures with Web Services. Security is a major concern when implementing mission-critical business transactions and such concern motivated the development of Web Services Security (WS-Security). However, the existing tools for configuring the security properties of Web Services give a technology-oriented view, and only assist in choosing the data to encrypt and selecting an encryption algorithm. The users must construct their own mental models of how the security configurations actually relate to business policies. In contrast, the tool described here gives a simplified, business-policy-oriented view. It models the messaging with customers and business partners, lists various threats, and presents best-practice security patterns against the threats. A user can select among variations on the basic patterns according to the business policies, and then apply them to the messaging model through the GUI. The result of the pattern application is described in the Web Services Security Policy Language (WS-Security Policy)."
587,"Services have been increasingly used as the building blocks for decoupled and flexible applications. Service evolution is a critical issue because even small changes, if not compatible, can potentially affect a huge number of client applications. However, particularly in the context of large scale usage of a service, changes cause different impact on client applications according to its use. This paper proposes to focus on compatibility from the point of view of usage patterns in order to deal with service evolution issues in more flexible and less costly way. The idea is to summarize the behavior of client applications into usage profiles, from which metrics that represent the impact of changes can be derived. This valuable information may support service providers on decisions about service lifecycle. The paper discusses the adoption of usage profiles and presents a framework for the automatic evaluation of service changes impact during its lifecycle."
588,"The rapid expansion of Web services and broadband network has enabled more and more useful services for the users with different systems at distributed locations to share applications over the network. However, the operation of such services has become more and more difficult in proportion to the complexity of the total system with applications and networks that need to be managed. Therefore, it requires a lot of effort for the operators to manage the total system if some trouble occurred with an application or a network. This paper presents an autonomic Web service platform with the dynamic multi-layer control that can manage both the network layer and the application layer simultaneously to offer the most preferred service for the users under any circumstances. It consists of three major components: 1) environment information database, which stores the status of networks and applications, 2) dynamic control manager, which maintains service policies on both networks and applications, and controls the service delivery based on the service policy that conforms to the current situation, 3) dynamic control agent, which works as a Web service proxy and controls SOAP messages of the applications based on the service policy. The communications and controls of this platform are all Web services based, and therefore, are quite open and applicable to any field of services. The result of an experiment on the disaster information sharing service proved the effectiveness of this Web service platform."
589,"In QoS-aware service selection, a service requester seeks to maximize its utility by selecting a service provider that charges the lowest service price while meeting the requester's QoS requirements. In existing selection approaches, a service requester focuses on finding providers based on their QoS and thereby ignores their service prices that could change with their QoS. High QoS may provide more benefits, but may require a high service price. As a result, the highest QoS may not produce the maximum utility. A service requester and candidate service providers have a conflicting interest over service prices. Since a provider would not reveal its minimum acceptabl price, it is important for a requester to predict the minimum price for a service that meets its QoS requirements. We propose a collaborative approach to predicting a provider's minimum price for a desired QoS based on prior usage experience. The experimental results show our approach can find the optimal service providers efficiently and effectively."
590,"Performance analysis is important for service clouds serving composite service application jobs containing parallelizable tasks, for optimizing the degree of parallelism (DOP) and resource allocation schemes could improve performance obviously. In this paper, we describe a novel tandem queuing network with a parallel multi-station multi-server system as an analytical model for service clouds serving composite service application jobs. We design a partition method (termed the 'pleasing partition') to help us propose an analytical model for parallelizable service which is the vital fraction of composite service. After that, we could obtain a complete probability distribution of response time, waiting time and other important performance metrics calculated by our proposed analytical model. Thus, to use this model, cloud operators could determine proper job configurations and resource allocation schemes, for achieving specific QoS (Quality of Service). Extensive simulations are conducted to validate that our analytical model has high accuracy in predicting performance metrics of composite service application jobs."
591,"In real-world scenarios, the evolution of Web services to meet functional and non-functional changes ultimately leads to multiple versions of the same original service. Thus, design and implementation of version management techniques, such as version description, directory, etc, play a critical role in realizing the full promise of SOA. To address the version management issues in Web services, we propose a version-aware service model based on some architectural extensions to WSDL and UDDI. WSDL would be enhanced to describe the attributes of the service versions. UDDI would be augmented to use versions in a service directory with an event-based notification/subscription mechanism. We also design a proxy, residing in the service consumer side which can dynamically update the client application instance at runtime. We have implemented a prototype to demonstrate these models and used a weather forecast web service as an example to illustrate the usefulness of the proposed architecture."
592,"Spatial alarm services are essential components of many location-based applications. One of the key technical challenges for supporting spatial alarms as a service is performance and scalability. This paper shows that the Euclidean distance-based spatial alarm processing techniques are inadequate for mobile users traveling on road networks due to the high overhead in terms of server load for alarm checks and the high energy consumption in terms of client wakeups. We design and develop RoadAlarm, a road network aware spatial alarm processing service, with three unique features. First, we introduce the concept of road network-based spatial alarms using road network distance measures and a set of metrics specialized for spatial alarm processing. Second, we develop the basic model for spatial alarm processing by exploiting two types of filters: subscription filter and Euclidean lower bound filter. Third and but not the least, we develop a suite of optimization techniques to further reduce the frequency of wakeups at mobile clients and the number of alarm checks at the alarm processing server, while ensuring high accuracy of spatial alarm processing. Our experimental results show that RoadAlarm outperforms existing Euclidean space-based approaches with high success rate (accuracy) and significantly increased hibernation time."
595,"Security enforcement framework is an important aspect of any distributed system. With new requirements imposed by SOA-based business models, adaptive security enforcement on the application level becomes even more important. Our work on the enforcement framework to date has resulted in a comprehensive middleware-based solution leveraging on Web services technologies. However, potential merits of hardware-based solutions to further secure application exposure have not been considered so far. This paper describes a method for combining software resource level security features offered by Web services technologies, with the hardware-based security mechanisms offered by trusted computing platform and system virtualisation approaches. In particular, we propose trust-based architecture for protecting the enforcement middleware deployed at the policy enforcement endpoints of Web and grid services. The main motivation is to additionally secure execution environment of the applications, by providing virtual machine level separation that maps from logical domains imposed by Web services level enforcement policies."
596,"While many process mining algorithms have been proposed recently, there does not exist a widely-accepted benchmark to evaluate and compare these process mining algorithms. As a result, it can be difficult to choose a suitable process mining algorithm for a given enterprise or application domain. Some recent benchmark systems have been developed and proposed to address this issue. However, evaluating available process mining algorithms against a large set of business models (e.g., in a large enterprise) can be computationally expensive, tedious and time-consuming. This paper proposes a novel framework that can efficiently select the process mining algorithms that are most suitable for a given model set. In particular, it attempts to investigate how we can avoid evaluating numerous process mining algorithms on all given process models."
597,"Due to the limited capabilities and resources, edge servers cannot meet the increasingly complex and diverse service requirements in mobile edge computing environments. In this circumstance, how to dispatch the component tasks of service requests to edge and cloud servers to reduce the time delay has become a crucial problem. Therefore, we focus on this problem and propose a heuristic algorithm called GAMEC (combined Genetic algorithm and simulated Annealing algorithm for service selection in Mobile Edge Computing systems). The simulated experiments have demonstrated the high effectiveness of the method."
598,"Service matchmaking and composition has recently drawn increasing attention in the research community. Most existing algorithms construct chains of services based on exact matches of input/output types. However, this does not work when the available services only cover a part of the range of the input type. We present an algorithm that also allows partial matches and composes them using switches that decide on the required service at runtime based on the actual data type. We report experiments on randomly generated composition problems that show that using partial matches can decrease the failure rate of the integration algorithm using only complete matches by up to 7 times with no increase in the number of directory accesses required. This shows that composition with partial matches is an essential and useful element of Web service composition."
599,"Web services in different trust boundaries interact with each other via SOAP messages to realize functionality in a collaborative environment. Exchanging SOAP messages for remote service invocation has gained wide acceptance among web service developers. Several web service security standards are widely deployed aiming at securing exchanges of a single SOAP message and a conversation of SOAP messages among partners in a collaborative environment. Concerns have been raised about the possibility of XML rewriting attacks within this context and their early detection. In this paper, we demonstrate such possible attacks with respect to WS* policy based scenarios to set a security context and to use a security context for conversations of SOAP messages. We show how our proposed SOAP Account [21] solution could be applied for early detection of XML rewriting attacks, specifically regarding secure SOAP-based conversations. A simulation-based performance analysis and comparison of our SOAP Account approach vs. a WS* policy based approach complements our observations."
600,"Recommending trusted services to users is of paramount value in service-oriented environments. Reputation has been widely used to measure the trustworthiness of services, and various reputation models for service recommendation have been proposed. Reputation is basically a global trust score obtained by aggregating trust from a community of users, which could be conflicting with an individual's personal opinion on the service. Evaluating a service's trustworthiness locally based on the evaluating user's own or his/her friends' experiences is sometimes more accurate. However, local trust assessment may fail to work when no trust path from an evaluating user to a target service exists. This paper proposes a hybrid trust-aware service recommendation method for service-oriented environment with social networks via combining global trust and local trust evaluation. A global trust metric and a local trust metric are firstly presented, and then a strategy for combining them to predict the final trust of service is proposed. To evaluate the proposed method's performance, we conducted several simulations based on a synthesized dataset. The simulation results show that our proposed method outperforms the other methods in service recommendation."
601,"Given the increase in web services, Quality of Service (QoS) attributes have been widely addressed. In dynamic Internet environment, service providers rarely deliver the QoS as declared. Runtime adaptations of execution plans become necessary in order to recovery from web service failures, maintain SLAs, and/or continuously improve the overall QoS attributes. As QoS-aware service composition methods are mainly employed to fulfill runtime adaptations, the time efficiency usually becomes a challenge since the execution of a composite application is blocked until the new execution plan is found. Although existing works have provided high-quality methods to deal with QoS-aware service composition at design time, a suitable mechanism has not been established to utilize these methods coping with runtime adaptations in an efficient way. In this paper, we propose a novel empirical approach to accelerate QoS-aware runtime adaptation. Based on historical records, our approach uses Support Vector Machines (SVMs) to capture the relationship between candidate services and adaptation scenarios which are used at runtime to predict the probabilities that candidate services will be used for upcoming adaptation scenarios. Then candidate services are pruned based on these probability estimates to reduce the search space. The experimental results revealed the proposed approach can achieve significantly acceleration while satisfying considerable correctness."
602,"Web services are becoming prevalent nowadays. Finding desired Web services is becoming an emergent and challenging research problem. In this paper, we present WSExpress (Web Service Express), a novel Web service search engine to expressively find expected Web services. WSExpress ranks the publicly available Web services not only by functional similarities to users' queries, but also by nonfunctional QoS characteristics of Web services. WSExpress provides three searching styles, which can adapt to the scenario of finding an appropriate Web service and the scenario of automatically replacing a failed Web service with a suitable one. WSExpress is implemented by Java language and large-scale experiments employing real-world Web services are conducted. Totally 3,738 Web services (15,811 operations) from 69 countries are involved in our experiments. The experimental results show that our search engine can find Web services with the desired functional and non-functional requirements. Extensive experimental studies are also conducted on a well known benchmark dataset consisting of 1,000 Web service operations to show the recall and precision performance of our search engine."
603,"While Web Services Security (WSS) enhances the security of web services, it may also introduce additional performance overheads to standard web services due to additional CPU processing and larger messages transferred. In this paper, we aim at clarifying this concern by conducting a quantitative performance evaluation of WSS overhead. Based on the evaluation, we extend our existing web services performance model by taking the extra WSS overheads into account. The extended performance model is validated on different environments with different messages sizes and WSS security policies."
604,"Service-oriented way of building customizations to packaged applications is an emerging alternative to the traditional way of modifying the package application directly to implement customizations. Estimating the effort at an early stage (before detailed design) in the services engagement is important to reduce the perceived risk of the new approach and to demonstrate the potential cost efficiencies. In this paper we present a method to estimate the effort and cost involved in developing customizations to packaged application software (such as SAP and Oracle ERP software) using service-oriented architecture/design (SOA) style. Taking only a description of business processes to be customized, we estimate the effort and costs of implementing those customizations. We use novel artifact-centric and linguistic analysis approaches to estimate the business object count which lies at the center of our cost estimation model. The model is currently being piloted at a large IT services organization involved in implementing packaged application software for clients. Initial experiments reveal that the cost and effort estimates are within 10% range of the benchmark estimates developed after requirements gathering and detailed design."
605,"Web services technology for interoperability, dynamic discovery and integration of distributed components is to be applied in various environments, such as networks of relatively narrow and unstable wireless connections. However, in such environments it is necessary to deal with resource limitation. Our work adopts mobile agent technology in response to this problem and presents a mobile agent framework for Web Services integration. This framework utilizes BPEL (Business Process Execution Language for Web Services) for description of the integration logic, and provides declarative descriptions for physical behaviors (migration and cloning). This facilitates addition or change of physical behaviors according to the environmental conditions while preserving the integration logic. This paper especially focuses on formal definition and discussion of our framework."
606,"High-Performance network technology, Remote Direct Memory Access (RDMA), has revealed its tremendous advantage over traditional TCP/IP. With its ultra-low latency and high bandwidth, RDMA has been extensively adopted in distributed environment, especially for in-memory key-value stores. However, although RDMA does provide the ability to interact with remote user space memory directly, memory copy still exists between data memory area and communication memory area with two-sided communication semantics in inmemory key-value store. In addition, using high performance one-sided communication semantics will expose memory totally, hence an inadvertent corrupt data operation could crash system. In this paper, we propose a tactful cache mechanism for RDMA-based in-memory key-value store - InnerCache. Our design concerns two dimensions with respect to improve the system performance with two-sided communication semantics and make system less vulnerable. It merges one-sided and two-sided communication model through making communication memory cacheable. Experimental results show that InnerCache can efficiently improve the performance of RDMA-based in-memory key-value store."
607,"Cloud computing services have been increasingly considered by business as a viable option for reducing IT expenditure. The highly automated and agile nature of cloud services offer businesses low cost, high efficiency and flexibility benefits. However, there are often associated problems with unmanaged accountability such as lack of disclosure of service obligations, mechanisms for detection for obligation fulfilment or determination of liable party if an obligation is violated. This paper analyses the accountability properties of a cloud service and proposes an accountable cloud service (ACS) model to address those problems. The ACS model is underpinned by a hybrid logic system called Dynamic Logic for Accountability (DLA) extended from Dynamic Logic. ACS provides an intuitive notation for modeling service collaboration diagrams based on a reduced version of BPMN2.0 to capture the fulfillment of service obligations. We also propose an Obligation Flow Diagram (OFD) as a simple method for conflict resolution and verification for the ACS model. The ACS model enables obligation specification, decomposition, validation, machine-interpretation, monitoring and reasoning, and ultimately facilitates accountability in cloud service consumption. Using Amazon S3 service as a case study, we show how to address those known accountability problems using our ACS model. Finally we discuss the applicability of our model to cloud services in general."
608,"Web services are supported by a set of protocols that have been designed with the main goal of providing interoperable communication to applications. In typical business-critical services environments the occurrence of interoperability issues can have disastrous consequences, including direct financial costs, reputation, and client fidelity losses. Despite this, experience suggests that interoperability is still quite difficult to achieve, since the heterogeneity of frameworks for providing web services is quite large. In addition, current tools have limited testing capabilities and, in many cases do not specialize in this problem. In this paper we present ITWS, an extensible Interoperability Testing tool for Web Services that is able to assess the interoperability of a web service, supported by any given framework. We have used ITWS to test the interoperability of a set of home-implemented TPC-App web services and a set of thousands of web services created in .NET C# against 11 client-side web service frameworks, including frameworks for mainstream programming languages. Numerous issues have been disclosed, showing the benefits of using ITWS and the importance of testing services for interoperability."
609,"MASC (manageable and adaptive service compositions) is a policy-based middleware for monitoring of Web service compositions and their dynamic adaptation to various runtime changes. MASC policies are described in our new WS-Policy extension called WS-Policy4MASC. Compared with recent related works, MASC has several distinctive characteristics, such as coordination of adaptation on the SOAP messaging layer and the business process orchestration layer, use of both technical and business metrics for adaptation decisions, and extending the power and flexibility of the new Microsoft .NET 3.0 platform. In this paper, we focus on MASC support for adaptation to address business exceptions and manage runtime faults. For example, a sub-process (or an activity) can be added, deleted, replaced, skipped, or retried. We have been implementing a MASC proof-of-concept prototype and evaluating it on adaptation scenarios from a stock trading case study. Our performance studies of the prototype indicate that overheads introduced by MASC are acceptable."
610,"Current Web services composition approaches do not take into account transactional requirements defined by designers. The transactional challenges raised by the composition of Web services are twofold: relaxed atomicity and dynamicity. In this paper, we propose a new process to automate the design of transactional composite Web services. Our solution enables the composition of Web services not only according to functional requirements but also to transactional ones defined using the acceptable termination states model. The resulting composite Web service is compliant with the consistency requirements expressed by designers and its execution can easily be coordinated using the coordination rules provided as an outcome of our approach"
611,"Today, Web services play a dominant role in Enterprise Application Integration (EAI) and for realizing Service Oriented Architectures (SOA), which define the architectural foundation for various kinds of distributed applications. In many business domains, Web services must exhibit quality attributes such as robustness, security, and maintainability. As a consequence, there is a high demand to develop Web services with Quality of Service (QoS) properties. Several specifications of the WS-* family, e.g. WS-Security and WS-Reliable Messaging, aim at simplifying the development and deployment of QoS-aware Web services. Although these approaches cover some typical application scenarios, their expressivity and flexibility are limited by the underlying WS-Policy framework. This paper proposes the syntax and semantics of a new WS-Policy operator that enables the introduction of conditional assertions. Secondly, a framework for managing different types of custom assertions is presented. Finally, this paper describes a proof of concept implementation demonstrating the practical feasibility of both extensions."
612,"The goal of Web service effort is to achieve universal interoperability between applications by using Web standards: this emergent technology is a promising way to integrate business applications. A business process can then be seen as a set of Web services that could belong to different companies and interact with each other by sending messages. In that context, neither a global model nor a global mechanism is available to monitor and trace faults when the business process fails. In this paper, we address this issue and propose to use model-based reasoning approaches on discrete-event systems (DES). This paper presents an automatic method to model Web service behaviors and their interactions as a set of synchronized discrete-event systems. This modeling is the first step before tracing the evolution of the business process and diagnosing business process faults."
614,"In the Big Data environment, more and more services with identical functionalities are emerging on the Internet, but the Quality of Service (QoS) of these services are different, therefore, service selection based on QoS becomes a challenge problem. In the past few years, extensive studies have been carried out on this problem and several approaches are proposed. However, existing research work mainly focuses on service selection for single request. In fact, with the rapid growth of service users, concurrent requests for the same service are taking place frequently. How to find the global optimal services selection scheme for concurrent requests so as to improve users' satisfaction and service broker's revenue has become a difficult problem. For this issue, this paper firstly transforms the problem of global optimal services selection for concurrent requests to a combinatorial optimization problem, then, it proposes a new evaluation model including admission control operator, users' satisfaction and service broker's revenue evaluation operators, finally, based on two important service domain characteristics(Similarity and Priori), a new Service-Oriented Artificial Bee Colony algorithm (S-ABC) is constructed for solving the global optimal services selection problem. Experimental results show that proposed approach in this paper is effective."
615,"Reliability prediction is an important task in software reliability engineering, which has been widely studied in the last decades. However, modelling and predicting user-perceived reliability of black-box services remain an open research problem. Software services, such as Web services and Web APIs, generally provide black-box functionalities to users through the Internet, thus leading to a lack of their internal information for reliability analysis. Furthermore, the user-perceived service reliability depends not only on the service itself, but also heavily on the invocation context (e.g., service workloads, network conditions), whereby traditional reliability models become ineffective and inappropriate. To address these new challenges posed by blackbox services, in this paper, we propose CARP, a new contextaware reliability prediction approach, which leverages historical usage data from users to construct context-aware reliability models and further provides online reliability prediction results to users. Through context-aware reliability modelling, CARP is able to alleviate the data sparsity problem that heavily limits the prediction accuracy of other existing approaches. The preliminary evaluation results show that CARP can make a significant improvement in reliability prediction accuracy, e.g., about 41% in MAE and 38% in RMSE when only 5% of the data are available."
616,"The growing number of RESTful web services available on the web raises a challenging search problem as to how the desired web services should be located. Traditional keyword searching is inaccurate, and its limitations have been noted for several years. We propose a combination method of WADL and a learning ontology mechanism to enable RESTful semantic web services. These syntactic and semantic descriptions allow search engines to support a similarity search for RESTful web services. We describe an experimental study on a collection of 168 RESTful web services. The experimental results show that our method has higher performance in terms both of the rate of recall and precision compared to existing methods."
617,"In recent times, automated business processes and web service technologies have become popular and ubiquitous for catering to diverse user needs. While providing a service, the service providers are typically expected to furnish promised QoS values for the services they deliver. However, when the services are physically deployed and invoked during a query resolution, these parameter values vary largely depending on different factors like network load, number of applications running in the server etc. In this work, we present a stochastic model of the web service composition problem. Experimental results on Web Service Challenge (WSC) benchmarks show the efficiency of our proposed mechanism."
618,"A current trend in the web services community is to define coordination mechanisms to execute collaborative tasks involving multiple organizations. Following this tendency, this work presents a dependable (i.e., intrusion-tolerant) infrastructure for cooperative web services coordination that is based on the tuple space coordination model. This infrastructure provides decoupled communication and implements several security mechanisms that allow reliable coordination even in presence of malicious components.This work also investigates the costs related to the use of this infrastructure and possible web service applications that can benefit from it."
619,"Cluster computing has become popular in academia and industry. Clusters of commodity servers are used for a variety of distributed applications like simulation, data analysis, and web services. No single framework can fit every distributed application. Even an Infrastructure as a Service (IaaS) like the Openstack cloud framework cannot fit all of them because, for example, some researchers would likely want to deploy hadoop clusters on bare metal servers. One solution to this problem is to assign a layer below the IaaS/PaaS (Platform as a Service) layer the job of handling cluster deployment. This idea is called the Cluster-as-a-Service layer for allocating servers and deploying IaaS and PaaS. This paper introduces dodai-deploy, which handles Cluster as a Service deployments."
620,"Automated composition of services is a key functionality for the adoption of the service-oriented development paradigm. Solving this problem in practice requires the ability to consider asynchronous stateful services and to express complex composition requirements which may span different phases of the life-cycle of component services. In this paper we present a novel automated service composition approach which addresses these challenges by associating so-called 'objects' to services, and by introducing a simple yet powerful notation to express composition requirements on them. We recast this view of the problem as a specific form of planning; our experiments on a prototype implementation witness the ability of our approach to deal with realistic scenarios and requirements that cannot be tackled by other current approaches."
621,"Web services and service oriented architecture (SOA) represent a radical departure from traditional monolithic, tightly bound, customized applications with proprietary interfaces. Their range however is limited as yet by the very structure and standardization which support their interoperability. While they are able to support heterogeneity of technical components, their ability to support complex business interactions has yet to mature. This paper summarizes research in progress which asks how might the Web services model be mapped to other kinds of IT services and extended to account for services of greater human and organizational complexity through their lifecycle. The research proposes the construction of a generic IT service model and a taxonomy IT services to provide a frame of reference for analysis and evaluation for the extension of the Web services model and for a transition to a service oriented view of IT."
623,"Online information sources including Weibo and Wechat bring huge impact to the society. Only a few words of network information can expand rapidly and catalyze the generation of a huge amount of information. Semantic computing on online social networks and research on topics about emergencies have great significance. In this article, a numerical model of text semantic analysis based on artificial neural network is proposed, and a semantic computational algorithm for social network texts as well as a discovery algorithm for emergencies is provided with reference to the information provided by the social nodes itself and the semantic of the text. Semantic vector of micro-information for nodes and closure extension of semantic extensions are defined in order to build up an equivalence of short sentences, and in turn realize the discovery of emergencies."
624,"Business processes continue to play an important role in today's service-oriented enterprise computing systems. Mining, discovering, and integrating process-oriented services has attracted growing attention in the recent year. In this paper we present a quantitative approach to modeling and capturing the similarity and dissimilarity between different process designs. We derive the similarity measures by analyzing the process dependency graphs of the participating workflow processes. We first convert each process dependency graph into a normalized process matrix. Then we calculate the metric space distance between the normalized matrices. This distance measure can be used as a quantitative and qualitative tool in process mining, process merging, and process clustering, and ultimately it can reduce or minimize the costs involved in design, analysis, and evolution of workflow systems"
625,"Soft resource allocation is an important factor of system configuration which plays a critical role in guaranteeing the performance of multi-tier web service systems. There is a tradeoff between real-time performance and resource consumption, and thus the real-time adjustment of soft resource allocation in response to dynamic workload is quite challenging. In this paper, we propose a real-time soft resource allocation method that integrates both model-based analysis and real-time optimization. Specifically, a multi-tier web service system is firstly formulated by a queueing network model, and theoretical analyses are provided. Then, an optimization approach for real-time soft resource allocation is designed by applying sliding window techniques, in order to cope with dynamic workloads and performance demands. Based on the RUBiS benchmark system, model parameters are obtained by measurements and the efficacy of our approach is finally validated."
626,"The word ""Web X.o"" represents ""Web eXpandoo"" which is derived from ""Web"" and a misspelled word ""Expandnoon"". ""noon"" is interpreted as ""the highest, brightest, or finest point or part"" or ""the highest point"". This paper introduces some basic features of Web X.o that expands the current Web to the ""highest and brightest"" point. The key concept of Services HyperChain in the Web eXpandoo framework is introduced to provide an enabling technology architecture that supports the future Web, which coordinates multimedia presentations, human interactions, business processes, computing resources, and business applications in a uniform fashion. Today's Web 2.0 technologies are mainly dealing with the presentation and community-based interaction aspects in the Web X.o framework. The boundary among Web, desktop applications, and backend resources are removed by creating Services HyperChain in the Web eXpandoo framework. The IEEE Body of Knowledge on Services Computing system is used as a prototype to illustrate how the Service-Oriented Architecture (SOA) and Web 2.0 technologies are used to to realize the Services HyperChain with unified services resource and services ecosystem enablement architecture of Web X.o."
627,"SOAP has been widely adopted as a simple, robust and extensible XML-based protocol for the exchange of messages among web services. Unfortunately, SOAP communications have two major performance-related drawbacks: i) verbosity, related to XML, that leads to increased network traffic, and ii) high computational burden of XML parsing and processing, that leads to high latency. In this paper, we address these two issues and introduce a novel framework for Differential SOAP Multicasting (DSM). The main idea consists in identifying the common pattern and differences between SOAP messages, modeled as trees, so as to multicast similar messages together. Our method is based on the well known concept of Tree Edit Distance, built upon a novel filter-differencing architecture to reduce message aggregation time, identifying only those messages which are relevant (i.e., similar enough) for similarity evaluation. In addition, our technique exploits a dedicated differencing output format specifically designed to carry the minimum amount of diff information, in the multicast message, so as to minimize the multicast message size, and therefore reducing the network traffic. The battery of simulation experiments conducted to evaluate our approach shows the relevance of our method in comparison with traditional and dedicated multicasting techniques."
630,"Hadoop, the open-source implementation of MapReduce, has been widely used in different projects. However, when users want to use this parallel computing framework, they have to spend time on the Hadoop cluster configuration, learning the programming API, and the MapReduce job operations. This paper proposes the Parallel Computing Framework as a Cloud Service (PCFCS) to provide the users parallel computing cluster, and simplify the configuration, programming, uploading, and operating procedures. Especially, PCFCS defines a set of annotations, with which users can quickly build their own MapReduce job."
631,"In Web service composition, the benefit conflicts between the user and the service provider ask the so-called both win to be supported. To address this concern, a novel QoS-guaranteed service composition approach based on a win-win strategy is proposed in this paper. First, a QoS model based on the probability interval is built to adapt to the dynamic nature of the Internet, and a corresponding user satisfaction evaluation method is designed. Next, a mathematical model of service composition based on game theory is proposed. Finally, a Genetic Algorithm (GA) is used to search an appropriate composite service with the Pareto optimum under the Nash equilibrium on both the user utility and the service provider utility achieved or approached. Simulation results have shown that the proposed approach is feasible and effective."
633,"Pervasive computing environments are nowadays more and more used as a supporting tool for cooperative workflows, e.g., in emergency management. A typical problem in these scenarios is the synthesis of workflows in presence of sets of services (hosted on mobile devices) with constrained behaviors, just before the collaborating team is dropped off in the operation field. In this paper, we propose a technique able to automatically synthesize distributed orchestrators, each one coordinating a service and synchronizing with the other orchestrators, given a target generic workflow to be carried out and a set of behaviorally-constrained services."
634,"We propose a comprehensive framework for adaptivity of service-based applications, which exploits the concept of process fragments as a way to model reusable process knowledge and to allow for the dynamic, incremental, context-aware composition of such fragments into adaptable service-based applications. The framework provides a set of adaptation mechanisms that, combined through adaptation strategies, are able to solve complex adaptation problems. An implementation of the proposed solution is presented and evaluated on a real world scenario from the logistics domain."
635,"Managing a highly connected and heterogeneous computing environment requires the federation of various management systems (managers) across different platforms, technologies and systems. This research work is focusing on a generic manager-to-manager (M2M) interface, which enables a seamless cooperation between autonomous managers on the basis of Web services. Next to the standard Web services protocol stack, further emerging Web services specifications are reviewed in depth for the usability in this case study."
636,"Currently, BPEL is the de-facto standard for the Web service composition. Because Web services are autonomous and loosely coupled, BPEL processes are susceptible to a wide variety of faults. However, BPEL only provides limited constructs for handling faults, which makes fault handling a time-consuming and error-prone task. In this paper, we propose a declarative approach to enhancing the reliability of BPEL processes. Our solution specifies fault handling logic through a set of event- condition-action (ECA) rules which build on an extensible set of fault-tolerant patterns. These ECA rules are integrated with normal business logic before deployment to generate a fault-tolerant BPEL process. We also develop a GUI tool to assist designers to specify ECA rules. Experiments show our approach is feasible."
637,"Employees are increasingly participating in business processes using mobile devices. Often, this is supported by a mobile web application, which accesses various web services in the back-end. The high latency of the mobile network (e.g., EDGE) is perceived by the user each time a web service is called, in addition to the time needed to invoke the service itself. This high latency may lead to low usability, low acceptance rate, and finally compromises the overall process quality. To reduce the latency perceived by the user, we present a caching architecture for web services and an adaptive prefetching algorithm. The key characteristics of our approach are the compatibility with major mobile browsers and the independence of the caching proxy from the front-end application and the back-end services. We evaluate our approach on realistic traces of web service calls in an IT service management scenario. The traces were generated from handling real incidents according to the ITIL (ISO 20000) processes. The results confirm that using our approach, the latency perceived by the user is reduced by 24%."
638,"Service-oriented architecture is emerging as a compelling paradigm for developing Web-based software applications. In this style, the functional components of the system are implemented in various programming languages as network-accessible ""services"" declaratively specified (in WSDL) and declaratively composed in workflows (using BPEL4WS). Despite this fundamentally distributed conceptualization of service composition, most current middleware assumes that the specification of the service composition is interpreted at run time by a central middleware node. This implies inflexible composition evolution: all parties must be updated concurrently to avoid interaction failures. This paper introduces an intelligent-agent framework that wraps Web services in a conversation layer and is capable of a simple workflow-adaptation function. The conversation layer implements protocols and consults globally shared, declarative policy specifications to resolve conversation failures. Two case studies illustrate this approach."
639,Provides a listing of current committee members.
640,"End-users conduct various on-line activities. Quite often they re-visit websites and use services to perform repeated activities, such as on-line shopping. The end-users are required to enter the same information into various web services to accomplish such repeated tasks. Typing redundant information repetitively into such services negatively impacts the user experience. In this study, we propose an approach to prevent end-users from such unnecessary interruption. Our approach propagates user inputs across services by linking similar input and output parameters. Our approach also pre-fills values to the input parameters which could not be filled by the values from other input or output parameters. We propose a meta-data model for storing user inputs and an Input Parameter Context Model for identifying similar input or output parameters. We have implemented our approach and evaluated it on the real world services through an empirical study. Our overall approach can reduce on average 37% of input parameters through the execution of composed services."
642,"Web services are rapidly becoming popular as a vehicle for the design, integration, composition, and deployment of distributed and heterogeneous software. However, while industry standards for the description, composition, and orchestration of Web services have been under discussion (and development) for quite some time already, their conceptual underpinnings are still in their infancy. Indeed, conceptual models/or service specification are rare so far, as are investigations based on them. This paper presents a multilevel service composition model that perceives service specification as going through several levels of abstraction, from transactional operations to the end user. Importantly, the model allows for specification of desirable composition properties at all levels. Different ways of achieving these properties as well as implications of the model are addressed."
643,"In the service oriented paradigm applications are created as a composition of independently developed Web services. Since the same service may be offered by different providers with different non-functional Quality of Service (QoS) attributes, a selection process is needed to identify the constituent services for a given composite service that best meet the users QoS requirements. In this paper, we consider a broker that offers a composite service with multiple QoS classes to several users each generating a flow of requests over time. We propose a service selection scheme which optimizes the end-to-end aggregated QoS of all incoming flows of requests by means of a simple linear programming problem which scales as the number of users, request volumes and/or services grows. This approach differs from most of the current proposals which may not scale well since: a) requests, even from the same user, are handled independently from one another; and b) the selection process often requires the solution of an NP-hard problem."
644,"Tracking the execution of composite Web services to identify and adjust their specification according to the current features of the environment is a challenging issue. The concept of views, as a dynamic snapshot over this specification according to a given context, is proposed in this paper. A view is used as a support means for tracking the execution progress of composite Web services and deploying the corrective measures in case of non-compliance with users' requirements. Our contributions are a definition of what a view means in the context of Web services composition, an approach for specifying user context and its respective view, and mechanisms for extracting and visualizing views over specifications of composite services."
645,"Currently process algebras have been effectively used for describing and reasoning web services and their composition, but they are lack of cost modeling and analyzing capability and can't support web services composition with optimal cost. This paper proposes a process algebra called PPA (Priced Process Algebra) basing on CCS (Calculus of Communicating Systems) [1]. We discuss the syntax and semantics of PPA extended with priced information and provide an algorithm to support web services composition with optimal cost."
646,"This paper proposes a new model for caching Web service response data in a mobile ad hoc network (MANET). The aim is to enable any node joining the ad hoc network to either contribute services to other nodes or to consume services offered by other nodes. This model attempts to coordinate the service discovery and service use processes while maintaining minimal communication among nodes. The system comprises proxy caches (PCs), for acting as the interface to remote Web services and the internally cached service responses, request directories (RDs)for caching the cache keys that act as indexes to the responses, and finally, the caching nodes (CNs) that cache the responses. The CNs are the mobile nodes that requested the cached responses while the RDs are the ones that store the cache keys generated from the submitted requests and hold pointers to the CNs that store the responses. The PCs, RDs, and CNs are added (assigned) based on need according to capability criteria. Experimental results show the superiority of the system over non-caching and illustrate its performance in terms of delay and load versus hit ratio, which is a function of the system's caching capacity"
647,"An important issue in QoS-aware Web service composition is how to select a set of Web services to perform the tasks within a requested service while meeting global QoS constraints. We consider the Web services are self-interested and will use dynamic pricing strategy. In general, the service cost is the minimum price acceptable to a Web service. We can obtain a composite Web service with the maximum utility by assigning the tasks to the Web services with the lowest costs. A Web service usually will not expose his cost, and thus, we face a decision making problem with incomplete information. Recent approaches use iterative combinatorial auction to address the problem. However, truthful bidding is not optimal strategy for Web services in these approaches. In this paper, we propose an incentive mechanism for choosing the optimal Web service for each task and show there exists a Bayesian Nash equilibrium of Web services, in which each Web service will bid truthfully. Finally, the experimental results show that our mechanism outperforms the existing combinatorial auction-based approaches."
648,"Web services are considered reusable elements that can be exploited stand-alone or as part of wider, generic or domain-specific, applications. With a vast variety of Web Services being offered by service providers, annotation techniques that add tags to services automatically are frequently exploited with the aim of retrieving relevant services faster. Tagging of Web Services at finer levels of granularity can assist in drawing more detailed conclusions, making their use easier. In this paper, we go beyond global Web Service tagging by proposing a classification technique for the annotation of Web Service sections. The process focuses on port type, operation and message sections of the service description. Service information as offered through WSDL and the provider and user descriptions is considered. The initial evaluation results demonstrate the efficiency of the approach in providing annotations at these different service levels."
649,"In the Internet of Things (IoT) era, an increasing number of data management applications, such as for connected vehicles and smarter cities, face the challenge of querying and analyzing massive volumes of spatiotemporal data. These applications frequently perform queries that join moving objects with spatial data, such as selecting sub-tracks crossing a bridge. However, spatiotemporal queries are not well supported or natively supported by current state-of-the-art relational database systems. Most of existing systems build a spatial index directly over the raw spatiotemporal data, which leads to performance issues when scaling out for both indexing and query. In this paper, we focus on building a Spatio Temporal historian as a Service (STaaS) by extending the IBM Blue mix Time Series Database service. The STaaS service manages to process spatiotemporal queries over high volume historical data. The experiments show that STaaS service could easily scale out by adding shards, and achieve dramatic speed-up on spatiotemporal query with support of our hybrid data store. Moreover, we have already deployed STaaS on Blue mix Staging (Internal User Testing) Zone to collect feedback for improvement before porting it into the product zone in the future."
650,"Boeing is an aerospace company, and both airplanes and aviation services are our core businesses. Founded in 1916, Boeing has had a long relationship with China. In fact, the first engineer hired by the company, Wong Tsoo, was a Chinese national. Boeing is projected to sell more airplanes to China than other countries outside of United States in the next 20 years. Boeing's central research organization supports the enterprise by developing technology in support of the company's current and future products, streamlining operations, and improving tools and processes. In the past few years, we have been collaborating with Chinese universities in Beijing, Shanghai, Nanjing, and Hong Kong to develop technologies for net-enabled applications for commercial aviation. We expect this effort will continue in the years to come. Boeing provides a long list of services to airlines in streamlining their operations. Recent models of Boeing aircraft such as the Next-Generation 737 and 787 Dreamliner will be net-enabled in the air and on the ground. We are just beginning to explore the opportunities net-enabling the fleet can bring to the end customers in terms of safety, cost, and passenger experience. Airplanes are mobile nodes in an IP world, offering tremendous opportunities and challenges for researchers. In this speech, we will explore how net-enabling and Web services can help us to exploit the opportunities and overcome the challenges."
651,"Service-oriented computing (SOC) suggests that the Internet will be an open repository of many modular capabilities realized as Web services. Organizations may be able to leverage this SOC paradigm if their employees are able to ubiquitously incorporate such capabilities and their resulting information into their daily practices. It is impractical to assume that human users will be able to search vast distributed repositories at real-time. In addition, automated search tools may invasively present too much information. This paper presents an architecture, software agent-based groupware using e-services (SAGE), that incorporates the use of intelligent agents to integrate human users with Web services. SAGE provides background search and discovery approaches thus enabling human users to exploit service-based capabilities that were previously too time-consuming to locate and integrate. We present a multi-agent system where each agent learns the rule-based preferences of a human user and manages the incorporation of Web services"
652,"To adapt systems in response to new business requirements with SLA, business process performance simulation will play a key role. This paper proposes a performance model for business process that comprises control flow structure (such as loop) and parallel message execution under shared resources. A network queuing model is inappropriate to model dynamic workload in the business process execution since messages are processed in parallel consuming shared resources. In our previous work, we have proposed a statistical service model to estimate execution time based on workload overlap analysis. In this paper, we extend this approach to model a business process with control flow. We integrate a Timed Petri Net model and our statistical models for services and process engines. We have implemented a prototype system for supply chain scenario. Experiments with this prototype show high accuracy of performance simulation."
653,Service computing has emerged as a primary platform for packaging and delivering computing and information technology to various enterprises ranging from business to governments and organizations. In recent years we have witnessed many emerging standards and industrial developments in the area of service computing.
654,"This paper describes an approach to discover and verify two fundamental relationships in web service message exchange flow patterns, i.e. support and correlate, based on the abstract WSDL specification of the service operations. Compared to methods used in previous studies, this approach is a significant improvement in that flow chain relations can be discovered and identified based on flexible and inexact match of XML Schemas using differential information in plain WSDL and XML Schema files. This approach is also efficient and extensible as it is based on a tree model of schemas and modular node equivalence rules. Pilot experiments on some web services standards, e.g. WS-Session, WS- Eventing, Parley X, etc., indicated that the proposed method is accurate and effective in identifying the message flow dependencies and related issues in service descriptions. The derived flow pattern relations can be represented as Petri nets that have some practical applications for web service verification, implementation and consumption."
655,"Developers write Web service composition programs in terms of functionalities (e.g., ""WebSearch"") to postpone choosing which services of the same functionality to invoke (Google or Yahoo). We provide a higher level of abstraction than this for higher reuse. We express high-level ""patterns"" (e.g., ""SearchAndCollectData"") as both objects that can be ""specialized"" to particular applications (""SearchAnd-DownloadPapers"" vs. ""SearchAndAddBooksInCart"") and objects that are reusable in the construction of higher-level ones. Our approach lets developers write patterns in terms of high-level functionalities (e.g., ""CollectData "") and later decide on services to compose that have lower-level functionalities (e.g., ""DownloadPapers"" or ""addBooksIn-Carts""). We describe our prototype and show an example of nested pattern specialization. We also discuss a reuse trade-off, showing that too much abstraction makes the pattern less expressive. Rather, we suggest developers capture what must be guaranteed in every context of invocation, regardless of the service selection."
656,"The Web service paradigm promotes the delivery of software as a service via the Web, instead of a product to end-users. The increase in assigned responsibilities for supporting organizational operations drives the needs for higher operational efficiencies (i.e. performance and reliability) and flexibilities (i.e. customizations). This paper makes two contributions. An infrastructure to support the development and operation of customized cost model components, used to manage cost efficiency and reliability issues. Secondly, a generic cost model algorithm that can be used as the foundation algorithm for cost model components. Further, the process of Web service relocation is also demonstrated and implementation has also been discussed."
657,"The configuration of a service-based system has a significant impact on the nonfunctional requirements of the system as a whole. However, finding the best configuration is very challenging and sometimes impossible for administrators because so many factors have to be considered. More importantly, a service based system has to be frequently reconfigured to adapt to rapid and continuous changes in user requirements and runtime environments. In this paper we propose an autonomic computing approach to the problem of reconfiguration, that is, enabling the service based system to configure itself by means of a loop of monitoring, analyzing, planning and executing actions. We begin by formalizing the definition of configuration and reconfiguration. Then, we describe how we implemented the autonomic computing mechanisms for reconfiguring service-based systems to satisfy service level agreements with minimal resource consumption. The approach is demonstrated on a resilient service provisioning environment. Finally, the preliminary experiments are evaluated to determine the effectiveness of proposed approach."
658,"With an ever-increasing number of Web services being available, finding desired Web service is crucial for service users. Current keyword search and most existing approaches are inefficient in two main aspects: poor scalability and lack of semantics. Firstly, users are overwhelmed by the huge number of irrelevant services returned. Secondly, the intentions of users and the semantics in Web services are ignored. Inspired by the success of the divide and conquer approach used to handle the complex information decomposition, we use a novel approach to partition a large set of search results into a set of smaller groups by employing a clustering approach. Then we utilize singular value decomposition (SVD) to capture the main semantics hidden behind the words in a query and the descriptions in the services, so that service matching can be carried out at the concept level. We report here on the preliminary experimental evaluation that shows improvements overall precision."
659,"Service identification meets with new challenges with overwhelming rise of categories and numbers of services in big data scenarios. Most of the current service identification approaches have paid little attention to the granularity of indicator for service identification, neither do they provide with any trustworthy monitoring mechanism during the process of service identification. To address the problems above, we propose a user requirements based service identification approach for big data (URBSI-BD). In the proposed URBSI-BD, we firstly cluster massive services with BIRCH clustering algorithm to obtain a number of service sets. We then employ PSO algorithm with MapReduce mechanism to achieve a fine-grained evaluation of indicator for service identification. Based on the integration, candidate services which can better meet with user requirements will be selected. Finally, we use Beth trust model on the quality of experience of users and set up a monitoring mechanism to better obtain required services. Simulation results and analysis demonstrate that the proposed approach has better performance in service identification compared with other current approaches in big data scenarios."
660,"The recent surge of popularity has established mashups as an important category of Web 2.0 applications. Mashups are essentially Web services that are often created by end-users. They aggregate and manipulate data from sources around the World Wide Web. Surprisingly, there are very few studies on the scalability and performance of mashups. In this paper, we study caching as a vehicle for enhancing the scalability and the efficiency of mashups. Although caching has long been used to improve the performance of Web services, mashups pose some unique challenges that necessitate a more dynamic approach to caching. Towards this end, we present MACE - a cache specifically designed for mashups. In designing the MACE framework this paper makes three technical contributions. First, we present a model for representing mashups and analyzing their performance. Second, we propose an indexing scheme that enables efficient reuse of cached data for newly created mashups. Finally, this paper also describes a novel caching policy that analyzes the costs and benefits of caching data at various stages of different mashups and selectively stores data that is most effective in improving system scalability. We report experiments studying the performance of the MACE system."
661,"Service-orientation paves the way for the Internet of Services (IoS), where millions of services will be available for building novel applications. As such, the service non-functional aspect should be considered for filtering and selecting among the great number of functionally-equivalent services that will be available for a specific user task. Until now, the state-of-the-art work in non-functional service discovery has exploited constraint solving techniques to optimize the matchmaking time between a non-functional service offer and demand pair. However, as matchmaking time is proportional to the offer number, this work does not scale well so it is not yet appropriate for the IoS. To this end, two alternative techniques are proposed to improve the overall matchmaking time. Both techniques were theoretically and experimentally evaluated. The results show that both techniques optimize the matchmaking time without sacrificing accuracy, while the second technique is quite scalable."
662,"In this paper, we have studied a common Web service composition problem, the syntactic matching problem, where the output parameters of a Web service can be used as the input parameters of another Web service. Many automatic Web service composition algorithms based on AI planning techniques have been proposed. However, most of them do not scale well when the number of Web services increases, or may miss finding a solution even if one exists. The planning graph, another AI planning technique, provides a unique search space. We have found that when we model the Web service composition problem as a planning graph, it actually provides a trivial solution to the problem. Instead of following the usual way to find a solution by a backward search, we put our efforts into removing the redundant Web services contained in the planning graph. Our approach can find a solution in polynomial time, but with possible redundant Web services. We have tested our algorithms on the data set used in ICEBEpsila05 and compared our results with existing methods."
663,"Telecommunication is a key area where the use of web services has an enormous potential to facilitate the development of powerful and complex functionalities on top of existing ones. At the same time, the specific features of TelCo applications (asynchronous, event-driven communications, handling concurrent, long-running transactions, dynamically evolving sets of partners) pose difficult challenges to the adoption of standard languages and tools for web services. In this work, based on our analysis of TelCo standards and applications, we discuss a set of orchestration patterns, and show how these can be modeled using the WS-BPEL language. We also address the issue of guaranteeing the correctness of such applications, providing an automated support for the formal analysis of their behavior."
664,"Web service composition is a topic bringing several issues to be resolved. Our work deals with the effectiveness and scalability of service composition. During composition we consider QoS and pre-/post-conditions of single services to create a composite service satisfying the user needs the best. Regarding pre-/post-conditions we propose an approach to fast determination of which services produce results expected by the user, i.e. the post-condition of which services implicates the desired condition defined in the user goal. This paper proposes also an approach to restriction on the service space which provided a dramatic improvement in terms of composition time."
665,This paper introduces an automatic Web service composition method based on logical inference of Horn clauses in Petri net models. The available services and user request described in SA WSDL are translated into a set of Horn clauses and the composability rules of the services' input/output parameters are established using ontology reasoning. We choose Petri net as the model of this set of Horn clauses. T-invariant method of Petri nets is used to determine the existence of composite Web services that can fulfill user's requirement.
666,"The information service is a key component of a grid environment and critical to the operation of a computational grid. In this work, an OGSA (Open Grid Services Architecture) based information service that complies with OGSI (Open Grid Services Infrastructure) is presented. The main functionality of this information service is the provision of information essential for applications running on a computational grid such as resource information, job status, resource workload, service meta-information, and queue status. This OGSI-compliant information service is built on Globus Toolkit MDS-3, and it works with meta-scheduling services and local job scheduling systems to support resource discovery, job scheduling, and execution management. In this paper, the architecture of the Information Service and the models of information data organization are presented. Some implementation issues are discussed as well."
667,"Dynamic service composition has emerged as a promising approach to build complex runtime-adaptable applications as it allows for binding service providers only shortly before service execution. However, the dynamic and ad hoc nature of mobile computing environments poses a significant challenge for dynamic service composition. In particular, the lack of central control and the potential volatility of service providers increase the complexity and failure probability of the composition process. Although, current research has led to decentralised composition algorithms and failure recovery strategies, the key question of how to reduce the failure probability of a composition still remains. We address this question and propose opportunistic service composition, an optimised execution model for complex service requests. The model merges the execution phase into the dynamic binding phase and supports the immediate fulfilment of partially composed service requests. We evaluated our model in mobile ad hoc network simulations. The results show an improvement over a baseline approach regarding composition success, response time, and communication effort."
668,"Event notification plays an important role in the orchestration of distributed systems. Web services-based event notification is an important step to achieve an interoperable messaging system on the Internet scale. However, different incompatible specifications have been proposed to specify the Web services interfaces for Web services-based event notification systems. The WS-Eventing specification and the WS-Notification specifications are two major initiatives. WS-Messenger is our implementation of both specifications. A mediation approach is used to reconcile the differences between these two specifications. In this paper, we propose a ""normalization-processing-customization"" (NPC) model for mediation among competing Web services and show how this model is used in WS-Messenger to reconcile the incompatibility between these two specifications. The NPC model is flexible and scalable. The mediation implementation in WS-Messenger is efficient with a very small overhead"
669,"Recently an increasing number of web applications especially cloud computing systems utilize representational state transfer (REST) API to deploy their services for simplicity and clarity. Users can employ the same interface to invoke various applications from the Internet. For security purposes, service providers would control the access to the provided interface through policy enforcement. Yet the access control of REST interfaces lacks a uniform standard regarding the policy language and corresponding enforcement implementation, which brings two limitations: i) Users have to deal with totally different types of policies to accommodate certain systems. ii) Service providers have to design their own platform-specific authorization policy language and the related enforcement mechanisms. In this paper, we propose a REST Policy Language (RestPL) to express the authorization policies especially for REST APIs. RestPL is ensured to be request-oriented, based on our definition of the standard request form. This indicates that a RestPL policy can be automatically generated from an actual request, which helps mitigate a user's pressure during policy designing. Furthermore, we also provide a reference implementation for the enforcement code of RestPL based on regular expressions and deploy it on OpenStack Liberty to demonstrate its feasibility. The experimental results indicate the enforcement overhead of RestPL can be reduced to 80.6% compared with the original policy. In addition, we show that an end-user can also benefit from RestPL for reducing the learning effort by at least 41.6%."
671,"Semantic Web services (SWS) aim at the automated discovery and orchestration of Web services on the basis of comprehensive, machine-interpretable semantic descriptions. However, heterogeneities between distinct SWS representations pose strong limitations w.r.t. interoperability and reusability. Hence, semantic level mediation, i.e. mediation between concurrent semantic representations, is a key requirement to allow SWS matchmaking algorithms to compare capabilities of distinct SWS. In that, semantic level mediation requires to identify similarities across distinct SWS representations. Since current approaches to mediate between distinct service annotations rely either on manual one-to-one mappings or on semi-automatic mappings based on the exploitation of linguistic or structural similarities, these are perceived to be costly and error-prone. We propose a mediation approach enabling the implicit representation of similarities across distinct SWS by grounding these in so-called mediation spaces (MS). Given a set of SWS and their respective MS grounding, a general-purpose mediator automatically computes similarities to identify the most appropriate SWS for a given request. A prototypical application illustrates our approach."
672,"A XML service is a software component that supports interoperable application-to-application interaction over a network. Each service makes its functionality available through well-defined or standardized XML interfaces. Aviation XML services refer to the services that make operating an airplane in air and on ground possible. In this paper, we present an emergency response framework to organize the aviation XML services to work cooperatively on mobile ad hoc networks (MANETs). A MANET is defined as a self-organized and rapidly deployed network of XML services in order to exchange information without using any pre-existing fixed network infrastructure. Note that the framework does not have to be limited to the aviation sector. The methodology can also be adopted into other MANET computing scenarios including: natural disaster communications (e.g., tsunami, earthquakes), emergency relief scenarios, car-based networks, and the provision of wireless connectivity in remote areas."
673,"The infiltration of the Web as the preferred choice for online e-commerce and IT applications has motivated the need to introduce innovative mechanisms to enforce strong security monitoring and control on transactions conducted over a distributed environment. The smart card technology presents an attractive solution for providing strong security and access control mechanisms that are tightly bound and associated to the individual carrying and owning the card. Despite its potential, smart card services have not been integrated into the networked environment in the way other portable computing devices, such as notebooks, PDA, mobile phones, and so on, have been. This has motivated our design for a distributed computing platform for smart card services to operate over a service-oriented architecture (SOA) based on evolving Web services technology. In particular, the paper presents the design of the WSCard (in short for Web services card) architecture and its implementation based on the application of Web services wrapping technology. Importantly, the design allows us to exploit Web services flexibility to provide a heterogeneous and promote rapid integration of smart card services as first class citizens of Web services. With WSCard, existing service-oriented applications can seamlessly interoperate with smart card services by treating the card services as native Web services, while employing established Web services standards - SOAP, WSDL and UDDI."
675,"A conversation protocol is a top-down specification framework which specifies desired global behaviors of a Web service composition. In our earlier work (Fu et al., 2003) we studied the problem of realizability, i.e., given a conversation protocol, can a Web service composition be synthesized to generate behaviors as specified by the protocol. Several sufficient realizability conditions were proposed by Fu et al. (2003) to ensure realizability. Conversation protocols studied by Fu et al. (2003), however, are essentially abstract control flows without data semantics. This paper extends the work by Fu et al. (2003) and achieves more accurate analysis by considering data semantics: to overcome the state-space explosion caused by the data content, we propose a symbolic analysis technique for each realizability condition. In addition, we show that the analysis of the autonomy condition can be done using an iterative refinement approach."
676,"We propose a 'Sentiment Analysis as a Service' (SAaaS) framework that abstracts sentiments from social information services, analyses and transforms into useful information. We propose a dynamic service composition mechanism for sentiment analysis based on the social information service classification. We also propose a new quality model to assess the quality of social information services. We use social media based public health surveillance as a motivating scenario. In particular, we focus on the spatio-temporal properties of social media users' sentiments to identify the locations of disease outbreaks. Experiments are conducted on the real-world datasets. Analytical results preliminarily show the performance of our proposed approach."
677,"With more and more Web services available on the Internet, many approaches have been proposed to help users discover and select desired services. However, existing approaches heavily rely on the information in UDDI repositories or WSDL files, which is quite limited in fact. The limitation of information weakens the effectiveness of existing approaches. In this paper, we present a novel Web services search engine named CoWS, which enriches Web services information using the information captured from the Internet to provide quality-aware Web services search. The information captured can be classified into two groups: functional descriptions and subjective feedbacks. We use the functional descriptions to enrich descriptions of Web services and the subjective feedbacks to calculate Web services' reputation. CoWS first ranks the services according to their functional similarities to a user's query, which are calculated using both descriptions in WSDL files and the enriched descriptions, and then refines and re-ranks the services with both objective quality constraints (QoS) and subjective quality constraints (reputation). The experiments on a large-scale dataset (including 31,129 Web services) show that CoWS can improve the effectiveness of both Web services discovery and selection comparing with existing approaches."
678,"Object-oriented programming languages are losing pace with the rapidly evolving Web Services paradigm. They are unable to deal with the distributed, volatile nature of web services, and they lack expressive language constructs for complex web service interactions. In this paper, we present ServiceJ, a Java extension with integrated support for web service programming. By extending the type system with type qualifiers, ServiceJ realizes important objectives from both paradigms. On one hand, ServiceJ resembles Java by promoting reuse and abstraction, and by supporting type- safe web service invocations. On the other hand, ServiceJ supports late web service binding, Quality of Service negotiation, and transparent web service failover. We provide an in-depth discussion of our type system extension, formally prove its type soundness, and compare our language extension with related object-oriented and service-oriented programming languages."
680,"We propose a six dimensional QoWS model including expected QoWS, agreed QoWS, delivered QoWS, perceived QoWS, transmitted QoWS, and statistic QoWS to assess quality of Web services comprehensively and objectively. Meanwhile, the proposed assessment mechanism evaluates Web services from the view of compliance, end-to-end performance, and long-term performance."
681,The growing need for enterprises to have instantaneous access and visibility to data is fuelling the need for enterprise data integration. An enterprise having fragmented systems on varied technologies is a very commonly occurring scenario. In this paper we take a business case scenario with heterogeneous systems and describe a non-intrusive service oriented approach for achieving enterprise wide data integration. We take a holistic view of the problem statement and propose an end to end architectural approach that encompasses the ETL activities to the shared data access layer. This meta-data based architecture is highly extensible requiring minimal change in existing applications and business process yet adhering to the long term architectural strategy. We also propose the creation of fine grained shared business services which are supported by underlying shared data services.
682,"Service evolution is the process of maintaining and evolving existing Web services to cater for new requirements and technological changes. In this paper, a service evolution model is proposed to analyze service dependencies, identify changes on services and estimate impact on consumers that will use new versions of these services. Based on the proposed service evolution model, four service evolution patterns are described: compatibility, transition, split-map, and merge-map. These proposed patterns provide reusable templates to encourage well-defined service evolution while minimizing issues that arise otherwise. They can be applied in the service evolution scenario where a single service is used by many, possibly unknown, consumers' applications. In such a scenario, providers evolve their services independently from consumers, which might cause unexpected errors and incur unpredicted impact on the dependent consumers' applications. Therefore, providers can use these patterns to estimate the impact that changes to be introduced to their services may cause on their consumers, and to allow consumers smoothly migrate to the newest version of the service."
683,"The concept of abstract services has been widely adopted in service computing to specify the functionality of certain types of Web services. It significantly benefits key service management tasks, such as service discovery and composition, as these tasks can be first applied to a small number of abstract services and then mapped to the large scale actual services. However, how to generate abstract services is non-trivial. Current approaches either assume the existence of abstract services or adopt a manual process that demands intensive human intervention. We propose a novel approach to fully automate the generation of abstract services from a service community that consists of a set of functionally similar services. A set of candidate outputs are first discovered based on predefined support ratio, which determines the minimum number of services that produce the outputs. Then, the matching inputs are identified to form the abstract services. We propose a set of heuristics to effectively prune a large number of candidate abstract services. An comprehensive experimental study on real world web service data is conducted to demonstrate the effectiveness and efficiency of the proposed approach."
685,"Due to the popularity of smartphones, finding and recommending suitable services on mobile devices are increasingly important. Recent research has attempted to use role-based approaches to recommend mobile services to other members among the same group in a context dependent manner. However, the traditional role mining approaches originated from the domain of security control tend to be rigid and may not be able to capture human behaviors adequately. In particular, during the course of role mining process, these approaches easily result in over-fitting, i.e., too many roles with slightly different service consumption patterns are found. As a result, they fail to reveal the true common preferences within the user community. This paper proposes an online role mining algorithm with a residual term that automatically group users according to their interests and habits without losing sight of their individual preferences. Moreover, to resolve the over-fitting problem, we relax the role mining mechanism by introducing quasi-roles based on the concept of quasi-bicliques. Most importantly, the new concept allows us to propose a monitoring framework to detect and correct over-fitting in online role mining such that recommendations can be made based on the latest and genuine common preferences. To the best of our knowledge, this is a new area in service recommendation that is yet to be fully explored."
686,"During the process of building business agility, the concept of Service Oriented Architecture (SOA) is proposed and widely lauded as an innovative business oriented solution. One of the most fundamental components in SOA based applications is a service, which represents the repeatable business functionalities to help the development of loosely coupled applications. Due to its important role in implementing SOA based applications for achieving dynamic business process, how to build a scalable, reliable service efficiently has become a vital challenge, while the service lifecycle management becomes one of the critical mechanisms to ensure the service quality. Currently, there are a large number of service lifecycle models proposed in the literature to fit their own purposes. However, few of them clearly indicate the integration of the lifecycle processes with stakeholders. To meet this gap, in this research a conceptual stakeholder analysis framework is proposed to associate the stakeholders with different lifecycle stages. It is believed that this method is able to offer the researchers in the community further insight into service lifecycle management from the stakeholder's perspective."
687,"Implementing and maintaining non-functional properties related to the monitoring of Quality of Service (QoS) can be expensive and complex tasks; with this paper, we present a model-based approach to the implementation of QoS monitors. Our approach uses platform-independent models in order to describe the system and its related QoS constraints. Then, we conduct model-driven development (MDD) transformations for the implementation of both the system and its QoS constraints. Following the aspect-oriented programming (AOP) paradigm, QoS constraints are implemented as aspects, which are weaved into the system implementation. Finally, we present a case study in order to support the evaluation of our approach."
688,Provides a listing of current committee members.
690,"In this paper, we present the design and the architecture of the CloudDB AutoAdmin system which aims to fill the existing gaps between the provided cloud database services and the requirements of the consumer applications. In particular, it focuses on facilitating the job of the cloud database consumers in implementing database applications as distributed, scalable, and elastic services with a minimum effort on the side of the application developer and a limited footprint in the application code."
691,"The response time of SOAP invocation over Http is an important designing factor and evaluating metric for QoS and Web services related computing. This paper presents a novel model to compute response time of SOAP over Http/1.1. It takes several influencing parameters into account, including compression methods, maximum segment size, round trip time, initial value of time-out sequence, the number of packets per ACK, maximum congestion control window size and packet loss rate. How to set these parameters to compute the response time of SOAP over Http/1.1 is illustrated and the model is validated with TPC-H benchmark based simulation data."
692,"For some, the ultimate goal of the semantic web service initiative is automated interoperation and process composition. Research activity is intense in this field and many approaches have been published and discussed in Academia. However, the predominant view is focused on technical issues, although the business aspects seem to be equally important. The introduction of interoperation and process composition is complex and has a great impact on the enterprise and its value chain network. Hence, industrial scale business cases are missing, a significant drawback that is preventing new technologies from entering the market. A fundamental problem in defining a business case is the lack of a conceptual framework. We need a model of the various factors that influence the enterprise and its value chain network when automated process composition is introduced. This paper presents such a framework, which may be a help in identifying future business cases and in attracting business partners."
693,"UDDI registries are intended to become the world-wide lookup mechanism for Web-services. As such, the registry has to provide high throughput, low response times, high availability, and access to accurate data. Replication is often used to satisfy such requirements. Various replication strategies exist, favoring different subsets of the above performance metrics. In this paper we have a closer look at two very different replication strategies. One strategy follows the UDDI specification, the second uses a middleware based replication tool. In this paper, we provide a comparison of these two approaches focusing on performance and ease of integration with an existing UDDI implementation."
694,"Data-intensive applications are calling for (1) significant performance improvement in data processing, and (2) lowered entries for domain expertise to create higher value. Pursuing a database approach to the challenges, the paper presents an automated database extension mechanism, which allows one to seamlessly program application level workflows, and computations to be pushed down to (heterogeneous) database systems, in one programming language. We thus greatly ease the job of implementing scalable data intensive computations, with multiple databases as the computation platform."
695,"As the use of services available on the Web is becoming mainstream, contracts and legal aspects of the relationship between providers and consumers need to be formalized. However, current proposals to model service level agreements are mostly focused on technical aspects, do not explicitly provide semantics to agreement terms, and do not follow Web principles. These limitations prevent take-up, automatic processing, and effective sharing of agreements. Linked USDL Agreement is a Linked Data based semantic model to describe and share service agreements that extends Linked USDL, which offers a family of languages to describe various technical and business aspects of services. We followed a use case driven approach, evaluating the applicability of our proposal in a cloud computing scenario, and comparing its expressiveness with existing models. Finally, we show a concrete tool that helps to model and check the validity of agreements."
696,"With the rapid growth of cloud computing, developing business applications on Platform-as-a-Service (PaaS) systems is increasingly popular among industry companies. Various services are developed to support different business requirements on PaaS systems. However, to the best of our knowledge, currently there is no service that provides mobile social messaging services to enable users of their apps to share messages in their social networks (e.g., We Chat, Whats App, Kaka Talk). Lack of such mobile social messaging services prevents industry companies from succeeding in drastic market competitions (e.g., Capture high customer satisfactory). In this paper, we propose a service-based framework to enable the mobile social messaging in PaaS systems (e.g., IBM Blue mix). Using this framework, developers can focus on the service encapsulation of existing applications, and define their business process flows via the conversation management in our platform (no coding work is needed). As such, our framework can effectively reduce the development workload for mobile social messaging in PaaS systems."
697,Within the past years the Web service technology emerged into more and more fields of application. In some cases the classical approach of using HTTP as a transport binding for SOAP seems no longer adequate. As a full scale application protocol HTTP causes a high amount of protocol overhead and is too inflexible for many Web Service scenarios. In this paper we initially give an in-depth review of existing transport bindings with a strong focus on data rate efficiency. Then we introduce an advanced UDP binding called PURE that significantly reduces the protocol overhead and enables interesting additional features such as point-to-multipoint communication via IP multicast and broadcast.
698,"In an emergency, response services require guaranteed availability regardless of failures in data transport layer and cloud service providers. In the work-in-progress reported here, we discuss three projects that have requirements that stress web infrastructure in preparing for and responding to emergencies. Our goal is to highlight the tradeoffs in costs and benefits of web services in each case, and to provide an indication of our approaches in addressing these issues."
699,"Service Oriented Computing enables distributed applications that orchestrate existing services exported by remote providers. This paradigm requires to explicitly handle possible changes that may affect the orchestration. They include changes that impact its functional behavior (e.g., services being retired by their providers), but also changes in the non-functional behavior of the orchestrated services (e.g., an increased execution time). In the past we developed DSOL: it combines a declarative language to model the orchestration with planning mechanisms to decide at run-time the best flow of actions. In this paper we extend DSOL to support QoS attributes and requirements. In particular, we combine the DSOL planning techniques with a linear optimizer to calculate the optimal plan w.r.t. the QoS requirements. Moreover, we leverage the DSOL ability to adapt the orchestration flow at run-time, to further optimize the QoS perceived by the end users depending on the actual situations encountered."
700,"As REST architectural style gains popularity in the web service community, there is a growing concern and debate on how to design Restful web services (REST API) in a proper way. We attribute this problem to lack of a standard model and language to describe a REST API that respects all the REST constraints. As a result, many web services that claim to be REST API are not hypermedia driven as prescribed by REST. This situation may lead to REST APIs that are not as scalable, extensible, and interoperable as promised by REST. To address this issue, this paper proposes REST Chart as a model and language to design and describe REST API without violating the REST constraints. REST Chart models a REST API as a special type of Colored Petri Net whose topology defines the REST API and whose token markings define the representational state space of user agents using that API. We demonstrate REST Chart with an example REST API. We also show how REST Chart can support efficient content negotiation and reuse hybrid representations to broaden design choices. Furthermore, we argue that the REST constraints, such as hypermedia driven and statelessness, can either be enforced naturally or checked automatically in REST Chart."
701,"With the proliferation of Web services as a business solution to enterprise application integration, ranking and selecting the best Web services among the providers become an important factor in the success of the business solution. Quality of service (QoS) determines the quality and usability of a service including its price, performance, reliability, integrity, accessibility, availability, interoperability, and security. Given a set of QoS attributes from a variety of sources, it is a challenge to sort through all of them and be able to get the best services that meet QoS requirement. In this paper, we describe a novel method by which Web services can be ranked and selected automatically based on a number of observed QoS parameters and feedback responses learned from prior knowledge. This new approach treats the observed Web services QoS attributes and target Web services relationship, represented by a matrix, as a statistical problem. Using singular value decomposition (SVD) technique, and an user assisted weighting system, implicit higher order correlations among Web services and their associated QoS attributes are extracted and used to estimate the selection of recommended Web services."
703,"Composite web services are usually coordinated according to a workflow, composed by several activities, each of which carried out by a service. A way to coordinate this cooperation is orchestration, which implies that the workflow underlying the composite web service is processed by a broker hosting a workflow engine (e.g., BPEL engine). According to the orchestration paradigm, the broker coordinates the invocation of services involved in the composition by passing the needed parameters. In general, all previous proposals for the service orchestration model consider the broker as a trusted entity. As such, they never payed attention to the fact that the broker is able to access several pieces of sensitive data. We believe there is the need to protect them against improper access and usage from partner services as well as the broker. To cope with these issues, in this paper, we propose a protocol based on a selective encryption able to ensure that both the broker and service partners can access only the information needed to fulfill their activities."
704,"As the amount of Web services over the Internet grows continuously, these services can be interconnected to form a service overlay network (SON). On the basis of SON, building value-added services by service composition is an effective method to satisfy the changeable functional and non-functional QoS (quality of service) requirements of customers. However, the previous research on QoS- aware service composition in SON mainly focuses on the context where services have simple interactions, and it can not support application scenarios with complex business collaboration in electronic business. In this paper, we propose the HOSSON (hierarchical service composition framework in SON) framework, which can be used to construct more general-purpose SON through describing the relations among services using business protocols. In HOSSON, business protocols instead of interactive messages are adopted to simplify the description of service composition requirements and a novel approach named protocol computing are proposed to implement service composition on demand. Furthermore, two algorithms, OSS and MCSS, are designed to support service selection for QoS-aware service composition. Finally, comprehensive simulations are conducted to evaluate the performance of algorithms."
705,"In order to choose from a list of functionally similar services, users often need to make their decisions based on multiple QoS criteria they require on the target service. In this process, different users may follow different decision making strategies, some are compensatory in which only an overall value on all the criteria is evaluated, some evaluate one criterion at a time in the order of their importance levels, while others count on the number of winning criteria. Most of the current QoS-based service selection systems do not consider these decision strategies in the ranking process, which we believe are crucial for generating accurate ranking results for individual users. In this paper, we propose a decision strategy based service ranking model. Furthermore, considering that different users follow different strategies in different contexts at different times, we apply a machine learning algorithm to learn a personalized ranking model for individual users based on how they select services in the past. Our experiment result shows the effectiveness of the proposed approach."
706,"UDDI registration center provides a set of management mechanism for Web services providers to publish their Web service and for Web service consumers to inquire what they needs. It solves the problem of Web service description, discovery and Integration. Web services graph is the semantic index established on the UDDI registration center according to the logical relations between web services [1,2]. It can enhance the Web service discovery efficiency. jUDDI is an open source Java implementation of UDDI specification for Web Services. Based on the research on UDDI and Web service graph, this article takes jUDDI as an example to show how the traditional UDDI registry center can be extended so as to supporting Web service graph."
707,"This paper describes a Web service that automatically parses and extracts data records from Web pages containing structured data. The Web service allows multiple users to share and manage a Web data record extraction task to increase its utility. A recommendation system, based on the probabilistic latency semantic indexing algorithm, enables a user to find potentially interesting content or other users who share the same interests with the user. A distributed computing platform improves the scalability of the Web service in supporting multiple users by employing multiple server computers. A Web service interface allows users to access the Web service, and allows programmers to develop their own applications and, thus, extend the functionality of the Web service."
708,"This paper proposes a model-based technique for lowering the entrance barrier for service providers to register services with a marketplace broker, such that the service is rapidly configured to utilize the brokerpsilas local service delivery management components. Specifically, it uses process modeling for supporting the execution steps of a service and shows how service delivery functions (e.g. payment points) ldquolocalrdquo to a service broker can be correctly configured into the process model. By formalizing the different operations in a service delivery function (like payment or settlement) and their allowable execution sequences (full payments must follow partial payments), including cross-function dependencies, it shows how through tool support, the non-technical user can quickly configure service delivery functions in a consistent and complete way."
709,"The emergence of heterogeneous big data in the last decade calls for a hybrid data service that can manage all different kinds of data, including relational data, JSON data, and text data in a unified way. Among them, text data play an important role in many fields such as Internet-of-Things, biology, social network, and etc. For example, a smart meter application detecting the anomaly of the electricity use might want to link each anomaly of a certain area to a meaningful social event mined from the news in plain text. As a result, text data services have raised more and more attentions by the research community. Most of such services are implemented based on a content management system such as ElasticSearch and Solr. However, we found that the mere content management capabilities are not enough. On one hand, text data query often requires join operations to relational data or JSON data in an existing DBMS. On the other hand, users often have to pull the big text data out to an independent system or service for further text analytics. In this paper, we present our Text-DataBase-as-a-Service (TDBaaS), which is built on top of the Hybrid Data Service (HDS) from IBM Research. The TDBaaS is designed to manage the text data together with relational data and JSON data in a single service. Basic text analytics can be conducted directly inside the database in the form of general SQLs. Moreover, the extensible framework allows the service to have abundant text analytic capabilities with high performance. As a case study, we investigate in the implementation of the top-k word algorithm, and show how the common computations are shared across different tenants in the TDBaaS. The experimental results demonstrate the high performance of the TDBaaS on both text data management and text data analytics."
710,"The functionality of applications is increasingly being made available by services. General concepts and standards like SOAP, WSDL, and UDDI support the discovery and invocation of single Web services. State-of-the-art process management is conceptually based on a centralized process manager. The resources of this coordinator limit the number of concurrent process executions, especially since the coordinator has to persistently store each state change for recovery purposes. In this paper, we overcome this limitation by executing processes in a peer-to-peer way exploiting all nodes of the system. By distributing the execution and navigation costs, we can achieve a higher degree of scalability allowing for a much larger throughput of processes compared to centralized solutions. This paper describes our prototype system OSIRIS, which implements such a true peer-to-peer process execution. We further present very promising results verifying the advantages over centralized process management in terms of scalability."
711,"Complex services can be described by service compositions and executed by service orchestrations. Changing service compositions is a frequent requirement in practical settings. Changing the composition must not result in a violation of its functional or non-functional properties. Whereas functional aspects such as soundness have been studied quite well, non-functional properties such as transactionality have been paid little attention to in the context of change. However, in practical applications it is impossible to separate the aspects of transactionality and change. In this paper, we investigate the effects of applying changes in transactional service compositions. For this we analyze the combination of concepts from the worlds of transactional service compositions and process change. Based on the analysis results, we derive algorithms to deal with change in transactional service compositions. We discuss the algorithm design and their practical applicability."
712,"One of the key challenges to successful systems-of- systems integration using Web services technologies is how to address crosscutting architectural concerns such as policy management, governance, and authentication, while still maintaining the lightweight implementation and deployment flavor that distinguishes Web services from earlier attempts at providing interoperable enterprise systems. To address this challenge, this article introduces the notion of a Rich Service, an extension of the standard service notion, based on an architectural pattern that allows hierarchical decomposition of system architecture according to separate concerns. Rich Services enable the capture of different system aspects and their interactions. By leveraging emerging Enterprise Service Bus technologies, Rich Services also enable a direct transition from a logical to a deployed service-oriented architecture (SOA). This results in immediate benefits not only in SOA design, implementation, deployment, and quality assurance, but also in the traceability of architectural requirements to an SOA implementation."
713,"Delivering services cost-effectively benefits from economies of scale, not dissimilar to the production of goods. Customizing services to the specific needs of a customer - either the service interfaces or the delivery system - incurs additional costs of setting up the service delivery system to be able to fulfill a specific variant and the excess costs of operating this additional system. Service providers need to understand the trade-off between service standardization and customization. We propose a domain independent approach to identify the variation scope of existing instances and derive a normative model of admissible configurations."
714,"This paper addresses issues relating to the efficient access and discovery of Web services across multiple UDDI business registries (UBRs). The ability to explore Web services across multiple UBRs is becoming a challenge particularly as size and magnitude of these registries increase. As Web services proliferate, finding an appropriate Web service across one or more service registries using existing registry APIs (i.e. UDDI APIs) raises a number of concerns such as performance, efficiency, end-to-end reliability, and most importantly quality of returned results. Clients do not have to endlessly search accessible UBRs for finding appropriate Web services particularly when operating via mobile devices. Finding relevant Web services should be time effective and highly productive. In an attempt to enhance the efficiency of searching for businesses and Web services across multiple UBRs, we propose a novel exploration engine, the Web service crawler engine (WSCE). WSCE is capable of crawling multiple UBRs, and enables for the establishment of a centralized Web services' repository which can be used for large-scale discovery of Web services. The paper presents experimental validation, results, and analysis of the presented ideas."
715,"Summary form only given. Software as a service (SAS) was introduced with the promise of lowering the costs associated with business software applications. To enable SAS and similar software service deployments to function smoothly, service-oriented architectures (SOAs) were introduced and have been quickly evolving for the past five years. Now, with SOAs well understood and software development environments so efficient, the return to insourcing, especially in the larger enterprises, is cutting into the earlier SAS gains. Providers for the mid-market, on the other hand, are wrestling with the dual problem - how to survive with on-demand requirements in a low margin arena? For many of the early pioneers in SAS and SOA, the past year has been one of tough demands from customers and harsh reactions from financial markets, as they continue to figure out how to survive in challenging, unchartered waters."
716,"The advent of SOA and Grid technology has brought new challenges to workflow operation and performance evaluation. In this paper, the characteristics of service-oriented workflow are presented, based on which the service-oriented workflow performance evaluation model is described and the performance analysis methods are depicted. Finally the design and implementation of our prototype system are introduced briefly."
717,"Distributed SOA computing environments usually use SOAP intermediaries that sit between senders and receivers to mediate SOAP messages. The intermediaries may add support services to the SOAP message exchange, such as routing, logging, and security. The typical processing by a SOAP intermediary is parsing the incoming SOAP messages, checking the data in each message, and then serializing the messages to put them back into the network. DOM is one of the popular interfaces to navigate an XML tree. Existing DOM implementations are not efficient for SOAP intermediary processing. Existing DOM implementations parse XML data to create tree data and traverse the tree data for serialization. Typically, a SOAP intermediary rarely modifies the tree data. In such situations, creating the tree data and serializing it back into XML data is computationally expensive. We propose a DOM implementation based on a hybrid data representation that uses both literal XML and DOM objects. In our implementation, a SOAP intermediary stores the original literal XML representation and reuses it to avoid traversing all of the tree data during serialization. We prototyped the DOM implementation and evaluated its performance."
718,"Event logs are of paramount significance for process mining and complex event processing. Yet, the quality of event logs remains a serious problem. Missing events of logs are usually caused by omitting manual recording, system failures, and hybrid storage of executions of different processes. It has been proved that the problem of minimum recovery based on a priori process specification is NP-hard. State-of-the-art approach is still lacking in efficiency because of the large search space. To address this issue, in this paper, we leverage the technique of process decomposition and present heuristics to efficiently prune the unqualified sub-processes that fail to generate the minimum recovery. We employ real-world processes and their incomplete sequences to evaluate our heuristic approach. The experimental results demonstrate that our approach achieves high accuracy as the state-of-the-art approach does, but it is more efficient."
719,"Web services make tools which used to be merely accessible to the specialist available to all, and permitting previous manual data processing and analysis tasks to be automated. One of key problem is Web services composition in terms of Quality of Service (QoS). There are many task concurrencies, such as remote sensing image processing, in computation-intensive scientific applications. However, existing Web service optimal combination approaches are mainly focused on single tasks by using ""selfish"" behavior to pursue optimal solutions. This causes conflicts because many concurrent tasks are competing for limited optimal resources, and the reducing of service quality in services. Based on the best reply function of quantified task conflicts and game theory, this paper establishes a mathematical model to depict the competitive relationship between multitasks and Web service under QoS constraints and it guarantees that every task can obtain optimal utility services considering other task combination strategies. Moreover, an iterative algorithm to reach the Nash equilibrium is also proposed. Theory and experimental analysis show the approach has a fine convergence property, and can considerably enhance the actual utility of all tasks when compared with existing Web services combinatorial methods. The proposed approach provides a new path for QoS-aware Web service with optimal combinations for concurrent tasks."
720,"How to select the optimal composited service from a set of functionally equivalent services but different QoS attributes has become a hot research in service computing. However existing approaches are inefficient as they search all solution spaces. More importantly, they neglect the QoS inherently uncertainty due to the dynamic network environment. In this paper, we propose a fast and reliable Web service selection approach that attempts to select the best reliable composited service on the basis of filtering low reliable Web services according to the uncertainty of QoS. The approach first employs information theory and variance theory to abandon high QoS uncertainty services and downsize the solution spaces. A reliability fitness function is then designed to select the best reliable service for composited services. We experimented with real-world and synthetic datasets and compared our approach with other approaches. Our results show that our approach is not only fast, but also find more reliable composited services."
721,"Service (API) discovery and recommendation is key to the wide spread of service oriented architecture and service oriented software engineering. Service recommendation typically relies on service linkage prediction calculated by the semantic distances (or similarities) among services based on their collection of inherent attributes. Given a specific context (mashup goal), however, different attributes may contribute differently to a service linkage. In this paper, instead of training a model for all attributes as a whole, a novel approach is presented to simultaneously train separate models for individual attributes. Meanwhile, a latent attribute modeling method is developed to reveal context-aware attribute distribution. Experiments over real-world datasets have demonstrated that this fine-grained method yields higher link prediction accuracy."
722,"As more and more users outsource their job executions to service clouds, effective job scheduling and pricing models are needed to solve resource and service competitions between users. Considering the particularity of scheduling and pricing problems in a service cloud whose goal is generally social welfare maximization, current commonly used models, such as fixed-pricing schemes, have obvious shortcomings and thus are unfeasible. Therefore, in this paper, we propose a randomized mechanism to schedule and charge job executions in service clouds. Our proposed mechanism can schedule jobs in a flexible way to achieve approximate social welfare maximization while guaranteeing non-preemption. Flexibility means the number of instances which are allocated to a job can be changed over time. The mechanism is truthful in expectation, computationally efficient and individually rational. The theoretical analysis shows that our mechanism can achieve an expected social welfare approximation ratio α, which can be 2 in some situations. Extensive simulations show that our proposed mechanism can efficiently solve the job scheduling problem in service clouds."
723,"As the green house gas emission becomes a serious problem, a lot of researches now focus on how to monitor and manage carbon footprint (CF) of a production process or transportation, especially in the supply chain. Usually, most of the carbon footprint management systems are based on databases. But database is not sufficient in describing the production and transportation processes and the facilities used in these processes. In this paper, we develop an SOA based model for the carbon footprint management and labeling (CFML), using ontology and OWL-S techniques. We use OWL-S to describe the processes and workflows for production and transportation and extend it to specify the methods for deriving CF of them. We use the existing energy conversion formula to derive the CF when the energy data can be collected separately. We also derive an approach to separate the CFs when data of different processes have to be collected together. In a supply chain, a production company may have different choices of suppliers to provide certain components. To balance the tradeoff between carbon dioxide emission and cost of the overall production process, we design a supplier selection algorithm to derive the optimal solution."
725,"The QoS (quality of service) of a cloud service is not always trusted as advertised, due to the variable network environment or fake advertisement reasons. Therefore, to promote the trusted service selection, we should first evaluate the trust of each cloud service, based on its historical QoS records from past invocations. However, different from the traditional web service whose historical QoS record is a fixed value (e.g., a historical latency record of a web service is 2s), the historical QoS record of a cloud service is usually not fixed, but fluctuant during the long-running period of a single service invocation. For example, an virtual organization O rents cloud service WeatherInquiry between 8:00 am and 8:00 pm so that the employees of O can access WeatherInquiry. In this scenario, the latency of WeatherInquiry is fluctuant from the perspective of organization O, during the served 12 hours. In this situation, it is a challenge to evaluate the trust of service WeatherInquiry, based on WeatherInquiry's multiple historical QoS records that fluctuate continuously. In view of this challenge, we introduce a novel concept of flexible SLA, to better accommodate the fluctuant QoS of cloud service, and further put forward a trust evaluation method based on fluctuant QoS and flexible SLA, i.e., FL-FL (FLuctuant QoS-FLexible SLA-based trust evaluation method, FL-FL). Finally, a set of experiments are designed and deployed to validate the feasibility of our proposal, in terms of effectiveness and efficiency."
726,"Due to multi-tenancy, access control is a very important component in SaaS (Software as a Service), especially for controlling cross-tenant accesses. Due to the potential information flow among multiple tenants, information flow control should also be carefully addressed. Existing models for SaaS access control have some limitations, especially in information flow control. In this paper, we define a new SaaS-AIFC model to provide comprehensive and improved access and information flow control in SaaS. SaaS-AIFC incorporates two advanced features. First, SaaS-AIFC integrates the advanced role mapping technique to govern the cross-tenant accesses. Role mapping is very flexible and can be very efficient for SaaS with a large number of tenants. We integrate role mapping in SaaS by developing a detailed process for mapping establishment and retrieval during validation. Second, we propose a new IFC model in SaaS-AIFC, which tracks the dependency of data objects and uses the dependency information to achieve flexible information flow control. An architecture design for realizing the SaaS-AIFC model is also proposed."
727,"Mobility profiles of users play a crucial role in a wide range of context-aware computing and services. Travel mode choice, as a representative feature of mobility profiles, is one of the important components in travel demand and future planning of transportation systems. Transportation mode choice has been widely studied based on the random utility model and decision making methods which haven't considered the correlation among features influencing transportation mode choice. This paper presents a data driven model to analyze transportation mode choice given transportation information. The contributions of this paper lie in the following two aspects. On one hand, we propose a travel mode choice model considering the correlation among influencing features of mode. And the relevant features related to the mode choice are redefined and considered to improve the final efficiency and effectiveness. On the other hand, we propose a directed-graph-guided fused lasso method to depict the correlation rules among features. The lasso method can reduce the redundant information to improve the speed of convergence and accuracy of analysis. Three different models namely standard lasso, graph-guided fused lasso and spatio-functionally weighted regression based models, are compared with our model and tested with the GPS trajectory data in Beijing. As a result, we achieved better performance than other compared models."
728,"With the semantic Web and Web services we have potential technologies to develop architectures that support the broad integration of business processes. A growing number of standards like BPEL or WSCI enable us to develop integrated business process architectures on a corporate as well as on an industrywide level. Service-oriented architectures (SOA) permit us to develop new generations of integrated business applications from reusable components and along structures that represent mundane as well as sophisticated business processes. However, experiences in business process modeling show that developing Web services and the corresponding orchestration layer is quite challenging. The coexistence of semantic Web standards and natural language could help to lower the language barrier in design and orchestration, allowing even non-tech people to be involved in application design. Translating natural language descriptions into the semantics of the respective SOA can help to facilitate and fasten the design process."
729,"While the traditional model driven development techniques are useful for building solutions in a reusable manner, they do not say much about how the existing assets in a client environment can be leveraged effectively and efficiently. In this work, we enhance model driven transformation techniques to generate implementation artifacts on a given platform from platform independent models while leveraging the existing assets in a client environment. We apply semantic Web service matching technology to achieve automatic binding of generated artifacts with available client assets. By generating implementation artifacts that are bound where appropriate with clientspsila existing functionality, our approach helps cut down the development time during project implementations and thereby resulting in reduced project durations and costs. We demonstrate the feasibility of the two platforms: IBM WebSphere and SAP NetWeaver. Lessons learned are presented."
730,"In this paper we propose a technique for the automated synthesis of new composite Web services. Given a set of abstract BPEL4WS descriptions of component services, and a composition requirement, we automatically generate an executable BPEL4WS process that, once deployed, is able to interact with the components to satisfy the requirement. We implement the proposed approach exploiting efficient synthesis techniques, and experiment with some case studies taken from real world applications and with a parameterized domain. We show that the technique can scale up to cases in, which the manual development of BPEL4WS composite services is not trivial and is time consuming."
731,"Thanks to the advent of smart devices and the emergence of 3G/4G wireless technologies, services over mobile phones are becoming in many respects similar to those available over the PC based internet. Indeed mobile Web-based services, such as search, maps, presence, messaging, emails, productivity, social networking, and entertainment are becoming available in high-end phones from several device manufacturers. Mobile computing, however, promises richer applications and services based on location and context. Web technologies are being adapted and extended to support emerging mobile Internet services. However, location based mobile services present new significant challenges in terms of implementation and management complexity. In this presentation we will discuss the business trends of mobile services. Through a discussion of current projects at IBM Research, we will also present examples of the technology trends supporting scalable location based services."
732,"Analytical solutions are considered as increasingly important for modern enterprises. Currently, systematical adoption of analytical solutions is limited to only a small set of large enterprises, as the deployment cost is high due to high performance hardware requirement and expensive analytics software. Moreover, such on-premises solutions are not suitable for the occasional analytics consumers. In order to accelerate the prevalence of analytical solutions, this paper explores the feasibility of leveraging SaaS (Software-as-a-Service) delivery model to provide analytics capabilities as services in a cost-effective way. The main contributions of our work include: (1) proposing a framework to enable enterprise tenants to consume analytics capabilities as services; (2) developing a method to enhance existing analytics platform to support multi-tenancy so that a single software instance can effectively support multiple concurrent tenants; (3) designing an SLA (Service Level Agreement) customization mechanism to satisfy the diverse analytics capability demands of tenants. A prototype system has been developed to evaluate the feasibility of our approach."
734,"One of the most interesting research directions in service computing is to leverage current recommendation system solutions to suggest web services for a mashup application. Existing approaches are mainly based on collaborative filtering techniques, which can suffer from the heavy rely on human input, data sparsity and cold start issues, resulting in low accuracy. In this paper, we leverage advanced probabilistic model based approaches to tackle these issues. Our solution is to make service recommendation based on the service features and historical usage. We use the Hierarchical Dirichlet Process (HDP), a nonparametric Bayesian approach to intelligently discover the functionally relevant services based on their specifications. We leverage Probabilistic Matrix Factorization (PMF) to recommend services based on historical usage and tackle the cold start issues for new mashups through their top-K neighbors. We integrate the suggesting results from these two approaches through the Bayesian theorem and take the indicator of quality of service into account to make the final suggestion. We compared our approach with some existing approaches using a real world data set and the result indicates that our approach performs the best."
735,"In Online Social Networks (OSNs), the important participants, the trust relations between participants, and the interaction contexts between participants greatly impact a participant's decision-making in many applications, such as service provider selection and crowdsourcing service invocation. However, predicting the trust between two unknown participants based on the whole large-scale social network can lead to very high computation costs. Thus, prior to trust prediction, extracting a small-scale sub-network containing the important participants and the corresponding contextual information with a high density could make the trust prediction more efficient and effective. However, extracting such a sub-network has been proved to be an NP-Complete problem. To address this challenging problem, we propose a strong social component-aware trust sub-network extraction model, So-BiNet, to search for near-optimal solutions effectively and efficiently. Our method can extract a trust sub-network without any decompression, which can in turn greatly save the search time of trust sub-network extraction. The experiments, conducted on four social network datasets, demonstrate that our approach can efficiently extract sub-networks covering important participants and contextual information while keeping a high density. Our approach is superior to the state-of-the-art approaches in terms of the quality of the sub-networks extracted within the same execution time."
736,"In semantic Web technologies, searching for a service means to identify components that can potentially satisfy the user needs in terms of outputs and effects (discovery), and that, when invoked by the customer, can fruitfully interact with her (contracting). In this paper, we present an application framework that encompasses both the discovery and the contracting steps, in a unified search process. In particular, we accommodate service discovery by ontology-based reasoning, and contracting by automated reasoning about policies published in a formal language. To this purpose, we consider a formal approach grounded on computational logic, and abductive logic programming in particular. We propose a framework, called SCIFF reasoning engine, able to establish, by ontological and abductive reasoning, if a semantic Web service and a requester can fruitfully inter-operate, taking as input the behavioral interfaces of both the participants, and producing as output a sort of a contract."
737,"In this paper, we propose an operational model to support the security of Web services. In addition to satisfying the basic security requirements, including authentication, confidentiality, data integrity, and nonrepudiation, the proposed model supports security mechanisms such as element-wise encryption and temporal-based element-wise digital signatures. Furthermore, the proposed model supports a flexible key specification scheme called explicit key definition, which can be used to define three different types of keys: static keys, dynamically selected keys, and keys applied to digital signatures. The service requester can determine the identity of the keys used without negotiating with the service provider. The implementation and experimental results demonstrate the feasibility of the proposed system."
738,"Although web services have been espoused due to their many benefits, it is known that overhead delay, particularly the communication delay, associated with invocation and execution of web services is high. Consequently, much research has been expended on minimizing those delays. In many situations an application invokes a web service repeatedly such that some or most of the data returned by the web service does not change. For instance, many web services that return schedules, such as bus or train schedules, exhibit this property. We present Differential Caches, with the accompanying Differential Updates method and the Mobile SOAP (MoSOAP) protocol, to avoid transfer of repeated data, sent by a web service to an application. The protocol is flexible in that other optimization techniques, such as encoding, can also be applied. We report on results of experiments, using a research prototype, to evaluate the method's potential benefits and also its overhead. The results of experiments show clearly that potential benefits outweigh the overhead. Under optimal conditions, the MoSOAP protocol with Differential Caches obtained a speedup of close to 800%, in delivery of the web services' replies in comparison to the SOAP communication. Further improvements in delays were gained when encoding was used in conjunction with Differential Caches."
739,"The Web services research community has proposed a number of approaches for service composition, ranging from manual to semi-automatic to completely automatic. However, it is often difficult to take independently developed services and compose them, since they may not work together correctly. For service composition to occur, the services in question must be designed and developed in a manner that facilitates their composition. In this paper, we propose a novel approach for service design and composition that combines top-down and bottom-up elements. Our approach is driven by faceted, tag-based functional requirements provided by end-users. These requirements describe, at a high-level, the families of compositions that end-users desire. The requirements kick off a top-down service development lifecycle, where enterprise architects and service developers design, develop and test workflows and services, possibly reusing existing flows and services in the process. At runtime, end-users can specify goals, which are satisfied through a bottom-up composition of flows from the available services. The composed flows include those explicitly designed by the architects as well as new ones that are assembled in a serendipitous manner from the available services. With examples from a case study in the financial services domain, we demonstrate our approach for designing and developing services that can be composed into myriad workflows based on end-user goals."
740,"Discovering and matching services is an area that has been extensively explored. In this paper we envision services that advertise not only their functional parameters, but also highlight the Quality of Service(QoS) guarantees (or non-functional parameters) they can provide. As a result users can also incorporate QoS requirements along with the service request. Given the vast pool of services available today, leading to complex ontologies, the search space becomes extremely large, increasing the complexity of the search. We construct an overlay reflecting the relationships between the services, which facilitates the pruning of the entire search space. Additionally our system takes into consideration the user context which provides information pertaining to the users preferences. (eg: A user could be performance-savvy or functionally-cautious etc) We propose an algorithm CCD (Context based Complex service Discovery), which utilizes the inputs provided by the users and determines the similarity quotient for the functional and QoS parameters. Our experiments show that CCD significantly improves the scalability of the search by aggressively pruning the search space, achieved by visiting only relevant nodes. CCD further uses the requester context to improve the recommendations provided to the requester. We also compare CCD with two baseline approaches based on the depth-first search algorithm on a travel ontology, which was created using real service definitions from the Open Travel Alliance (OTA)."
741,"With the rapid growth of Web services in recent years, the optimal service selection from functionally-equivalent service candidates has become more critical for building high quality service-oriented systems. To provide accurate QoSvalues for service selection, user-side QoS prediction thus becomes an important research problem. Although collaborative filtering based prediction approaches have been studied in several previous works, these methods suffer from the limitation of the sparsity of available historical QoS data, which greatly degrades the prediction accuracy. To address this problem, this paper proposes a Web service positioning (WSP)framework for response time prediction, which is one of the most important QoS properties. In our approach, a small set of landmarks are deployed to periodically monitor the response times of the Web service candidates and provide references to the numerous service users. By combining the advantages of network coordinate based approaches and collaborative filtering based approaches, the response times between users and Web services can be accurately predicted using their corresponding Euclidean distances. Extensive experiments are conducted based on our real-world QoS dataset collected on PlanetLab, comprising about 359,400 response time values from 200 users on 1,597 Web services. The experimental results show that our WSP approach outperforms the other existing approaches, especially when the historical data is sparse."
742,"The number of networked multimedia platforms that are introduced into the market has increased dramatically in recent years. Current approaches to multimedia distribution do not scale to this growing set of client configurations and heterogeneous dynamic networks. We propose a distributed architecture that offers a scalable solution to multimedia publication and distribution in such heterogeneous environments. It builds upon recent standardization efforts related to Web services. This paper details the multimedia Web services at the proxy server, that cooperate on a loosely coupled basis to tailor content creators' multimedia presentations to clients' environments. The experiments show that our Web service-oriented architecture offers a significant added value in heterogeneous multimedia environments."
743,"Quality of Service (QoS) has been widely used for personalized Web service recommendation. Since QoS information usually cannot be predetermined, how to make personalized QoS prediction precisely becomes a challenge of Web service recommendation. Time series forecasting and collaborative filtering are two mainstream technologies for QoS prediction. However, on one hand, existing time series forecasting approaches based on Auto Regressive Integrated Moving Average (ARIMA) models do not take the latest observation as a feedback to revise forecasts. Moreover, they only focus on predicting future QoS values for each individual Web service. Service users' personalized factors are not taken into account. On the other hand, collaborative filtering facilitates user-side personalized QoS evaluation, but rarely precisely models the temporal dynamics of QoS values. To address the limitations of existing QoS prediction methods, this paper proposes a novel personalized QoS prediction approach considering both the temporal dynamics of QoS attributes and the personalized factors of service users. Our approach seamlessly combines collaborative filtering with improved time series forecasting which uses Kalman filtering to compensate for shortcomings of ARIMA models. Finally, the experimental results show that the proposed approach can improve the accuracy of personalized QoS prediction significantly."
744,"Since the emergence of Web services and the diversity of user's devices, an emerging need for adapted web services appeared. In fact, users aim to find web services meeting their requests, contexts and desired QoS. In order to achieve this goal, we propose in this paper a framework to find and select adaptable web services. Our framework is named QoS-ASF and it is based on a multi-agent system."
745,"In scientific collaboration platforms such as caGrid, workflow-as-a-service is a useful concept for various reasons, such as easy reuse of workflows, access to remote resources, security concerns, and improved execution performance. We propose a solution for facilitating workflow-as-a-service based on Taverna as the workflow engine and gRAVI as a service wrapping tool. We provide both a generic service to execute all Taverna workflows, and an easy-to-use tool (gRAVI-t) for users to wrap their workflows as workflow-specific services, without developing service code. The signature of the specific service is identical to the corresponding workflow's input/output definition and is therefore more self-explained to workflow users. These two categories of services are useful in different scenarios, respectively. We use a tumor analysis workflow as an example to demonstrate how the workflow-as-a-service approach benefits the execution performance. Finally a conclusion is drawn and future research opportunities are discussed."
747,"The service-oriented approach is becoming more and more popular to integrate highly heterogeneous systems. Web services are the natural evolution of conventional middleware technologies to support Web-based and enterprise level integration. Formal testing of such Web-based technology is a key point to guarantee its reliability. In this paper, we choose a non-intrusive approach based on monitoring to propose a conformance passive testing methodology to check that a composed Web service respects its functional requirements. This methodology is based on a set of formal invariants representing properties to be tested including data and time constraints. Passive testing of an industrial system (that uses a composition of Web services) is briefly presented to demonstrate the effectiveness of the proposed approach."
748,"With the wide adoption of SOA (Service Oriented Architecture), a massive amount of innovative applications emerge in the Internet. One of the popular representations is mashup. It is a new application created by combining different kinds of services. There exist multiple typed objects (e.g., mashup, service, category, tag, provider and description) and relations (e.g., compose and composed by relation between mashups and services, provide and provided by relation between services and providers), which constitute a heterogeneous information network (HIN) naturally. Several approaches already exist for recommending services for users but they are limited to consider only one or two kinds of relations between mashups and services. To apply the rich semantics and enhance recommendation performance, in this paper, we propose a Factorization Machine based service Recommendation approach, called FMRec, on HIN. Specifically, we firstly apply counting-based similarities for meta paths to capture the multiple semantic meanings between mashups and services. And then, we employ matrix factorization to the similarity matrices built by different kinds of meta paths to obtain the mashup latent features and service latent features. Finally, we leverage factorization machine model with a group lasso regularization term to learn the ratings between mashups and services. Comprehensive experiments are conducted on a real-world dataset, indicating that our proposed service recommendation approach significantly improves the quality of the recommendation results compared with existing methods."
750,"REST Chart is a Petri-Net based XML modeling framework for REST API. This paper presents two important enhancements and extensions to REST Chart modeling - Hyperlink Decoration and Hierarchical REST Chart. In particular, the proposed Hyperlink Decoration decomposes resource connections from resource representation, such that hyperlinks can be defined independently of schemas. This allows a Navigation-First Design by which the important global connections of a REST API can be designed first and reused before the local resource representations are implemented and specified. Hierarchical REST Chart is a powerful mechanism to rapidly decompose and extend a REST API in several dimensions based on Hyperlink Decoration. These new mechanisms can be used to manage the complexities in large scale REST APIs that undergo frequent changes as in some large scale open source development projects. This paper shows that these new capabilities can fit nicely in the REST Chart XML with very minor syntax changes. These enhancements to REST Chart are applied successfully in designing and verifying REST APIs for software-defined-networking (SDN) and Cloud computing."
751,"We propose the PREvent framework, which is a system that integrates event-based monitoring, prediction of SLA violations using machine learning techniques, and automated runtime prevention of those violations by triggering adaptation actions in service compositions. PREvent improves on related work in that it can be used to prevent violations ex ante, before they have negatively impacted the provider's SLAs. We explain PREvent in detail and show the impact on SLA violations based on a case study."
752,"Question answering (Q&amp;A) communities have gained momentum recently as an effective means of knowledge sharing over the crowds, where many users are experts in the real-world and can make quality contributions in certain domains or technologies. Although the massive user-generated Q&amp;A data present a valuable source of human knowledge, a related challenging issue is how to find those expert users effectively. In this paper, we propose a framework for finding such experts in a collaborative network. Accredited with recent works on distributed word representations, we are able to summarize text chunks from the semantics perspective and infer knowledge domains by clustering pre-trained word vectors. In particular, we exploit a graph-based clustering method for knowledge domain extraction and discern the shared latent factors using matrix factorization techniques. The proposed clustering method features requiring no post-processing of clustering indicators and the matrix factorization method is combined with the semantic similarity of the historical answers to conduct expertise ranking of users given a query. We use Stack Overflow, a website with a large group of users and a large number of posts on topics related to computer programming, to evaluate the proposed approach and conduct extensively experiments to show the effectiveness of our approach."
753,"Local State Transfer (LoST) is a simple, declarative approach for enacting communication protocols. LoST is perfectly distributed and relies only upon the local knowledge of each business partner. It involves a novel treatment of the information bases of protocols, especially in terms of how their parameters are specified. As a result, LoST can capture subtle patterns of interaction that more complex approaches cannot handle well. Further, LoST lends itself to implementations that are robust against unordered and lossy message transmission."
754,"The rapid development of cloud computing service allows data and computation intensive applications to be easily moved into cloud. Users pay for computing and storing resources to deal with their data in the cloud, therefore how to manage those resources has become an important issue. In order to satisfy different demands of users, various storage strategies have been proposed. Aiming at presenting a cost-effective and computation-efficient strategy, the paper considers both the monetary cost and response time to decide whether a generated data set should be stored or not based on the proposed data importance model. The main focus of this model is to compare the attributes and relationships between data sets. To better illustrate the effectiveness and efficiency of our strategy, both theoretical analysis and simulations are conducted in this paper. Various experiments show excellent results that our strategy outperforms its counterparts in both cost and computation."
755,"We study the problem of synthesis of a choreographer in Web service composition for a given set of services and a goal. Services and goal are represented using I/O automata which can succinctly and precisely describe the interfaces of the services. Our technique considers existence and synthesis of two types of the choreographers: a simple choreographer capable of only relaying outputs from one service to input of another and a transducing choreographer which is capable of storing and reusing inputs/outputs from the services. The central theme of our technique relies on generating I/O automata representation of all possible choreographed behavior of existing services (captured in form of universal service automaton, a concept introduced in this paper) and verifying that the goal can be simulated by the universal set of choreographed behaviors."
756,"With the proliferation of mobile application (app) markets (e.g., Google Play, Apple App Store), predicting user preferences on apps becomes a challenging problem. Different from previous work, we assume that a user likes an app because he/she likes certain features of the app (e.g., permission, genre, topic). Based on this assumption, we propose a feature-oriented approach to predict user preferences on apps. Specifically, we transform the original app rating matrix to feature rating data and predict the unknown ratings on the features through a latent factor model, instead of directly predicting ratings on apps. The predicted user ratings on features can be used to generate the ratings on apps. Two integration methods are presented to give different significance for feature preferences. The approach has some obvious advantages: as it integrates feature information to analyze the details of user preference, it can generalize better as the feature rating data is denser, and improve the interpretation of the prediction of app ratings. Experimental results on a real-world dataset demonstrate the effectiveness of the proposed approach."
757,This paper reports several key challenges and solutions when we apply Web 2.0 mashup technology to build a university-oriented services portal. A two-layer mashup service model is proposed as the underlying basis to support multiple granularities of services mashup. We explore a caching technique to facilitate personalizable services requests. We also report our preliminary practice of exploiting Facebook as a social relationship data source.
758,"In this paper, we propose a preliminary approach for automating web service verification. We use Semantic Markup for Web Services (OWL-S) to describe web service behavior. We parse the OWL-S file and transform it automatically to a corresponding Markov chain diagram or Markov decision process, which are then transformed to a PRISM model to be used as input by PRISM, a probabilistic model checker, to verify automatically the web service behavior. We provide an implementation of the transformation algorithm through a developed software tool automating all the transformation and verification activities."
759,"For companies and government agencies alike, the emergence of Web services technologies and the evolution of distributed systems toward service oriented architectures (SOA) have helped promote collaboration and information sharing by breaking down ""stove-piped"" systems and connecting them via loosely coupled, interoperable system-to-system interfaces. Such architectures, however, also bring about their own security challenges that require due consideration. Unfortunately, the current information security mechanisms are insufficient to address these challenges. In particular, the access control models today are mostly static and coarsely grained; they are not well-suited for the service-oriented environments where information access is dynamic and ad-hoc in nature. This paper outlines the access control challenges for Web services and SOA, and proposes an attribute based access control (ABAC) model as a new approach, which is based on subject, object, and environment attributes and supports both mandatory and discretionary access control needs. The paper describes the ABAC model in terms of its authorization architecture and policy formulation, and makes a detailed comparison between ABAC and traditional role-based models, which clearly shows the advantages of ABAC. The paper then describes how this new model can be applied to securing Web service invocations, with an implementation based on standard protocols and open-source tools. The paper concludes with a summary of the ABAC model's benefits and some future directions."
760,Service-oriented architecture (SOA) promotes a paradigm where ad-hoc applications are built by dynamically linking service-based software capabilities. Service providers follow specification standards to advertise their services' capabilities and to enable loosely coupled integration between their services and other businesses over the Web. A major challenge in this domain is interpreting the data that must be marshaled between consumer and producer systems. We propose a framework to support formal modeling and contracts for data-centric Web services. We demonstrate how this framework can be used to verify correctness properties for composition of services.
761,"The development of services-based systems starts from defining goals for business processes to be implemented, e.g., as a Web service orchestrations specified in WS-BPEL. In this paper, we propose a scenario-driven approach for modeling business processes. We aim for simplicity in the notation and leverage example-like modeling principles in order to improve process sketching. The first step in our approach is to identify the essential business requirements and model them using a simple scenario notation. The scenarios, given as UML sequence diagrams, are synthesized into a state machine, which is translated into a WS-BPEL flavored process skeleton given as UML activity diagram. The process skeleton can be further refined into executable process model."
762,"Web ontologies as the foundation of the semantic Web were proposed to solve the problem of integrating and sharing heterogeneous information resources in the Web. Massive amount of domain specific ontologies have been constructed and published in different domains on the Web. However, several limitations make existent ontologies not suitable for high-level and large-scale applications. In this paper, we described a context-based ontology service with a large-scale traditional Chinese medicine (TCM) ontology, which provides clients with an interactive interface and intelligent inter-operations to assist users in sharing and exploiting large-scale TCM information and can be used as a semantic view for domain specific problem solving."
763,"Solution design has been more of an art than an engineering discipline. Lots of researchers and practitioners have proposed and exercised different kinds of approaches with varied success. Most of these methods seem to have focused on building new solutions from scratch. However, enterprise solutions today are mostly built on top of an existing IT infrastructure. The notion of SoA is trying to pave a way to integrate heterogeneous components together to meet new business needs. When a new requirement is given to a system developer in the form of business processes, it would be ideal if she/he can make the best of existing services for many reasons. In this paper we propose a data driven approach to provide service composition guidance to implement the given requirement. Based on the relations among business domain data and service domain data, we can generate additional data mediations according to three composition rules. With these data relations and composition rules, we give a formal approach to devise choreography of services from current service portfolio, plus additional data mediation artifacts to realize a given requirement. Our work can be seen as an effort to bridge the gap between business and service domain"
764,"A critical issue in service composition area is how to achieve an optimized overall end-to-end quality of service(QoS) requirements by effectively coordinating QoS constraints for individual service. However, this issue has not yet been well addressed. In this paper, we propose a novel method by employing a recursive bargaining Strategy to gradually remove QoS constraint violations for Optimizing service composition execution Path. Our method mainly exploits the hidden market competitive relationships which widely exist in real business world for developing a novel bargaining strategy. Based on this strategy, concessions can be made by service providers to offer better QoS values. By recursively using bargaining strategy, an initial execution path built by a local optimization policy for service composition, can be continually updated to be close to the optimal one by reselecting better service providers for meeting overall end-to-end QoS requirements. An experiment and evaluation have been made to demonstrate the feasibility and effectiveness of our proposed method."
765,"In large distributed monitoring and management systems that involve a large number of entities across multiple trust domains, the problem of establishing a secure conversation effectively between any two entities is outstanding when these two entities do not have a direct trust relationship. In this paper, we present a conversation establishment protocol that uses forwarded trust relationships to solve this problem. In this protocol, security assertion markup language (SAML) based authentication assertions are used to encapsulate the conversation context as well as the conversation target identity authentication information into a secure context token. Our protocol is conformant to the emerging Web services standards of WS-trust and WS-secure conversation. The implementation of this framework on Java platform and its application to secure a Web services based grid monitoring system are presented"
766,"In service oriented architectures, complex applications are composed from a variety of functionally equivalent Web services, which may differ for quality parameters. Under this scenario, applications are defined as high level business processes and service composition can be implemented dynamically by identifying the best set of services available at run time. In this paper, we model the service composition problem as a mixed integer linear problem where both local constraints and global constraints can be specified."
767,"Service discovery becomes a key to hastening the evolution of web services as the number of services is expected to increase dramatically. In this paper, we propose to index all the ontology-annotated outputs in registered services. For each ontology-annotated output, there is a service list which records all the services in the registry that deliver the output. Based on the indexing, we propose a composition-oriented service discovery algorithm, which greatly accelerates the filtering of irrelevant atomic services by making use of the inverted indexing, and increases the likelihood of finding a possible candidate by exploring service composition. Experimental results show that the proposed algorithm provides a better performance on response time than the sequential matchmaking, and a better recall rate than the algorithms without the exploration of composition."
768,"Web services offer an interoperability model that abstracts from the idiosyncrasies of specific implementations; they were introduced to address the increasing need for seamless interoperability between systems in the business-to-business domain. We analyse the requirements from this domain and show that to fully address interoperability demands we need to make use of semantic descriptions of Web services. We therefore introduce the Web service execution environment (WSMX), at software system that enables the creation and execution of semantic Web services based on the Web service modelling ontology. Providers can use it to register and offer their services and requesters can use it to dynamically discover and invoke relevant services. WSMX allows a requester to discover, mediate and invoke Web services in order to carry out its tasks, based on services available on the Internet."
769,"A panel of distinguished information technology executives describes how Web services, services computing, service-oriented architectures, and services-centric models are changing their organizations. The panelists are members of IT Professional Magazine's Advisory and Editorial Boards, and hold (or have held) C-level positions in government, industry, and academia."
770,"The quality of service oriented architecture (SOA) solutions is becoming more and more important along with the increasing adoption of SOA. Continuous integration testing (CIT) is an effective technology to discover bugs as early as possible. However, the diversity of programming models used in an SOA solution and the distribution nature of an SOA solution pose new challenges for CIT. Existing testing frameworks more focus on the integration testing of applications developed by a single programming model. In this paper, a unified test framework is proposed to overcome these limitations and enable the CIT of SOA solutions across the whole development lifecycle. This framework is designed following the model driven architecture (MDA). The information of an executable test case is separated into two layers: the behavior layer and the configuration layer. The behavior layer represents the test logic of a test case and is platform independent. The configuration layer contains the platform specific information and is configurable for different programming models. An extensible and pluggable test execution engine is specially designed to execute the integration test cases. A global test case identifier instrumentation approach is used to merge the distributed test case execution traces captured by ITCAM - an IBM integrated management tool. A verification approach supporting Boolean expression and back-end service interaction verification is proposed to verify the test execution result. Initial experiments have shown the effectiveness of this unified test framework."
771,"The design of applications that comply to the REST architectural style requires observing a given set of architectural constraints. Following these constraints and therefore designing REST compliant applications is a non-trivial task often not fulfilled properly. There exist several approaches for the modeling and formal description of REST applications, but most of them do not pay any attention to how these approaches can support or even force REST compliance. In this paper we propose a model-driven approach for modeling REST services. We introduce a multi layered model which enables (partially) enforcing REST compliance by separating different concerns through separate models. We contribute a multi layered meta-model for REST applications, discuss the connection to REST compliance and show an implementation of our approach based on the proposed meta-model and method. As a result our approach provides a holistic method for the design and realization of REST applications exhibiting the desired level of compliance to the constraints of the REST architectural style."
772,"The huge demand for situational and ad-hoc applications desired by the mass of business end users cannot be fully implemented by IT departments. New approaches that allow for end user development (EUD) are needed to overcome this ldquolong-tailrdquo dilemma. More specifically, most existing approaches insufficiently support EUD for infrequent, situational, and ad-hoc B2B collaborations. Enterprise mashup and lightweight composition approaches and tools are promising solutions to unleash the huge potential of integrating the mass of users into development. Within the current research project FAST, a Web based mashup/Gadget development tool is in development that allows for different options to realize B2B collaborations via mashups. In this work, five patterns for the development of enterprise mashups are identified, characterized, and evaluated with focus on their adequacy for B2B collaborations."
773,"Web API is a modern approach for exposing service data to use for applications, however, decision on Uniform Resource Identifiers (URIs) from an existing web application is still a manual and very time consuming task. Depending on the existing web application, thousands of lines of code has to be read and discussed to decide on what data can be exposed as web API resources. An automated approach is named SPEC2REST and proposed here for eliciting web API resources which uses class relations for path elicitation and filters web API resources using word occurrence. Evaluation results showed that SPEC2REST can elicit around 90% of actual existing web APIs for four applications by using class relations, as well as, helps inexperienced developers at their first step of creating RESTful resources."
774,"Automatic Web services composition can be achieved by using AI planning techniques. HTN planning has been adopted to handle the OWL-S Web service composition problem. However, existing composition methods based on HTN planning have not considered the choice of decompositions available to a problem which can lead to a variety of valid solutions.In this paper, we propose a model of combining a Markov decision process model and HTN planning to address Web services composition. In the model, HTN planning is enhanced to decompose a task in multiple ways and hence be able to find more than one plan,taking both functional and non-functional properties into account. Furthermore, an evaluation method to choose the optimal plan and some experimental results illustrate that the proposed approach works effectively."
775,"We introduce a logic model to formally specify the semantics of workflows and their composite tasks described as BPEL4WS abstract processes. Based on the model, we present a set of inference rules to deduce the strongest postcondition and weakest precondition of a workflow and demonstrate that automatic workflow verification is possible due to the restrictions on data manipulation in an abstract process. We then sketch an algorithm that automatically synthesizes a workflow given its specification and a task library."
776,"With the increasing presence of web services on the Internet, Quality of Service (QoS) is becoming important for describing non-functional characteristics of web services, and is often employed in web service composition. As QoS is an aggregated concept consisting of several attributes, service composition on enormous candidate sets is a challenging multi-objective optimization problem. In this paper, we study the problem from a general Pareto-optimal angle, seeking to reduce the search space in service composition. QoS attributes are systematically studied according to their different types of aggregation pattern in service composition, and QoS-based dominance relationships between candidates and between workflows are defined. Taking advantage of pruning candidates by dominance relationships and constraint validations at candidate level, a service composition algorithm using partial selection technique is proposed, which is able to significantly reduce the search space and achieve great performance gains. A careful analysis of the optimality of our approach is provided, and its efficacy is further validated by empirical evaluation."
777,"The myriad of SOA platforms and the complexity of the Web services standards has meant that it is difficult for users to ensure that their deployments are appropriately secure. Despite the compilation of various SOA security ""best practices''', detecting violations of such practices has proven difficult. To address this need, we developed a tool that can analyze the deployment configurations of multiple SOA platforms and report potential SOA best practice violations. In this paper, we compare, contrast and categorize SOA platforms, and describe the analysis challenges posed by each category. We describe our framework architecture for our multi-platform analyses, and further describe our prototype implementation of this architecture."
778,"The growing complexity of software, combined with demands for greater productivity and shorter cycles, creates an increasing demand for more automation and integration within the software engineering (SE) domain. When viewed holistically, the heterogeneous nature, implicit feature cross-dependencies, and manual administration of the toolchain infrastructure results in unnecessary complexity, inefficiencies, and reduced reliability for the SE process. A common infrastructure is missing that provides an interoperable and distributed tool environment, addresses feature dependency selection, and automates toolchain workflow composition and execution. To address these challenges, this paper explores the practicality of a unifying semantic Web services approach towards automated software engineering (SWS-ASE)"
779,"Web service composition enables seamless and dynamic integration of web services. The behavior of participant web services determines the overall performance of a composition. Therefore, it is important to choose the high quality participants for service composition. The state of the art in service discovery and selection rely on non-functional aspects also known as quality of service (QoS) e.g., response time and availability. Though these parameters are crucial for selecting web services, they do not reflect the end-user's perspective on quality. In this paper, we explore the feasibility of adopting the perceived quality from end-user's perspective for service selection and composition. We name such quality parameters as quality of experience (QoE). First, we propose a solution that automatically mines and identifies QoE parameters from the web. Second, we study the application of such dynamically extracted QoE attributes for service selection. For the evaluation purpose, we collected more than 24,000 reviews from 22 different services from four domains. Our result shows the automated approach identifies QoE attributes with an average precision and recall 90% and 79% respectively. Our study shows that there is a strong positive correlation between QoS and QoE. Hence QoE can be used during service selection especially when QoS data are not available."
780,"The emergence of Web services has led to more interest into Web service composition, which is an active area of research. The formidable problem of efficient and effective composition of existing Web services is the subject of much current attention. The study of workflow model is one of the most important parts and a key layer of Web service composition. The existing approaches have difficulties in modeling dynamic and complex service composition process, so we propose a novel workflow model named Service/Resource Net (SRN) for Web service composition, which is an extended Petri-net-based model with some new elements, such as time, resource taxonomy, condition, etc. To establish formal concept system of Web service taxonomy in SRN, we present Web service semigroup based on group theory. Moreover, meta-service is proposed based on the definition of generating element in cyclic monoid. As the supplement of traditional analysis methods of basic Petri net, some new methods based on graph theory are presented for SRN analyzing and evaluating. For system and methodology validation purpose, SRN is applied to a case study in our research project."
781,"The Internet of Things (IoT) changes many sectors of our lives. In the healthcare domain, IoT presents as mobile medical applications over various sensors that update healthcare professionals on patients' health information. However, IoT-based healthcare systems also face major challenges in protecting patients' privacy via an effective access control system. This paper presents an ambient home solution framework for privacy-preserving monitoring of patients' health status. We focus on two major points: 1) how to use the data collected from ambient and biometric sensors, to perform the high-level task of activity recognition, and 2) how to secure the collected healthcare data via effective access control. We achieve multi-level access control by using Public Key Infrastructure (PKI) for authentication and Attribute-Based Access Control (ABAC) for authorisation. Our access control system regulates access to healthcare data by classification over healthcare professionals and data. Our system provides guidelines to define data classes and healthcare professional groups and specifies security policies to control access to the data classes. The system is flexible and can incorporate more policy rules, professionals, and data classes."
782,"Service composition has emerged as a fundamental technique for developing Web applications. Multiple services, often from different organizations or trust domains, may be dynamically composed to satisfy a user's request. Access control in the presence of service compositions is a challenging security problem. In this paper, we present an access control model and techniques for specifying and enforcing access control rules on Web service compositions. A key advantage of our approach is that past histories of service invocations can be used to make access control decisions. Our approach allows role hierarchies and separation of duty constraints. Access controls rules may be parameterized by one or more arguments. We have implemented our access control model via a declarative policy specification language which uses pure-past linear temporal logic (PPLTL). We describe an implementation of our approach using a supply chain management (SCM) application. Our experiments show that our approach can enforce expressive and flexible access control policies while incurring reasonable performance overhead on the application."
783,"Information visualization has been acknowledged as an important tool in software decision support. But usually visualizations are static and just used for presentation rather than exploration. Interactive statistical data visualization can be used as a powerful tool which reaches beyond the limits of static graphs. We developed code mapping, a software platform for visualization and manipulation of complex and large software portfolios. Code mapping builds data-rich, graphical representations that are color-coded for function, call position and experimental interaction data. The functions implemented in code mapping allow collaboration and organization of software architecture diagrams in a visual model format; this feature is especially useful for Web services development and other Internet application code. The aim of this paper is to describe a tool which helps to visualize the software development and to present a new way to visualize software in a three-dimensional CAVE/spl reg/ environment."
784,"During past few years, how to achieve load balance using efficient software on cloud architecture is posing significant challenges to the research community. Due to the access conflictions among the shared hardware resources like distributed file systems and database transactions, creditable measures like mutex based locks, semaphore schemes, and global run queues have been widely applied. However, growing with the data scale and integration of multiprocessors in the cloud computing environment, each processor has to obtain the global lock of the system run queue, which brings inevitable burden for the runtime support. In this paper, we propose a novel lock free structure, named FairPlay, which is able to support service migration through duplex buffers between processors. Based on the buffer based structure, a load balance scheduling algorithm is presented to handle the service allocation asynchronously. Experimental results on the modified Linux operating system kernel demonstrate that the lock free mechanism could efficiently reduce the overheads on the locks with great scalability and affordable overheads."
785,"How we manage Web services depends on how we understand their variable parts and invariable parts. Studying them separately could make Web service research much easier and make our software architecture much more loose-coupled. We summarize two variable parts that affect Web service compositions: uncertain invocation results and uncertain quality of services. These uncertain factors affect success rate of service composition. Previous studies model the Web service problem as a planning problem, while this problem is considered as an uncertain planning problem in this paper. Specifically, we use Partially Observable Markov Decision Process to deal with the uncertain planning problem for service composition. According to the uncertain model, we propose a reinforcement learning method, which is an uncertainty planning method, to compose web services. The proposed method does not need to know complete information of services, instead it uses historical data and estimates the successful possibilities that services are composed together with respect to service outcomes and QoS. Simulation experiments verify the validity of the algorithm, and the results also show that our method improves the success rate of the service composition."
786,"The security and dependability of cloud applications require strong confidence in the communication protocol used to access web resources. The mainstream service providers nowadays are shifting to REST-based services in the detriment of SOAP-based ones. REST proposes a lightweight approach to consume resources with no specific encapsulation, thus lacking of meta-data descriptions for security requirements. Currently, the security of RESTful services relies on ad-hoc security mechanisms (whose implementation is error-prone) or on the transport layer security (offering poor flexibility). We introduce the REST security protocol to provide secure service communication, together with its performance analysis when compared to equivalent WS-Security configuration."
787,"Summary form only given. Service-oriented architecture (SOA) has been proven to be a flexible and extensible architecture for designing and realizing industry solutions and applications. Enterprise Service Bus (ESB) is a hub for integrating different kinds of services through messaging, event handling, and business performance management. This tutorial will focus on a SOA solution framework; the critical role and value proposition of an ESB in SOA and Web services; ESB (and SOA) analysis and design methodology; best practices for the practical design and implementation of an ESB, including ESB design using the enterprise integration and application integration patterns; ESB and business process integration tools and techniques for ESB implementation; and performance, security and transaction management. This tutorial is based on numerous projects and solution architectures that the authors and colleagues have been engaged in the last 3 years in various industries, including government, financial, retail, electronics and distribution."
788,This paper presents the results of a brief survey of tools for increasing the level of automation of service composition. Our main emphasis in this paper is on tools for building compound services. Tools for both non-semantic and semantic services are considered.
789,"The most commonly deployed web service applications employ client-server communication patterns, with clients running remotely and services hosted in data centers. In this paper, we make the case for Service-Oriented Collaboration applications that combine service-hosted data with collaboration features implemented using peer-to-peer protocols. Collaboration features are awkward to support solely based on the existing web services technologies. Indirection through the data center introduces high latencies and limits scalability, and precludes collaboration between clients connected to one-another but lacking connectivity to the data center. Cornellpsilas Live Distributed Objects platform combines web services with direct peer-to-peer communication to eliminate these issues."
791,"In this paper, the user experience in added value location-based mobile music service called the City Night Life (CNL) is evaluated and analyzed. The CNL service provides recommendations about entertainment premises in the proximity by matching their music styles with the user's personal music preferences. Entertainment premises are shown on a map interface and are marked with colors according to their match. The paper presents the components of the CNL service, the music profile, matching algorithm and the map-based user interface in a web browser. The user evaluation with 53 test users assessed the feasibility and business potential of the CNL service. Data was collected with both quantitative questionnaires and short interviews. The results indicate that the CNL service will be most useful in relatively large, unfamiliar cities. Furthermore, the matching algorithm was perceived to perform quite well."
792,"This panel aims to explore the intrinsic and multi-facet relationships between software and services and the effects of such relationships on the coupling and transformation of computing and business and the embedding of service oriented architecture (SOA) into future computing and business management environments. Today, more and more software are augmented with service oriented packaging. At the same time, more and more business and government services are provided and offered in the form of software. The key focus of this panel is to discuss and debate: (1) Where software and services will meet? (2) Can and how the dynamic and multi-facet relationships between software and services are modeled and exploited? (3) What can be leveraged in this endeavor from the perspective of Web services, business transformation, and consumer demand?"
793,"Given a process specification, it is a complex task to dynamically select constituent services and compose them in an execution plan to satisfy users' non-functional preferences. Process scheduling approaches assume users can clearly specify their non-functional preferences and there are formulas (e.g., utility functions) to compute process level QoS from the QoS of constituent services and their connections. However, these assumptions are not always true. Users' preferences can be subjective, implicit, vague, mixed and different for various types of processes. Besides, not all the preferences for example easy-to-use can be computed using formulas. We proposed a machine learning based approach to evolutionarily learn user preferences according to their ratings on historical execution plans, recommend existing or generate new execution plans for business processes that adapt to user preferences."
794,"This paper describes a scalable approach to the enabling of legacy scientific applications on computing grids using a service-oriented architecture. In the context of this paper grid-enabling means turning an existing application, installed on a grid resource, into a service and generating the application-specific user interfaces to use that application through a Web portal. Scalability is achieved by providing a common abstraction for a category of applications and providing a ""generic"" application service to wrap those applications as services. The focus of this paper's approach is on grid-enabling ""command-oriented"" scientific applications. The novel aspect of the approach is that the entire process -from turning an application into a service to the user-interface generation for that application - is done automatically, without requiring coding or grid-system downtime. Portlet technology is used to dynamically generate application-specific interfaces. Further, the approach makes it possible to customize the applications for different user groups by way of simplifying, restricting or composing the functionalities of applications. The approach is useful for building grid portals on which a large number of applications need to be dynamically enabled."
795,"The three core problems that can be identified with any SOA based system are: (i) service discovery, (ii) service selection, and (iii) service composition. Most of the previous researches have considered these three problems separately. However, we argue that these three problems are intrinsically related and hence, cannot be treated separately. We propose a proactive event-driven model where user activities and services are treated as events. We then graphically model a SOA system into a network of activities called ""activity network"" (ANet). The nodes of such a network are the services and the edges their mutual causal dependency. We show how both functional as well as contextual information can be incorporated into the network and then justify why that is necessary for solving the three problems mentioned hereby. We then propose an abstraction technique that helps us to simplify the ANet and provide an architectural platform where the three problems can be solved in an efficient manner."
796,"Model-driven security is a framework to configure WS-security easily. It generates a security policy written in WS-security policy to be transformed into platform-specific configuration files. Since the WS-security policy specification is quite complicated, it is difficult to directly map between a security policy and a configuration. We propose a generic security policy transformation framework using an intermediate model. The intermediate model structure is designed based on the WS-security message structure, because both a security policy and the configuration files correspond to one WS-security message, even though the WS-security policy is flexible in specifying security requirements. Our contributions are simpler transformation rules compared to direct mapping, the support for various platforms, and more flexible updates if the WS-security policy specification changes. We demonstrate the transformation using the intermediate model for WebSphere application server 6.0."
797,"In order to offer adequate guidance to the emerging reboot-based self-healing processes in Web services, this paper presents probabilistic models to estimate the recovery time and disruption of recovering a service using multiple levels of reboot. A case study is conducted to demonstrate benefits of the models."
798,"Service-oriented computing (SOC) enables organizations and individual users to discover openly-accessible capabilities realized as services over the Internet. However, service registries can potentially be very large preventing organizations from discovering services in real-time. In fact, consumers may not be aware of the services that can be of most benefit to them. In our work, we introduce a web service recommender system that proactively discovers and manages web services. This paper focuses on the underlying search and ranking algorithms that enable the recommendations. As an innovation, we have analyzed real, fully-operational web services currently available on the Internet and, as a result, have discovered insights into how real web service messages are defined. Using these general naming tendencies coupled with enhanced syntactical methods, we are able to aggregate services by their messages and accurately suggest candidate services to users as a part of daily routines."
799,"Web Services standard-based system resource management is a new direction. In this paper, in order to verify the WS-Management standard whether to satisfy the realistic system resource management requirements, we use IBM MAPE categories to find more WS-Management related use cases. We design three typical use cases based on MAPE principles. Finally, we make a conclusion and propose our next-step work."
800,"The Web services transaction protocol family includes the WS-atomic transaction and the WS-business activity specifications in order to carry out distributed transactions in a Web services (WS) environment. The WS-atomic transaction specification defines all necessary interfaces to carry out transactional work. In contrary, the WS-business activity specification for long-running transactions intentionally left the interface between initiator and coordinator undefined. This allows vendors to integrate WS-business activity coordinators into their business process engines. However, it requires proprietary protocols between initiator and coordinator. We propose an extension protocol to the WS-business activity specification that explicitly defines this interface between initiator and coordinator. This extension allows coordinators and initiators from different vendors to interoperate transparently. Accordingly, participants no longer need to trust an initiator-selected and likely initiator-run coordination service, but may use commonly trusted, third-party coordination services."
801,"Big Data analytics provide support for decision making by discovering patterns and other useful information from large set of data. Organizations utilizing advanced analytics techniques to gain real value from Big Data will grow faster than their competitors and seize new opportunities. Cross-Industry Standard Process for Data Mining (CRISP-DM) is an industry-proven way to build predictive analytics models across the enterprise. However, the manual process in CRISP-DM hinders faster decision making on real-time application for efficient data analysis. In this paper, we present an approach to automate the process using Automatic Service Composition (ASC). Focusing on the planning stage of ASC, we propose an ontology-based workflow generation method to automate the CRISP-DM process. Ontology and rules are designed to infer workflow for data analytics process according to the properties of the datasets as well as user needs. Empirical study of our prototyping system has proved the efficiency of our workflow generation method."
803,"The concept of complex event processing (CEP) and complex-event-aware service have been extensively studied to retrieve relevant information from massive amount of realtime streaming events. In mobile environment, the Mobilityaware CEP (MCEP) system was proposed to address the issue of synchronization problem between different query ranges and MCEP operators. We noticed that MCEP systems lack the ability to process event in parallel and scale out when system load is high. In this paper, we proposed a parallel architecture for MCEP. The architecture can handle the synchronization problem and guarantee the correctness of event processing result. We also proposed a scaling strategy that can automatically scale out operators while ensures semantic transparency. An empirical evaluation based on up to 10 ViMs demonstrated that our approach is able to achieve higher throughput while keeping the MCEP synchronization mechanism valid."
804,"Cloud computing, as a concept, promises cost savings to end-users by letting them outsource their non-critical business functions to a third party in pay-as-you-go style. However, to enable economic pay-as-you-go services, we need Cloud middleware that maximizes sharing and support near zero costs for unused applications. Multi-tenancy, which let multiple tenants (user) to share a single application instance securely, is a key enabler for building such a middleware. On the other hand, Business processes capture Business logic of organizations in an abstract and reusable manner, and hence play a key role in most organizations. This paper presents the design and architecture of a Multi-tenant Workflow engine while discussing in detail potential use cases of such architecture. Primary contributions of this paper are motivating workflow multi-tenancy, and the design and implementation of multi-tenant workflow engine that enables multiple tenants to run their workflows securely within the same workflow engine instance without modifications to the workflows."
805,"A key challenge in Web services security is the design of effective access control schemes that can adequately meet the unique security challenges posed by the Web services paradigm. Despite the recent advances in Web based access control approaches applicable to Web services, there remain issues that impede the development of effective access control models for Web services environment. Amongst them are the lack of context-aware models for access control, and reliance on identity or capability-based access control schemes. In this paper, we motivate the design of an access control scheme that addresses these issues, and propose an extended, trust-enhanced version of our XML-based role based access control (X-RBAC) framework that incorporates context-based access control. We outline the configuration mechanism needed to apply our model to the Web services environment, and also describe the implementation architecture for the system."
806,"Service composition is an emerging technology in System of Systems Engineering (SoS Engineering or SoSE), which aims to construct a robust and value-added complex system by outsourcing external component systems. A serviceoriented SoS runs under a dynamic and uncertain environment. To assure the overall Quality of Service (QoS), online reliability time series prediction, which aims to predict the reliability in near future for service-oriented SoS arises as a grand challenge in SoS research. In this paper, we tackle the prediction challenge by exploiting two Markov independence assumptions resulted from the special system dynamics of a SoS environment. A novel motifs-based Dynamic Bayesian Networks model is proposed that supports the independence assumptions. Experimental results conducted on real Web services demonstrate the effectiveness of our approach."
807,"The semantic Web and Web services technologies have provided both new possibilities and challenges to automatic information processing. There are a lot of researches on applying these new technologies to current personal Web information retrieval systems, but no research addresses the semantic issues from the whole life cycle and architecture point of view. Web services provide a new way for accessing Web resources, but until now, they have been managed separately from conventional Web contents resources. In this paper, we point out new system requirements and propose a conceptual architecture for a personal semantic Web information retrieval system. It incorporates semantic Web, Web sendees, and multiagent technologies to enable not only precise location of Web resources but also the automatic or semiautomatic integration of hybrid Web contents and Web service resources."
808,"The growing popularity of service oriented computing based on Web services standards is creating a need for paradigms to represent and design business processes. Significant work has been done in the representation aspects with regards to WSBPEL. However, design and modeling of business processes is still an open issue. In this paper, we present a novel designer for business processes, which allows for intuitive modeling of Web processes, as well as using a template based approach for semi-automatically integrating partners either at design time or at deployment time. This work has been done as part of the METEOR-S project, which concentrates on adding semantics to the entire Web process lifecycle."
809,"The loosely coupled nature of service-oriented architectures raises the question how information for access control can be managed in an efficient way. Several specifications for Web services exist to describe security requirements and to facilitate a provision of identity information. However, the integration of different standards regarding the expression of identity information in policies, claims and assertions comes along with an increased complexity. In order to identify and address the problems occurring with the combined use of standards as XACML, SAML and WS-Trust, we designed and implemented an architecture for identity- and attribute-based access control in decentralized environments. Our implementation provides an automated generation of access control policies in a format called XACML, a way to communicate required user attributes as claims across different domains based on the standards WS-Trust and WS-Policy, and a consistent mapping of retrieved attribute assertions to the XACML attributes in the access control policy."
810,"Quality of Service (QoS) prediction is an important task in services computing, which has been extensively investigated in the past decade. Many time-aware QoS prediction approaches have been proposed and achieved encouraging prediction performance. However, they did not provide effective model updating mechanisms, and thus have to periodically retrain the whole models to deal with the newly coming data. How to timely update the prediction model to precisely predict missing QoS values of candidate services becomes an urgent issue. In this paper, we propose a novel personalized LSTM based matrix factorization approach for online QoS prediction. Our approach can capture the dynamic latent representations of multiple users and services, and the prediction model can be timely updated to deal with the new data. Experiments conducted on a real-world dataset show that our approach outperforms several state-of-the-art approaches in online prediction performance."
811,"In this paper, we consider the widespread multi-stage job scheduling problem (e.g., in Big Data processed by MapReduce) in which jobs arrive at hybrid cloud systems stochastically. The objective is to minimize the number of elastic computing instances. Along with hard deadlines of jobs, the problem under study is NP-hard in strong sense. In terms of initial job priorities, timetables are constructed by adjusting job priorities adaptively and generating feasible schedules iteratively. Job sequences are generated by two simple dispatching rules. A fast local search heuristic and a rescheduling process are developed for improving the obtained sequences. Experimental results show that the proposed heuristics improve the utilization of computing resources effectively while meeting the cloud service quality requirements."
812,"Collaborative filtering based recommender systems are very successful on dealing with the information overload problem and providing personalized recommendations to users. When more and more web services are published online, this technique can also help recommend and select services which satisfy users' particular Quality of Service (QoS) requirements and preferences. In this paper, we propose a novel collaborative filtering based service ranking mechanism, in which the invocation and query histories are used to infer the user behavior, and user similarity is calculated based on similar invocations and queries. To overcome some of the inherent problems with the collaborative filtering systems such as the cold start and data sparsity problem, the final ranking score is a combination of the QoS-based matching score and the collaborative filtering based score. The experiment using a simulated dataset proves the effectiveness of the algorithm."
813,"Preserving privacy during Web transactions is a major concern for individuals and organizations. One of the solutions proposed in the literature is to maintain anonymity through group cooperation during Web transactions. The lack of understanding of incentives for encouraging group cooperation is a major drawback in such systems. We propose an anonymizing club mechanism, and sequential economic strategy for trusted collaboration. We model the individual transactions as a Prisoners' Dilemma, where two players either cooperate or defect while maintaining each other's anonymity. The activities of the participants over a series of transactions can be modeled as a sequential repeated game. We determine conditions to ensure cooperation among the participants in the sequential repeated game, even if defecting is a dominant strategy in each individual Prisoners' Dilemma game. Our results show that by adopting an appropriate initiation fee and adequate fine for malicious behavior, both enforced through a trusted central authority, we can sustain cooperation in the proposed anonymizing club mechanism."
814,"Today's service systems are growing more complex and dynamic due to the coupling of functionalities required under different contexts, which asks for extra effort from both service users and services developers. An ideal service system shall provide just right functionalities and just enough qualities to its user under a given usage context. Moreover, the service shall adapt itself when requirements change. Service systems targeting at different application domains contain domain knowledge as well as reusable system assets that are currently available. In this paper, we propose an approach that integrates domain ontology reasoning and feature configuration during system service design and execution."
815,"Current technologies aimed at supporting processes - whether it is a business or learning process - primarily follow a metadata- and data-centric paradigm. Whereas process metadata is usually based on a specific standard specification - such as the business process modeling notation (BPMN) or the IMS learning design standard - the allocation of resources is done manually at design- time, and the used data is often specific to one process context only. These facts limit the reusability of process models across different standards and contexts. To overcome these issues, we introduce an innovative semantic Web service-based framework aimed at changing the current paradigm to a context-adaptive service-oriented approach. Following the idea of layered semantic abstractions, our approach supports the development of abstract semantic process model-reusable across different contexts and standards - that enables a dynamic adaptation to specific actor needs and objectives. To illustrate the application of our framework and establish its feasibility, we describe a prototypical application in the e-learning domain."
816,The conference offers a note of thanks and lists its reviewers.
817,"The Internet of Things, large scale sensor networks or even in social media, are now well established and their use is growing daily. Usage scenarios in these fields highlight the requirement to process, procure, and provide information with almost zero latency. This work is introducing new concepts for enabling fast communication by limiting information flow through filtering concepts combined with data processing techniques adopted from complex event processing. Specifically we introduce a novel mediation services architecture using filter policies to reduce latency. The filter policies define when and what data services need to provide to the mediator and thus save on bandwidth. The filter policies describe temporal conditions between two events removing the need to keep a complete history while still allowing temporal reasoning. Promising experimental results highlight the advantages to be gained from the approach."
818,"Current Web services approaches have many limitations, especially with description, discovery and integration mechanisms. In this paper we present a novel software architecture called aspect-oriented Web services (AOWS) which addresses these problems. AOWS uses descriptions of cross-cutting concerns between Web services to give more complete descriptions of services, supporting richer dynamic discovery and seamless integration. We describe our architecture, a formal specification of it and an implementation using .NET Web services technology."
820,"One of the main challenges that encounter Web services is how to ensure reliable compositions. In this paper we present an approach that starts from a composite service effective executions to improve its reliability. Basically, we propose a set of mining techniques to discover its model and its transactional behavior from an event based log. Then, based on this mining step, we use a set of rules to improve its recovery mechanisms"
821,"Predicting the Quality of Service (QoS) values is important since they are widely applied to Service-Oriented Computing (SOC) research domain. Previous research works on this problem do not consider the influence of user location information carefully, which we argue would contribute to improving prediction accuracy due to the nature of Web services invocation process. In this paper, we propose a novel collaborative QoS prediction framework with location-based regularization (LBR). We first elaborate the popular Matrix Factorization (MF) model for missing values prediction. Then, by taking advantage of the local connectivity between Web services users, we incorporate geographical information to identify the neighborhood. Different neighborhood situations are considered to systematically design two location-based regularization terms, i.e. LBR1 and LBR2. Finally we combine these regularization terms in classic MF framework to build two unified models. The experimental analysis on a large-scale real-world QoS dataset shows that our methods improve 23.7% in prediction accuracy compared with other state-of-the-art algorithms in general cases."
822,"Many cloud providers offer on demand applications as Business Process as a Service (BPaaS), allowing companies to outsource their processes. For cost saving, some process fragments can be reused on the cloud regardless of privacy risks. In this paper, we propose an anonymization-based approach to preserve client business activity while sharing process fragments between organizations on the cloud."
823,"Web service composition is a new technology for creating value-added services in the dynamic network environment and implies great application value, with the rapid development of Web services and Cloud Computing, composite service requests are growing sharply. However, how service integrators intelligently to handle continuous arrival of composite service requests, ensure service quality of existing users while improving resource utilization is a challenge for urgent solution. This paper provides an intelligent admission control framework for composite services, which mainly including three aspects: Firstly, it proposes a global QoS constraints optimal decomposition method which decomposes global QoS constraints into local QoS constraints that each task in the composition process should satisfy, this step provides critical basis for admission judgment, Secondly, it proposes a reliable admission judgment method based on QoS dynamic prediction and QoS interference estimation, this method can improve the reliability of admission control. Finally, it provides a context-aware service resource adaptive scheduling method for assigning the most suitable resource to the new accepted request so as to improve the utilization of the whole service resources. Our study aims to reveal the general rules of admission control for composite service, provide methods to improve the quality of composite service and increase the utilization of the whole service resource."
824,"Service recommendation plays a critical role in fostering the growth of service ecosystems. However, existing methods are mainly in favor of a small number of popular services while newly emerged ones (i.e., newborn services) are largely ignored, which hurts the systems in two aspects. First, the potential of many services, especially the newborn ones, is wasted. Second, service ecosystems highly depending on a few kernel services are not diversified nor robust. To address this issue, we propose to proactively recommend collaborative services for newborn ones. The aim is to illuminate how to use the newborn services and fertilize their proper usages. While this is a cold start problem, frequent collaboration among newborn or dissimilar services makes it more difficult. In this situation, a Divide-and-Conquer approach is adopted utilizing category tags and collaboration records (DCCC). For each newborn service, the approach first produces one ranked list of old services and one list of newborn services, separately. DCCC then merges the two lists into one for recommendation. Experiments over a real-world dataset from ProgrammableWeb demonstrate that the proposed approach achieves significant improvement in recommendation accuracy compared with baseline methods."
825,"While service oriented computing is gaining popularity and design methodology for functional features is becoming mature, design of non-functional features and governance capabilities is increasingly important. WS-policy has been touted as a standard way to specify service policy for these features and map service policy to upper layers of SOA policy model for SOA governance. This paper introduces a novel model-driven method to orchestrate service policy within system context in design time. By applying top-down SOA design methodology, firstly the logical service policy model is created to describe relation among service policy in logical level, and then it is transformed to physical service policy model with more factors of real system topology considered. Finally, service policy deployment model is generated to describe the relation among policy related artifacts and policy repositories, and guide and automate the deployment of policies in runtime environment as well. Leveraging system topology and behavior information, service policies are orchestrated vertically and horizontally according to service policy dependency, business policy, service level agreement (SLA), and domain specific constraints. Meta-policy based service policy conflict detection and resolution is highlighted. We demonstrate how this approach significantly enhances the capability to ensure the correctness and consistency of service policy within service oriented computing environment."
827,This study introduces a sound-based mobile payment system as a wireless Web service. Using a mobile phone for payment for mobile commerce is essential for its success. The proposed system will use sound that is generated from the existing mobile phone and the sound can be recognized by the existing credit card reader of the shop with an installation of an ordinary microphone and simple software to process sound input. This system is a better system since it doesn't require customers to buy a new mobile phone and the cost for the microphone and the software is very low.
828,"Service negotiation allows a service client to negotiate with a service provider on the terms of service. Much existing work on service negotiation assumes that a provider will define its negotiation strategy in terms of the state of its resources. This approach can lead to complex strategy and assumes, unrealistically, that providers have full knowledge and control of their resources. We propose a hierarchical model of service negotiation in which negotiation strategy is defined in terms of sub-negotiations with internal or external agents. This model helps to manage the complexity of negotiation strategy by allowing it to be decomposed, with each component having well-defined scope. In this paper we present our hierarchical negotiation model, and a negotiation protocol and negotiation policy language based on it."
829,"Although the use of workflow techniques based on Web services (WS) has grown rapidly for the past several years, there is still a significant gap to be bridged before such an approach is widely accepted as the standard way to build and represent WS-based applications. One of the reasons for this is the inability in most current workflow languages to support large and complex applications that require workflow integration and data sharing between workflows. In this paper, this issue is addressed by the introduction of the two approaches used in the Service Workflow Language (SWFL): the Procedure-Oriented approach, and the Objected-Oriented approach, for workflow integration. The data sharing and related synchronization issues that are associated with these workflow integration approaches are also discussed."
830,"Web services technology has received much attention in the last few years, and a lot of research efforts have been devoted to utilizing services on the Internet to fulfill consumers' requirements. However, little research has been done on the current status of web services on the Internet, which has a great impact on current research. Enlightened by this situation, we made an exploratory study of the current status of web services on the Internet. Our study mainly focused on the investigation of four aspects, including the number, complexity, quality of description and the function diversity of available web services on the Internet. A web services investigation system is built up to harvest web services from the Internet and calculate the statistical results. The investigation results are reported in this paper, and, based on our study, the development trend of web services technology is also discussed in this paper."
831,"Microservice based cloud architecture becomes a promising solution to deal with the challenges of large-scale intelligent video applications. However, the current service selection methods usually do not consider both the fine-grained online service capability and the features of video tasks, and this will result in the degradation of the overall efficiency of the service composition. In this paper, we propose a novel Performance-aware Service PAth Selection (PSPAS) approach for the microservice based video cloud computing platform. Firstly, we establish a fine-grained time estimation model which synthetically considers the processing capability of microservice instances, the characteristics of video processing tasks, and the data transfer conditions between microservice instances. Then, based on the proposed performance model, we search and update the optimal microservice path by using the shortest path algorithm. Finally, the experiment evaluation results demonstrate the effectiveness of our method."
832,"Web service orchestration engines need to be more open to enable the addition of new features into service-based applications. In this paper, we illustrate how, in a BPEL engine with aspect-weaving capabilities, a process-driven application based on the Google Web service can be dynamically adapted with new features and hot-fixed to meet unforeseen post-deployment requirements. Business processes (the application skeletons) can be enriched with additional features such as debugging, execution monitoring, or an application-specific GUI. Dynamic aspects are also used on the processes themselves to tackle the problem of hot-fixes to long running processes. In this manner, composing a Web service 'on-the-fly' means weaving its choreography interface into the business process."
833,"This paper proposes WS-attestation, attestation architecture on Web services framework. We aim at providing software oriented, dynamic and fine-grained attestation mechanism that leverages TCG technologies to increase trust and confidence in integrity reporting. In addition, the architecture allows efficient binding of attestation with application context, privacy protection, as well as infrastructural support for attestation validation."
834,"Web Services are emerging technologies that enable application-to-application communication and reuse of autonomous services over the Web. Recent efforts, OWLS, model the semantics of Web Services that includes the capabilities of the service, the service interaction protocol, and the actual messages for service exchanges. However, there is a need to automate discovery, selection and execution of OWL-S services. Further, a framework that meets the quality of service (QoS) requirements for ad hoc Internet based services is rarely provided. In this paper, we have proposed a rule-based framework, called SetnWebQ, which manages workflows composed of Semantic Web Services. SemWebQ is capable of conducting QoS-based adaptive selection as well as dynamic binding and execution of Web Services according to the semantics of workflow, thereby rendering a resilient and adaptive Web based service flow. A series of experiments performed on the SemWebQ with real Web Services have confirmed the effectiveness of proposed framework with respect to adaptive selection and execution of the Web Services in Web based workflows."
835,"With the increasing popularity of web services on the Internet, besides functionalities, Quality of Service (QoS) is becoming an important concern for describing characteristics of web services. QoS rankings provide valuable information for making optimal service selection and recommendation from a set of functionally similar or equivalent service candidates. However, in order to obtain such rankings, a huge number of invocations on the services are usually required, which is extremely expensive and even impractical in reality. To tackle this challenge, this paper proposes a scheme to derive the global ranking from observations of QoS rankings on subsets of the services, while the observations may also be contaminated by noise and errors. We introduce a pairwise comparison model to describe the relationships between services, and thus the ranking can be formulated as random walks over the services. A Markov chain based approach is proposed, and algorithms for deriving global rankings are designed. The efficacy of our approach is validated by both mathematical analysis and simulation experiments."
837,"Web service evaluation is one of the key problems in web service discovery and selection research fields. In this paper, we propose an approach to evaluate the web service performance. Different from traditional evaluation methods, our approach is based on users experience, and we apply the idea and result in common webs evaluation fields to help construct our evaluation system. We import the Alexa ranking to help evaluate the information providing performance of web services, we import the idea of Page Rank to help designing our evaluation method for the function sharing performance of web services."
838,"The execution time of computationally-intensive applications such as protein folding and fractal generation can be reduced by implementing these applications as Web services that run in parallel. Additionally, some of these Web services may save state periodically to resume execution later on. However, currently, there is no solution to load balance this class of Web services, and to replicate the saved state for the purposes of resumption. This paper describes the architecture of JULIET, a system that load balances .NET Web services across a Windows cluster in a distributed fashion. The system is also fault tolerant since it supports failovers and replication of data generated by the Web services at the application level. The system is designed to be minimally-visible to the Web service and the client that consumes it."
839,"Massive parallel business workflows running in the cloud are prone to temporal violations (namely intermediate runtime delays) due to various reasons such as service performance fluctuation and resource conflicts. To deliver satisfactory on-time completion, cloud workflow temporal verification is employed to accurately detect time delays of workflow activities and timely handle temporal violations before final deadline is violated. While most of the existing works only monitor the time delays of individual workflow activities or workflow instances, the effect of time delay propagation (similar to ""Butterfly Effect"") in cloud workflow systems has been overlooked, which has significant impact on the accuracy of temporal verification. In this paper, we present a propagation-aware temporal verification strategy for parallel business cloud workflows. Specifically, we first analyze the effect of time delay propagation in cloud workflow systems. Then, we present the novel temporal verification strategy based on a new propagation-aware throughput consistency model which includes the propagation effect. Experimental results demonstrate that compared with the traditional strategy, our propagation-aware strategy has higher success rate in achieving target on-time completion rate for massive parallel business cloud workflows."
840,"As available services accumulate on the Internet, QoS-aware service selection (SSP) becomes an increasingly difficult task. Since Artificial Bee Colony algorithm (ABC) has been successful in solving many problems as a simpler implementation of swarm intelligence, its application to SSP is promising. However, ABC was initially designed for numerical optimization, and its effectiveness highly depends on what we call optimality continuity property of the solution space, i.e., similar variable values (or neighboring solutions) result in similar objective values (or evaluation results). We will show that SSP does not possess such property. We further propose an approximation approach based on greedy search strategies for ABC, to overcome this problem. In this approach, neighboring solutions are generated for a composition greedily based on the neighboring services of its component services. Two algorithms with different neighborhood measures are presented based on this approach. The resulting neighborhood structure of the proposed algorithms is analogical to that of continuous functions, so that the advantages of ABC can be fully leveraged in solving SSP. Also, they are pure online algorithms which are as simple as canonical ABC. The rationale of the proposed approach is discussed and the complexity of the algorithms is analyzed. Experiments conducted against canonical ABC indicate that the proposed algorithms can achieve better optimality within limited time."
841,"QoS aware service composition necessitates an effective pricing mechanism in regulating service providers in public cloud computing environments. However, due to the fact that service providers are usually autonomous, strategic and self-motivated, it is far from trivial to deal with the pricing issues between them. In this paper we formulate a non-cooperative service pricing game to understand the performance of a QoS aware service composition model, for which multiple providers strategically bid how to provide and price their elementary services and establish the Nash equilibrium as the final service composition scheme. We also develop a proportional revenue division rule to incentivize elementary service providers to contribute in improving the QoS of the final composite service delivered to end users. Concerning privacy conservation, we develop a decentralized and recursive bidding algorithm, allowing service providers to reach an equilibrium without disclosing their private information. Through theoretical analysis, we show that a Nash equilibrium exists in a QoS aware service composition game. Through extensive simulations, we show that the proposed recursive bidding process can converge quickly to a Nash equilibrium service composition scheme, and its efficiency is generally high."
842,"We propose a language CDL as a formal model of simplified WS-CDL. The operational semantics of CDL is given, and static validation and verification of choreographies is studied. Some properties of the proposed model are verified using the SPIN model-checker, which illustrates the potential usage and benefits of the formal model"
843,"The task abstraction and aggregation in a business process can help to (1) obtain customized descriptions of a business process for different users, and (2) derive user interfaces of a business process related to the participating users. This paper proposes an approach for task abstraction and aggregation of a business process based on a role-enriched business process model. For each user role, tasks of a business process are abstracted and aggregated according to the identified control flow patterns. A set of elementary operations for task abstraction and aggregation in business process are specified. The algorithm for deriving the abstracted and aggregated business process is developed. The structural consistency between the business process and the abstracted and aggregated business process has been analyzed. The derived abstracted and aggregated business processes with the proposed approach can be used to support analysing, developing, and updating software components such as user interfaces related to different user roles."
844,"In service selection, an end user often has his or her personal preferences imposing on a candidate service's non-functional properties. For a service selection process promoted by a group of users, candidate services are often evaluated by a group of end users who may have different preferences or priorities. In this situation, it is often a challenging effort to make a tradeoff among various preferences or priorities of the users. In view of this challenge, a multi-criteria decision-making method, named AHP (Analytic Hierarchy Process), is introduced to transform both qualitative personal preferences and users' priorities into numeric weights. Furthermore, a QoS-aware service evaluation method is presented for a shared service's co-selection taking advantage of AHP theory. At last, a case study is presented to demonstrate the feasibility of the method."
845,"For realizing non-intrusive protection of open SCADA systems, a non-intrusive solution for distributed open SCADA systems is proposed. The solution consists of three functionality parts: Abstract Execution, Refine State, and Behavior Checking. The approach provides a runtime verification of the system by combining cyclic semantic reconstruction of VM and abstract execution of SCADA services. First, all Internet packets through virtual network bridges are extracted and symbolically linked to specific service model to get simulated traces. Then, cyclic semantic reconstruction is performed to acquire the current service runtime state. According to the service instance state of semantic reconstruction, the simulated traces are refined. When a trace is identified, behavior checking is adopted to verify whether the runtime state is compliant to the system specification that is defined based on milestone events for meeting SCADA real-time requirements."
846,"Identity delegation is an act whereby an entity delegates his or her authority to use identity information to another entity. It has most often been implemented in enterprise environments, but previous studies have focused little on the dynamic data and access management model as well as the design from a practical viewpoint. An identity delegation framework is described for using access tokens across security domains. The framework enables fine-grained access control with limited overhead cost for access management and permission assignment for delegated access."
848,"Web Services constitute a set of technologies that many believe will change the Web communication landscape within the next few years. They offer standardized and easy communications for distributed systems over the Internet. However their dynamic and distributed nature requires a well-managed system, and pending security issues prevent their widespread adoption. Meanwhile there is a big rage toward the use of Virtual Private Networks (VPNs) to secure communications in a cost-effective environment like the Internet. In this paper we explain how to merge these two technologies in a new powerful hybrid model that: (1) enables an easy management of Web services, (2) provides Web services security thanks to the use of dynamic and programmable VPNs, and (3) remains simple and fully integrated."
849,"In service-oriented mobile social networks (S-MSN), many location-based services are developed to provide various applications to social participants. Services can in turn be composed with the help of these participants. However, the composite structure, the subjective interpretation of trust demand, and the opportunistic connectivity make service composition a challenging task in S-MSN. In this paper, we propose a novel approach to enable trustworthy service evaluation and invocation during the process of composition. By analyzing dependency relationships, our approach can decentralizedly evaluate the trust degree of each service based on a lattice-based trust model to prevent data from being transmitted to untrustworthy counterparts. Besides, service consumers and vendors are able to specify their global and local constraints on the trust degree of service components on demand for more effective composition. Finally, by introducing acquaintances to the neighbors iteratively, social participants form a trust-aware acquaintance graph to forward invocation messages."
850,"Reflection is a powerful tool for the adaptation of applications at runtime. The modification of Web services is a task that entails the modification and compilation of the source code, as well as the deployment of the new version of the Web service in the application server. In this paper, we introduce RAWS (Reflective and Adaptable Web Service), a Web service design model based on a reflective architecture of two levels. RAWS allows both the dynamic modification of the definition and implementation structure of the Web service, and the dynamic modification of the Web service behavior in order to change the existing code or to add new functionalities. All these dynamic modifications are performed directly on the code during execution, with no need to have the Web service source code. RAWS improves Web services adaptability and maintainability, as well as the ability for authorized clients to remotely modify them."
851,"A Web service is a software system designed to support interoperable application-to-application interactions over the Internet. Web services are based on a set of XML standards, such as Web Services Description Language (WSDL), Simple Object Access Protocol (SOAP) and Universal Description, Discovery and Integration (UDDI). Recently, there has been a growing interest in Web service composition, and some languages (e.g., WSBPEL, BPML) for modeling the composition have been proposed. In this paper, we focus on security constraints of Web service composition, which have not been deeply investigated so far. We propose a method for modeling security constraints and a brokered architecture to build composite Web services according to the specified security constraints"
852,"The objective of this research is to seek the exact nature of how an organization should apply a service-oriented design to systems that were previously standalone heterogeneous applications. For this objective, three applications are developed and integrated by using a service-oriented architecture: a biometric attendance system, a surveillance system, and a point of sale system. Each application is classified by a combination of two properties: input or output heavy application, local or remote running application. Based upon the combination, two types of service-oriented integration approach, fully or partially Web serviced integration, are suggested according to the extent of the usage of Web services. Based upon the experiences gained from developing the three applications, the guidelines for adopting Web services are proposed when an organization is integrating applications."
853,"This paper describes a new and generic REST service composition framework that uses 6 algebraic rules to drive resource-resource compositions in different domains. The generalization is achieved by two techniques borrowed from Category Theory: 1) decomposition of a REST system into cohesive link and communication layers, and 2) uniform representation of resource interfaces and resource compositions as categorial links. Within this framework, we introduce three types of categorial links: basic, nested and concurrent, and we show that many domain tasks in hyperlink pipeline, call control and media workflows can be accomplished efficiently by these links. The preliminary experiments indicate the approach is feasible."
854,"Large and complex workflow repositories include a series of interdependent workflows. In this scenario, it becomes hard to estimate the effort required to accomplish changes to workflows. Furthermore, ad-hoc changes may induce side and ripple effects, which ultimately hamper the reliability of the repository. In this paper, we introduce a static dependency-centric change impact analysis approach for workflow repository management. The approach relies on metrics and visualizations that makes it easy and quick to estimate change impact. We implemented the approach, incorporated it into HP Operations Orchestration (HP OO), and conducted an exploratory study in which we thoroughly analyzed the workflow repository of 8 HP OO customers. Besides being able to characterize and compare the repositories against each other, we found that while the out-of-the-box repository provided by HP OO has 10 flows with high change impact, 5 customer repositories had higher values that ranged from 11 (+10%) to 35 (+250%)."
855,"In the mobile Internet, users mostly work with handheld devices with limited computing power and small screens. Their access conditions also change more frequently. In this paper, we present a novel service supporting intelligent content adaptation to better suit handheld devices. The underlying technique is a unit of information (UOI)-based content adaptation method, which automatically detects semantic relationships among comprising components in Web contents, and then reorganizes page layout to fit handheld devices based on identified Ious. Experimental results demonstrate that our method enables more exquisite content adaptation."
856,"The discovery of suitable web services for a given task from a brokerage system is the core part of current Service Oriented Architecture (SOA). With the number of registered web services growing, organization of search results is critical for improving the utility of any service search engine. A clustering view of search results is more effective than traditional ranked-list style in helping users to navigate into relevant services quickly and accurately. In this paper, we will propose an efficient clustering algorithm for organizing returned services. Our experiments validated the efficiency of the proposed method."
857,"Information search in the Web can become cumbersome if the desired information is scattered across multiple websites. For instance, even though there exist pages listing track chairs of the past ICWS conferences and web accessible bibliography databases, compiling the list of recent journal publications of the ICWS track chairs with the help of existing search engines is still a time consuming task. It is even harder to find information from the Deep Web as it requires user interactions that are hard to simulate by automatic crawlers. Semantic search based on structured data aims at efficiently answering information needs but relies on the cooperation of providers to be able to access their data. We aim at providing an alternative solution by introducing web browsing recipes that are goal-oriented end user browsing processes containing instructions for accessing, extracting, and merging (dynamic) information from various websites. Browsing recipes can be shared and reused to allow users to benefit from the browsing efforts of others. In order to achieve this goal, the development of efficient search techniques is the main prerequisite for effectively sharing and reusing recipes. In this paper we propose an efficient search technique for finding browsing recipes from large recipe repositories. Our search technique necessitates a structured query specifying the required information along with constraints on the structure of the browsing processes. We augment explicit state representation based model checking technique by indexing structures tailored to the requirements of information search based on the recipes. The performance evaluation of our approach reveals the impact of the indexing structures on the overall recipe search efficiency."
858,"Adaptation of service-based applications (SBA) is not trivial in their heterogeneous and dynamic execution context. While different approaches exist, most of them focus on a specific part of the SBA ignoring the overall impact of the adaptation on the whole application. In this paper we propose a cross-layer adaptation manager (CLAM) that tackles this problem."
860,"This paper presents an infrastructure based in virtualization which provides support to intrusion tolerance (Byzantine or malicious faults) for Web Services. The introduced approach makes extensive use of virtualization technology and shared memory in order to tolerate intrusions at a low cost of messages. The intrusion tolerance is typically achieved using state machine replication (SMR). Our approach allows client requests to be performed with a variable number (between f + 1 and 2f + 1) of execution replicas which is different from more classical implementations of SMR. This paper describes the algorithms, presents details of a prototype, testing and a comparative study with the related work in the literature."
861,"The design, deployment and execution of business process models and their associated security models is expensive and time consuming. This is because these activities usually involve multiple stakeholders that include business domain experts, security experts, web service developers and IT operations teams, and there is no streamlined development environment to allow these stakeholders to work collaboratively on a business process. We have developed a cloud-based model-driven development and execution environment called SSC4Cloud to provide a shared business process modeling workspace and a business process execution environment. More specifically, with the shared modeling workspace, business process models can be developed, refined and shared. Within the shared execution environment, a business process model is translated into a WS-BPEL based executable model, which is then assigned for execution in a virtual machine container from a shared machine cluster. The common model execution environment supports both business process execution and enforcement of the security requirements attached to the business process models."
862,"This tutorial will share with the audience on how to leverage the foundational knowledge of service-oriented architecture (SOA) and Web services to build service-oriented business consulting method, enable software as services and services as software. Advanced SOA techniques are covered in the following topics: services publishing and discovery, business services relationship modeling, business requirements-driven services composition, services value chain collaboration, business process integration and modeling, enterprise modeling, and project-based business performance management. A services engineering method and delivery framework will be discussed as a case study. The target audiences are all-level researchers, practitioners, and students. From this tutorial, the audiences could learn the actual service delivery processes, technologies, and methodologies in the entire service delivery life cycle. This tutorial material is created for the IEEE Body of Knowledge initiative on Services Computing, which is sponsored by the IEEE Computer Society Technical Committee on Services Computing"
863,"The physical world is becoming smarter and smarter due to the advances in smart devices and CPS/IoT technologies. In this paper, we investigate the roles various cutting-edge technologies, such as service computing, big data analytics, crowd sourcing, gaming technologies, etc., can play to significantly enhance the intelligence of our physical environment and subsequently benefit the society. We consider a smart physical world (SPW) consisting of physical entities, cyber entities, and human. Service technologies can be used to model the entities in SPW and specify their capabilities. Service discovery and composition techniques can be used to, based on the modeling, compose these capabilities to solve real-world problems. To enable higher level intelligence, we further discuss how various technologies, such as big data analytics and artificial intelligence, can be used in the smart world to reason from sensor inputs to derive the situation facts, and from the situation facts to derive the reactive actions. From the derived actions (tasks), the service technologies can be used to compose the capabilities of the entities to realize the task. However, current technologies and artificial intelligence may fall short in many situations. We further propose a gaming-based crowd sourcing platform to make use of human intelligence to enable successful completion of some challenging reasoning and control tasks."
866,"Automatic composition of Web services requires that the providers publish an abstract version of their Web services to a registry. They offer this abstraction instead of the complete Web service to ensure the privacy of their internal know-how and trade secrets. Many studies have offered methods to do this, but none of them is able to formally prove their ability to keep the secret information hidden. In this article we turn to the verification of opacity, a formal security property that allows not only to preserve the secret but also to formally prove that it remains hidden. In particular, we investigate if the composition of two opaque Web services is also opaque. Our work consists in verifying the opacity of the composition of two Web services through the verification of the opacity of their individual abstractions represented by Symbolic Observation Graphs."
867,"We have designed and implemented a framework assisting client-side applications to utilize asynchronous Web services that deliver results by calling back the applications in the context of enterprise security. This framework can support a number of rich features such as the single-request/multiple-response (SRMR) message exchange pattern, resumable clients, intra-enterprise user and terminal mobility, and flexible interaction modes for client applications to communicate with asynchronous Web services. Practical deployment of this framework in a number of cases has illustrated that this framework can effectively minimize development efforts in facilitating client applications to use Web services with callbacks."
868,"The market is demanding seamless application integration. Those who can deliver on this demand will be rewarded. Web services will play a key role in making it possible, affordable, and ease to use."
869,"Web services promise to allow businesses to adapt rapidly to changes in the business environment, and the needs of different customers. However, the rapid introduction of new services paired with the dynamicity of the business environment also leads to undesirable interactions that negatively impact service quality and user satisfaction. In this paper, we propose an approach for modeling such undesirable interactions as feature interactions. Our approach for detecting interactions is based on goal-oriented analysis and scenario modeling. It allows us to reason about feature interactions in terms of goal conflicts, and feature deployment. Two case studies illustrate the approach. The paper concludes with a discussion, and an outlook on future research."
870,"Over the last few years, we have seen the transformation of the traditional monolithic enterprise, in which all operations were performed in-house, to the extended enterprise, which consists of a network of collaborating entities. Global operations, outsourcing, and increasing specialization have all contributed to this trend. One challenge facing the extended enterprise is how to reconnect the information flows and business processes that were disconnected as the enterprise disaggregated. The emergence of web services, service-oriented architectures, and business process modeling and execution standards are helping to address this challenge. Our contention is that the next phase of evolution is the rise of the intelligent enterprise, which is characterized by being able to adapt quickly to changes in its operating environment. The intelligent enterprise monitors its own business processes and its interactions with customers, partners, suppliers, and collaborators; it understands how this information relates to its business objectives; and it acts to control and optimize its operations to meet its business objectives. Decisions are made quickly and accurately to modify business processes on the fly, dynamically allocate resources, or change business partners (e.g., suppliers, service providers) and partnerships (e.g., establish new service level agreements). This talk will describe challenges in managing the business operations of an intelligent enterprise. While a plethora of tools exist for managing the IT infrastructure (servers, storage, and network resources) of the enterprise, there is little systematic support today for the closed loop management and control of business operations. We will describe technology approaches to intelligent business operations management that we are pursuing at HP Labs., the progress we have made, and some open research questions."
871,"WS-BPEL processes are usually overlapped in large Business applications composed of several Web Services. Such applications are more and more developed with respect of quality processes. Testability is an important quality degree, which evaluates the fault detection coverage during the testing process and the testing cost. In this paper, we focus on a well-known testability criterion called observability, which evaluates if enough distinguishable events can be observed while testing. We study the observability of ABPEL (Abstract-BPEL) specifications and we describe some ABPEL observability degradation properties. From these, we propose some observability enhancement methods which detect observability issues in ABPEL specifications and semi-automatically update the ABPEL code."
873,"We propose a fuzzy trust management framework for the Service Web. The proposed framework supports a natural way of representing and querying consumers' perception on services. We describe the underlying models, algorithms and an implementation architecture, mainly focusing on the key features and contributions of the proposed framework."
874,"DHTs are scalable, self-organizing, and adaptive to underlying topology changes, thus being a promising infrastructure for realizing efficient Web service discovery. Range queries play an important role in service discovery, and in recent years a number of DHT-based range query schemes have been proposed. However, most of them suffer from high query delay and high processing cost. This paper presents ERQ, an Efficient scheme for delay bounded Range Query processing over DHTs. We first emulate the PHT structure and design a balanced Kautz (BK) tree to uniformly map the m-dimensional data space onto DHT nodes, and then present a novel algorithm that processes range queries in a parallel fashion, where an on-the-fly space pruning mechanism is adopted to reduce the processing cost. In a DHT with N nodes, ERQ can answer any range query in less than logN (2loglogN+1) hops with low processing cost, irrespective of the queried range, the whole space size, or the number of queried attributes. The effectiveness of ERQ is demonstrated through extensive experiments."
877,"Currently Web services composition problems are addressed using AI planning techniques. The team-based approach, with emphases on the sharing of mental models and proactive collaboration, provides an alternative to current static approaches to Web service composition. The approach provides clear advantages for proactive handling of failures that may be encountered during execution of a complex Web service. The paper proposes a generic framework for dynamic Web-service composition, and extends the CAST architecture to realize the framework."
878,"In this work we present a case study of a SOA realization exercise at a business information provider firm, which deals with disparate sources of data in-order to provide reliable reports to its clients. Unlike typical enterprise scenarios, where applications are required to be service enabled, the key requirement here was to service enable its data acquisition, quality check, reporting and other processes which are either mostly manual or ETL based workflows. This paper also addresses how shared services, business processes, rules, and semantics are used to provide quality and agility to the internal processes many of which are entirely dependent on the type of data received. The case and the scenario are chosen specifically to emphasize the fact, that mere web-services implementation does not lead to service oriented architecture, but it is the appropriate usage of them."
879,"The fundamental challenge of e-commerce is enabling companies to do business with one another across a network, despite different business processes and computer systems. Traditionally, these problems were overcome through expensive custom point-to-point integration or Electronic Data Interchange (EDI) networks. The promise of the Internet, by contrast, is an open e-business platform where companies can do business spontaneously with anyone, anywhere, anytime. Business Services Networks fulfill that vision. This talk introduces the concept of Business Services Networks and their profound business and technology implications for e-commerce. BSNs are Internet business communities where companies collaborate through loosely coupled business services. Participants register business services (e.g., place an order, make a payment) that others can discover and incorporate into their own business processes with a few clicks of a mouse. Companies can build on each other’s services, creating new services and linking them into industry-transforming, network-centric business models. We also discuss evolution of BSNs since early 1990’s, emerging technological underpinnings, and CommerceNet’s role in catalyzing their adoption."
880,"In this paper, we investigate the combination of configuration and query rewriting for semantic Web service composition. Given a user query and a set of service descriptions, we rely on query rewriting to find services that implement the functionalities expressed in the user query (discovery stage). Then, we use configuration to capture dependencies between services, and to generate a set of composed Web services described as a directed acyclic graph, while maintaining validity with respect to business rules (orchestration stage).Finally, we propose a semantic ranking algorithm to rank results according to user preferences (classification stage).The techniques used in our approach take into account the semantics of concepts utilized to describe the elements (services, business rules, query and user preferences) involved in the composition process. We provide a formal approach and its implementation, together with experiments on Web services from different application domains."
881,"Author Name Disambiguation (AND) is the task of clustering unique author names from publication records in scholarly or related databases. Although AND has been extensively studied and has served as an important preprocessing step for several tasks (e.g. calculating bibliometrics and scientometrics for authors), there are few publicly available tools for disambiguation in large-scale scholarly databases. Furthermore, most of the disambiguated data is embedded within the search engines of the scholarly databases, and existing application programming interfaces (APIs) have limited features and are often unavailable for users for various reasons. This makes it difficult for researchers and developers to use the data for various applications (e.g. author search) or research. Here, we design a novel, web-based, RESTful API for searching disambiguated authors, using the PubMed database as a sample application. We offer two type of queries, attribute-based queries and record-based queries which serve different purposes. Attribute-based queries retrieve authors with the attributes available in the database. We study different search engines to find the most appropriate one for processing attribute-based queries. Record-based queries retrieve authors that are most likely to have written a query publication provided by a user. To accelerate record-based queries, we develop a novel algorithm that has a fast record-to-cluster match. We show that our algorithm can accelerate the query by a factor of 4.01 compared to a baseline naive approach."
882,"Web service-based workflow management systems have garnered considerable attention for automating and scheduling dependent operations. Such systems often support user preferences, e.g., time of completion, but with the rebirth of distributed computing via the grid/cloud, new challenges are abound: multiple disparate data sources, networks, nodes, and the potential for moving very large datasets. In this paper, we present a framework for integrating QoS support in a service workflow composition system. The relationship between workflow execution time and accuracy is exploited through an automatic workflow composition scheme. The algorithm, equipped with a framework for defining cost models on service completion times and error propagation, composes service workflows which can adapt to user's QoS preferences."
883,"The growth of the Internet has been accompanied by the growth of web services (e.g. e-commerce, e- health) leading to the need to protect the privacy of web service users. However, before privacy can be protected, it is necessary to understand the risks to privacy that come with the service. Indeed, such understanding is key to protecting privacy throughout the service lifecycle. Unfortunately, there does not appear to be any existing method for privacy risk analysis specifically designed for web services. This paper presents a straightforward method for web services privacy risk analysis that uses visual techniques to improve effectiveness and illustrates the method with an example."
884,"Service-Oriented Computing is benefit of interoperation among services. Current service-oriented computing research is much more concerning the low level interoperation among services, such as service discovery, service composition etc. However, the high level research issue-the feature interaction problem is also challenging the interoperation of service-oriented computing. Traditional feature interaction methods are focused on the service design phrase with formal methods or software engineering analysis. Autonomy and distribution of service deploying style have made the needs of runtime detecting and resolving feature interaction in SOC research community. This paper investigates the detection of feature interactions in web services at runtime and proposes ESTRIPs, an extended STRIPS operation conflict-free of services in business process detection method, which reasons from OWL-S and SWRL combined with runtime SOAP messages. First, we give the model of the feature interaction problem in business process during its execution and then the ESTRIPS method given in detail. The implementation of a prototype is illustrated. Using a real world scenario shows the plausibility of our method of detecting feature interactions of business process."
885,"BPEL (Business Process Execution Language) enacts a process-oriented web service orchestration, and multi-business processes can be regarded as BPEL composition. A business process can be regarded as a complex set of interactions among Web services to achieve a defined goal. The achievement of distributed agreement among multiple-participant services is an orthogonal problem outside the scope of BPEL, so the rationality for distributed coordination of multi-business processes is an urgent issue to study. The definition of the message exchanges that take place between the process and each one of its partners lack the precise definition which is required for performing a formal analysis and reasoning. An integrated approach supporting a formal verification of multi-business interactions is proposed. This paper first examines a rigorous approach for the formalization of the execution semantics of business process in the Pi-calculus. Then transforms the Pi-calculus expressions into equivalent SMV code and verifies the system whether a process satisfies given properties automatically using the NuSMV model checker, and the approach is illustrated using a concrete case study subsequently. The approach supports creating robust multi-business processes which are distributed or span multiple vendors and platforms."
886,"There is a significant number of IT failures per year because parts fail, products are used in ways they were not designed for, and humans make errors in using products. These failures result in incidents that product vendors service as a part of the warranty or contracts. Incidents incur significant costs for servicing them, including call centers, parts, and field engineers. Some of the major problems include lack of coherent incident information, leading to inaccurate service diagnosis and inability to forecast failures. At the same time, technology has evolved. Hardware is generally more reliable, failures are moving from hardware to firmware, software, and applications. The scale effect limits human operator engagement, prevents centralized approaches, and expands automation. Traditional ways of handling incidents are not appropriate any more. In this paper we present a set of tools and approaches that enable unified serviceability with self-healing, automated learning, and an analysis engine. Unified serviceability with self-healing results in clean incident data and it reduces criticality of incidents into deferred maintenance. Automated learning produces empirically proven actionable knowledge enabling cost reduction of automated incident resolution. Using clean data and actionable knowledge, the analysis engine helps predict failures and determine trends, resulting in preventive maintenance. Collectively, preventive and deferred maintenance and automated incident service significantly reduce the costs. This way we have aligned incidents cost with the technology trends."
887,"In the literature, many solutions for measuring the reputation of web services have been proposed. These solutions help in building service recommendation systems. Nonetheless, there are still many challenges that need to be addressed in this context, such as the ""cold start"" problem, and the lack of estimation of the initial reputation values of newcomer web services. As reputation measurement depends on the previous reputation values, the lack of initial values can subvert the performance of the whole service recommendation system, making it vulnerable to different threats, like the Sybil attack. In this paper, we propose a new bootstrapping mechanism for evaluating the reputation of newcomer web services based on their initial Quality of Service (QoS) attributes, and their similarity with ""long-standing"" web services. Basically, the technique uses regression models for estimating the unknown reputation values of newcomer services from their known values of QoS attributes. The technique has been experimented on a large set of services, and its performance has been measured using some statistical metrics, such as the coefficient of determination (R2), Mean Absolute Error (MSE), and Percentage Error (PE)."
888,"In cloud computing, it is an urgent problem to provide stable composition service which can satisfy the personalized requirements for large scale users. This paper takes several aspects of web service into consideration, including Quality of Service (QoS), user preference and the service relationships and proposes a method based viterbi algorithm to reason out the global optimal solution of web composition service. Result shows our method holds executive efficiency, stability as well as outstanding selecting result."
890,"WS are distributed software components that can be exposed and invoked over the Internet using standard protocols. They communicate with their clients and with other WS by sending XML based messages over the Internet. AI planning techniques can help solving the composition of WS problem. In fact, services can be modelled as actions and the business process as planning to connect the WS. The main contribution of this paper is the extension of the model of actions to handle the creation or elimination of objects as effects of actions. This contribution allows us to answer to new and more expressive requests, called implicit requests, in which goals may contain objects that have been generated by the plan."
891,"WS-Security is an essential component of the Web services protocol stack. WS-Security provides end-to-end security properties (integrity, confidentiality, and authentication) through open XML standards. End-to-end message security assures the participation of non-secure transport intermediaries in message exchanges, which is a key advantage for Web-based systems and service-oriented architectures. However, point-to-point message security based on TLS (transport layer security) is known to significantly outperform WS-Security. In this paper we analyze the overhead of the WS-Security protocol processing stages and evaluate existing and new techniques for WS-Security signature performance optimizations to speed up end-to-end message integrity assurance and authentication."
892,"Maintaining data availability is one of the biggest challenges in Decentralized Online Social Networks (DOSN). In the existing work of improving data availability in DOSN, it is often assumed that the friends of a user are always capable of contributing sufficient storage capacity to store all the data published by the user. However, this assumption is not always true for today's Online Social Networks (OSNs) for the following reasons. On one hand, the increasingly more data are being generated on the OSNs nowadays. On the other hand, current users often use the smart mobile devices to access the OSNs. These two factors cause the shortage of the storage capacity in DOSN, where the published data are supposed to be stored within a friend circle. The limitation of the storage capacity may jeopardize the data availability. Therefore, it is desired to know the relation between the storage capacity contributed by the OSN users and the level of data availability that the OSN can achieve. This paper addresses this issue. In this paper, the data availability model over storage capacity is established. Further, a novel method is proposed to predict the data availability on the fly. Extensive simulation experiments have been conducted to evaluate the effectiveness of the data availability model and the on-the-fly prediction. The data availability model can be used by the OSN designers to determine the storage capacity for the published data in order to achieve the desired data availability. The on-the-fly prediction method can help the data replication and storage policies make judicious decisions at runtime."
893,"In real life, a tester can only afford to apply one test case prioritization technique to one test suite against a service-oriented workflow application once in the regression testing of the application, even if it results in an adverse scenario such that the actual performance in the test session is far below the average. It is unclear whether the factors of test case prioritization techniques known to be significant in terms of average performance can be extrapolated to adverse scenarios. In this paper, we examine whether such a factor or technique may consistently affect the rate of fault detection in both the average and adverse scenarios. The factors studied include prioritization strategy, artifacts to provide coverage data, ordering direction of a strategy, and the use of executable and non-executable artifacts. The results show that only a minor portion of the 10 studied techniques, most of which are based on the iterative strategy, are consistently effective in both average and adverse scenarios. To the best of our knowledge, this paper presents the first piece of empirical evidence regarding the consistency in the effectiveness of test case prioritization techniques and factors of service-oriented workflow applications between average and adverse scenarios."
894,"Location-sensitive information monitoring services are a centerpiece of the technology for disseminating content-rich information from massive data streams to mobile users. The key challenges for such monitoring services are characterized by the combination of spatial and non-spatial attributes being monitored and the wide spectrum of update rates. A typical example of such services is ""alert me when the gas price at a gas station within 5 miles of my current location drops to 4 per gallon"". Such a service needs to monitor the gas price changes in conjunction with the highly dynamic nature of location information. Scalability of such location sensitive and content rich information monitoring services in the presence of different update rates and monitoring thresholds poses a big technical challenge. In this paper, we present SLIM, a scalable location sensitive information monitoring service framework with two unique features. First, we make intelligent use of the correlation between spatial and non-spatial attributes involved in the information monitoring service requests to devise a highly scalable distributed spatial trigger evaluation engine. Second, we introduce single and multi-dimensional safe value containment techniques to efficiently perform selective distributed processing of spatial triggers to reduce the amount of unnecessary trigger evaluations. Through extensive experiments, we show that SLIM offers high scalability for location-sensitive, content-rich information monitoring services in terms of the number of information sources being monitored, number of users and monitoring requests."
895,"For Web-services to become practical, an infrastructure needs to be supported that allows users and applications to discover, deploy, compose, and synthesize services automatically. This automation can take place only if a formal description of the Web-services is available. In this paper we present an infrastructure using USDL (universal service-semantics description language), a language for formally describing the semantics of Web-services. USDL is based on the Web Ontology Language (OWL) and employs WordNet as a common basis for understanding the meaning of services. USDL can be regarded as formal service documentation that will allow sophisticated conceptual modeling and searching of available Web-services, automated service composition, and other forms of automated service integration. A theory of safe service substitution for USDL is presented and proved sound and complete. The rationale behind the design of USDL along with its formal specification in OWL is presented with examples. We also compare USDL with other approaches like OWL-S and WSML and show that USDL is complementary to these approaches."
896,Presents the welcome message from the conference proceedings.
897,"At present, a growing number of web applications especially cloud computing systems employ representational state transfer (REST) API as the interface to expose their services for simplicity and clarity. For security purposes, service providers prefer to control the access to the provided interface based on the principle of least privilege. However, how to divide the administrative privileges remains a difficulty in practice. In this work, we simplify the privilege partitioning problem into a classification problem of RESTful functions, so the permission to call a category of functions can be granted to a specific administrator. We propose a RESTful API classification approach called RestSep based on genetic algorithm. A classification is represented as a 2-dimensional matrix, which is used as the chromosome. Customized operators of selection, mutation and crossover are designed. The fitness function is designed to balance parameters such as number of categories, test case coverage, function overlapping, etc. Experiments on popular clouds like OpenStack and Kubernetes indicate RestSep can generate a self-explanatory classification result, which can serve as a guideline for privilege partitioning. The overhead of test generation is at most 13.1% and the overhead of genetic algorithm is at most 183.29s, which are acceptable for practical use."
898,"In the provision of dynamic data-intensive services, the cost and response time of data sets as well as the states of services may change over time. An ant colony system for this problem is studied in this paper. Specifically, we consider changing the QoS attributes of services and replacing a certain number of services with new ones at different frequencies. In order to adapt the ant colony system to handle the dynamic scenarios, several pheromone modification strategies in reaction to changes of the optimization scenarios are investigated. The aim of the strategies is to find a balance between preserving enough old pheromone information to speed up the search process, and resetting enough new pheromone information to facilitate the ants to find a new solution for the changed scenarios. The strategies differ in their degree of reinitialized pheromone values with respect to the information that has been used to decide the amount of pheromone values. Moreover, the behaviors of different strategies for modifying pheromone information are compared."
900,"The automatic extraction of metadata and other information from scholarly documents is a common task in academic digital libraries, search engines, and document management systems to allow for the management and categorization of documents and for search to take place. A Web-accessible API can simplify this extraction by providing a single point of operation for extraction that can be incorporated into multiple document workflows without the need for each workflow to implement and support its own extraction functionality. In this paper, we describe CiteSeerExtractor, a RESTful API for scholarly information extraction that exploits the fact that there is duplication in scholarly big data and makes use of a near duplicate matching backend. The backend stores previously extracted metadata and avoids extracting metadata from a document if it has already been extracted before. We describe the design, implementation, and functionality of CiteSeerExtractor and show how the duplicate document matching results in a difference of 8.46% in the time required to extract header and citation information from approximately 3.5 million documents compared to a baseline."
901,"Organizations are nowadays more and more adopting Cloud computing to execute their multi-tenant business processes in order to quickly adapt to changes of requirements at lower costs. In such environment, using configurable process models allows for various tenants to share a reference process which can be customized depending on their needs. However, there is a lack of support for cloud-specific resource configuration with considering quality of service requirements. In this paper, we cope with this gap by proposing a genetic-based approach that enables to optimally extract cloud resource configuration allocation w.r.t Cloud resource features (i.e., elasticity and shareability), and process non functional properties (i.e. QoS properties)."
902,"The increasing usage of smart embedded devices is blurring the line between the virtual and real worlds. This creates new opportunities for applications to better integrate the real-world, providing services that are more diverse, highly dynamic and efficient. Service Oriented Architecture is on the verge of extending its applicability from the standard, corporate IT domain to the real-world devices. In such infrastructures, composed of a large number of resource-limited devices, the discovery of services and on demand provisioning of missing functionality is a challenge. This work proposes a process, its architecture and an implementation that enables developers and process designers to dynamically discover, use, and create running instances of real-world services in composite applications."
903,"Web service models are increasingly being used in the Grid community as way to create distributed applications exposing data and/or applications through self describing interfaces. Scientific research is one key field in which the benefits are apparent as individual services can be orchestrated into experimental workflows that model the research process and facilitate verification and extension. However, many applications are not web enabled and the task of creating services from scratch is cumbersome in part due to the range of complex technologies, tools, standards and languages involved. In this paper we present gRAVI, a WSRF Web service wrapping tool that allows scientists to rapidly expose applications, scripts and workflows as Web services. gRAVI generated services include GSI security, Grid scheduling, state notifications, persistence and data staging. All service code, scripts and definition files are created automatically without any developer input. gRAVI services are created in standard Grid Archive files and are able to be moved and deployed to any compliant container with no requirement for any gRAVI or Grid infrastructure on the target machine. gRAVI supports deployment to the open science cloud Nimbus, whilst also being able to parse Taverna workflow definition files to create strongly typed services."
904,This paper presents a middleware that supports reliable Web services built on active replication. The middleware is responsible for maintaining the consistency of the replicas' states. A Java package for handling the interactions with the middleware and the failures of the Web services is provided for programmers to use when writing client applications. The package reduces the complexity in developing client applications.
905,"In this paper we describe JClarens; a Java based implementation of the Clarens remote data server. JClarens provides Web services for an interactive analysis environment to dynamically access and analyze the tremendous amount of data scattered across various locations. Additionally this research is aimed to develop a service oriented grid enabled portal (GEP) that provides interface and access to several grid services to give a homogeneous and optimized view of the distributed and heterogeneous environment. Other than showing platform independent behavior provided by Java, the use of XML-RPC based Web services enabled JClarens to be a language neutral server and demonstrated interoperability with its Python variant. Extreme care has been taken in the usage and manipulation of various Java libraries to cater the needs of high performance computing. The overall exercise has yielded in a prototype with strong emphasis on security and virtual organization management (VOM). This shall provide a common platform to support development of larger, more flexible framework with future aims to integrate it with a loosely coupled, decentralized, and autonomous framework for grid enabled analysis environment (GAE)."
906,"In the Web there are a large number of (business) services with complex behavior, such as e-commerce Web sites that require multiple interactions with the user, as well as an increasing number of Web automation scripts to coordinate the execution of multiple complex services. However, while there are a quite a few search techniques for atomic Web services, search techniques for complex services are still rare and only foundational. In this paper, we present \textit{behavior classes} that have formal semantics as well as human comprehensible names in order to foster usability of specification of constraints, and efficiency of search for complex services and processes. Our approach enables automatic methods for (i) assigning behavior classes to complex behavior descriptions, (ii) checking consistency of such a classification, and (iii) computing behavior class hierarchies. Furthermore, human comprehensible names for the behavior classes increase usability by allowing for shorter service descriptions and requests. Our evaluation results show that a behavior class hierarchy can be exploited as an indexing structure to gain performance of search."
907,"A core paradigm of the Web is information exchange via persistent publication, i.e., one party publishes a piece of information on the Web, and any other party who knows the location of the resource can retrieve and process the information at any later point in time and without the need for synchronization with the original publisher. This functionality significantly contributed to the scalability of the Web, since it reduced the amount of interaction between the sender and the recipient. Current approaches of extending the World Wide Web from a collection of human-readable information, connecting humans, into a network that connects computing devices based on machine-processable semantics of data lack this feature and are instead based on tightly-coupled message exchange. In this paper, we (1) show that Web services based on the message-exchange paradigm are not fully compliant with core paradigms of the Web itself, (2) outline how the idea of persistent publication as a communication paradigm can be beneficially applied to Web services, and (3) propose a minimal architecture for fully Web-enabled semantic Web services based on publication in shared information spaces, which we call triple space computing."
908,"Use cases are a key technique to elicit software requirements from the point of view of the user of a system. Their prevalence is noticeable ever since the onset of agile programming techniques. Within SOA projects however, business process models are used for capability analysis and gap detection. Business process models present a global view of the system and hence are more suited for gap detection. Therefore, in practice both these forms of requirements continue to be useful and coexist. Often in big software projects and in distributed development environment such coexisting requirement specifications can grow out of synch. We present here a technique to semi-automatically transform use cases into business processes and to create mapping between them. By preserving the mapping between these forms one can enforce consistency between the two forms of requirements."
909,"Social media have been used in the detection and management of natural hazards such as earthquakes. However, disasters often lead to other kinds of disasters, forming multi-hazards. Landslide is an illustrative example of a multi-hazard, which may be caused by earthquakes, rainfalls, water erosion, among other reasons. Detecting such multi-hazards is a significant challenge, since physical sensors designed for specific disasters are insufficient for multi-hazards. We describe LITMUS -- a landslide detection service based on a multi-service composition approach that combines data from both physical and social information services by filtering and then joining the information flow from those services based on their spatiotemporal features. Our results show that with such approach LITMUS detects 25 out of 27 landslides reported by USGS in December and 40 more landslides unreported by USGS. Also, LITMUS provides a live demonstration that displays results on a web map."
910,"Web applications have played an important role in mobile computing since they are developed by standard web technology, easy to achieve portability and always up to date. As a widely adopted development fashion, mashup integrates data, computation and UI elements from multiple web-delivered services into a single web application, which is a promising way to develop applications for mobile devices. However, developing mobile web applications by mashup has faced two challenges: on one hand, the growing popularity of web-delivered services, especially those equipped with mobile features like LBS, increases the difficulty of finding proper services and connections between services to create situational applications, on the other hand, mobile computing yields for agile development and fast iteration to meet users everchanging requirements, imposing more burden on developers. This paper presents a graph-based approach to helping mobile developers create mobile web applications. Based on this graph model, algorithms by synthesizing different recommendation patterns are designed to assist mashup completion, such as finding the missing components, connections between them, or potentially relevant options."
911,"Asynchronous invocation and continuation are common patterns in some middleware infrastructures for object-based distributed computing. Their benefits are particularly significant in distributed environments characterized by high communication latencies and coarse-grained operations. Therefore, Web services could strongly benefit from the adoption of these patterns to (1) overlap communication with computation, (2) reduce the high number of interactions typically needed to handle stateless services by migrating the state of a service as parameters of service operations, (3) intercept at run-time data dependencies among consecutive services in a composition not visible from service descriptions. Unfortunately, current semantics of Web services do not directly support the patterns, but some specifications (i.e. WS-addressing) can simplify their implementation. In the paper we present the patterns, their benefits, and a module that implements a flexible schema useful to perform asynchronous invocations in several contexts. This way, modelling composed services can benefit from abstractions whilst more sophisticated low-level interactions among services are automatically handled at run-time."
912,"Web application hybrids, popularly known as mashups, are created by integrating services on the Web using their APIs. Support for finding an API is currently provided by generic search engines or domain specific solutions such as Google and ProgrammableWeb. Shortcomings of both these solutions in terms of and reliance on user tags make the task of identifying an API challenging. Since these APIs are described in HTML documents, it is essential to look beyond the boundaries of current approaches to Web service discovery that rely on formal descriptions. In this work, we present a faceted approach to searching and ranking Web APIs that takes into consideration attributes or facets of the APIs as found in their HTML descriptions. Our method adopts current research in document classification and faceted search and introduces the serviut score to rank APIs based on their utilization and popularity. We evaluate classification, search accuracy and ranking effectiveness using available APIs while contrasting our solution with existing ones."
913,"Quality of individual services is substantial to guarantee high performance and availability of overall composite service oriented systems. This requires the process of diagnosing service quality degradation and responding to it in a timely and non-stopping manner. This paper presents a modeling-based approach to coordinate the process of Web service management, configuring parameters or invoking other tasks. This approach uses executable process models to represent diagnosis logic and orchestrate the replacement. The process models can interact with the system and accept administrators' instruction at the process level. As a result, a service can be automatically replaced by the best available strategy without any downtime of the overall system. Our approach is implemented on an architecture of Enterprise Service Bus (ESB) that allows intercepting services and redirecting messages with little performance penalty. The experiment demonstrates the efficiency of this approach using a loan-brokering Web service on ESB."
914,"In Big Data era, continuous data with low latency and high throughput makes high-availability essential for stream computing. Traditional availability guarantee is tightly-coupled and inefficient for customization and reuse. In this paper, a framework is proposed to improve the availability of stream computing, in which basic functions are provided as general services like reliable point-to-point communication and distributed status management. With its help, high-level patterns can be achieved effectively. Comprehensive experiments have been designed and evaluated to show the availability improvement with acceptable extra overheads."
915,"For service-based applications which are composed of multiple independent third-parties, continuous monitoring is required to assure that runtime behavior of the systems complies with specified properties. However, most existing work only detects the violation while not consider how to enforce the properties so that the constraint can not be violated at runtime. To address this limitation, this paper presents EnforceBCL, a framework for enforcing data-centric properties for concurrent service-based applications. Users of EnforceBCL can specify the properties to be enforced using the expressive behavior constraint enforcement language. Data-centric property is enforced at runtime by blocking the process whose next action would violate it. The impacted processes can be unblocked and allowed to execute when the specified property eventually reaches a safe state. EnforceBCL also provides the mechanism to detect possible deadlock during the enforcement of the property, and executes corresponding handler to solve the deadlock. To evaluate the effectiveness and efficiency of the proposed approach, we conducted several experiments. Results show that EnforceBCL is able to effectively enforce data-centric properties for concurrent service-based applications and also incurs less performance overhead."
916,"Web APIs provide interfaces for interaction among systems based on the existing infrastructure for hosting Web sites and applications. The REST architectural style is the most employed approach for building Web APIs. However, the flexibility provided by REST may result in implementations with low quality design, limited reuse and poor documentation. This paper describes a maturity model for classifying Web APIs, aimed at promoting the adherence to REST architectural principles and the adoption of semantic Web technology in order to improve the design, reuse and documentation of Web APIs."
917,"Online Social Networks (OSNs) have provided an infrastructure for a number of emerging applications in recent years, e.g., for the recommendation of service providers, where trust is one of the most important factors for the decision-making of service consumers. In order to evaluate the trustworthiness of a service provider (i.e., the target) without any prior interaction with a service consumer (i.e., the source), the trust network from the source to the target need to be extracted firstly before performing any trust evaluation, as it contains some important intermediate participants, the trust relations between the participants, and the social context, each of which has an important influence on trust evaluation. However, the network extraction has been proved to be NP-Complete. Towards solving this challenging problem, we first propose a complex contextual social network structure which considers some social contexts, having significant influences on both social interactions and trust evaluation between participants. Then, we propose a new concept called QoTN (Quality of Trust Network) and a social context-aware trust network discovery model. Finally, we propose a Heuristic Social Context-Aware trust Network discovery algorithm (H-SCAN) by adopting the K-Best-First Search (KBFS) method and our optimization strategies. The experimental results illustrate that our proposed model and algorithm outperform the existing methods in both algorithm efficiency and the quality of the extracted trust networks."
918,"CAPTCHA challenges are used all over the Internet to prevent automated scripts from spamming web services. However, recent technological developments have rendered the conventional CAPTCHA insecure and inconvenient to use. In this paper, we propose vCAPTCHA, a voice-based CAPTCHA system that would: (1) enable more secure human authentication, (2) more conveniently integrate with modern devices accessing web services, and (3) help collect vast amounts of annotated speech data for different languages, accents, and dialects that are under-represented in the current speech corpora, thus making speech technologies accessible to more people around the world. vCAPTCHA requires users to speak their responses, in order to unlock or use different web services, instead of typing them. These user responses are analyzed to determine if they were indeed naturally produced, and transcribed to ensure that they contain the challenge sentence. We build a prototype for vCAPTCHA in order to assess its performance and practicality. Our preliminary results show that we are able to achieve an attack success rate as low as 2.3% while maintaining a human success rate comparable to current CAPTCHAs, on ASVspoof datasets."
919,"Although universal description, discovery and integration (UDDI) is the de jure Web service registry standard, it is not suitable for handling semantic markups due to its flat data model and limited search capabilities. In this paper, we introduce an approach to support semantic service descriptions and queries using registries that conform to the UDDI version 3 specification. Specifically, we present a scheme that allows users to store OWL-S service descriptions in the UDDI data model and use that information to perform semantic query processing. Our approach does not require any modification to the existing UDDI registries. The add-on modules only reside on the client-side machines that wish to take advantage of the semantic capabilities. This approach is completely backward compatible and can integrate seamlessly into the existing service-oriented architecture (SOA) infrastructure"
920,"In this paper we describe a platform that supports context aware runtime service discovery. The platform supports service discovery based on structural and behavioural service models as well as complex context related service discovery conditions which are specified in a newly introduced query language. During discovery, context information is obtained through a uniform scheme of calling ""context operations"" and is subsequently used in the evaluation of service discovery queries."
921,"Global IT services and consulting organizations like Tata Consultancy Services (TCS) have been investing considerable resource into the emerging discipline of services computing. The address focuses on the business drivers for services computing across multiple industries across the world and why this could be the 'silver bullet' for many of our challenges. The address uses the TCS framework for services computing as a backdrop - across services strategy, services implementation and most importantly services governance. This is a progressive approach towards bridging the gap between 'what IT delivers' and 'what the business wants' through a well defined 'value measurement framework'. This framework links organizational readiness, solution maturity and delivery assurance. Finally, the importance of aligning the competencies and roles across different parts of the organization and its stakeholders is examined"
922,"Summary form only given. A business process is a collection of related structures and activities, undertaken by organizations in order to achieve certain business goals. The Web services-based business processes with a new set of protocols bring a new set of security challenges. As security has become an essential component for all software, several security solutions for XML and Web services have been proposed. In general, a security threat model is an organized representation of relevant threats, attacks, and vulnerabilities to a system. In this context, security threat modeling is an engineering technique which can be used to shape the Web service-based business processes with security requirements. The topic of security threat modeling in business process is becoming increasingly important to industry. This tutorial strives to reflect recent trends in research and developments of business processes integration and management with security concerns. In addition this tutorial will cover the fundamental concepts of security threat modeling from the perspectives of Web service-based business process. This tutorial will also address the common practices and related tools/procedures for addressing the security vulnerabilities, especially in XML attacks. A research prototype of security assessment will also be presented and demonstrated in the tutorial."
924,"Recently, most enterprises adapt Web services technologies for designing and building complex inter-enterprise business applications. These applications are built by the coordination between set of Web services. Therefore, checking the compatibility between two services to guarantee that they can interact correctly is an important issue. In case of service update or replacement, there is a need for checking the replaceability to ensure that the new service is compatible with all the services which were compatible with the replaced one. These two type of checking are based on the services descriptions. Enriching services descriptions by including their behaviours is becoming more and more important. This behaviour can be described by business protocols representing the possible sequences of message exchanges. Since a lot of Web services use access control policies to restrict the access to authorized consumers, these policies should be a part of the service description. Checking compatibility and replaceability between Web services by analyzing their business protocols after assigning the access control policies is the main contribution of this work. Access control policies will be presented using ontology."
925,"Service-oriented computing (SOC) enables organizations and individual users to discover openly-accessible capabilities realized as services over the Internet. Research in this area focuses on techniques for managing the messages that flow into and out of these services to ultimately compose higher-level functions. In our work, we investigate the nature of message definitions by analyzing real, fully-operational Web services currently available on the Internet (i.e., from the wild). By leveraging insights into how real Web service messages are defined, we develop enhanced syntactical methods to best aggregate these messages and ultimately the Web services"
926,"In environments with limited network bandwidth or resource-constrained computing devices the high amount of protocol overhead caused by SOAP is disadvantageous. Therefore, recent research work concentrated on more compact, binary representations of XML data. However, due to the special characteristics of SOAP communication most of these approaches are not applicable in the field of Web services. First, we give a detailed overview of the latest developments in the field of XML data compression. Then we will introduce a new approach for compressing SOAP data which utilizes information on the structure of the data from an XML schema or WSDL document to generate a single custom pushdown automaton. This cannot only be used as a highly efficient validating parser but also as a compressor: its transitions are tagged with short binary identifiers which replace XML tags during compression. This approach leads to extremely compact data representations as well as low memory and CPU utilization"
927,"We propose, in this paper, to consider the problem of Web service antipatterns detection as a multi-objective problem where examples of Web service antipatterns and well-designed code are used to generate detection rules. To this end, we use multi-objective genetic programming (MOGP) to find the best combination of metrics that maximizes the detection of Web service antipattern examples and minimizes the detection of well-designed Web service design examples. We report the results of an empirical study using 8 different types of common Web service antipatterns. We compared our multi-objective formulation with random search, one existing mono-objective approach, and one state-of-the-art detection technique not based on heuristic search. Statistical analysis of the obtained results demonstrates that our approach is efficient in antipattern detection, on average, with a precision score of 94% and a recall score of 92%."
928,"Spurred by Web 2.0 paradigm, there emerge large numbers of service mashups by composing readily accessible data and services. Mashups usually address solving situational problems and require quick and iterative development lifecyle. In this paper, we propose an approach to composing data driven mashups, based on tag-based semantics. The core principle is deriving semantic annotations from popular tags, and associating them with programmatic inputs and outputs data. Tag-based semantics promise a quick and simple comprehension of data capabilities. Mashup developers including end-users can intuitively search desired services with tags, and combine several services by means of data flows. Our approach takes a planning technique to retrieving the potentially relevant composition opportunities. With our graphical composition user interfaces, developers can iteratively modify, adjust and refine their mashups to be more satisfying."
929,"This paper presents a REST service composition framework based on functional programming with monads. It is motivated by the acute needs for dynamism and compos ability in concurrent control of large number of REST resources, especially in the areas of Cloud computing, software-defined networking (SDN), and Internet-of-Things/Web-of-Things. Dynamism gives us the ability to write a REST service composition program to invoke REST services whose identifications and operations are unknown at the design time, and compos ability gives us the ability to combine existing programs for different purposes. The key idea of our approach is to treat the primitives of a composition program, including resource identifications, operations, and control structures, as unknown functions and monads that can be dynamically composed from known functions and monads. We present the algebraic operators and rules that govern the compositions and its translation to XML, to combine the modularity and flexibility of functional programming with the portability and extensibility of XML. A prototype composition engine has been implemented and used to compose REST services in the Cloud computing domain. The initial experimental results indicate that the approach is feasible and promising."
930,"Formal concept analysis is a mathematics research field introduced in the beginning of the 1980s by Rudolf Wille, that has been applied in several different knowledge areas, including Computer Science. FCA is a data analysis theory that identifies conceptual structures within data sets or formal contexts. In this work, we propose an FCA-based approach to build minimal implication rules-based computational models for social networks. As an application example, in this work we constructed canonical models using data extracted from user sessions in one of the most popular social networks in Brazil, Orkut. These models represent the patterns of access to Orkut, about a certain problem domain, and are composed by a minimal rule set."
931,"With the rapid development of services and software, how to share, integrate and discover them properly in open and dynamic network environment is one of the most important challenges for software technology. With monad techniques, we present a novel formal semantic model for service oriented computing in a black-box observation way. The monad-based model can help us formally describe and further study on software components and services through monads' properties such as abstraction, reflection and composability. This model relatively improves service reuse and discovery, and it significantly facilitates web service composition and enables integration of legacy applications."
932,"Enterprises value monitoring as it provides dependable e-services, whether it is an interactive Web site or programmatic Web service. However, this task becomes non-trivial when enterprises begin to require support from thousands of servers across geographical areas. How can the communications between a monitoring systems and remote servers be minimized? Could the task be achieved easily based on a readily available technology such as SNMP? How can we monitor thousands of servers powered by, say, Tomcat, which lacks SNMP support? How may a poorly responsive site be identified prior to being reported as a failure by SNMP? In this paper, we propose a unified e-monitoring system that enables system administrators to remotely monitor the health of distributed e-services in both the form of Web site and Web services. We further discuss our implementation experience based on a pragmatic prototype."
933,"In this paper, we propose an event-based framework that allows to specify and reason about the monitoring properties during composition process execution. The proposed approach is highly expressive and allows to specify monitoring properties that can be based on either functional or non-functional requirements, allows multi-level detection of any violation, allows to calculate effects of any such violation on the overall process execution and to recover from it using a set of recovery actions. The choice of a reasoning based approach allows to foresee the effects of violations and respects any functional and non-functional constraints associated with the process, when performing recovery. In addition, as the approach builds upon an event-based declarative framework called DISC, it results in an integrated approach as both composition design and monitoring framework are event-based."
934,"Internet of Things (IoT) interconnects billions of smart sensors, devices, actuators, as well as people, over a distributed environment to work together towards a better and smarter physical world. However, technologies are still under development to allow us to most effectively make use of the ubiquitous ""smart things"" to accomplish the desired goals in various human activities. Service discovery is one of these technologies that can help identify the desired IoT devices for the given task. In this paper, we design an efficient peer-to-peer (p2p) service discovery algorithm in an IoT network with a mix of static and mobile IoT nodes. Our algorithm is based on unstructured p2p routing with routing information caching (RIC). Due to the limited memory on IoT nodes, we bound the cache size to fit the IoT nodes and maximize the usefulness of their RIC content. First, we build an ontology of IoT device capabilities and design a hierarchical Bloom-filter based Indexing (HBFI) to index the capabilities in the ontology. HBFI is flexible in incorporating new capabilities and offers much more space efficient representation of the routing information. Also, the cache replacement scheme of RIC considers the reference time of the entries (as conventional), the mobility of the IoT nodes, and the inclusiveness of the capability concept. Experimental evaluations show that with the same memory size constraint, our scheme can provide more effective routing information and minimize the service discovery time compared to centralized, supernode based, DHT based, flooding based, and other information caching based solutions."
935,"In a business process execution language (BPEL) process definition the sequence of exchanged messages typically originates from the sequence of business process activities and from the need of coordination of those activities across the participants of the process. As such business concerns (e.g. the sequence of business process steps) are of en mixed with technical aspects (e.g. the sequence of coordination messages). In this article we present an architecture to separate business and technical concerns, which results in a clearer overview of the high-level business process and improves the flexibility and maintainability of the orchestration architecture. The described architecture depends on existing Web service standards. Different eventing and coordination specifications are discussed. The ultimate architecture is mainly based on the WS-Brokered Notification and WS-Coordination framework specifications."
936,"While a streaming-XPath engine provides a more efficient XPath evaluation performance than a DOM based engine, it still requires time to parse a whole XML document at least once. Taking into consideration that XPath is used to extract data from Web service messages, the target element may be in the first part of documents, such as the SOAP headers. Therefore, it is wasteful to parse a document all the way to the end. We present a new XPath engine named ""XScope"", which eliminates parsing XML documents by schema information. In this paper, we describe the XScope approach, then evaluate its performance and show that the engine works effectively in Web services."
937,Service Level Agreements (SLAs) are used to specify the negotiated conditions between the provider and the consumer of services. In this paper we present a stepwise method to identify and categorize a set of test requirements that represent the potential situations that can be exercised regarding the specification of each isolated guarantee term of an SLA. This identification is addressed by means of devising a set of coverage levels that allow grading the thoroughness of the tests. The utilization of these test requirements would focus on twofold objectives: (1) the generation of a test suite that allows exercising the situations described in the test requirements and (2) the support for the derivation of a monitoring plan that checks the compliance of these requirements at runtime. The approach is illustrated over an eHealth case study.
938,"Web service is an Internet-based software component that can shield all sorts of resources on basis of standard protocol stack. It helps to raise the level of abstraction and simplify conventional COTS middleware for resource sharing and cooperation across organization. In this paper, a Web service container framework (WSCF) is presented to offer an effective systematic solution for Web service-based application supporting environment, referencing from CORBA and J2EE system managing architecture and using SOAP interoperation protocol. The proposed framework focuses on addressing problems in two aspects: 1) Web service runtime supporting technologies: unified resource mapping strategy, flexible service adaptation mechanism, and SOAP engine scheduling algorithm; 2) several application supporting services: publication and discovery service, Web service composition service, and security service, etc. In particular, it describes ongoing project StarWebService that has followed WSCF. Finally, some open issues about WSCF are introduced and the future direction of WSCF is pointed out."
939,"To solve the difficulties of the service operators on how to manage services and achieve a continuable operation goal, we propose and develop the 4th party service platform. We also develop a rule-based customizable charge service management component for the platform. The website www.foode.cn is the demonstration. The charging service is used to charge for the 3rd services such as search services, electronic authentication services, online communication services, online payment services and so on. The results show that the 4th party service platform can greatly shorten the construction time of a business application, reduce construction and operating costs, and enable service providers to share in the financial rewards properly."
940,"Mashup is becoming a powerful approach for end-users to meet their ad-hoc requirements based on existing services. Quite a few researches have been performed to achieve rapid, on-demand, intuitive development of mashups, which mainly focus on finding suitable quality components from a large number of available services. However, for mashups with procedure and context features, it is more crucial and difficult to construct an effective mashup structure, rather than selecting individual components. In this paper, we propose a context-based autonomous construction approach for procedural mashup based on pattern mining. In our approach, the mashup composition process is divided into 2 phases: schema construction phase and component binding phase. First, context-based mashup schemas with probability are extracted and recovered by applying pattern mining tasks to historical mashup logs. Then, according to user goal and awareness of user context, an optimal mashup schema is composed progressively by top-k recommendations for the next behavior/activity, which will be grounded to Web-API based components later. The proposed approach can autonomously generate context-based mashup schema with quality components and high probability of success without dependence on user professionalism."
941,"Composite business applications typically combine characteristics of workflow and transactional applications. In order to handle multiple concurrent sessions and conversations across different nodes and over an extended time period, these applications pass dedicated identifiers inside their messages such as ""OrderNumber"". Debugging and understanding these conversations and the flow of conversation identifiers can be challenging in realistic business applications. We present a new heuristic algorithm to find these conversation identifiers and a method to reverse-engineer the conversations from the content of traced messages. Visualizing the results of this analysis provides a deeper understanding of the data flow, the business process and the interactions between business partners in composite business applications."
942,"Semantic annotations for WSDL (SAWSDL) is a recently adopted W3C recommendation that provides a mechanism by which WSDL documents can reference external, domain-specific semantic models in order to provide concept-level interoperability of Web Services. Moby is an established protocol for providing semantic Web Services developed by the bioinformatics community: we use Moby to provide a grounding for a SAWSDL implementation in bioinformatics. Our software (Daggoo) allows users to create Moby-compliant semantic Web Services by simply adding SAWSDL markup to existing WSDL files. These new services are compatible with existing Moby services and client software. The Java software we present consists of a proxy servlet, a URI-resolution mechanism, and rule systems for converting back and forth between Moby and XML Schema data formats. As an early implementation of SAWSDL, Daggoo reveals shortcomings in the notation, and several additional technologies needed to achieve real-world semantic interoperability of WSDL-based services. Based on our experience, we suggest how to improve the semantic annotation mechanism, and how to reduce the programming burden for individual service providers. Furthermore, we demonstrate the importance of a semantically-enabled registry for services and data types in facilitating scientist-driven, rather than programmer-driven, Web service choreography."
943,"The data exchanged among the Web services participating to a composition are clearly very relevant for a correct behavior of the composition. Nevertheless, most of the approaches existing in the literature for the static verification of Web service compositions ignore data, or require very small ranges to be associated to the data types. In this paper, we propose an approach for the verification of Web service compositions that takes into account the dataflows among the component process. The approach exploits abstraction techniques for modeling those aspects of data that are relevant for the correctness of the composition and hiding the aspects that are irrelevant. We show that building the right abstraction corresponds to defining those assumptions on the data manipulations performed by the component services which are crucial for the correctness of the composition"
944,"Much has been written on the promise of Web service discovery and (semi-) automated composition. In this discussion, the value to practitioners of discovering and reusing existing service compositions, captured in workflows, is mostly ignored. This paper presents one solution to workflow discovery. Through a survey with 21 scientists and developers from the myGrid workflow environment, workflow discovery requirements are elicited. Through a user experiment with 13 scientists, an attempt is made to build a gold standard for workflow ranking. Through the design and implementation of a workflow discovery tool, a mechanism for ranking workflow fragments is provided based on graph sub-isomorphism matching. The tool evaluation, drawing on a corpus of 89 public workflows from bioinformatics and the results of the user experiment, finds that the average human ranking can largely be reproduced"
945,"Mashup is a Web technology that combines information from more than one source into a single Web application.This technique provides a new platform for different data providers to flexibly integrate their expertise and deliver highly customizable services to their customers. None the-less, combining data from different sources could potentially reveal person-specific sensitive information. In this paper, we study and resolve a real-life privacy problem in a data mashup application for the financial industry in Sweden. Therefore we propose a service-oriented architecture for privacy-preserving data mashup together with a multi-party protocol to securely integrate private data from different data providers, whereas the integrated data still retains the essential information for supporting general data exploration or a specific data mining task, such as classification analysis. Experiments on real-life data suggest that our proposed method is effective for simultaneously preserving both privacy and information usefulness."
946,"Web Services providing access to datasources with structured data have an important place in the SOA. In this paper we focus on modeling and discovery of generic data providing services (DPS), with the goal of making data providing services available for interactions with service requesters in contexts such as service composition and mediation. In our model RDF Views are used to represent the content provided by the DPS. A characterization of match between description of DPS as RDF Views and the OWL-S service request is specified, based on which we developed a flexible matchmaking algorithm for discovery of data providing services. Finally, we propose a realization of the DPS using a SOAP version of the SPARQL protocol and a dynamic configuration interface allowing easy interactions of service requesters with data providing services."
947,"Web services composition environment is highly dynamic with new services being deployed, existing ones becoming unavailable or their QoS and other non-functional properties (cost, availability, etc.) changing dynamically. However, current approaches for web services composition and execution, such as WS-BPEL, can neither tailor the execution automatically as per the required QoS nor can they adapt to the highly dynamic environment. Though there have been some recent efforts in this direction they are piecemeal and insufficient. They either do not take user's non-functional requirements (NFRs) into consideration for adaptation, or there is no standard way of specifying these requirements for a BPEL process. In this paper, we propose an integrated approach for dynamically adapting web service compositions based on NFRs. We first give a specification for representing NFRs for each partner service of a BPEL process, and then describe a system that dynamically adapts the BPEL process based on these requirements by selecting suitable services at runtime. The selected services only need to be semantically equivalent and the system automatically takes care of the syntactical differences between their interfaces. We integrated our system in an existing BPEL engine using aspect oriented approach and demonstrate via experiments that it has very little performance overhead even at high loads."
948,"We propose a service mining framework for exploring interesting compositions of existing Web services. The framework first screens Web services for composition leads using a ""coarse-grained"" filtering approach. It then verifies these leads based on runtime conditions. Top candidates are selected from the verified leads and evaluated for their interestingness. We present algorithms to automate the screening phase of the framework. Finally, we study the effects of key variables on lead compositions' interestingness. As a motivating example, we apply these algorithms to the field of biological pathway discovery and rely on knowledge obtained from reverse engineering online resources to assess their effectiveness."
949,"Dynamic Web service selection refers to determining a subset of component Web services to be invoked so as to orchestrate a composite Web service. Previous work in Web service selection usually assumes the invocations of Web service operations to be independent of one another. This assumption however does not hold in practice as both the composite and component Web services often impose some orderings on the invocation of their operations. Such orderings constrain the selection of component Web services to orchestrate the composite Web service. We therefore propose to use finite state machine (FSM) to model the invocation order of Web service operations. We define a measure, called aggregated reliability, to measure the probability that a given state in the composite Web service will lead to successful execution in the context where each component Web service may fail with some probability. We show that the computation of aggregated reliabilities is equivalent to eigenvector computation. The power method is hence adopted to efficiently derive aggregated reliabilities. In orchestrating a composite Web service, we propose two strategies to select component Web services that are likely to successfully complete the execution of a given sequence of operations. Our experiments on a synthetically generated set of Web service operation execution sequences show that our proposed strategies perform better than the baseline random selection strategy."
950,"In many decades, many organizations, especially large consulting companies, have been designing, implementing and managing business solutions for every industry around the globe. But due to numerous limitations in process, tooling and skills, most of those solutions were made very specific to individual industry and client needs at its early design stage. Therefore, reuse and more importantly, managing the ever changing business requirements, become almost impossible. Service-orientation and architecture, model-driven business development provides us a new and powerful approach to facilitate asset based industry solution design and development. To further accelerate this, this tutorial will discuss an innovative approach that take advantage of many proven best software engineering practices, from object/component based technology, meta-data driven architecture types (archetypes) that are used to model the common structural and in some cases non-structural business entities such as customer, product, payment, etc. In order to address the consequences introduced by abstracting those common elements out of the specific industry model and be able to enable easy and meta-data based transformation, we properly decompose business components/services into a multi-layered business architecture. Therefore, process/components/services can be decomposed accordingly to facilitate the decomposition and abstraction, while maintaining certain level of necessary traceability across various artifacts. In the realization phase, existing assets/operational systems will be mapped and transformed to the required business components and services to best leverage those existing valuable industry/client investments. To support such a SOA based, model and business driven development process, existing tooling, especially the necessary transformation and integration capability, needs to be significantly enhanced. This tutorial will also present some recommendation based on some recent design a..."
951,"Developing efficient solutions to achieve automatic service composition has drawn significant attentions in service computing. As a composite service typically runs in a dynamic environment, adaptability of the composition solution arises as a central concern. Reinforcement Learning (RL) is one commonly used approach in service composition to achieve self-adaptability. However, traditional RL methods cannot guarantee good efficiency for large-scale composition problems. Hierarchical RL (HRL) has appeared to be a viable solution to address the efficiency issue. The applicability of existing HRL methods (e.g., MAXQ) requires a task graph, which can be generated by decomposing a composition plan into a task hierarchy. Current approaches that apply HRL to service composition generate the task graph manually, which will not scale to large service composition problems. In this paper, we address the above issue by systematically integrating automatic task decomposition and MAXQ HRL, resulting in an adaptive composition solution with good efficiency. Our experimental results demonstrate the effectiveness of the proposed service composition approach."
952,"Over the past few years, Cloud computing has been receiving much attention as a new computing paradigm for providing flexible and on-demand infrastructures, platforms and software as services. In Cloud computing, challenges in searching cloud services need to be renewed due to a number of unique characteristics of cloud services such as the dynamic, diverse services offering at different levels, as well as the lack of standardized description languages. In this paper, we present the Cloud Service Crawler Engine that we used to collect metadata of 5, 883 valid cloud services through search engines after parsing more than a half million possible links. Based on the collected data, we conducted a set of statistical analysis and present the results in this paper. These statistical results offer an overall view on the current status of cloud services. Some most intriguing findings from our investigation include: i) the scarcity of standardization in Cloud computing, and ii) little evidence on a strong support of Cloud computing from the established service-oriented computing (SOC) technologies. Our findings provide some further insights on improving cloud services discovery and the datasets collected from this study will be valuable to the research community."
953,"This paper presents the WS-Talk interface layer, a concept for a structured natural language interface which virtualizes the composition of Web services at the end-user's side. As the number of available Web services grows, the representation of the context in which they are used becomes more and more an issue for both service providers and service consumers. While providers concentrate more on the technical levels of activation and communication within a service network, the users, i.e. the service consumers, want to form ad-hoc collaborations between Web services. To shield users of Web services from the technical details of finding and combining them, a semantic level has to be found that suits their specific needs and expertise. Through a semantic layer, WS Talk tries to create a level of abstraction that enables views on services expressed in natural language. To reduce the complexity of natural language, domain-specific languages are defined and their semantics are mapped onto Web services and their attributes. The domain of economics is currently used for testing architecture and usage aspects of the WS-Talk interface layer."
954,"Volunteered Service Composition (VSC) refers to the process of composing volunteered services and resources. These services are typically published to a pool of voluntary resources. Selection and composition decisions tend to encounter numerous uncertainties: service consumers and applications have little control of these services and tend to be uncertain about their level of support for the desired functionalities and non-functionalities. In this paper, we contribute to a self-awareness framework that implements two levels of awareness, Stimulus-awareness and Time-awareness. The former responds to basic changes in the environment while the latter takes into consideration the historical performance of the services. We have used volunteer service computing as an example to demonstrate the benefits that self-awareness can introduce to self-adaptation. We have compared the Stimulus- and Time-awareness approaches with a recent Ranking approach from the literature. The results show that the Time-awareness level has the advantage of satisfying higher number of requests with lower time cost."
955,"Privacy is still among the key challenges that keep hampering DaaS service composition solution. Indeed services may follow different, conflicting privacy specifications with respect to the data they use and provide within a composition. In this paper, we propose an approach for privacy-aware composition of DaaS services. Our approach allows verifying the compatibility of privacy specifications of services involved in a composition. In the case when any composition will be incompatible in terms of privacy, we introduce a novel approach based on negotiation to reach compatibility of concerned services. The negotiation approach is cautiously operated with without any privacy damaging of services. We validate the applicability of our proposal through a set of experiments."
956,"This paper discusses the quality of service (QoS)-aware composition of Web services. The work is based on the assumption that for each task in a workflow a set of alternative Web services with similar functionality is available and that these Web services have different QoS parameters and costs. This leads to the general optimization problem of how to select Web services for each task so that the overall QoS and cost requirements of the composition are satisfied. Current proposals use exact algorithms or complex heuristics (e.g. genetic algorithms) to solve this problem. An actual implementation of a workflow engine (like our WSQoSX architecture), however, has to be able to solve these optimization problems in real-time and under heavy load. Therefore, we present a heuristic that performs extremely well while providing excellent (almost optimal) solutions. Using simulations, we show that in most cases our heuristic is able to calculate solutions that come as close as 99% to the optimal solution while taking less than 2% of the time of a standard exact algorithm. Further, we also investigate how much and under which circumstances the solution obtained by our heuristic can be further improved by other heuristics"
957,"Web services evolve over time to fix bugs or update and add new features. However, the design of the Web service's interface may become more complex when aggregating many unrelated operations in terms of context and functionalities. A possible solution is to refactor the Web services interface into different modules that help the user quickly identifying relevant operations. The most challenging issue when refactoring a Web services interface is the high number of possible modularization solutions. The evaluation of these solutions is subjective and difficult to quantify. This paper introduces the use of a neural network-based evaluation function for the problem of Web services interface modularization. The users evaluate manually the suggested modularization solutions by a Genetic Algorithm (GA) for a number of iterations then an Artificial Neural Network (ANN) uses these training examples to evaluate the proposed Web services design changes for the remaining iterations. We evaluated the efficiency of our approach using a benchmark of 82 Web services from different domains and compared the performance of our technique with several existing Web services modularization studies in terms of generating well-designed Web services interface for users."
958,"Significant consumption and cost savings can be made by better managing energy usage within small commercial properties and individual dwellings. By combining Web services and off-the-shelf home automation equipment, it is now possible to build a cost-effective infrastructure to support the delivery of energy management services to small consumers. In this paper we treat residential energy management as a resource management problem in a distributed computing system. Energy consumers are able to delegate the energy management of smart-meter connected appliances to these energy service providers and specify their energy consumption preference through access policies. We give an optimal scenario in terms of energy cost and efficiency in this service model. We also design an algorithm that makes use of aggregated user information to achieve near optimal energy use among residential electricity users."
959,"Web services represent an important technology for distributed applications and will replace various other technologies for distributed application development soon. A lot of problems of the early days of Web services are solved now. However, for authorization no appropriate solution is available and ready to use. We define requirements for authorization of Web services and investigate existing authorization solutions concerning these requirements. Based on existing authorization solutions and the defined requirements, a Web service authorization framework is developed. We describe concepts and the design of the proposed framework and give an overview of selected implementation aspects (e.g. authorization data access, descriptive deployment). The framework emphasizes easy deployment of Web Service authorization and is ready to use. Practical experience using the framework concludes the paper."
960,"The paper presents an extension of solution plans generated by the graph-plan algorithm developed for AI planning. We post process the solution plans in order to generate executable workflows. We make the following main contributions: (i) we generate a Workflow Tree from the solution plans by encapsulating the actions into abstract workflow Blocks, (ii) we develop a methodology that uses the specification of workflow patterns in terms of Abstract State Machines in order to find the appropriate pattern to replace the abstract Block constructs in the workflow tree. (iii) We provide semi-automated support for agents in order to select the most specific pattern that fits their behavior at run-time. We have implemented the approach and our experiments with some real-world case studies show the viability of the proposed approach."
961,"As service-oriented architecture has become popular, security has been a critical issue in multiple security domains using the WS-security framework. The authentication requirements depend on the application semantics, but configuring authentication is very difficult for someone who is not a security expert, such as an application developer, because it is necessary to understand platform-specific security features and authentication mechanisms. To resolve these difficulties, we propose a framework for platform-independent security configuration based on the model driven architecture. In this paper, we introduce a security qualifier, which is an abstract annotation for specifying authenticated identity on a platform-independent model, and a security infrastructure model which is a model including the platform information required for creating security policies. These ideas make authentication configuration possible without understanding the platform-specific information, such as the federation of the security domain and the relationships of trust between the servers. Our framework allows a non-security expert to configure security easily. We show how to configure the authentication for an ID propagation scenario and discuss advantages of our framework compared to existing tools"
963,"A mobile business process is a special case of a business process where most of the human interaction is done using mobile devices. In this paper, we propose UML-based support for developing such mobile business processes. Here, the business process is first modeled using UML. Then the process model is translated into a BPEL description, which can be run in mobile and/or network-based workflow engines. We propose rules to guide modeling of mobile business processes, to import existing WSDL decriptions into UML models, and to generate executable BPEL descriptions with appropriate WSDL definitions. In addition, we introduce our implementation of the approach. The practical applicability is demonstrated by designing a group messaging process. It provides a customizable mobile device based communication service offering a business case for mobile operators."
964,"Geospatial data and analytical functions are essential to geospatial modeling. There are increasing interests in publishing both geospatial data and analytical functions as Web services and use them as the building blocks for domain specific geospatial modeling. While the advantages of using the Web services technologies have been well recognized and a number of prototype systems have been built to demonstrate the feasibility, very few performance evaluations have been reported in the previous studies. Compared with business data, geospatial data is rich in data types, large in data volumes and complex in semantics. On the other hand, the Web services technologies are known to have significant overheads with respects to deployment and invocation. The answers to how effective the Web services technologies can be, and, to what extent they are effective under the typical computation environments for geospatial modeling remain largely unknown. In this study we have set up an experimental system by deploying several geospatial Web services on top of popular commercial and open source spatial databases and geographical information systems (GIS). The Kepler scientific workflow system is used for geospatial Web services composition and invocation. We have conducted experiments to chain the geospatial Web services into a geospatial model under two data volume levels and two network settings. Our experiments show that the geospatial modeling using the Web services technologies remains effective in the wired LAN computation environment for data volume as large as 10000 points. However, the same data volume level incurs significant response lags under the wireless WAN computation environment. The experimental results may be used as a guideline for geospatial modeling using the Web services technologies when performances need to be taken into considerations."
965,"We study the access control integration problem for web services. Organizations frequently use many services, each with its own access control policies, which must interoperate while maintaining secure access to information. The integration problem is to take the set of such services and to find a globally consistent access control policy that ensures that the system composed from the services does not have any authorization failures or information disclosures. We give a sound and complete algorithm for access control integration by reducing the problem to Boolean constraint solving. We have implemented ROLEMATCHER, a tool to infer global role-based access control schemas for a set of services, and show on examples that it can quickly infer global roles for composed systems, or determine the absence of a globally consistent role schema."
967,The Web services stack of standards is designed to support the reuse and the interoperation of software components on the Web. A critical step in the process of developing applications based on the service oriented architecture is the service discovery. This paper shows how service composition can be used as a technique to support service discovery. The paper discusses the current state of research in this area and introduces a semantic matching algorithm that exploits the possibility to compose multiple services in order to satisfy a service request.
968,"Structured financial messaging solution (SFMS) is a secure and common messaging solution that serves as the basic platform for intrabank and interbank applications in the Indian financial sector. SFMS connects around 25 banks, with 500 branches each on average, spread across the length and breadth of the country. SFMS was envisaged in 1999 with MQSeries as the messaging oriented middleware. In this paper, we discuss how the middleware in SFMS has evolved from a proprietary messaging middleware to an open middleware to handle the ever-increasing demands of the Indian financial service industry. SFMS has evolved from being a CUG application to being an open system that can be used by any of the bank's strategic business partners. The paper evaluates the technologies at each stage of SFMS transition and discusses how the then-existing anomalies have been removed and highlights the power of Web services in achieving straight through processing, across various players of financial system."
969,"Collaborative filtering is one of widely used Web service recommendation techniques. There have been several methods of Web service selection and recommendation based on collaborative filtering, but seldom have they considered personalized influence of users and services. In this paper, we present an effective personalized collaborative filtering method for Web service recommendation. A key component of Web service recommendation techniques is computation of similarity measurement of Web services. Different from the Pearson Correlation Coefficient (PCC) similarity measurement, we take into account the personalized influence of services when computing similarity measurement between users and personalized influence of services. Based on the similarity measurement model of Web services, we develop an effective Personalized Hybrid Collaborative Filtering (PHCF) technique by integrating personalized user-based algorithm and personalized item-based algorithm. We conduct series of experiments based on real Web service QoS dataset WSRec [11] which contains more than 1.5 millions test results of 150 service users in different countries on 100 publicly available Web services located all over the world. Experimental results show that the method improves accuracy of recommendation of Web services significantly."
970,"The purpose of this panel is to present a broad range of best practices of SOA strategization and operationalization in the use of Web services in real-world SOA implementations. The focus will be on the common challenges and issues encountered in SOA projects. Topics include, but are not limited to, tenets, methodology, architecture, service management, standards, tools, process, organization, governance, security, and quality of services. Practitioner's guides and anti-patterns as well as trends will be discussed in the context. Real-life pragmatic solutions to business problems will be exemplified and illustrated in case studies."
971,"One of the main issues in service collaborations among business partners is the possible lack of trust among them. A promising approach to cope with this issue is leveraging on blockchain technology by encoding with smart contracts the business process workflow. This brings the benefits of trust decentralization, transparency, and accountability of the service composition process. However, data in the blockchain are public, implying thus serious consequences on confidentiality and privacy. Moreover, smart contracts can access data outside the blockchain only through Oracles, which might pose new confidentiality risks if no assumptions are made on their trustworthiness. For these reasons, in this paper, we are interested in investigating how to ensure data confidentiality during business process execution on blockchain even in the presence of an untrusted Oracle."
972,"This article demonstrates how to build a globalSOA infrastructure, using a step by step approach and Design Patterns. The proposed architecture results in a minimalistic use of the SOA concepts and a novel solution for SOA management and governance which goes beyond current vendor products."
973,"This research proposes to modernize a legacy software system by using Web services as the main building blocks of the software reengineering. For this purpose, a legacy theorem proof checking and derivation tool called Bertie3 is reengineered in terms of service-oriented architecture, service-oriented componentization, and external data representation and serialization. With the Web services of derivation checking engines and first-order markup languages, a minimal amount of development time can be spent working on well-known and well-developed components. More time can be spent updating the features that make the tool unique. This case study shows that modernizing a software system with Web services will allow the business components of the system to be easily expanded and integrated with other application components for future demands."
974,"Service evolution is a critical issue because even small changes, if not compatible, can potentially affect a huge number of client applications. However, particularly in the context of large scale service usage, changes have different impact on clients according to its use. This paper proposes a change management framework that supports service providers to scope and quantify the impact of changes based on usage analysis. The framework adopts a finer-grained versioning model in order to easily locate and assess the compatibility of changes in service descriptions. The framework also clusters client applications based on similar patterns of usage, summarizing them in usage profiles. A usage profile quantifies the functionality of the service used by the corresponding applications, enabling to assess the impact of incompatible changes against the profile."
975,"Recently, the mediation-aided approach is attracting more attention in Web service composition, in the meanwhile, temporal constraints are regarded as an important aspect to ensure the correctness and QoS in service compositions. This combination leads to a new challenge in analyzing the timed compatibility of mediation-aided service composition. Unfortunately, existing model checking based approaches are lack of the ability of transform mediation-aided service composition to Time Automata (TA) models, and suffer from state space explosion for large-scale and complex compositions. In this paper, we present a new model checking approach to analyzing timed compatibility. Firstly, mediation-aided service composition is automatically decomposed into fragments. Secondly, each fragment is transformed into a TA. Finally, the temporal constraints are checked by the queries of observing TAs. Compared with existing approaches, our approach is able to check timed compatibility of mediation-aided service composition, and is more efficient than them."
976,"With an increasing number of Web services providing similar functionalities, quality of service (QoS) is becoming an important criterion for selection of the best available service. Currently the problem is twofold. The Universal Description, Discovery and Integration (UDDI) registries do not have the ability to publish the QoS information, and the authenticity of the advertised QoS information available elsewhere may be questionable. We propose a model of reputation-enhanced QoS- based Web services discovery that combines an augmented UDDI registry to publish the QoS information and a reputation manager to assign reputation scores to the services based on customer feedback of their performance. A discovery agent facilitates QoS-based service discovery using the reputation scores in a service matching, ranking and selection algorithm. The novelty of our model lies in its simplicity and in its coordination of the above mentioned components. We present experiments to evaluate the effectiveness of our approach using a prototype implementation of the model."
977,"Mobile users generate ever-increasing amounts of digital data, such as photos, which they upload, while on the go, to online services. 3G connectivity enables mobile users to upload their data while on the go but drains the battery of their devices and overloads mobile service providers. Wi-Fi data offloading overcomes the aforementioned issues for delay-tolerant data, at the cost of constrained mobility for users as they are required to stay within a given area while the data is uploaded. The up-link of the broadband connection of the access point is a bottleneck and incurs significant waiting times. In this paper, we advocate the exploitation of the storage capabilities of common devices located on the Wi-Fi access point LAN, typically residential gateways, to decrease the waiting time. We propose Hoop, a system for offloading upload tasks onto such devices. Hoop operates seamlessly on HTTP(S) POSTs, making it highly generic, it also requires limited changes on the gateways and on the web server and none to existing protocols or browsers. Hoop is secure and, in a typical setting, reduces the waiting time by up to a factor of 46. By correlating mobility traces with the positions of the Wi-Fi access points of a major community network, we show that Hoop drastically decreases the delay between the time a photo is taken and the time it is uploaded, compared to regular Wi-Fi offloading."
978,"The problem of systematically capturing and managing business provenance across loosely coupled systems has received insignificant attention despite of its relevance. Business provenance gives the flexibility to selectively capture information required to address specific compliance, security or QoS goals. This paper presents a native cloud-based architecture named Cirrus, it may collect retrospective provenance generated by business processes across multiple internal and external organizational boundaries."
979,"Web service substitution refers to the problem of identifying a service that can replace another service in the context of a composition with a specified functionality. Existing solutions to this problem rely on detecting the functional and behavioral equivalence of a particular service to be replaced and candidate services that could replace it. We introduce the notion of context-specific substitutability, where context refers to the overall functionality of the composition that is required to be maintained after replacement of its constituents. Using the context information, we investigate two variants of the substitution problem, namely environment-independent and environment- dependent, where environment refers to the constituents of a composition and show how the substitutability criteria can be relaxed within this model. We provide a logical formulation of the resulting criteria based on model checking techniques as well as prove the soundness and completeness of the proposed approach."
980,"Service Level Agreements (SLAs) play an important role in service-based systems. However, traditional approaches to establish SLAs are mostly manual and predefined which is not suitable for the highly dynamic and unpredictable service-oriented environment. In this paper, we propose a policy-based framework for supporting dynamic and automated SLA negotiations for Web services. In our framework, we extend the WS-Policy framework to provide a domain-independent policy language for specifying QoS constraints over the QoS attributes that are to be negotiated. Negotiation agents are dynamically created to perform SLA negotiations on behalf of each negotiating party in a P2P way using standard web services invocations. Decision making models of negotiation agents are also defined in a declarative way and can be reconfigured easily. We have implemented a prototype of our framework and demonstrated our approach through a case study."
981,"We present a case study to illustrate our formalism for the specification and verification of the method-invocation behavior of web-service applications constructed from asynchronously interacting multi-threaded distributed components. Our model is expressive enough to allow the representation of recursion and dynamic thread creation, and yet permits the algorithmic analysis of the following two questions: (1) Does a given service satisfy a safety specification? (2) Can a given service be substituted by a another service in an arbitrary context? Our case study is based on the Amazon.com E-Commerce Services (ECS) platform."
982,"With the incessant growth of Web services on the Internet, designing effective Web service recommendation technologies based on Quality of Service (QoS) is becoming more and more important. Neighborhood-based Collaborative Filtering has been widely used for Web service recommendation, in which similarity measurement and QoS prediction are two key steps. However, traditional similarity models and QoS prediction methods rarely consider the influence of time information, which is an important factor affecting the QoS of Web services. Furthermore, traditional similarity models fail to capture the actual relationships between users or services due to data sparsity. These shortcomings seriously devalue the performance of neighborhood-based Collaborative Filtering. In order to make high-quality Web service recommendation, we propose a novel time-aware approach, which integrates time information into both the similarity measurement and the final QoS prediction. Additionally, in order to alleviate the data sparsity problem, a hybrid personalized random walk algorithm is employed to infer more indirect user similarities and service similarities. Finally, we conduct series of experiments to validate the effectiveness of our approaches."
983,"In this paper, we propose an Internet public Web service matching approach that paves the way for (semi-)automatic service mashup. We first provide the overview of the solution, which requires a detailed review of two fundamental models - schema/graph matching and semantic space. Based on the conceptual model and the literature study, the complete service matching approach is then provided with four essential steps - semantic space, parameter tree, similarity measures, and WSDL operation matching. The system demonstration that proves the concept proposed in this approach is finally presented. The solution has the potential to facilitate the Internet services mashup."
984,"Trust and reputation mechanisms offer a promising way to solve the web services selection problem. Currently, many researches on this topic focus on collecting, storing and aggregating feedbacks which can be execution data from monitoring web services or user ratings reported by service consumers for web services evaluation. However, besides considering the evaluation methods, the reliability of feedbacks becomes equally crucial. It is pointed out that the accuracy of a web service's reputation evaluation will be reduced as unreliable feedbacks are used. Therefore, the elicitation of trusted feedbacks and feedbackers becomes very important. In this paper, we managed to elicit trusted users in our service registry through building and maintaining a ""Web of Trust"". We mined this web of trust by our proposed algorithm which is based on the Kalman Filter Algorithm and analyzed the trustworthiness for each user. Feedbacks reported by trusted users then should be considered more reliable than others to ultimately facilitate the web services selection process."
985,"Key resources are important, influential and powerful performers in a social network structure. Identifying them in the social network of a business process activity is beneficial and rewarding. One of the most effective centrality measures for identification of the key nodes in a social network is to rank resources based on a selection of criteria. PageRank is a representative example of such algorithms, which was first utilized in the Google search engine in 1998. However, the PageRank approach merely assumes a single link as a vote, which allows one originator to link or transfer his work to others more than once in a handover work scenario. We argue that this problem can lead to inaccurate influence based ranking in the context of business processes for resources in a social network. In this paper, we propose f-PageRank, a new approach specifically designed to identify the key resources in a social network generated from a business process activity. We evaluate our proposed method by comparing it with the existing approaches in process mining tools, such as degree centrality, betweenness centrality, BaryRanker, and HITS. The experimental results show that our approach can obtain a satisfying outcome."
986,"Perception of trustworthiness of service providers is a fundamental need in service selection. Trust propagation has been used to predict trustworthiness of service providers in service-oriented social networks. However, existing trust propagation methods may suffer from a scalability problem, i.e., their computation time is likely too high to be acceptable in practice, especially when they are applied to very large-scale service-oriented social networks. Moreover, they rarely consider the structural properties of social networks to optimize their performance. This paper proposes an efficient trust propagation scheme for predicting trust in service-oriented social networks. It exploits the specific structural properties of social networks and builds an advanced data structure from preprocessing to improve the efficiency of trust propagation. Our scheme can support multiple trust propagation strategies. Experiments show that our scheme is much more efficient than well-known trust propagation methods in trust prediction, while its trust prediction results are as accurate as theirs in service-oriented social networks."
987,"Automated negotiation among Web services not only provides an effective way for the services to bargain for their optimal customizations, but also allows the discovery of overlooked potential solutions. A number of negotiation supporting techniques have been used to find solutions that are acceptable to all parties in the negotiation. However, employing these solutions for automated negotiations among Web services has its own challenges. In this paper, we present the design of a Negotiation Web service that would be used by both the consumers and providers of Web services for conducting negotiations. This negotiation service uses a genetic algorithm(GA) based approach for finding acceptable solutions in multi-party and multi-objective negotiations. In addition to the traditional genetic operators of crossover and mutation, the search is enhanced using anew operator called the Norm. Norm operator represents the cumulative knowledge of all the parties involved in the negotiation process. GA performance with the new Norm operator is compared to the traditional GA, hill-climber and random search techniques. Experimental results indicate the practicality of our approach in facilitating the negotiations involved in a Web service composition process. Specifically, the proposed GA with Norm operator performs better than other approaches."
988,"BPEL/WSBPEL is the predominant approach for combining individual Web services into integrated business processes, allowing for the specification of their sequence, control flow and data exchanges. BPEL however does not include mechanisms for considering the invoked servicespsila Quality of Service (QoS) parameters and thus BPEL scenarios can neither tailor their execution to the individual userpsilas needs or adapt to the highly dynamic environment of the WEB, where new services may be deployed, old ones withdrawn or existing ones changing their QoS parameters. Moreover, infrastructure failures in the distributed environment of the Web introduce an additional source of failures that must be considered in the context of QoS-aware service execution. In this work we propose a framework for addressing the issues identified above; the framework allows the users to specify the QoS parameters that they require and it undertakes the task of locating and invoking suitable services. Finally, the proposed framework intercepts and resolves faults occurring during service invocation, respecting the QoS restrictions specified by the consumer."
989,We address the problem of optimizing mediator-based service composition where the services and the desired composition (goal) functionality are represented as i/o automata with loops. The objective of optimization is to minimize the costs of communications and computations necessary to realize the goal from the existing services. We develop an algorithm to compute the minimum cost of an automaton representing the choreographed behavior of services realizing the goal. This forms the central theme of our technique for developing automatically a strategy of decentralized mediation that will result in the optimized composition of services.
990,"As mobile devices become more pervasive they will emerge as a standard platform for hosting Web service clients. Unlike their ""static"" counterparts, mobile devices are typically connected via a wireless network forcing them to deal with constrained bandwidth and the sudden loss of connectivity. This paper focuses on the use of caching SOAP request-response pairs in order to compensate for fluctuating bandwidth and loss of connectivity. We introduce the concept of embedded SOAP caching, highlighting the need for meta-data as a means to support it. A novel SOAP cache (CRISP) that can be embedded into the application or used as an independent proxy-cache is presented and evaluated under various loads and settings. The evaluation of CRISP shows that caching of SOAP traffic is not only an effective means to compensate for loss of connectivity but also enables reducing network loads which is particularly interesting when dealing with bandwidth constraint wireless connections."
991,"As Web services proliferate, the discovery and integration of appropriate services will become a challenging task particularly with services that share common similarities and functionalities. To assist requesters to find Web services of interest easily, we need to enhance the present service discovery mechanisms with ranking method. Enlightened by disciplines of Web 2.0, the anchor semantics enabled ranking method is proposed. In this method, the service requestor, as a consumer, can also become the producer to contribute not only the request description but also the usage reputation as part of service description. Based on case study and analogical analysis, we argue that this method improves recall and precise metrics at the same time."
992,"In this paper, we propose a novel framework WSCRec, in which historical information (i.e., execution logs) are utilized to facilitate the process of Web service composition. According to the execution logs of composite services, appropriate services which have been proved to be more reliable and robust and have higher probability to fulfill users' demands are located."
993,"Service composition has become an important paradigm for building distributed applications and e-business processes. While effort has been reported to verify a posteriori whether a given composition such as a BPEL schema satisfies the predefined behavioural properties, little effort has been made to utilise the properties to assist the designer in developing a correct service composition in the first place. This paper reports our first attempt towards this goal by presenting a framework and associated techniques to provide automated guidance to the designer during the composition design process. The guidance can be suggestions on the next valid steps in the business process, identifications of missing/misplaced steps, and/or propositions for inserting, deleting or reordering activities. The guidance is provided based on the temporal business rules which state the temporal/sequential relationships between business activities."
994,"Web service conformance testing checks the correctness of a black box service implementation, and it is the basis of other testings. An efficient formal method for conformance testing is the Chinese postman traversal algorithm that can find minimum-cost test sequences. However, the applicability of this algorithm is in question if data dependences are present in the protocol specification. Also, it suffers from the limited observability problem which is not uncommon in web services. Despite its optimality, the traversal algorithm does not take advantages of the special patterns in web service interfaces observed by developers. To address these issues, we propose an abstract GFSM (guarded finite-state machine) model that unifies and augments the commonly used Moore and Mealy machines with dataflows. Using this abstract model, we formalize the conditions under which the tours with complete data and control coverage are guaranteed, and address the limited observability problem using the equivalence of Moore and Mealy machines. Furthermore, the authors propose a recursive descent traversal algorithm that explores the inverse operation pattern of web services to facilitate incremental development of Web services."
995,"Mobile applications have been widely used in life and become dominant software applications nowadays. However there are lack of systematic recommendation systems that can be leveraged in advance without users' evaluations. We present AppReco, a systematic recommendation system of iOS mobile applications that can evaluate mobile applications without executions. AppReco evaluates apps that have similar interests with static binary analysis, revealing their behaviors according to the embedded functions in the executable. The analysis consists of three stages: (1) unsupervised learning on app descriptions with Latent Dirichlet Allocation for topic discovery and Growing Hierarchical Self-organizing Maps for hierarchical clustering, (2) static binary analysis on executables to discover embedded system calls and (3) ranking common-topic applications from their matched behavior patterns. To find apps that have similar interests, AppReco discovers (unsupervised) topics in official descriptions and clusters apps that have common topics as similar-interest apps. To evaluate apps, AppReco adopts static binary analysis on their executables to count invoked system calls and reveal embedded functions. To recommend apps, AppReco analyzes similar-interest apps with their behaviors of executables, and recommend apps that have less sensitive behaviors such as commercial advertisements, privacy information access, and internet connections, to users. We report our analysis against thousands of iOS apps in the Apple app store including most of the listed top 200 applications in each category."
996,"WSO (Web-Services Orchestration) addresses a broad set of failure scenarios that become relevant when web-service implementations encapsulate multiple web-service calls to other systems. Network timeouts introduce uncertainty about the success or failure of a given API call: in certain scenarios, simply retrying the operation may make the situation worse. In order to guarantee pseudo-atomicity of a web-service call sequence, the failure of one API call implies that other, successfully completed, API calls may have to compensated for before retrying the original set of web-service calls. Even if no failure occurs, orchestration of a web-service to deal with edge-cases (such as expiration of credentials) can be difficult to implement correctly. In this paper we describe an approach to solving such scenarios which we have implemented in our WSO project. To facilitate use of WSO, the programming model is designed to wrap existing code rather than requiring rewriting code. We show the benefits of WSO in the context of a motivating scenario and also present a before/after comparison."
997,"Online Social Networks (OSNs) have become an integral part of daily life in recent years. OSNs contain important participants, the trust relations between participants, and the contexts in which participants interact with each other. All of these have a great influence on the prediction of the trust between a source participant and a target participant, which is important for a participant's decision-making process in many applications, such as seeking service providers. However, predicting the trust from a source participant to a target one based on the whole social network is not really feasible. Thus, prior to trust prediction, the extraction of a small-scale sub-network containing most of the important nodes and contextual information with a high density rate could make trust prediction more efficient and effective. However, extracting such a sub-network has been proved to be an NP-Complete problem. To address this challenging problem, we propose BiNet: a social context-aware trust sub-network extraction model to search for near-optimal solutions effectively and efficiently. In this model, we first capture important factors that affect the trust between participants in OSNs. Next, we define a utility function to measure the trust factors of each node in a social network. At last, we design a novel binary ant colony algorithm with newly designed initialization and mutation processes for sub-network extraction incorporating the utility function. The experiments, conducted on two popular datasets of Epinion and Slash dot, demonstrate that our approach can extract sub-networks covering important participants and contextual information while keeping a high density rate. Our approach is superior to the state-of-the-art approaches in terms of the quality of extracted sub-networks within the same execution time."
999,"Web service composition can make use of distributed services with various functions to accomplish a specific task. Service binding and composition become keywords to service computing. In the context of workflow management, service binding plays an important part in forming a concrete workflow after an abstract one is submitted. Existing research is devoted to selecting a set of servers to run the corresponding services in an application and achieving better QoS. Due to the data intensity of data flow, the communication between two server nodes may bring significant delay that can not be neglected when considering the performance of the whole application process. In this work, we consider not only the QoS on each server but also the transfer delay of data that affects the total latency. Algorithms are designed to determine a set of service candidates for web service workflow with different structural characteristics. We measure and utilize indispensable parameters to implement these algorithms. Further, we propose Lagrange and ARP algorithms to make comprehensive decisions when taking into account other QoS metrics besides latency."
1000,"In today's fast and ever changing business environments, it is very important to quickly adapt business processes to new business requirements, and to optimize their performance. From this viewpoint, business process performance prediction plays a key role in directing the business. Previous studies have shown that a network queuing model is able to predict the business process performance. In order to do this, the model requires details of the service's inside behavior. This paper presents a business process performance model called `BizCast', which enables us to estimate business process execution time under any given condition and not to require details of the service's inside behavior. We especially focus on the overlap between business process instances to predict the performance based on the outside behavior. Prototype evaluation of the service model for supply chain services shows high accuracy, which is, at least, almost 0.9 correlation coefficient"
1001,"In this paper, we introduce Web service application session services, WS-session, for Web service based application session management in communication. A two-way Web service framework is described. It supports the use of Web service in the situation of telecommunication where the communication endpoint can be both a client and a peer-to-peer server. We describe the interface design in two-way Web service interaction based on the concept of tightly coupled interface and loosely coupled interface solutions. The proposed approach provides full support of asynchronous outbound operations and event notifications in communication services. It is implemented in a research prototype system that provides both standard based (ECMA-348) services and Avaya service extensions. In addition, we investigated the integration of our proposed approach with BPEL4WS. It allows the user to create new services through a drag-n-drop based service creation environment (ECLIPSE/BPEL4WS). It leads to a paradigm of service creation through Web service composition. More sophisticated telecommunication services are created from composing the basic services."
1002,This work deals with services composition. We propose an approache based on Symbolic Observation Graphs (SOG) allowing to decide whether two (ore more) web services can cooperate safely. The compatibility between two web services is defined by the well known soundness property on open workflow nets and checked on the composition of SOGs instead of the original web services composition. This allows to respect the privacy of the services since SOGs are base on collaborative activities only and hide the internal structure and behavior of the corresponding service.
1004,"In the Internet of Things (IoT) era, with ubiquitous remote sensing devices and other diverse data sources, nearly everything can forward voluminous data continuously, in real-time, which drives demand to perform real-time analytics on uninterrupted IoT data flows. The typical resultant approach is a cloud-centered architecture providing an analytic service for real-time IoT data processing. However, a cloud-centered IoT analytic service cannot guarantee real-time responsiveness has a high-fee pay-as-you-go business model, and opens data privacy concerns. Hence, it becomes rational to shift analytic workloads to the edge and provide a management service for edge analysis. Existing work on providing edge analytics as a service encountered challenges such as lacking a lightweight way to compose IoT applications based on multiple service providers, lacking a flexible and unified way to define domain-specific analytic logic, and maintaining efficiency when processing data on a resource-limited edge. This paper presents EAaaS, a scalable analytic service for enabling real-time edge analytics in IoT scenarios. In this work, we propose a unified rule-based analytic model to ease user's programming efforts in specifying rule-based analytic logic. Moreover, we also designed and implemented a high performance edge engine to apply rule-based analytic on incoming device data streams. To simplify the access to EAaaS service, a group of RESTful web interfaces is also designed for edge analytic management on cloud and flexible composition with external services. EAaaS is implemented as a part of IBM Watson IoT Platform, which is a cloud service for elementary IoT application development on IBM Bluemix cloud announced by IBM recently. We have conducted proof of correctness (PoC) of EAaaS with customers from boat racing in the U.S. and collected valuable feedback from customers for further enhancement of EAaaS's flexibility and usability."
1005,"Web service plays an important role in implementing service oriented architecture (SOA) for achieving dynamic business process. With the increased number of Web services advertised in public repository, it is becoming vital to provide an efficient Web service discovery and selection mechanism with respect to a userpsilas requirement. Considerable efforts have been made to solve this problem among which semantic based Web service discovery has been attained much importance by researchers in academic and industry community. However, there is a challenge in the semantic based web service discovery process, that is, among the retrieved set of semantically equivalent Web service candidates, how to discern which one is the best? In this paper, inspired by collaborative filtering idea, a web service ranking framework is proposed in which a set of users with similar interest will be firstly identified. Afterwards, association rules will be found out by analyzing all Web service composition transactions related to that set of users. By combining user group and association rule mined from that group, a personalized Web service ranking mechanism is achieved and the experiment shows the promising result."
1006,Provides an abstract of the keynote presentation and a brief professional biography of the presenter. The complete presentation was not made available for publication as part of the conference proceedings.
1007,"Rigorously trusted services depend on reliable evidences to describe and check service behaviors. In this paper, we propose the pi-SOA framework, which delivers mutually trusted services in a rigorous way. The framework allows clients to verify service behaviors remotely according to their trust policies and uniquely identify the verified service at all times during its executions. On the other hand, service providers in this framework can check that clients have agreed with the service behaviors before using the service."
1008,"QoS prediction has become an important step in service recommending and selecting. Most QoS prediction approaches are using collaborative filtering as a prediction technique. But collaborative filtering may suffer from data sparsity problem which degrade the prediction accuracy. In order to alleviate the data sparsity problem of collaborative filtering, we presented a hybrid QoS prediction approach by applying clustering on web services before applying collaborative filtering (named services clustering QoS prediction, SCQP). The clustering process cluster web services in to service clusters in which services have the same physical environment. Then the similarity between users is calculated based on these service clusters instead of individual services. So that there are more information to be used when calculate the similarity and it will contribute to elevate the prediction precision. The experimental results showed that our hybrid approach could not only achieve higher prediction precision, but also reduce the computation time than other collaborative filtering based prediction methods."
1009,"The service-oriented architecture (SOA) has been successfully applied in enterprise environments. Due to decentralized set-ups, mergers and acquisitions and organizational boundaries, many enterprises today operate multiple, fragmented and heterogeneous service infrastructures that are administered by different organizational units. This fragmented infrastructure causes service duplication and unnecessary redundancy. This paper proposes an approach of cross-domain service integration through an automated federation of Enterprise Service Buses (ESBs). ESBs are the mediation centers within a service domain that enable service interaction across technological boundaries by using service proxies. We present DISCE, a configuration engine prototype that enables an operator to configure service connectivity in such an environment in a declarative form, by specifying simple rules. The engine produces a configuration consisting of a set of proxies interconnecting clients and services."
1010,"Several approaches have been proposed to introduce self-management capabilities for Web service compositions.However, most of these works are limited as they are not extensible, i.e., new self-adaptation features cannot be supported, and even if that is possible then still this cannot be done dynamically while the composite services are running.In addition, many of these works are not based on the service composition standard WS-BPEL. In this paper, we propose a plug-in architecture for self-adaptive Web service composition, in which self-adaptation features are well-modularized in aspect based plug-ins. Our approach supports application-specific adaptation scenarios, is easily extensible, and allows self-adaptation logic to be hot-deployed on running process instances. We have implemented this architecture and several plug-ins using the dynamic aspect-oriented workflow language AO4BPEL."
1011,"Application delays caused by abnormal tasks arecommon problems in big data computing frameworks. Anabnormal task in Spark, which may run slowly withouterror or warning logs, not only reduces its resident node'sperformance, but also affects other nodes' efficiency.Spark log files report neither root causes of abnormal tasks,nor where and when abnormal scenarios happen. AlthoughSpark provides a “speculation” mechanism to detect stragglertasks, it can only detect tailed stragglers in each stage. Sincethe root causes of abnormal happening are complicated, thereare no effective ways to detect root causes.This paper proposes an approach to detect abnormality andanalyzes root causes using Spark log files. Unlike commononline monitoring or analysis tools, our approach is a pureoff-line method that can analyze abnormality accurately. Ourapproach consists of four steps. First, a parser preprocessesraw log files to generate structured log data. Second, ineach stage of Spark application, we choose features relatedto execution time and data locality of each task, as well asmemory usage and garbage collection of each node. Third,based on the selected features, we detect where and whenabnormalities happen. Finally, we analyze the problems usingweighted factors to decide the probability of root causes. In thispaper, we consider four potential root causes of abnormalities,which include CPU, memory, network, and disk. The proposedmethod has been tested on real-world Spark benchmarks.To simulate various scenario of root causes, we conductedinterference injections related to CPU, memory, network,and Disk. Our experimental results show that the proposedapproach is accurate on detecting abnormal tasks as well asfinding the root causes"
1013,"A design issue that often appears in real-world services is that their interfaces are not cohesive, i.e., they consist of many and possibly unrelated operations. This issue may complicate the comprehension of the services functionalities and the maintenance of the applications that use them. Currently, the state of the art on case studies that focus on the evaluation of the cohesion of services offered by major service providers is limited, while research efforts on corresponding cohesion metrics are at a quite early stage. In particular, there exist coarse-grained metrics of cohesion lack, which consider that the operations of a service interface are related if the types of certain of their input/output data exactly match. The problem in this approach is that operations which operate on data characterized by similar, but not exactly matching, types are treated as being totally unrelated. Consequently, the aforementioned metrics may overestimate the cohesion lack of service interfaces. In this paper, we undertake a more elaborate approach to evaluate a set of real world services provided by Amazon. Specifically, we propose two fine-grained metrics of cohesion lack, which are defined with respect to the structural similarity of the input/output data types of interface operations. The proposed metrics are formally defined and analytically assessed with respect to fundamental properties of software metrics. Finally we report the results from our case study."
1014,"Service oriented architectures (SOA) are becoming the prevalent approach for realizing modern services and systems. SOA offers superior support for autonomy (decoupling) and heterogeneity compared to previous generation middleware systems, resulting in more scalable and adaptive solutions. However, SOA have not adequately addressed management, while traditional management solutions do not sufficiently scale to address the needs of (global) Web services. We propose scalable management based on models and industry standards. We discuss a use case for global service management, we present its design, implementation and preliminary evaluation. We retain all the benefits of SOA while also enabling global scale manageability. Our approach provides manageability that is comprehensible for administrators yet automated enough for integration into autonomous systems."
1015,"Summary form only given. Services businesses have become very exciting growth opportunities for the industry. How to design, model, and implement business services using IT technology is becoming a challenging issue. With the introduction of service-oriented architecture (SOA) and Web services, componentizing enterprises and services based on patterns has paved a way to running a successful services business. In this talk, the author describes the services ecosystem, methodology and supporting techniques used to build, operate, and manage business services."
1016,"caBIGtrade (the cancer Biomedical Informatics Gridtrade) is an open-source, open-access information network enabling cancer researchers to share tools, data, applications, and technologies. caGrid is the underlying service-based grid software infrastructure for caBIG, integrating distributed data and analytic resources into a virtual collaborative platform for cancer research. Within caGrid, many cancer-related data analysis and aggregation tasks can make use of ""canned"" sets of service invocations, or workflows. As a result, there is a need to orchestrate the invocation of caGrid services through the use of both a workflow language and tooling. In this paper, we first explain why we select Taverna as a candidate for workflow authoring and invocation. We then review the development of Taverna plug-ins in general, and describe how we extend Taverna to use caGrid services. We then detail a real-world example and the lessons learned from our research. Finally we conclude with a summary and a description of potential next steps."
1017,"In this paper we propose a solution for optimizing (Web service) business workflow response times through dynamic resource allocation. On-the-fly monitoring is combined with a novel workflow modeling algorithm that discovers critical execution paths and builds ""dynamic"" stochastic models in the associated ""critical graph"". One novel contribution of this work is the ability to naturally handle parallel workflow execution paths. This is essential in applications where workflows include multiple concurrent service calls/paths that need to be ""joined"" at a later point in time. We discuss the automatic deployment of on-the-fly monitoring mechanisms within the resource management mechanisms. We implement, deploy and experiment with a proof of concept within a generalized Web services business process (BPEL4WS/SOAP) framework. In the experimental setup we explore and show the natural adaptation to changing workflow conditions and appropriate automatic re-allocation of resources to reduce execution times."
1018,"This tutorial presents the foundational knowledge for the researchers and practitioners on service-oriented architecture (SOA) and Web services. The traditional ""triangle"" SOA and variations that better support SOA services and solutions will be examined. Critical Web services infrastructures will be covered, such as WSDL, BPEL, WSRF, Discovery, Composition, Registry, and Web services invocation and relationship binding. How Web 2.0 and SOA can benefit with each other will also be explored. An IEEE SOA solution reference architecture standardization initiative will be introduced in this tutorial to illustrate how different pieces of technology components can be used to build reusable, flexible, and extensible SOA solutions. Finally, the presenter will depict research and development challenges and directions in the field of SOA and Web services. The target audiences are all-level researchers, practitioners, and students. This tutorial material is created for the IEEE Body of Knowledge initiative on Services Computing, which is sponsored by the IEEE Computer Society Technical Committee on Services Computing"
1019,"Prevalent approaches for automatically composing Web services (WSs) into Web processes predominantly utilize planning techniques to achieve the composition. However, many of the planning methods do not scale efficiently to large processes. In addition, they lack the capability to operate directly on the WS descriptions, and specifically on the preconditions and effects which may be represented using methods ground and propositionalize the higher level logic resulting in exponentially many more states. In this paper, we present a new framework for composing Web services into processes, called Haley, that exploits the natural hierarchy often found in Web processes. Haley uses symbolic techniques that operate directly on first order logic based representations of the state space to obtain the compositions. In addition to providing an approach that handles the uncertainty inheret in Web services, Haley guarantees cost-based optimality and offers an approach potentially scalable to large real world processes."
1020,"Quality-of-Service (QoS) is widely employed for describing non-functional characteristics of Web services. Although QoS of Web services has been investigated in a lot of previous works, there is a lack of real-world Web service QoS datasets for validating new QoS based techniques and models of Web services. To study the performance of real-world Web services as well as provide reusable research datasets for promoting the research of QoS-driven Web services, we conduct several large-scale evaluations on real-world Web services. Firstly, addresses of 21,358 Web services are obtained from the Internet. Then, invocation failure probability performance of 150 Web services is assessed by 100 distributed service users. After that, response time and throughput performance of 5,825 Web services are evaluated by 339 distributed service users. Detailed experimental results are presented in this paper and comprehensive Web service QoS datasets are publicly released for future research."
1021,We propose a novel composition framework for an Infrastructure-as-a-Service (IaaS) provider that selects the optimal set of long-term service requests to maximize its profit. Existing solutions consider an IaaS provider's economic benefits at the time of service composition and ignore the dynamic nature of the consumer requests in a long-term period. The proposed framework deploys a new multivariate HMM and ARIMA model to predict different patterns of resource utilization and Quality of Service fluctuation tolerance levels of existing service consumers. The dynamic nature of new consumer requests with no history is modelled using a new community based heuristic approach. The predicted long-term service requests are optimized using Integer Linear Programming to find a proper configuration that maximizes the profit of an IaaS provider. Experimental results prove the feasibility of the proposed approach.
1023,"We present in this paper a functional model based framework for customizable Web services description, discovery and composition. Web services are described here at a high level abstraction and can be seen as ""black boxes"" labeled by their names and input/output parameters. Complementary to this formalism, we defined a set of mechanisms for Web services discovery and composition."
1024,"In this paper, we explore a novel problem to identify timely new knowledge triples. In the literature, the need of knowledge enrichment has been recognized as the key to the success of semantic search. However, previous work of automatic knowledge extraction, such as Google Knowledge Vault, aim at identifying the unannotated knowledge triples from the full web-scale content in the offline execution. In our study, we show that most people demand the updated knowledge soon after the information is announced. However, the number of queries of such knowledge dramatically declines after a few days, meaning that the most people cannot obtain the precise knowledge from the execution of the offline knowledge enrichment when they inquire the answer of the timely events. To remedy this, we propose the SCKE framework to extract new knowledge triples which can be executed in the online scenario. We model the 'Query-Click' bipartite graph to extract the query correlation and to identify temporally coexistent entity pairs. Our experimental studies show that new triples can also be identified effectively and efficiently."
1026,"Automatic Service Composition (ASC) provides a new value-added service from existing services by user's request dynamically and automatically. User's requests consist of functional and nonfunctional requirements. During service composition, services that fulfill the functional requirements are located at the discovery stage. Abstract nonfunctional requirements should be identified mainly before the selection stage for service execution. Our research was motivated by the identification of abstract nonfunctional properties (NFPs) for a seamless ASC and proposes transformation from the abstract NFPs to intermediate-level NFPs based on the model of three levels of abstractness of NFPs. To solve the vagueness of the abstractness, we adapt approaches based not only on ontology but also on term similarity. The transformation between the intermediate and the concrete levels is carried out by a deterministic algorithm based on mapping of domain ontology. To evaluate the effectiveness of term similarity metrics for nonterminal terms, vector-based and large corpus-based approaches were investigated. The transformation performance based on precision over our test data set and ontology was evaluated."
1027,"The service selection for automatic dynamic service composition with client's requirements oriented service selection becomes more intense. The existing planning and selection algorithms are mostly designed for service discovery. Further, to our knowledge, there are only a few works that incorporate end-user requirements into service composition. In this paper, we propose a graph based multi-grain clustering and selection model for service composition."
1028,"This paper presents an approach for providing a reliable distributed Web service execution engine. Instead of using a Web service execution engine running on a single host to conduct the execution of a composite service, the responsibility of conducting the execution of the composite service has been delegated to the service providers that are chosen to provide the main functionalities of the tasks in the composite service. The operations of the tasks are represented as a set of XML-based notations. The notations are platform independent. Thus, they can be (a) retrieved by service providers based on different platforms, and, (b) interpreted and executed by the service providers. The approach also provides a mechanism for coping with possible failure in the system"
1029,"Service contracts function as interfaces which bridge IT implementation, business modeling, and economical analysis among stakeholder. We report a review of recent research/practice(to be completed) with focus on contract content and contract management. Dimensions of { Quality of data(QoD), Quality of service(QoS), Legal issue,&nbsp;&nbsp;Context, Business term } and { Description of contract, Monitor/Control, Selection, Matchmaking, Composition } are empirically chosen for evaluation purpose. We also propose future directions."
1030,"The publish/subscribe paradigm can be used to build IoT service communication infrastructure owing to its loose coupling and scalability. Its features of decoupling among event producers and event consumers make IoT services collaborations more real-time and flexible, and allow indirect, anonymous and multicast IoT service interactions. However, in this environment, the IoT service cannot directly control the access to the events. This paper proposes a cross-layer security solution to address the above issues. The design principle of our security solution is to embed security policies into events as well as allow the network to route events according to publishers' policies and requirements. This solution helps to improve the system's performance, while keeping features of IoT service interactions and minimizing the event visibility at the same time. Experimental results show that our approach is effective."
1032,"We define a formal model for information services that incorporates the concept of service similarity. The model places services in metric spaces, and allows for services that have arbitrarily complex inputs and output domains. We then address the challenge of service substitution: finding the services most similar to a given service among a group, possibly large, of candidate services. To solve this nearest neighbor problem efficiently we embed the space of services into a vector space and search for the nearest neighbors in the target space. We report on an extensive experiment that validates both our formalization of similarity and the methods used for finding service substitutions."
1034,"In order to build a low-latency lightweight publish/subscribe (pub/sub) system for IOT services, we propose an efficient and scalable broker architecture, called Grid Quorum-based pub/sub system (GQPS). As a core component in the event-driven SOA framework for IOT services, this architecture organizes multiple pub/sub brokers into a quorum-based peer-to-peer topology for efficient topic searching. It also leverages a topic searching algorithm and a caching strategy to achieve a small and constant search latency. Lightweight RESTful interfaces make our GQPS more suitable for IOT services. Cost analysis and experiment study demonstrate that GQPS achieves a significant performance gain in search satisfaction without compromising search cost. We applied GQPS in the District Heating Control and Information Service System in Beijing, China, which validates the feasibility and availability of our architecture."
1038,"Summary form only given. The emerging concept of semantic Web services aims at more sophisticated Web Service technologies: on basis of semantic description frameworks, intelligent mechanisms are envisioned for discovery, composition, and contracting of Web services. The tutorial explains the current state of the art in semantic Web services on basis of the Web service modeling ontology WSMO and related initiatives. Commencing from the vision and arising challenges for semantic Web services, the tutorial in detail explains the specifications of recent frameworks for semantic Web services and presents the Web service execution environment WSMX as the WSMO reference implementation. The tutorial consists of three main sections that subsequently provide a complete overview of semantic Web services and the latest status of WSMO. The tutorial addresses academic as well as industrial researches and developers that are working with Web services and are interested in semantic Web services."
1039,"QoS computation plays an important role in Web service selection. It involves property value preprocessing aspect, user satisfaction calculation aspect, and aggregation of multiple QoS properties aspect. However, little attention has been paid to users participating in QoS computation. In this paper, we examine QoS computation from the angle of experienced users and novice users. An experienced user is able to be more active in providing configuration information such as expected boundary of a QoS property, distribution function of user satisfaction, and the aggregation weight of each QoS property. While a novice user has limited experience to do this. Based on the study of user-centered factors in QoS computation, we propose a user-centered QoS computation, which provides a new choice of normalization in property value preprocessing aspect, an approach of approximation in user satisfaction calculation aspect, and a weight suggestion way in aggregation of multiple properties aspect. A case study in translation service selection shows that the proposed user-centered QoS calculation is more efficient for novice users than random configuration, and much more efficient for experience users."
1040,"Service software deployed in E-commerce and finance fields needs working under 7*24 houses mode. If any failure occurs, service reconfiguration should be immediately executed to find appropriate services from candidates in order to guarantee the availability of core business. Thus, service software cries for an effective approach to constantly adjust its form for responding to varying user requirements and instable runtime environments. To this end, this paper proposes a probabilistic model checking-based Web service reconfiguration architecture. First, it proposes a predictive Web service monitoring approach based on probabilistic model checking. Second, it gives a Web service dynamic service selection approach which takes compatibility checking into account. The single-source service selection works to execute service replacement, while the multi-source service selection carries out service simulation. Third, it discusses a Web service dynamic reconfiguration verification approach where the Probabilistic Counterexample-Guided Abstraction Refinement (Probabilistic CEGAR) is introduced to alleviate the state space explosion problem."
1041,"The information technology has been recognized as one of the most important means to improve health care and curb its ever-increasing cost. However, existing efforts mainly focus on informatization of hospitals or medical institutions within organizations, and few are directly oriented to individuals. The strong demand for various health services from customers calls for the creation of powerful individual-oriented personalized health care service systems. Web service composition (WSC) and related technologies can greatly help one build such systems. This paper aims to present a newly developed platform called a Public oriented Health care Information Service Platform (PHISP) and several novel WSC techniques that are used to build it. Among them include WSC techniques that can well support branch and parallel structures."
1042,"Process algebra are a set of formal languages that are suitable to describe concurrent and communication systems including Web services. Nowadays, although process algebra have been effectively exploited for modeling and verifying functional aspects of Web services composition, non-functional aspects have been ignored due to process algebra lack of capability of modeling them. Since execution of Web services need to consume resource (and energy, time, fee, etc), we propose an abstract concept, that is, cost, to model this non-functional aspect. We introduce this abstract concept into TCCS(temporal calculus of communicating systems) that is a classical process algebra and propose a new process algebra called PTCCS(priced temporal calculus of communicating systems). We present syntax and semantics of PTCCS, and prove that PTCCS extends TCCS with cost modeling capability. And an algorithm is proposed to construct cost state space that is used to select Web services composition with optimal cost. Experiment results show that PTCCS can model both functional aspects and non-functional aspects of Web services composition."
1043,Presents the welcome message from the conference proceedings.
1044,"Mobile web interaction using multiple modalities such as speech, gesture and touch is a well-researched area with examples of multiple framework deployments within the domain. Here, we explore usage of semantics of web content as a new modality for mobile web content discovery and navigation. In addition to semantics-based interactivity, the framework enables dynamic cross-language switching between web pages represented in different languages representing same topics through use of their inherent semantic relations. The approach uses transforming natural web content representations to an intermediate index of latent topic distributions. As the system is semantically indexed, no translation or transliteration techniques are required for cross language switch. The multi-dimensional topic index is built using content-topic distributions which in turn are inferred using supervised topic models. The end-to-end system has been developed as an industrial application with enhanced navigational capability provided as an extension to Samsung Browser v4.0 for Android platform. The service also allows standard browsers to query limited services using the web services RESTful API. The current system provides semantic navigation along with cross-language support for English, Hindi, and Spanish. The semantics based navigation modality, benchmarked against state of the art semantic browsers on objective and usability metrics by users demonstrate the superiority of this system. The cross-language system has been evaluated by language experts for both cross-mapping accuracy as well as relevance order mapping resulting in a subjective cumulative gain score of over 72%."
1045,"Most human centric business activities, like the handling of an insurance claim or the design of an IT solution, do not follow a formal process model word-by-word. While there are often underlying processes in place, the real business operation typically encompasses a wider scope of actions. It includes unstructured segments of human activities, additional documents and interactions, the integration of remote systems and services, and the reactions to exceptional situations. With the case management approach, the knowledge worker in the center takes control and acts as an orchestrator of available services rather than being a part of an assembly line. This paper presents how data-centric web services can be used to build a service-oriented case management application, and how advanced analytics can leverage historical information to improve the effectiveness of business processes execution."
1046,"In order to improve productivity and quality of the products, there are many efforts to integrate and automate design/analysis processes based on various distributed engineering resources. KIMM (Korea Institute of Machinery and Materials) constructed SOA (Service Oriented Architecture)-based e-Engineering framework for flexible integration and automation of the engineering processes on the Web services environment. However, guaranteeing reliability in the engineering process is one of the new problems that must be solved in the e-Engineering framework. This paper presents two approaches to handle various exceptions that happen in the middle of the execution of the process. One is to introduce recovery activities into the engineering process modeling to handle exceptions caused by failures of the engineering resources; the other is to use log-based recovery techniques to recover from the system exceptions such as network failures or system crashes."
1047,"Building mission critical applications and services (e.g. e-commerce) using process-oriented approaches has had successes and difficulties. These applications automated successfully the important frequent cases such as purchases, but the code needed for handling exceptions such as cancellations and failures tend to grow to disproportionate size and complexity. These difficulties lead to non-automated and expensive solutions such as call centers, which resolve data inconsistency problems manually. In this paper we describe the WED-flow (work, event, and data-flow) approach, which provides transactional recovery through incremental evolution of exception handling, by combining the concepts of advanced transaction models, events, and data states. By carefully recording the detailed data states of each execution step, WED-flow composes backward and forward recovery mechanisms as reusable exception handling services to preserve the consistency of all databases involved in the application with well-defined correctness properties. A practical application of the automated recovery in WED-flow is the real-time recovery of failed cases for mission-critical applications and services."
1048,"Dynamic description logic (DDL) is among the few emerging service composition solutions through logical reasoning. To overcome low efficiency and lacking context-aware support of DDL reasoning, we propose a new DDL-based service composition model, which supports context-based service pre-filtering over DDL reasoning space. The pre-filtering runs under the BPEL workflow and a distributed reasoning algorithm need to reasoning different contexts after pre-filtering."
1049,"In service oriented architecture, workflow discovery and web service composition (WSC) are two major challenges and many works have been carried out in the recent years. In this work, we address the problem of WSC in a multi-cloud environment. All existing methods assume that a feasible solution exists and solves the WSC problem, however when user constraints are hard, a feasible solution might not exist and models fail to return an optimal solution. To address this problem, we propose an Integer Linear programming model where violations in global constraints are permitted but with an associated penalty. The results show that our method performs significantly better than the standard method when user constraints are hard."
1050,"A key research challenge in Web services concerns (semi-) automatic discovery and composition of Web services, in order to construct new Web services with desired properties or capabilities. This talk provides a survey of key developments that work towards this ambitious goal. The fundamental work in this area has centered on three models, each coming with a different approach to the composition problem. The OWL-S model for Web services focuses on how Web services interact with the ""real world,"" represented abstractly using (time-varying) first-order logic predicates and terms. A representative composition result here uses a translation into Petri nets. The ""Roman"" model for services focuses on an abstract notion of ""activities"" (without modeling how they impact the world), and in essence model Web services as finite state automata with transitions labeled by these activities. A powerful composition result is obtained using a reduction to propositional dynamic logic (PDL). The conversation model focuses on messages passed between Web services, and again uses finite state automata to model the internal processing of a service, with transitions labeled by message sends and message reads. A key result here concerns determination of the ""local"" behavior of individual services, if they are required to conform to a given ""global"" behavior. The talk also discusses two ongoing efforts to unify the three models just described. One activity, by the semantic Web services intiative (SWSI), is to develop a semantic Web services ontology (SWSO). This is based on the Process Specification Language (PSL), a first-order ontology for sharing descriptions of manufacturing processes, which recently became an ISO standard."
1051,"In recent years, freelancer economy has been a new normalcy. In the supply-driven freelancer marketplace, people sell their capabilities or labor as service on the internet platform to help others with some particular micro-tasks. As this kind of human service ecosystem is at the fast growth stage, it is inundated with a variety of services whose quality is uneven. Quite often, when facing these services, customers hesitate to make the decision. The root causes of this hesitation are: (1) customers do not know these services well, even the explicit service category and description are provided, (2) customers do not know their own demands well. Most of the time, customers only have a general/fuzzy goal, but have no sense of the requirements in detail. Therefore, this study aims at proposing a human services recommendation method for fuzzy customer requirement. The experimental data of this study is collected from Fiverr.com, which is one prominent supply-driven human services marketplace. By analyzing the transaction data, any details of services, freelancers, customers, and their relations will be extracted to construct a supply-demand relation graph. In this study, customer's fuzzy requirement description will be transferred into a query subgraph, which is the input of an evolved subgraph matching algorithm. This algorithm will help to retrieve the recommendable services (combinations). In addition, a guided Q&amp;A approach is designed to complement customer's fuzzy requirement, so that subgraph matching algorithm can retrieve better results."
1052,"Computerized workflow systems have attracted considerable research interest. More recently, there have been several XML-based languages proposed for specifying and orchestrating business processes, culminating in WS-BPEL. A significant omission from WS-BPEL is the ability to specify authorization information associating users with activities in the business process and authorization constraints on the execution of activities such as separation of duty. In this paper, we address these deficiencies by developing the RBAC-WS-BPEL and BPCL languages. The first of these provides for the specification of authorization information associated with a business process specified in WS-BPEL, while BPCL provides for the articulation of authorization constraints"
1053,"In this paper, we propose a methodology to drive innovation from isolated service islands into the global social service network to connect the islands. First, we propose Linked social service-specific principles based on Linked Data principles for publishing services on the open web as linked social services using our new service model, and then an approach is proposed to enable exploitation of a global social service network, providing Linked social service as a service."
1054,"Big data poses challenges to the technologies required to process data of high volume, velocity, variety, and veracity. Among the challenges, the storage and computing required by big data analytics is usually huge, and as a result big data capabilities are often provisioned in cloud and delivered in the form of Web-based services. Solid-state drive (SSD) is widely used nowadays as an elementary hardware feature in cloud infrastructure for big data services. For example, Amazon Web Service (AWS) offers EC2 instances with SSD storage, and its key-value data store, DynamoDB, is backed up by SSD for superior performance. Compared to hard disk drive (HDD), SSD prevails in both access latency and bandwidth. In the foreseeable future, SSD would be readily available on commodity servers though its capacity would be neither large enough nor cost effective to accommodate big data on its own. Therefore, it is essential to investigate how to efficiently leverage SSD as one layer in a storage hierarchy in addition to HDD. In this paper, we investigate the effectiveness of using SSD in three workloads, namely standalone Hadoop MapReduce jobs, Hive jobs, and HBase queries. Firstly, we device an approach to enable Hadoop Distributed File System (HDFS) having a SSD-HDD storage hierarchy. Secondly, we investigate the IO involved in different phases of Hadoop jobs and design different schemes to place data discriminatively in the aforementioned storage hierarchy. Afterward, the effectiveness of different schemes are evaluated with respect to job run time. Finally, we summarize best practices of data placement for examined workloads in a SSD-HDD storage hierarchy."
1055,"As the Web service technology matures, other computing paradigms such as peer-to-peer gradually adopt the service-oriented approach and are beginning to expose functionality as services. Hence there is a need for integration of these heterogeneous services, for the development of service-oriented applications. The first step to accomplish this is to establish a unified approach for service discovery. In this paper, we briefly present a query language along with its enacting service search engine which is used for effectively discovering Web and P2P services in a unified manner"
1056,We propose an hierarchical state machine (HSM) model for specifying behavioral interfaces of peers participating in a composite Web service. We integrate the HSM model to a design pattern which is supported by a modular verification technique that can 1) statically analyze the properties about global interactions of a composite Web service and 2) check the conformance of the Java implementations of the participant peers to their interfaces. We extend the synchronizability analysis to HSMs to efficiently identify composite Web services whose global interactions can be analyzed with respect to unbounded queues using finite state model checkers. We also discuss automated translation of behavioral interfaces specified as HSMs to BPEL specifications to be published and used by other services.
1057,"The WS-BPEL specification focuses on business processes the activities of which are assumed to be interactions with Web services. However, WS-BPEL processes go beyond the orchestration of activities exposed as Web services. There are cases in which people must be considered as additional participants to the execution of a process. The inclusion of humans, in turn, requires solutions to support the specification and enforcement of authorizations to users for the execution of human activities while enforcing authorization constraints. In this paper, we extend RBAC-WS-BPEL, a role-based authorization framework for WS-BPEL processes with an identity attribute-based role provisioning approach that preserves the privacy of the users who claim the execution of human activities. Such approach is based on the notion of identity records and role provisioning policies, and uses Pedersen commitments, aggregated zero knowledge proof of knowledge, and Oblivious Commitment-Based Envelope protocols to achieve privacy of user identity information."
1058,"Capability specification is key problem for Web service discovery. Conventional one-step process based capability specification has its limitations. This paper proposes an approach for semantic behavior-based capability specification of Web service to stride over the limitations. Meta-level environment ontology is proposed to provide formal and sharable specifications of environment resources in a particular domain. For each environment resource, there is a corresponding hierarchical state machine specifying its dynamic characteristics. Then, effects on the environment resources are modelled with the hierarchical state machines. On the basis of the environment ontology, forest-structured communicating hierarchical state machines (FCHM) are defined and expected to be semantics of capability specification of Web services, which can be derived from the effects that Web services impose on their environments"
1059,"A fundamental problem that confronts QoS-aware service selection and composition is the efficient and timely QoS information obtainment. Current research on this problem usually involves query-based or monitoring-based methods. However, in a dynamic and volatile service oriented Computing (SOC) environment, these solutions suffer some or all of the limitations, such as cost, timeliness guarantee and salability. This paper presents Pat, a P2P based publish/subscribe system to disseminate new revised QoS information. Pat aims at reliable and efficient QoS information dissemination in large-scale SOC environments. It exploits specialized rendezvous points (RP) and a replicas mechanism to reduce the risk of subscriptions loss and consequently improve reliability. A reverse RP ring is designed to quicken subscription delivery and QoS information publication. In addition, an optimization mechanism for composite services is built into Pat, which helps to reduce notification traffic. Simulation results show that Pat is reliable, efficient and scalable."
1060,"Web services are increasingly hosted on peer to peer networks, to facilitate resource sharing and cooperation. In these settings, each peer hosts a set of services which can be invoked by other peers of the network through a service query. In a pure peer to peer network, it might not be possible to maintain directory for publishing the services, rather, the peers need to search for the required service through query forwarding in the network. For such non-directory based peer to peer networks, in this paper, we introduce an efficient and fully decentralized policy-compliant search query routing method for service discovery. Our main goal is to protect the search query from traversing unwanted peers in the network, while achieving service discovery. To this end, we design and develop a policy-driven approach that allows distributed searches through service queries taking into account any security, routing or other functional criteria a peer may have with respect to routing a query. We have developed a prototype of the query protection and search protocol, and tested it on large networks. Our tests demonstrate accuracy and efficient execution times."
1061,"Monitor and control operations demand deep interaction between users and devices, while they require the adoption of high interoperable solutions that only SOA-based Web services can offer. When the access is performed via Internet using Web services calls, the remote invocation time becomes crucial in order to understand if a service can be controlled properly, or the delays introduced by the wire and the serialization/deserialization process are unacceptable. We propose methodologies, based on a 2 factorial analysis and a Gaussian majorization of previous service execution times, which enable the estimation of a generic remote method execution time."
1062,"In the field of service computing, user preferences and service quality may change with time, environment and other factors. A recommendation algorithm that considers both the dynamic characteristics of users and the dynamic quality of services (QoS) is proposed in this paper. On the one hand, this algorithm uses the temporal LDA (Latent Dirichlet Allocation) model to mine dynamic user preferences. On the other hand, it considers the dynamic changes of QoS and focuses on the latest QoS. The service recommendation list is then generated for the user based on dynamic user preferences and dynamic QoS. Experimental results on a real-world dataset show that the proposed algorithm outperforms some classic algorithms and the state-of-the-art algorithms in terms of accuracy, recall and diversity."
1063,"Improving performance of Web services interactions is an important factor to burst the adoption of SOAin mission-critical applications, especially when they deal with large business objects whose transfer time is not negligible. Designing messages dynamic granularity (offloading) is a key challenge for achieving good performances. This requires the server being able to predict the pieces of data actually used by clients in order to send only such data. However, exact prediction is not easy, and consequently lazy interactions are needed to transfer additional data whenever the prediction fails. To preserve semantics, lazy accesses to the results of a Web service interaction need to work on a dedicated copy of the business object stored as application state. Thus, dynamic offloading can experience an overhead due to a prediction failure, which is the sum of round-trip and storage access delays, which could compromise the benefits of the technique. This paper improves our previous work enabling dynamic offloading for both IN and OUT parameters, and analyses how attributes copies impact on the technique, by comparing the overheads introduced by different storage technologies in a real implementation of a Web services framework that extends CXF. More specifically, we quantitatively characterize the execution contexts that make dynamic offloading effective, and the expected accuracy of the predictive strategy to have a gain in term of response time compared to plain services invocations. Finally, the paper introduces the Attribute Loading Delegation technique that enables optimized data-transfers for those applications where data-intensive multiple-interactions take place."
1064,"Workflows have been used to represent a variety of applications that involve coordinating a set of business services or scientific services, which are generally geographically distributed. With the development of cloud computing, a workflow engine can be deployed as a cloud service, responsible for executing customers' workflow instances. In a cloud workflow service, workflow engine components can be placed into different cloud regions. Thus, one challenging problem that arises is how to select the appropriate cloud regions to place the workflow engine components in order to efficiently execute a service workflow instance. Because this is a typical nondeterministic polynomial-time hard (NP-hard) problem, we propose a heuristic algorithm to select the regions where to place workflow engine components in an optimal and efficient way, with the objective of reducing the execution time of the service workflow instance. The experimental results prove that our proposed algorithm has higher performance than other approaches in terms of the solution quality and the running speed."
1065,"Web services composition process needs business rules to regulate the behavior of the partner services. However, designing these rules is time-consuming and error-prone, especially under the condition that current standards barely provide any abstract and high-level guidance. In this spirit, rule analysis and verification for services composition is urgently required to augment its reliability and usability. In this paper, we choose a variant of Description Logics, called ALCO(Q*), as the underlying logic, and provide a formal mapping to transform ECA rules, so that the semantics in the original ECA rules can be captured and are computationally traceable. To this end, we further investigate some important properties for business rules, namely, redundancy, termination and conflict, and propose several sound and complete algorithms to resolve them."
1066,"Verifying Web service composition in a dynamic environment remains one of the most difficult tasks despite the efforts and the previous proposed research works because new services can be composed during the execution step and others can automatically appear, disappear, or be updated. To achieve the Web service composition specification and verification, we introduce a new concept, called dynamic pattern. A dynamic pattern is an extension of a static one. Then, we propose to formalize dynamic Web Service composition in Event-B using dynamic patterns. The resulting model is progressively verified using proofs. We use animator of model (ProB) to detect a variety of problems, such as deadlocks or other unexpected behavior of a model."
1067,"In this paper we present a Smart Push system with feedback enabled flow control suitable for web enabled mobile and IoT devices. Our push architecture incorporates a gateway client and gateway server component. The flow of the web push notifications is controlled so that they are delivered to the device when the user is most likely to open them. We present an overview and implementation details of the Smart Push system with enhanced web push features, and describe the algorithm running on the gateway server to enable the push flow control. This architecture also provides flexibility for users to move seamlessly across different geographies and mobile operating systems like Android and Tizen without any changes at the content provider's applications servers. We also present results of an experiment performed on multiple users to measure the effectiveness of the system to perform flow control of the notifications to ensure that more of them are clicked."
1068,"Network Centric Information Infrastructure (NCII) is a toolkit of prefabricated software components together with a composition framework for integration and experimentation. Typical distributed system applications of a toolkit such as NCH can be applied to air traffic management, shop floor control and in-flight connection to Internet. The composition framework has gone though through three generations of changes in the past five years and it is a good illustration of a paradigm shift from object-oriented architecture, through component-based architecture to service-based architecture."
1069,"Developing robust web services is a difficult task. Field studies show that a large number of web services are deployed with robustness problems (i.e., presenting unexpected behaviors in the presence of invalid inputs). Several techniques for the identification of robustness problems have been proposed in the past. This paper proposes a mechanism that automatically fixes the problems detected. The approach consists of using robustness testing to detect robustness issues and then mitigate those issues by applying inputs verification based on well-defined parameter domains, including domain dependencies between different parameters. This integrated and fully automatable methodology has been used to improve three different implementations of the TPC-App web services. Results show that this tool can be easily used by developers to improve the robustness of web services implementations."
1070,"Business processes are conventionally modeled as monolithic flows that capture the desired business logic. However, developing process flows is challenging. Because a flow specifies what its participants should do, it restricts the autonomy of its participants, thus limiting their ability to exploit opportunities or accommodate exceptions according to their business preferences. We take a dual perspective wherein business processes are modeled as compositions of (instantiated) business protocols. Each business protocol specifies interactions among its partners; each protocol serves a unique business purpose, e.g., processing a payment or shipping an item. Thus, modularizing a monolithic business process via business protocols allows clear separation of concerns for modeling and enacting the process. We develop an approach in which protocols are compiled into local skeletal flows for each participant that can be fleshed out with local business logic as needed. Such flows are naturally distributed but can be enacted using commercial business flow engines. Thus, our protocol-based approach combines the benefit of improved modeling with simplified implementations."
1071,"Pervasive Information System (PIS) provides a new vision of Information System available anytime and anywhere. The users of these systems must evolve in a space of services, in which several services are offered to him. However, PIS should enhance the transparency and efficiency of the system. We believe that a user-centric vision is needed to ensure a transparent access to the frequently changing space of services regardless of how to perform it. In this paper, we propose a new approach of PIS, both context-aware and intentional called IPIS. In this approach, services are proposed in order to satisfy user's intention in a given context. Then, we propose a context-aware intentional service discovery mechanism. Such mechanism is based on an extension of OWL-S taking into account the notion of context and intention. We present in this paper IPIS platform. Then, we detail the proposed service discovery mechanism and present experimental results that demonstrate the advantage of using our proposition."
1072,"The objective of this paper is to demonstrate that sharing the vocabulary for service description enhances the service discovery mechanism. The proposed solution is a distributed architecture for enhanced context-aware web services. The starting point is a motivation scenario in which university students are trying to share a solution about a specific problem in a campus environment. The proposed solution includes an ontology-based context model for describing service vocabulary. This model is shared among users to facilitate the description of their petitions. The Devices Profile for Web Services (DPWS) was integrated in the architecture as a framework for sending, describing and discovering web services. The adopted validation methodology consisted in comparing scenarios with the context ontology as vocabulary source and others that use synonyms from Word net. A series of discrete-event simulations were set up by specifying performance metrics related to the discovery mechanism, control parameters and user behavior models. The results have shown that using the context ontology enhances the discovery ratio as well as the mean discovered services per request."
1073,"Runtime monitoring of Web service compositions has been widely acknowledged as a significant approach to understand and guarantee the quality of services. However, existing runtime monitoring solutions consider only the constraints on the sequence of messages exchanged between partner services and ignore the actual data contents inside the messages. As a result, it is difficult to monitor some dynamic properties such as how message data of interest is processed between different participants. To address this issue, we propose an efficient, non-intrusive online monitoring approach to dynamically analyze data-centric properties for service-oriented applications involving multiple participants. By introducing Par-BCL - a Parametric Behavior Constraint Language for web services - to define monitoring parameters, various data-centric temporal behavior properties for Web services can be specified and monitored. This approach broadens the monitored patterns to include not only message exchange orders, but also the data contents bound to the parameters. To reduce runtime overhead, we statically analyze the monitored properties to generate parameter state machine from the event pattern automata to optimize monitoring. The experiments show that our solution is efficient and promising."
1074,"Dynamic composition of services provides the ability to build complex distributed applications at run time by combining existing services, thus coping with a large variety of complex requirements that cannot be met by individual services alone. However, with the increasing amount of available services that differ in granularity (amount of functionality provided) and qualities, selecting the best combination of services becomes very complex. In response, this paper addresses the challenges of service selection, and makes a twofold contribution. First, a rich representation of compositional planning knowledge is provided, allowing the expression of multiple decompositions of tasks at arbitrary levels of granularity. Second, two distinct search space reduction techniques are introduced, the application of which, prior to performing service selection, results in significant improvement in selection performance in terms of execution time, which is demonstrated via experimental results."
1075,"This paper presents an approach to the runtime management of decentralized service compositions. The goal of the approach is to transparently observe the behavior of peer-to-peer service interaction in order to permit the introduction of adaptive behavior. The design includes a monitoring component that utilizes a choreography model to formulate a global view of the application, and introduce adaptive behavior in a unique way. The design incorporates a mechanism for transforming the choreography to introduce redundancy, and dynamic selection in the interaction."
1076,"Secure multi-party computation (SMC) allows parties to jointly compute a function over their inputs, while keeping every input confidential. It has been extensively applied in privacy-preserving computation, such as privacy-preserving data mining (PPDM), to protect data privacy. However, most SMC-based solutions are ad-hoc. They are proposed for specific applications, and thus cannot be applied to other applications directly. To address this issue, we propose a privacy model DAG (Directed A cyclic Graph) that consists of a set of secure operators (e.g., Multiplication and division). Our DAG model is general -- its operators, if pipelined together, can implement various functions. It is also extendable -- secure operators can be defined to add new features to the model. As an application study, we have applied our DAG to kernel regression. Experiments on datasets of more than 680,000 tuples show that our DAG model is effective and its running time is nearly thrice that of non-privacy setting, where parties directly disclose data."
1077,"Workflow is an important way to mashup reusable software services to create value-added data analytics services. Workflow provenance is core to understand how services and workflows behaved in the past, which knowledge can be used to provide a better recommendation. Existing workflow provenance management systems handle various types of provenance separately. A typical data science exploration scenario, however, calls for an integrated view of provenance and seamless transition among different types of provenance. In this paper, a graph-based, uniform provenance model is proposed to link together design-time and run-time provenance, by combining retrospective provenance, prospective provenance, and evolution provenance. Such a unified provenance model will not only facilitate workflow mining and exploration, but also facilitate workflow interoperability. The model is formalized into colored Petri nets for verification and monitoring management. A SQL-like query language is developed, which supports basic queries, recursive queries, and cross-provenance queries. To verify the effectiveness of our model, A web-based, collaborative workflow prototyping system is developed as a proof-of-concept. Experiments have been conducted to evaluate the effectiveness of the proposed SQL-like graph query against SQL query."
1078,"Business processes realized by services rely on the functionality of these services. Maintaining the interoperability when services evolve independently is therefore an important and challenging problem. If adaptation is to happen automatically, then determining and categorizing changes and their solutions is crucial. In this paper, we make the following contributions: (1) we provide an overview of previously defined mismatches and provide a common representation based on model management operators. (2) We present a categorization of changes with the categories non-effective, solvable and problematic and validate our categorization. (3) We provide solutions for changes that fall under the solvable category in terms of adaptation operators and describe the process of (self-) adaptation."
1079,"Cross-organizational service-based processes are increasingly adopted by different companies when they can not achieve goals on their own. In order to guarantee that all involved partners are informed about errors that may happen in the collaboration, it is necessary to monitor the execution process by continuously observing and checking message exchanges during runtime. This allows a global process tracking and evaluation of process metrics. In this paper, we present an approach for decentralized monitoring of cross-organizational choreographies. We introduce the concept of External Flow Monitoring and define a hierarchical propagation model for exchanging external notifications between the collaborating parties. We also introduce the concept of EFM-view which allows partners to track the state of the choreography beyond their own processes. The collected monitoring data can be further used for the evaluation of global process metrics by expressing and evaluating statistical queries over execution traces."
1080,"Web service composition enables the provision of existing resources on the web without investing in new infrastructure. However, searching an optimal composition solution with both functional and non-functional requirements is a computationally demanding problem: the time and space requirements may be insufferable due to the high number of available services. To alleviate this problem, we propose the application of a skyline operation to reduce the search space and improve the scalability. We design a system to solve the composition problem with two separate processes. The Graphplan approach finds a solution in a short time, the database approach may take longer time to find a solution, but the solution returned by this approach always has fewer redundant services with a better QoS value. Full Solution Indexing using Database (FSIDB) approach pre-computes all services combinations and store them as paths in a database. Partial pre-composing approach chooses ""popular"" paths generated by FSIDB approach and store them in a separate table. If the problem can be solved by these paths, there is no need to search the table with whole paths. We evaluate our approach with a web service challenge dataset."
1081,"Many Web APIs (by which we mean ones using HTTP as the application protocol) do not publish a machine-readable API description (in a language such as WADL or WSDL) but only provide human-readable documentation, usually in HTML. This documentation may be machine-generated, or it may be hand-edited in which case there is the possibility of errors being introduced into the API description. In this paper we present a Web Interface Language (WIfL) vocabulary for API documentation, which is intended to be embedded in HTML using RDFa annotations. We present the semantics of WIfL, including a formal presentation of inheritance and validation. We discuss our WIfL tools, which include a dynamically generated console for interacting with an API's reference implementation, and a validator which can check an API for internal consistency."
1082,"The ever-growing choice in diverse services is making service orchestration variability an essential aspect of a composite web service. Influence of this variation on the Quality of Service (QoS) of a composite service is critical and the focus of our work. In this paper, we present a methodology to first model orchestration variability using a feature diagram (FD). The FD specifies a product line of orchestrations represented as configurations of invoked/rejected atomic services. Second, due to the potentially large set of configurations we employ combinatorial testing techniques to automatically generate configurations covering all valid pair wise interactions between services. Third, we analyze QoS variation for each configuration using probabilistic models of QoS. Using a crisis management system case study we experimentally show that pair wise generation covers all QoS outliers and eliminates analysis of &gt; 75% of all possible configurations. The QoS analysis of the pair wise configurations reveals unsafe/ineffective configurations, helps determine realistic Service Level Agreements (SLAs), and provides valuable feedback to help remodel an orchestration."
1083,"In this paper we discuss a model-based approach to verifying process interactions for coordinated Web service compositions. The approach uses finite state machine representations of Web service orchestrations and assigns semantics to the distributed process interactions. The move towards implementing Web service compositions by multiple interested parties as a form of distributed system architecture motivates the need for supporting compatibility verification of activities and transactions in all the processes. The described approach is supported by a suite of cooperating tools for specification, formal modeling and providing verification results from orchestrated Web service interactions."
1084,In this paper we describe how we encode our temporal aggregates ontology in OWL for Web services on the semantic Web. We also present one example to show how to use the ontology to represent temporal aggregates information.
1085,"Enterprise wide rather than departmental focus differentiates SOA from the earlier legacy technologies like client server and object oriented programming. This enterprise focus not only includes the IT personnel, but also the business personnel into the broader SOA ecosystem. This paper presents lessons learnt from three years of SOA implementation at a large city government in the US from an enterprise wide application perspective"
1086,"Context-awareness and adaptability are highly desirable features for services that are operating in dynamic environments. Recently, a number of approaches have been introduced to support the development of such services. But, validating the varying requirements of these services is still a major challenge. In this paper, we introduce a novel scenario-based approach to address this challenge. First, our approach captures a service's requirements as two sets of scenarios: functional and adaptation. The functional scenarios represent the service's core functionality, while the adaptation scenarios capture the service's runtime adaptation in response to context changes. The service properties that need to hold at runtime are also represented graphically in a form similar to the scenarios. Second, a technique is introduced to enumerate and generate the specifications of a service's variants from its scenarios. The generated variants are then validated against the service properties to ensure their validity. This technique also checks the consistency of the service's adaptation requirements (scenarios). Case studies have shown that with our approach, a small number of service scenarios specified by the software engineer is able to cover a large number of service variants, which are generated and validated automatically."
1087,"Web service is a modern technology commonly used to integrate software projects among different platforms, operating systems or even programming languages. This distributed and heterogeneous nature complicates the testing activity which is, in general, expensive and effort demanding. Adequate and cost effective testing methods are needed for Web services. An extended approach based on XML messages perturbation has been introduced to test pairs of Web services. Perturbation operators produce modified XML messages, which are used as test cases. This paper explores the use of such promising approach by introducing new perturbation operators for SOAP messages and describing a supporting tool, named SMAT-WS. An experimental study was accomplished with this tool. The obtained results allow an evaluation of the perturbation operators regarding cost and efficacy"
1088,"Graph Pattern Matching (GPM) plays a significant role in many real applications, where given a graph pattern Q and a data graph G, computing the set M(Q, G) of matching subgraphs of Q in G. However, many applications like the experts recommendation in social networks, often need to find Top-K matches of a designated node v
<sub>0</sub>
, rather than the entire set M(Q, G). Moreover, the existing GPM method for matching the designated node v
<sub>0</sub>
 does not consider the multiple constraints of the attributes associated with each vertex and each edge which commonly exist in real applications, like the constraints of social contexts for the experts recommendation in contextual social. In this paper, we first propose the Multi-Constrained Top-K Graph Pattern Matching problem (MC-Top-K-GPM), which involves the NP-Complete Multiple Constrained GPM problem. To address the efficiency and effectiveness issues of MC-Top-K-GPM in large-scale social graphs, we propose a novel index, called HB-Tree, which indexes the label and degree of nodes in G and can get candidates of v
<sub>0</sub>
 efficiently. Furthermore, we develop a Multi-Constrained Top-K GPM method, called MTK, which can identify Top-K matches of v
<sub>0</sub>
 effectively and efficiently. Using real-life data, we experimentally verify that MTK outperforms the existing GPM algorithms in both efficiency and effectiveness in solving the MC-TOP-K GPM problem."
1089,"With the advent of IoT (Internet of Things) age, considerable web services are emerging rapidly in service communities, which places a heavy burden on the target users' service selection decisions. In this situation, various techniques, e.g., collaborative filtering (i.e., CF) is introduced in service recommendation to alleviate the service selection burden. However, traditional CF-based service recommendation approaches often assume that the historical user-service quality data is centralized, while neglect the distributed recommendation situation. Generally, distributed service recommendation involves inevitable message communication among different parties and hence, brings challenging efficiency and privacy concerns. In view of this challenge, a novel privacy-preserving distributed service recommendation approach based on Locality-Sensitive Hashing (LSH), i.e., DistSRLSH is put forward in this paper. Through LSH, DistSRLSH can achieve a good tradeoff among service recommendation accuracy, privacy-preservation and efficiency in distributed environment. Finally, through a set of experiments deployed on WS-DREAM dataset, we validate the feasibility of our proposal in handling distributed service recommendation problems."
1090,"To achieve workflow retrieving and compliance checking based on behavior, it is necessary to identify the behavioral profiles of workflow models. Since the execution timing and the occurrence probability of tasks are crucial to choose appropriate workflows, it is significant to conduct exact quantitative analysis on the ordering relations by considering the time constraints and occurrence probabilities of tasks in workflow models. In this paper, the quantitative ordering relations are formally defined and an algorithm is proposed to efficiently compute the ordering relations with time and probability constraints based on complete firing sequences. Experiments show that the algorithm works well."
1091,"There are two approaches to specifying the composition of Web services: orchestration and choreography. Previous works in Web services selection are mostly based on the orchestration model which focuses on the interactions with a single party. However, in many application scenarios, business goals are achieved by a number of pair-wise interactions among a set of Web services, and there does not exist a single entity that is in charge of selecting Web services for all tasks. Each Web service will autonomously perform Web services selection. In such a choreographic environment, we study the kind of information that each Web service should provide to its partner Web services and how each Web service should perform Web service selection so as to maximize the chance of successfully accomplishing a business goal. The proposed approach is evaluated by simulation, and the experimental results show that our proposed method is close to centralized method and better than the other two distributed Web services selection methods."
1092,"With the development of next-generation sequencing (NGS), DNA/RNA sequencing has become cheaper and more efficient. Today, a whole human genome can be sequenced under $1,000, providing opportunities for large-scale bioinformatic analysis on big datasets. However, most of existing bioinformatic analysis tools are programmed for single server based computing platform and not suitable to process such big datasets. As Hadoop MapReduce and Spark are gaining popularity as cluster computing based big data processing platform, more and more bioinformatic applications start to explore cluster computing platform for large scale data analysis. In this paper we present an in-depth experimental study on deploying Spark clusters for high performance bioinformatic short sequence reconstruction. Our experimental results enable us to answer a number of challenging and yet most frequently asked questions regarding efficient management of bioinformatic data analysis services on Spark systems. Example questions include how to best split big dataset into multiple partitions, and how to distribute data partitions and bioinformatic analysis tasks on a Spark cluster for carrying out a high performance distributed analysis job? What types of memory models are effective for bioinformatic data analysis services on a Spark cluster? Why do different bioinformatic data analysis operations exhibit different throughput performance on the same Spark cluster? We conjecture that this experimental study not only demonstrates the feasibility of high performance bioinformatic data analysis on Spark platform, but also will help bioinformatic application developers to make more informed decisions on both design and configuration of Spark Cluster, managing and tuning parameters of Spark runtime system for enhancing the performance of large scale big data analytics."
1094,"Community discovery is a popular way to solve the personal service recommendation problem and has recently attracted more and more attentions of the researchers. The communities are often practically overlapping with each other, thus more and more research focus on the problem of overlapping communities detection. A common drawback of the existing algorithms to this problem is the low efficiency when dealing the large scale network. In this paper, we propose a graph compression based overlapping communities discovery algorithm, which greatly enhances the power of handling large networks even using a single computer. First, a graph compression based social network model, namely agglomerative graph, is introduced, which is a lossless compression to the original network. Then, inspired by the idea of iteration based on the selected seeds, the algorithm expands the selected seeds to the communities by optimizing the proposed community fitness function iteratively. Finally, it merges the communities of high similarity with each other to get the final results. Since the network is lossless compressed, and massive redundant computations are avoided, the results can be exactly obtained in an efficient and effective way. The experiments based on both real and synthetic datasets demonstrate efficiency and effectiveness of the proposal method in detecting overlapping communities over large scale networks."
1095,"OWL-S is an ontology that provides the necessary vocabulary for describing various components of Web services so that automated discovery, composition and invocation of Web services can be made possible. The main component, the process model, describes the interaction protocol between a Web service and its clients. Such protocol includes not only the inputs, outputs, preconditions and results of the service, but also the control flow and data flow within the service. In this paper, we propose an approach to verify various properties of the process model of an OWL-S service. We define a set of mapping rules to translate a process model into a process algebra model and use a dedicated model checker to check the properties of the translated model. We handle not only the control flow of the process model, but also the binding-based data flow. Pre-conditions and results are also included in our approach. As a case study, we use a reasonably complex online book shopping Web service."
1096,"This paper describes an architectural framework adopting the principles of service oriented architecture for deploying unified communications over the Internet. Also discuss the possible ecosystem based on this approach, and the business model that could probably emerge using such a model. In this architecture, we explore the benefits of the Open Services Gateway Initiative (OSGi) framework for providing us with an ideal service execution platform on the servers, with subscriber client devices on the Internet deploying their services dynamically on them. With a cluster of such networks, we form a distributed communication network for different regions of deployment, with an interaction model between them."
1097,"Cloud-of-clouds storage is a recent approach to improve the security and reliability of data storage for online applications. It encrypts and encodes the user data, and disperses the results to multiple clouds. Thus, the data can tolerate cloud failures, while cannot be inferred even when some clouds are compromised. However, efficiency is a well-known challenge to such a paradigm, since its data storing process (also known as the dispersal process) is time-consuming involving encryptions, encoding, and transmissions, posing a barrier to its wide application. How to speed up the dispersal process is yet to be well addressed. We observe that the dispersal process consists of two types of operations: calculation and transmission. We find that they can execute simultaneously. Hence, the process can be optimized with a pipelined architecture. To this end, we propose the pipelined versions of two state-of-the-art cloud-of-clouds storage approaches, i.e., AONT-RS and CAONT-RS. We implement both proposals and release them open-source online. To verify their effectiveness, extensive experiments are conducted on a prototype storage system with real-world traces. The results show that the pipelined architecture can improve the performance of the dispersal process."
1098,"The vision of the semantic Web has brought about new challenges at the intersection of Web research and data management. One fundamental research issue at this intersection is the storage of the resource description framework (RDF) data: the model at the core of the semantic Web. We present a data-centric approach for storage of RDF in relational databases. The intuition behind our approach is that each RDF dataset requires a tailored table schema that achieves efficient query processing by (1) reducing the need for joins in the query plan and (2) keeping null storage below a given threshold. Using a basic structure derived from the RDF data, we propose a two-phase algorithm involving clustering and partitioning. The clustering phase aims to reduce the need for joins in a query. The partitioning phase aims to optimize storage of extra (i.e., null) data in the underlying relational database. Our approach does not assume a particular query workload, relevant for RDF knowledge bases with a large number of ad-hoc queries. Extensive experimental evidence using three publicly available real-world RDF data sets (i.e., DBLP, DBPedia, and Uniprot) shows that our schema creation technique provides superior query processing performance compared to state-of-the art storage approaches. Further, our approach is easily implemented, and complements existing RDF-specific databases."
1099,"Service composition is becoming a prevalent way to building service-oriented business applications (SOBAs). In an open service environment, the profit of composition (PoC) is a primary concern of building such applications. How to improve the PoC is a significant issue in developing SOBAs but was largely overlooked by current research. Particularly, the modeling and prediction of PoC should play a key role to drive the composition process. In this paper, we focus on how to model and predict PoC in SOBAs. We regard the PoC of a composite service as a function of the quality of service (QoS) attributes, defined in the service level agreement (SLA) between the service and its external partners. Based on the PoC prediction approach, we further propose a profit-driven composition methodology to assist enterprises to make more profit in their SOBAs."
1100,"Service Oriented Architectures enable markets of functionally equivalent service providers delivering services at different Quality of Service (QoS) and cost levels. Under these circumstances, there is a need for mechanisms to optimally select service providers at run-time to support a business process execution so that a utility function for the business process is maximized subject to QoS and cost constraints. This is an NP-hard problem. This paper investigates this problem when the utility function is expressed in terms of multiple QoS metrics. An efficient optimal algorithm is presented, which eliminates the need to exhaustively search the state space. This algorithm can be used for small to medium size problems. A very efficient heuristic solution is also presented and scientifically evaluated against the optimal solution on a large set of randomly generated business processes."
1101,"Business Process Management (BPM) software provides visibility into business processes in organizations of all sizes and helps increase process efficiency continuously. However, the time and effort involved in modeling, deploying and executing a business process is tremendous and as a result organizations struggle to agilely adapt business processes to dynamic business requirements. On the other hand, the growing popularity of cloud computing poses opportunities and challenges on how business processes can leverage resource outsourcing and elasticity. In light of the above, this paper presents a business process management platform that assists business analysts lacking necessary programming expertise by automating manual steps and providing guidance and recommendations to quickly and efficiently design, implement, deploy and execute business processes in a hybrid cloud environment."
1102,"Quality of Service (QoS) in orchestrated web services compositions have been well studied with probabilistic and multi-dimensional models. Choreographies that involve message passing among services, on the other hand, require further analysis. In this paper, we begin with the set of QoS domains that may be studied in case of choreographies and the algebraic rules for their composition. As choreographies manage QoS composition in a distributed fashion, techniques to enrich functional specifications with QoS are examined using the model proposed in the CHOReOS project. These are further analyzed with choreographies that may reconfigure due to functional or QoS requirements. Studies on the effects of such reconfiguration on multiple QoS domains can lead to better understanding of optimal runtime configurations along with associated tradeoffs. A goal programming approach is also proposed to choose Pareto optimal solutions with respect to diverse QoS domains."
1104,"We investigate Web service composition as a planning problem and use the input-output parameter relations in order to select the constituent services that make up the composite service. Furthermore, we make use of ontological information between the input-output parameters such that a more specific concept can be used instead of a general concept to make the process more flexible. Our proposed approach is based on constructing a dependency graph including the service parameters and Web services themselves. By using this dependency graph, we perform backward chaining starting to search from the desired output parameters, which is in fact the goal, to the available input parameters. In addition to using semantic information through the search, our approach considers non-functional attributes of the services such as service quality. Considering the quality measures, we find the constituent services by making use of depth first search. After finding the required services, our algorithm generates a plan that shows the execution order of each service."
1105,"Information on the Web is heterogeneous and available in constantly increasing quantities. Consequently, there are numerous, partly redundant data analytics services, each optimized for data with certain characteristics. Often, analytics tasks require multiple services to be pipelined to find a solution, where combinations of exchangeable services for single steps might outperform one-service-predictions. This work proposes a Multi-Agent System (MAS) perception of prior setting, where decentralized agents are considered to manage services, having to coordinate their decisions to find a consensus. We, first, propose a supervised method for service accuracy estimation and, therefore, exploit locality-sensitive features of training data. Given a committee of services managed by agents, we develop coordination strategies to handle conflicting confidences and reduce erroneous predictions due to service correlation. We evaluate our approach with Named Entity Recognition (NER)- and Named Entity Disambiguation (NED) services on text corpora with heterogeneous characteristics (i.e. news articles and tweets). Our empirical results improve the out-of-the-box performance of the original services."
1106,"With the wide adoption of Service-Oriented Architecture (SOA), the number of web accessible services and their compositions is increasing rapidly. Among huge number of services, how to recommend appropriate ones for automatic composition satisfying users' need is challenging. We investigate services and their compositions in Programmable Web which characterize services as APIs and their compositions as mashups. We study the problem of recommending suitable APIs satisfying users' need for mash up creation. To this end, we propose a manifold ranking framework for API recommendation. First, we categorize existing mashups into functionally similar clusters. Then we recommend APIs for each mash up cluster using manifold ranking algorithm which incorporate the relationships between mashups, between APIs and between mashups and APIs. Intuitively, we take three factors into consideration: (1) We recommend APIs that are in functionally similar mashups. (2) We recommend APIs that are popular in the mashups. (3) We recommend APIs that are similar to each other. Finally, we map a user's requirement for mash up creation to a mash up cluster and recommend APIs generated by the algorithm to user. Experiments based on real dataset crawled from Programmble Web demonstrate the effectiveness of the proposed approach in terms of precision, recall, and NDCG."
1107,"The demands of migrating existing on-premises complex enterprise applications to cloud dramatically increase with the wide adoption of cloud computing. A recent research validates the possibility to combine multiple proprietary migration services offered by different vendors together to complete cloud migration. Pattern based service composition has been proven as an appealing approach to accelerate the service composition and ensure the qualities in the Service Oriented Architecture (SOA) domain and can be applied to the cloud migration service composition theoretically. However, current pattern generation approaches are not applicable for the cloud migration due to lack of either existing cloud migration business process knowledge or execution logs. This paper proposes a novel approach to generate cloud migration patterns from a set of service composition solutions. We formalize the pattern generation as a special graph similarity matching problem and present an algorithm to calculate the similarity of these service composition solutions. Patterns are chosen out of the solutions by similarity with designed criteria. The benchmark results and quantitative analysis show that our proposed approach is effective and efficient in pattern generation for cloud migration."
1108,"With an increasing number of Web services on-line, service recommendation becomes an important approach to help user discover suitable services. However, current service recommendation approaches only consider user's historical preferences and functional or non-functional properties of services. The compositional side of services to form mashups is overlooked. However, the service composition information is a valuable information for recommendation since it gives an implicit measure of relatedness of services. In this paper, we recognize the importance of mashups on service recommendation and propose to jointly model historical preference of users on mashups and services, compositional information of services as mashups, and functional information of services and mashups in a single framework. Specifically, we extract topics from functional descriptions of services and model the relations between user, mashup and service and topics as a quadripartite graph. We use a personalized ranking on graph algorithm to learn the proposed model and simultaneously recommend services and mashups for user. We conduct a comprehensive experimental study using real world data from ProgrammbleWeb and results show that our recommendation method outperforms other representative recommendation approaches."
1109,"In this paper, we propose a comprehensive trust management approach for Web services that covers the analysis/modelling of trust relationships and the development of trust management layer in a consistent manner. The specific characteristics of trust relationships in Web services are discussed. We introduce a separated trust management layer for Web services that can hold computing components for trust management tasks. A trust management architecture for Web services is proposed for building up the trust management layer. The proposed trust management architecture for Web services deals with trust requirements, trust evaluation, and trust consumption in Web services under a unified umbrella and it provides a solid foundation upon which may evolve the trust management layer for Web services."
1110,"Quality of Services (QoS) is an important criterion to evaluate Web services recommendation system. Due to factors including various network conditions, QoS values are dynamic and time-varying. In reality, the data is too spare to fit in with traditional time series forecasting model (e.g., ARIMA). To address this crucial challenge, this paper proposes a novel time-aware and sparsity-tolerant QoS values prediction approach based on collaborative filtering. Our approach combines limited historical QoS value with collaborative filtering method to forecast the personalized QoS values. Based on the limited data, our approach firstly forecasts user-service pairs that have historical usage experiences, and then uses CF-based method to predict personalized QoS values. Finally, we combine the results from temporal forecasting with those from CF prediction as the final forecasted QoS values. The extensive experiments show that the proposed approach efficiently improves the forecasting coverage and accuracy."
1111,"We propose a novel approach based on model checking for automated non-linear service composition. Modeling services as interleaved processes, we formulate the service composition problem as verifying a safety property and show that multiple non-linear compositions can be constructed from the counter-example. The state explosion problem is tackled by using service clustering and computing service closures."
1113,"Automatic or assisted workflow composition is a field of intense research for applications to the World Wide Web or to business process modeling. Workflow composition is traditionally addressed in various ways, generally via theorem proving techniques. The originality of this research stems from the observation that building a composite workflow bears strong relationships with finite model search, and that some workflow languages can be defined as constrained object metamodels (van der Aalst et al., 2003). This leads to consider the viability of applying configuration techniques to this problem. Our main contribution is to prove the feasibility of such an approach, with some advantages and drawbacks compared to logical based techniques. We present a constrained object model for workflow composition, based upon a metamodel for workflows and ontologies for processes and data flows. Experimental results are listed for a working implementation that generates complex interleaving composite workflows involving transformations, synchronization and branching constructs."
1114,"The need for flexibility in process-based applications, in particular during their execution, places the demand for enabling adaptability of processes. AOP is considered to be one of the approaches to flexibly switch on and off functionality on per-instance basis in applications during their execution; analogously, this paradigm can be applied in a BPEL environment to enable adaptation of running orchestrations. In the presented approach we strive towards reuse of as much concepts and technology already available in a Web service (WS) environment as possible. We combine standard BPEL, the publish/subscribe paradigm and WS-Policy so that WS operations play the role of aspects with respect to BPEL processes. We present the syntax for such aspects as an extension of the WS-Policy framework. We introduce the architecture of the supporting infrastructure and a prototypical implementation. The approach draws on the combined benefits of service orientation and the AOP paradigm to improve the state-of-the-art techniques for flexibility of service orchestrations in a non-intrusive manner."
1115,"As an emerging interdisciplinary subject which crosscuts business modeling, knowledge management and economic analysis, service engineering is increasingly demanded to take care of various stakeholders' proting goals of the short run vs. long run. We propose to work towards a value driven design based solution through introducing a form of service design patterns: the service value broker (SVB) patterns to shorten the distance from economical analysis to IT implementation. SVB patterns allow us to not only study the value added in terms of functional and business aspects, but also reason about the need for brokerage across various domains."
1116,"We address the problem of 3D medical volume reconstruction using Web services. The use of proposed Web services is motivated by the fact that the problem of 3D medical volume reconstruction requires significant computer resources and human expertise in medical and computer science areas. Web services were implemented as an additional layer to a dataflow framework called data to knowledge. In the collaboration between UIC and NCSA, pre-processed input images at NCSA were made accessible to medical collaborators for registration. Every time medical collaborators inspected images and selected corresponding features for registration, the Web server at NCSA was contacted and the registration processing query was executed using the image to knowledge library of registration methods. Co-registered frames were returned for verification by medical collaborators in a new window. This paper presents 3D volume reconstruction problem requirements, architecture of the developed prototype system and the tradeoffs of our system design."
1117,"Given the dynamic nature of the cloud, resulting from mapping virtual to physical resources, changes in the usage pattern of resources, migration of virtual resources and the dynamic nature of the applications themselves, the bottleneck resource in a given application changes over time. Promptly identifying the bottleneck of cloud application and consequently taking corrective actions (e.g. admission control) are essential requirements for cloud application performance management. The traditional threshold based bottleneck detection technology, which adopts a pre-defined target performance measure (e.g. response time, CPU utilization, etc.), requires a good understanding of the application. It is difficult to identify which performance measures need to be monitored and how to set accurate threshold values for them. The commonly used technique of model-based workload management also faces a big challenge in modeling the highly dynamic, cloud application behavior. In this paper, we propose a self-optimizing application workload management solution for cloud applications which adapts well to the cloud dynamics. It utilizes a target-less bottleneck detection mechanism, without the need to define target thresholds. It also contains a model-free controller for workload management, thus avoiding the complexity of dynamically changing the model as the cloud environment changes. We believe that this is the first time such a design principle to cloud application performance management is introduced. The validity and efficiency of this solution have been verified by a real-case study on an IBM cloud platform, using the RUBiS web application benchmark."
1118,"Mashup is a user-centric approach to create value-added new services by utilizing and recombining existing service components. However, as services become increasingly more spontaneous and prevalent on the Internet, finding suitable services from which to develop a mashup based on users' explicit and implicit requirements remains a daunting task. Several approaches already exist for recommending specific services for users but they are limited to proposing only services with similar functionality. In order to recommend a set of suitable services for a general mashup based on users' functional specifications, a novel social-aware service recommendation approach, where multi-dimensional social relationships among potential users, topics, mashups, and services are described by a coupled matrix model, is proposed in this paper. Accordingly, a factorization algorithm is designed to predict unobserved relationships, and as a result, a comprehensive service recommendation model can be readily constructed. Experimental results for a realistic mashup data set indicate that the proposed approach outperforms other state-of-the-art methods."
1119,"Publish/subscribe (pub/sub) systems are widely used in numerous Internet-Of-Things (IoT) applications such as environment monitoring, supply chain tracing, healthcare, and vehicle networks. In these applications, publishers (e.g. Smart devices, sensors) are continuously generating large volume of data with an extremely high throughput, whereas subscribers are only interested in a small portion of the data. Recently, content-based subscription systems have raised more and more attentions by the researchers where subscribers can specify rules on the content of messages that are composed of many attributes. For example, in traffic monitoring, an operator is only interested in the data within a specified area defined by constraints on latitude and longitude instead of the whole map. In this paper, we present COSS, the first Content-based Subscription Service for IoT with natural multi-tenant support and easy-to-use REST APIs. Moreover, we investigate in the problem of Balanced Rule Engine Partitioning for content-based subscription under the Tenant-Message-Rule (TMR) model. We show the NP-hardness of the problem and design a heuristics to enable COSS to adaptively adjust the message distribution according to the workload history, and to scale on both the high data throughput of IoT workloads and multi-tenant. Extensive experiments show that COSS offers high performance and scalability for content-based subscription in terms of the number of tenants, and the data throughput of the messages."
1120,"While Web services have been widely accepted as a platform-independent services-oriented technology, its performance remains a concern due to the verbosity and inefficiency inherent from using text-based XML. This paper presents a study of Web services performance by evaluating the current implementations of Web services and comparing them with a number of alternative technologies. This study gives a picture of the current Web services performance behaviors and develops a simple performance model that can be used to estimate Web services latencies"
1121,"With more proliferation of services and higher degree of personalization, higher accurate approaches to service recommendation are becoming more and more pivotal. Performance of existing service recommendation approaches is not satisfactory due to the sparseness of available data set or the incomplete information of the global service market, which make it difficult to identify a customer's potential preferences on available services. In this paper, we extract finegrained value features from customer reviews, and identify the personalized distribution of each value features to demonstrate the value preference of a specific customer. Then, a novel recommendation algorithm (VFDSR) is proposed. An algorithm VFMine based on text mining is presented to effectively extract value features from customer reviews. A VFDAnalysis algorithm based on sentiment analysis is employed to identify the value feature distributions. Based on it, VFDSR recommends top-satisfying services to customers. In addition, the value feature distributions are visualized in the form of ""heatmaps"". Comprehensive experiments are conducted on a Yelp dataset and the experimental results show the superiority of our approach."
1122,"Contrary to simple Web content, standard Web services do not offer their clients the possibility to use cached information without the risk that it may be out-of-date. This feature has not been worth its costs in realistic Web service usage scenarios until now. However, its absence may pose restrictions and impede possible benefits in a future scenario, where mediators are both willing and able to effectively minimize the amount of wirelessly transmitted data in the Internet of Services. This paper describes how developments in the Internet of Services start to motivate the automatic enablement of safe (i.e., always up-to-date) client-side caching for Web services. It presents our solution for generically adding this feature to any Web service, and, based on new experiments, reveals the limits beyond which the approach can offer significant benefits."
1123,"To enforce reliability of composite Web Services at run-time, this paper proposes a transaction management model based on Compensation Planning Graph for Web Service composition. First, a web service composition model with transactional properties is introduced. Second, in order to handle transaction when exceptions are occurred, compensation dependency relationships and Compensation Planning Graph is introduced, and an algorithm of automatic generation of compensation dependency relationships and transaction handling based on CPG is proposed. Finally, evaluation methods of QoS of web services and user satisfaction of transaction handling are given. During the execution of composite web services, this approach can guarantee compensation achieving through forward- or backward-compensation. In addition, in this model, a novel concept named Transfer Service is proposed to solve the problem that there are no compensation services or unsuccessful retriable services after many retry times. This model and method can improve self-adjustability and stability of composite services in the course of deployment and execution. Simulations prove that this approach can efficiently guarantee the reliability of composite services at run-time."
1125,"Mashing up Web services and RESTful APIs is a novel programming approach to develop new applications. As the number of available resources is increasing rapidly, to discover potential services or APIs is getting difficult. Therefore, it is vital to relieve mashup developers of the burden of service discovery. In this paper, we propose a probabilistic model to assist mashup creators by recommending a list of APIs that may be used to compose a required mashup given descriptions of the mashup. Specifically, a relational topic model is exploited to characterize the relationship among mashups, APIs and their links. In addition, we incorporate the popularity of APIs to the model and make predictions on the links between mashups and APIs. Moreover, the statistical analysis on a public mashup platform shows the current status of mashup development and the applicability of this study. Experiments on a large service data set confirm the effectiveness of this proposed approach."
1126,"As many software systems have been turned as Web services, the evolutionary changes of Web services are becoming an important issue. To understand the way in which the change affects the services, we must ascertain parts of the system that will be effected by the change and examine them for additional impacts. In this paper, we propose an impact analysis model based on service dependency. In particular, the service dependency graph model, service dependency and the relation matrix are examined. Based on the shift and calculation of the matrix, the dependency and impact of the service evolution can be analyzed and its quantity can be ascertained. Furthermore, we also represent an approach for service change annotation and for service evolution process. Overall, these works provide a foundation for the automatic management, control, and evaluation of service evolution."
1127,"A fundamental issue of human-centered coordinating services in ubiquitous computing is concerned with dynamic service coordination according to users' intentions. How can we coordinate the services to assist the user in receiving a coordinated service to maximize the user's satisfaction in an environment? In order to solve this issue, we have been developing a sort of agent-based coordination framework, called ""location-mediated agent coordination,"" that orchestrates Web services embedded in the real world and Web services on the Internet. In this paper, we show a prototype application of the framework, context-aware information assist services in a museum."
1128,"With the continuous development of cloud services, the types and the number of services have increased significantly. It has become the current focus on how to find a service to meet their needs in cloud market. However in cloud market, the difficult is that how to maximize the benefit of users and make users understand the reasonable pricing of their services. So this paper proposes a method based on the ARIMA (Autoregressive Integrated Moving Average Model) model to predict the future price of the cloud service, and choose the service is used ontology collaborative filtering algorithm. Finally, we take the cloud service of infrastructure as a service (IaaS) as an example to do experiment. The experimental results show our approach can find the optimal service price accuracy. This will make it possible to provide more accuracy service to user."
1129,"Because of load fluctuating, the performance of service-based system(SBS) in the cloud environment may deviate from service level agreement(SLA). In the cloud environment, it is important to dynamically allocate resource for SBS according to the predicted system load, so as to satisfy global SLA constraint and minimize resource cost. By the analysis of complex business logic in SBS and the feature of dynamic resource allocation problem, this paper models the dynamic resource allocation problem as composite optimization problem and proposes the cost optimization oriented dynamic resource allocation model. Then this paper applies genetic algorithm to solve the dynamic resource allocation model so as to improve the resolving efficiency. Finally, the approach proposed in this paper is evaluated and compared with some related algorithms. It reveals very encouraging results in terms of the quality of resource allocation."
1130,"Professional mashups that include complex choreographies, data mediation, and result publishing within Web pages are still affected by implementation and design practices that rely either on very simple models or on low-level scripting and programming skills of developers, thus hampering the use of mashups in business context as rapid solution to immediate problems. Indeed, industrialization of their development is still a hard objective to achieve.We propose a design methodology based on visual models to improve the quality and the productivity of service mashups and presentation of the results, thus increasing their acceptance as professional applications in the business scenario. Existing software engineering methods are combined together in an innovative mix, comprising standard business process modeling languages (namely, BPMN) to describe a high-level view of the mashup orchestration and on WebML (Web Modeling Language) to specify the detailed Web application model, including Web service interactions, hypertext navigation, event management, and rich user interfaces."
1131,"Building a composite application based on Web services has become a real challenge regarding the large and diverse service space nowadays. Especially when considering the various functional and non-functional capabilities that Web services may afford and users may require. In this paper, we propose an approach for facilitating Web service selection according to user requirements. These requirements specify the needed functionality and expected QoS, as well as the composability between each pair of services. The originality of our approach is embodied in the use of Relational Concept Analysis (RCA), an extension of Formal Concept Analysis (FCA). Using RCA, we classify services by their calculated QoS levels and composability modes. We use a real case study of 901 services to show how to accomplish an efficient selection of services satisfying a specified set of functional and non-functional requirements."
1132,"With the development of mobile Internet, mobile service is emerging one after another, and the problem of information overload is becoming ever more serious. As an important tool to alleviate information overload, mobile service recommendation has attracted more and more attention. However, traditional recommendation algorithms always recommend popular services to users, which result into a rich-get-richer problem and become a barrier for the unpopular services to startup and growth. In order to promote the healthy development of the service ecosystem, it is necessary to guarantee the fairness of unpopular services. To address this problem, this paper proposes a fairness-aware mobile service recommendation method (FMSR), which gives a relatively fair recommendation opportunity for unpopular services. FMSR makes a tradeoff between recommendation accuracy and fairness, and can recommend popular services and unpopular services respectively. For unpopular services, we design a fair efficiency function and use combinatorial optimization techniques to achieve recommendations. For popular services, bias matrix factorization is utilized to implement recommendations. Experimental results based on real-world demonstrate that FMSR significantly improve the fairness of mobile service recommendation in the evolving mobile service ecosystem."
1133,"Web services is a fast moving research field with a profound impact to many critical research areas, ranging from software to communication, from server platforms to mobile endpoints. It emerges as a disruptive technology to various enterprises to deliver services, computing, communication and information to their customers and partners. This talk intends to capture some recent advances of Web services and new Web services applications in software-as-a-services, cloud computing, communication, message centric SOAP engine design, and mobile services computing endpoints. In addition to present the technical perspectives and extension potentials in these new developments, I would like to fill the gap between academic research and industry applications through real examples and use cases, with a goal to demonstrate how these technical advances of Web services are applied to industry products and applications, Standards, servers, clients, and new paradigms in software design and testing."
1134,"Organizations of handling personal or sensitive information have the pressure of complying with relevant privacy laws or regulations. Since the laws or regulations are always written with complex legal terms, it is not easy for information system designers to understand precisely such laws and regulations and adopt them directly in their designs. In this paper, we propose the method of formalizing the Australian Privacy Act into executable processes and the method of modeling business in a privacy-aware way. Thus, by executing the processes over the privacy-aware business models, the information system designers can easily check the compliance of their designs with the privacy laws or regulations. In addition, the executable formalization of the Privacy Act makes it more efficient for law enforcement officers to process privacy violation cases. As an example, the clauses NPP 1.3 and NPP 2.1(s) of Australia Privacy Act are formalized and executed over a retailer's privacy-aware business model. The execution shows the same result as the investigation performed by the law enforcement officers."
1135,"In an IoT environment, process analysis becomes more difficult as a process usually spans over a set of autonomous and distributed sensors. This paper consummates our previous service hyperlink model, to encapsulate dependencies among events generated from services. To effectively discover service hyperlinks, we transform the service hyperlink discovery problem into a frequent sequence mining problem. Existing frequent sequence mining algorithms cannot be directly used because they do not take the temporal constraints in event dependencies into consideration. Based on the dataset from a real power plant as well as several synthetic datasets, we do lots of experiments to verify the effectiveness and efficiency of our algorithm."
1137,"Nowadays, system and network resource management software should deal with more and more heterogeneous specific interfaces of different resource. This is a tightly-coupled management model. Recent years, Web Services have become the major technology and architecture of SOA for enterprise applications. Web Services provides a loosely-coupled management model. In this paper, based on the Web Services-based management protocol-WSManagement, we propose the distributed System and Network Resource Management Middleware Model. In this model, every managed IT resource provides the manageability interfaces via WS-Management specification. Furthermore, we utilize WSManagement Java implementation prototype-wiseman and WMI management interface to carry out the scheme implementation and test case work of the novel model, and then analyze the experiment results. At last, we analyze the prospective research direction and challenges in this field."
1138,"In this paper we extend our previous work on soft probabilistic contracts for QoS management, from the particular case of ""response time"", to general QoS parameters. Our study covers composite QoS parameters dealing not only with time aspects but also with quality of data. We also study contract composition (how to derive QoS contracts for an orchestration from the QoS contracts with its called services), and contract monitoring. Our approach supports comprehensive and flexible QoS management within a probabilistic framework."
1139,"Service oriented architecture (SOA) is an architectural paradigm that enables dynamic composition of heterogeneous, independent, multi-vendor business services. A prerequisite for such inter-organizational workflows is the establishment of trustworthiness, which is mostly achieved through non-technical measures such as legislation, and/or social consent that businesses, or organizations simply pledge themselves to adhere. In our viewpoint, a business process can only be trustworthy if the behavior of all services in it is trustworthy. Trusted Computing Group (TCG) has defined an open set of specifications for the establishment of trustworthiness through a hardware root-of-trust. This paper has three objectives: firstly, the behavior of individual services in a business process is formally specified. Secondly, in order to overcome the inherent weaknesses of trust management through software alone, a hardware root of-trust devised by the TCG, is used for the measurement of the behavior of individual services in a business process. Finally, a verification mechanism is detailed through which the trustworthiness of a business process can be verified."
1141,"The accurate description of service semantics plays a crucial role in service discovery, composition and interaction. Most work in this area has been focussed on ontological descriptions, which are searchable and machine-understandable, but do not define service functionality in a verifiable and testable way. Formal specification techniques, having evolved over the past 30 years, can define semantics verifiably and testablly, but they have not yet been applied to service computing because formal specifications are not searchable. There is a huge gap between these two methods of semantics description. In this paper, we bridge the gap. Our technique is to specify services formally in an algebraic specification language and then to extract ontological description as profiles in the language OWL-S, with the associated searchability benefits. We present a prototype tool for performing this transformation and report a case study to demonstrate the feasibility of our approach. The algebraic specification language we use is SOFIA (Service Oriented Formalism in Algebras)."
1142,"Location-based services gained much popularity through providing users with helpful information with respect to their current location. As a recent trend, virtual location-based services consider webpages or sites associated as 'virtual locations' that online users can visit. The presence of links between virtual locations and the corresponding physical locations (e.g., geo-location information of a restaurant linked to its website), allows for novel types of services and applications which constitute virtual location-based services (VLBS). Their success largely depends on the existence of websites referring to physical locations. In this paper, we investigate the usefulness of linking virtual and physical locations. For this, we analyze the presence and distribution of virtual locations, i.e., websites referring to places, for two Irish cities. Using simulated tracks based on a user movement model, we investigate how mobile users move through the Web as virtual space. Our results show that virtual locations are omnipresent in urban areas, and that the situation that a user is close to even several such locations at any time is rather the normal case instead of the exception."
1143,"Automated matching of service descriptions is the key to service discovery and composition. In this paper, we propose an approach for web services discovery and composition. The approach relies on (1) SAWSDL, a simple and generic annotation language, (2) an XML representation of a web service that carries both syntactic (e.g., WSDL) and semantic (e.g., SAWSDL) information, and (3) the reuse of available schema matchers. The approach departs from exiting ones because it does not advocate a specific matchmaking algorithm, and it promotes the combination of different schema matchers, allowing multiple discovery and composition strategies."
1144,"In the wireless world there has been much interest in alternate serialization formats for XML data, mostly driven by the weak capabilities of both devices and networks. However, an alternate serialization format is not easily made compatible with XML security features such as encryption and signing. We consider here ways to integrate an alternate format with security, and present a solution that we see as a viable alternative. In addition to this, we present extensive performance measurements, including ones on a mobile phone, on the effect of an alternate format when using XML-based security. These measurements indicate that, in the wireless world, reducing message sizes is the most pressing concern, and that processing efficiency gains of an alternate format are a much lesser concern"
1145,"The advent of mobile computing devices and development of wireless and ad-hoc networking technologies has led to growth of infrastructure-less environments. Mostly, these environments lie at the edges of Internet i.e. they are disconnected/sparsely connected to rest of the world. In order to exploit the access to such edges of Internet, we propose and experimentally evaluate an interoperability middleware that synergizes P2P technology, message queuing support and a passive distributed UDDI for Web services discovery and invocation."
1146,"With the rapid development of Web 2.0 and its related technologies, more and more end-users start to participate in the development of World Wide Web. Yet, not enough attention from both academics and industries has been paid to allowing them to compose services to assist their daily work. This is mainly due to the lack of a high-level abstraction that defines what concepts, functions, and system components a platform should equip to support end-user service composition. This paper first defines the crucial requirements of end-user friendly service composition tools, and then presents End-User Service Composition Model, a platform-independent meta-model that serves as a basis for the design and development of such an end-user friendly platform based on these requirements. Finally, the paper applies this meta-model to four existing end-user service composition platforms to see if they meet these requirements."
1147,"Representational State Transfer (ReST) architecture provides a set of constraints that drive design decisions towards architectural properties such as interoperability, evolvability and scalability. Designing a ReSTful service API involves finding resources and their relationships, selecting uniform operations for each resource, and defining data formats for them. It is often a non-trivial exercise to refine a functional specification, expressed in terms of arbitrary actions, to a resource-oriented, descriptive state information content. We argue that this process can be described as a series of model transformations, starting from service functionality and gradually refining the phase products until a ReSTful service API is reached. This paper outlines the process phases, transformations and intermediate models based on our experiences in developing ReSTful services and service APIs at Nokia Research Center. The process captures our understanding on how to systematically transform functional specifications into ReSTful Web service interfaces."
1148,"In order for travelers to arrive at their destination comfortably, it is important that they plan their trip carefully. Our motivation in this paper is to free the traveler from tedious planning tasks. Several studies and applications have been proposed to help travelers plan their trip according to explicit preferences, but most ignore implicit preferences. For the first time, to the best of our knowledge, we propose a Personalized and Optimized Door-to-door TRavel Planning two-stage approach named PD-TRP, which combines different kinds of transportation services to provide high quality and cost-effective travel plan recommendations, taking into consideration individual explicit and implicit preferences in real time. In order to recommend a set of suitable plans with explicit attribute values, we propose an efficient heuristic search algorithm to generate the top-K candidate plans, and rank them according to the information entropy of implicit preferences discovered from the travelers' previous travel behaviour. We simulate information on transportation services, crawled from the Internet, on 2000 travelers and their past travel behaviour over the last three years, collected from an online travel company, and extensive experimental results show the two-stage approach PD-TRP can effectively improve the degree of satisfaction of the travelers."
1149,"Web service has emerged as a new solution for enterprise application integration. Automatic service composite algorithm is to build a composite service for user's request or complex business requirement. Most former work uses the dependence among services, however the relationship is often complicated and the composition algorithm is time consuming. This paper proposes a type based service composition algorithm, which can calculate a feasible solution in linear time. We conduct a series of experiments with real dataset and generated data. The experimental results demonstrate the efficiency of proposed method."
1150,"An aggregated privacy-preserving identity verification scheme is proposed for composite Web services. It aggregates multiple component providers' interactions of identity verification to a single one involving the user. Besides, it protects users from privacy disclosure through the adoption of zero-knowledge of proof of knowledge. This approach can dramatically reduce the computation time, independently on the number of identity attributes and component providers."
1151,"With the advent of Web 2.0 application, and the increasing number of browsers and platforms on which the applications can be executed, cross-browser incompatibilities (XBIs) are becoming a serious problem for organizations to develop web-based software. Although many techniques and tools have been proposed to detect cross-browser issues, there still lacks a comprehensive approach to locate the root causes of various cross-browser issues. To address this limitation, this paper proposes X-Diag, an automated technique for debugging XBIs based on our findings from an extensive study of the root causes of XBIs in real-world applications. The characteristic of X-Diag is that it narrows down the root causes of cross-browser issues step-by-step by checking whether such issues are caused by incompatible DOM APIs, CSS properties or Html elements. Our empirical evaluation shows that X-Diag is effective in locating the root causes of cross-browser issues, and can provide useful support to developers for (eventually) eliminate XBIs."
1152,"Service interoperability is a major obstacle in realizing the SOA vision. Interoperability is the capability of multiple, autonomous and heterogeneous systems to use each other's services effectively. It is about the meaningful sharing of functionality and information that leads to the achievement of a common goal. In this paper we identify requirements for semantic and pragmatic interoperability. We further propose a method for assessing whether a composite system meets these requirements"
1153,"To add descriptive ability to knowledge representation (KR) system based on OWL, many extensions to OWL are proposed to augment its expressive power. Many of these extensions take the approach by changing the syntax and semantic of the description logic corresponding to OWL. A new approach is proposed in this paper by using a dynamic concept interpretation method without changing the structure of original description logic. This approach provides a flexible extension mechanism outside the description logic framework, which will not bring any additional logical computational complexity or uncertainty to the original description logic. And various implementations can be realized through this approach for different extension purposes."
1154,"When a composition of Web services is designed, available services are put together to form a defined flow of executions. In a discovery process, a trader proposes available Web services as potential candidates. In a succeeding selection, for each task a trader chooses one candidate to form the optimal composition due to selection criteria. This paper discusses how the selection can consider different quality-of-service (QoS) categories to determine the most suitable candidates for the composition. If more than one category is used for optimisation, a multi-dimensional optimisation problem arises. This mentions similarities to similar combinatorial problems. Then, possible solutions are proposed and their performance is evaluated."
1155,"Users visit web services and compose them to accomplish on-line tasks. Normally, users enter the same information into various web services to finish such tasks. However, repetitively typing the same information into services is unnecessary and decreases the service composition efficiency. In this paper, we propose a context-aware ranking approach to recommend previous user inputs into input parameters and save users from repetitive typing. We develop five different ranking features constructed from various types of information, such as user contexts. We adopt a learning-to-rank approach, a machine learning technology automatically constructing the ranking model, and integrate our ranking features into a state-of-the-art learning-to-rank framework. Our approach learns the information of interactions between input parameters and user inputs to reuse user inputs under different contexts. Through an empirical study on 960 real services, our approach outperforms two baseline approaches on ranking values to input parameters of composed services. Moreover, we observe that textual information affects the ranking most and the contextual information of location matters the most to ranking among various types of contextual data."
1156,"Web services technologies are emerging as a new approach for supporting e-science and e-engineering by providing access to heterogeneous computation resources and integration of distributed scientific applications. In this paper, we propose a Web portal architecture that enables end users to access the distributed application through the Web interface and to compose new tasks by integrating a set of high level Web services provided by portal developers. The portal design demonstrates that flexibility, reusability and interoperability can be provided for both end users and portal developers by using semantic Web services technologies."
1157,"Indoor localization service based on magnetic field attracts increasing attention in the field of mobile computing in recent years. However, most of existing indoor localization methods based on magnetic field usually require the pedestrians to be equipped with a unified localization device, or take the magnetic value as the only characteristic of an indoor fingerprint map. Neither of these two kinds of methods could make sure that the indoor localization is ubiquitous, accurate and robust. Therefore, we propose a novel indoor hybrid fingerprint map and a robust Extended Particle Swarm Optimization Algorithm (EPSO). The proposed indoor hybrid fingerprint map is characterized by the variation trend of magnetic value and visible light intensity, which is suitable for most current smartphones that are usually integrated with the electronic compass and the light sensor. EPSO can correct not only the ""data drifting"" problem of sensors but also the logic error of localization called ""wall crash"". In addition, EPSO is still able to provide a high and robust localization accuracy when the attitude of smartphone is changed. Finally, we conduct a series of experiments and the experimental results show that EPSO is robust and it can achieve an accuracy within 1.7 m in the case of 80% localization errors."
1158,"Dynamic collaborations involve contributed resources across the organisational boundaries that are subjected to different set of policies. The management of such resources for dynamic collaborations including negotiation, validation, instantiation and termination is difficult. Existing approaches for collaborations using Web Services such as WSLA are designed to deal with scenarios involving two parties: a service provider and a service consumer. These approaches do not scale well to multiparty nature of dynamic collaborations. To address this problem, we propose a framework for a language called Web Service Collaborative Context Definition Language for dynamic collaborations. The language itself has been defined using XML Schema and has been implemented in a dynamic collaboration platform."
1159,"The extensibility, flexibility, expressiveness, and platform-neutrality of XML delivers key advantages for interoperability. The interoperability of XML Web services often comes at the price of reduced efficiency of message composition, transfer, and parsing compared to simple binary protocols. This paper presents a high-performance XML parsing and validation technique that is time and space optimal. A schema-specific parsing method is developed that uses a two-stack push-down automaton (PDA) for single-pass parsing and validation without backtracking. The schema validity constraints are packed in a compact parsing table derived from a permutation phrase grammar. This approach reduces both the space and time requirements of XML parsing and validation. By contrast, other XML schema-specific parsing methods trade efficiency for space (larger code and/or data size) or trade space for efficiency (backtracking). Performance results show that the method is significantly faster than traditional validating and non-validating XML parsers."
1160,"In a service-oriented online social network consisting of service providers and consumers as participants, a service consumer can search trustworthy service providers via the social network between them. This requires the evaluation of the trustworthiness of a service provider along a potentially very large number of social trust paths from the service consumer to the service provider. Thus, a challenging problem is how to identify K optimal social trust paths that can yield the K most trustworthy evaluation results based on service consumers' evaluation criteria. In this paper, we first present a complex social network structure and a concept, Quality of Trust (QoT). We then model the K optimal social trust paths selection with multiple end-to-end QoT constraints as the Multiple Constrained K Optimal Paths (MCOP-K) selection problem, which is NP-Complete. For solving this challenging problem, based on Dijkstra's shortest path algorithm and our optimization strategies, we propose a heuristic algorithm H-OSTP-K with the time complexity of O(m+Knlogn). The results of our experiments conducted on a real dataset of online social networks illustrate that H-OSTP-K outperforms existing methods in the quality of identified social trust paths."
1161,"Given the large amount of existing services and the diversified needs nowadays, it is time-consuming for end-users to find appropriate services. To help end-users obtain their desired services, context-aware systems provide a promising way to automatically search and recommend services using a user's context. However, existing context-aware techniques have limited support for dynamic adaption to newly added context types (e.g., location, time and activity). Due to the diversity of user's environment, the available context types may change over time. It is challenging to anticipate a complete set of context types while we design a context aware system. In this paper, we propose a context modeling approach which can dynamically handle various context types and values. More specifically, we use ontologies to enhance the meaning of a user's context values and automatically identify the relations among different context values. Based on the relations among context values, we capture the potential services which the user might need. A case study is conducted to evaluate the effectiveness of our approach. The results show that our approach can use contexts to find users' needs and recommend their desired services with high precision and recall."
1162,"All words are used in books, in which each are organized into a category through a library classification system. Therefore, words used by bloggers are also written in books, which in turn are categorized into specific categories. If a blogger uses a particular word often, we can anticipate that the blogger will have interest areas coinciding with the related book categories. This paper suggests that bloggerspsila interests can be known through extracting keywords from blog entry titles and using book classification schemes. Because there were instances in which the keywords alone did not provide adequate information, the Naver (Korean search engine) related keywords search function was used because of its collective intelligence properties. During the experiment, test subjects picked the blogs and entry titles, which were analyzed using the 'Naver OpenAPI'. The results show that it is possible to know a blogger's interests using blog entry titles and book classifications."
1163,"Quality of service (QoS) support in Web services plays a great role for the success of this emerging technology. In this paper, we present a QoS broker-based architecture for Web services. The main goal of the architecture is to support the client in selecting Web services based on his/her required QoS. To achieve this goal, we propose a two-phase verification technique that is performed by a third party broker. The first phase consists of syntactic and semantic verification of the service interface description including the QoS parameters description. The second phase consists of applying a measurement technique to compute the QoS metrics stated in the service interface and compares their values with the claimed one. This is used to verify the conformity of a Web service from the QoS point of view (QoS testing). A methodological approach to generate QoS test cases, as input to QoS verification is used. We have implemented a prototype that includes the verification and certification components of the broker. We performed experiments to evaluate the importance of verification and certification features in the selection process using real Web services."
1164,"With the number of available Web services is rapidly increasing, how to compose multiple Web services automatically to fulfill a given request has attracted much attention. This paper proposes a dedicated planner named AWSP (Automatic Web Service Planner) toward this problem. Compared with other AI planners for automatic Web service composition, AWSP is characterized by its two different heuristic functions to reduce the search space greatly. A series of experiments based on test sets generated by WSBen show that 1) AWSP performs well even when the scale of the test set expands significantly. 2) AWSP has a smaller search space and performs better when using the backward search strategy than using the forward search strategy, 3) AWSP with the A* heuristic function can get the solution with the shortest invocation path."
1166,"Network latency is one of the most critical factors for the usability of mobile SOA applications. This paper introduces prefetching and caching enhancements for an existing SOA framework for mobile applications to reduce the user perceived latency. Latency reduction is achieved by proactively sending data to the mobile device that could most likely be requested at a later time. This additional data is piggybacked onto responses to actual requests and injected into a client side cache, so that it can be used without an additional connection. The prefetching is done automatically using a sequence prediction algorithm. The benefit of prefetching and caching enhancements were evaluated for different network settings and a reduction of user perceived latency of up to 31% was found in a typical scenario. In contrast to other prefetching solutions, our piggybacking approach also allows to significantly increase battery lifetime of the mobile device."
1167,"For Web services to fulfil their promise in truly heterogeneous environments, the different base operations being included in Web services will need to appear in multiple Web Services with different names as well as different authorization and authentication mechanisms, different service level agreements, and even different formats. We describe service views and rule groups which together provide an adaptation layer at which these changes can be described and enforced. Service views include the ability to offer different views on WS components by altering the WSDL contract. Rule groups provide information directing these changes, as well as other policies to be automatically enforced underneath the services."
1168,"One of the major current trends in service-oriented systems is the emphasis given to the need of introducing runtime adaptation features, so that the system can meet its QoS requirements in a volatile operating environment. In this paper we present the design and implementation of a service broker that supports the QoS-driven runtime adaptation of SOA applications offered as composite services to users. We describe the functionalities provided by the broker components and present their design and implementation according to two different versions we have developed and that are both based on open source products. The components of the first version have been developed in Java as Web services, while the second version takes advantage of OpenESB. Since the broker needs to sustain a traffic of requests generated by several concurrent users, we also present the replicated architectures of the two broker versions. We discuss the design tradeoffs and the lesson we have learned in developing the broker."
1169,"As Web services become more prevalent, the need to ensure their quality increases. This paper explores the criteria of reliability of Web services-oriented systems, and discusses how to design and generate test cases to conduct tests over Web services. A prototype system is constructed to test the effectiveness and efficiency of our algorithms. The preliminary results show that our approach facilitates the testing of services-oriented systems."
1170,"With the steadily growing number of service providers the competition becomes more and more intense. In order to find a distinctive edge over other competitors, automatic service composition can be applied to further adapt to the requirements of the users. Most of the current composition approaches can be categorized as either planning or selection algorithms. The former automatically create workflows and tend to neglect Quality-of-Service (QoS) concerns, drawing compositions useless if user constraints are violated. Selection algorithms on the other hand optimize the QoS but provide no functional flexibility. In this paper we show how the strengths of both approaches can be combined by leveraging common characteristics of service registries. Therefore we utilize a data structure which arranges functionally similar services in clusters and computes the QoS of each cluster. Our planning tool composes workflows consisting of these clusters, taking the QoS of the clusters into account. This way, the utility in general and the reliability of the composed workflows are significantly increased. We prove the feasibility and the benefits of our functional clustering in our detailed evaluation."
1172,"The explosive growth of Internet Web services has intensified the study of Service-oriented Computing (SOC). The recommendation of a Web service has become extremely common. Current approaches utilize the Quality of Service (QoS), which is based on the performance of the Web service. However, acquiring average QoS values is impractical because these values are susceptible to the uncertain Internet environment. Additionally, the performance information of a Web service does not provide the discriminative power to the user when selecting identical service candidates. In this paper, we propose Creditability of a Web service (CoS) to recommend a credible Web service. A credible Web service satisfies three features: popularity, influence and authority. We measure CoS by aggregating Web services together to form a Web service society. Then, we capture the social information for the Web service and calculate the behavior of the Web services. We use about 30,000 Web services from Programmable Web, WSDream and Seekda in the experiments. Our approach effectively recommends credible Web services and displays a better discriminative power."
1173,"The initial specifications of Web services cope with the issues of service publishing and service discovery but not with the issue of service selection. Service discovery, handled by UDDI and WSDL, alone is not sufficient to find the most appropriate server that can deliver customers' required quality of service (QoS). In this paper, we consider a broker-based approach to provide QoS support in Web services and deal with the selection issue. The broker may implement various selection policies that can range from static policies to dynamic ones, which take into account the current state of servers. Besides, servers may deliver different levels of service to their customers. We model this QoS broker-based scheme by a multi-class queuing model and we study its performance with a probabilistic splitting policy for server selection."
1174,"The quality of service-oriented system relies heavily on the third-party service. Such reliance would result in many uncertainties, in consideration of the complex and changeable network environment. Hence, effective runtime monitoring technique is required by service-oriented system. Several monitoring approaches have been proposed. However, all of these approaches do not consider the influences of environmental factors such as the position of server and users, and the load at runtime. Ignoring these influences, which exist among monitoring process, may cause wrong monitoring results. In order to solve this problem, this paper proposes a novel QoS monitoring approach sensitive to environmental factors called wBSRM (weighted Bayesian Runtime Monitoring) based on weighted naive Bayesian and TF-IDF (Term Frequency-Inverse Document Frequency). The proposed approach measures influence of environmental factor by TF-IDF algorithm and then constructs weighted naïve Bayesian classifier by learning part of samples to classify monitoring results. Experiments are conducted based on both public network data set and randomly generated data set. The experimental results demonstrate that our approach is better than previous approaches."
1175,"This paper presents a novel approach to expedite a service discovery process. Since current service discovery approaches do not assume any preprocessing when service providers publish services into a service registry, service composition occurs at discovery time. Also, easily reusing a composed service for future discovery has not been considered. In this paper, instead of considering a service registry as a set of published services, we build a service knowledge base at publication time, which we call a service community. The service community consists of a set of service populations and their relationships in composition. A service population is a set of services in which the input and output parameters of all services are semantically equivalent. These pre-composed services expedite the on demand service discovery process. Also a newly composed service is injected into the service community, and easily reused for future discoveries"
1176,"Web-service-related techniques have become popular to improve system integration and interaction. In distributed and dynamic environment, Web services' availability has been regarded as one of the key properties for (critical) service-oriented applications. Quality of Service (QoS), including availability, has been regarded by IEEE as a user-perceived property. However, based on our investigation of monitoring invocation records of real Web services, existing availability metrics, which were proposed in traditional domains, have not addressed the ""user-perceived'' characteristics. Based on analyzing the limitations of the existing availability metrics, we propose a status-based user-perceived service availability metric and a corresponding estimation approach. Experiments on monitoring and analyzing the invocation records of real services demonstrate that the new metric and the corresponding estimation approach could lead to a feasible estimation on Web services' availability from the user side."
1177,"Extending the functionality of Web services without invalidating the code of existing clients has been a challenging task. This paper proposes a new interface design technique called ""Generic Web Services"" that preserves a service's backwards compatibility while enriching its functionality. This is accomplished by applying interface relaxation technique - shifting the semantics of a service's API from design time to runtime by using identity parameters. The paper gives an example illustrating the idea on a Web service from SAP Enterprise Services Workplace."
1178,This paper describes an approach to enterprise application integration (EAI) using extensible Web services. The approach is demonstrated by building a real-world application for EAI in the financial services domain. Business drivers for and approaches to EAI are presented first. The manifestation of Web services in general and their role in EAI are discussed next. Financial services domain characteristics are presented. Business drivers that entail a strong need for functional extensibility in the financial services domain are described. Our proposed architecture for EAI which addresses functional extensibility is described. This architecture is based on the notion of extensible Web services. We then present our implementation of the architecture and practical challenges encountered in EAI. A brief discussion of how our work relates to the current research in service-oriented computing (SOC) and semantic Web concludes the paper.
1179,"Scientific workflow systems facilitate scientific experiments by integrating and coordinating geographically distributed data and algorithmic services in a loosely coupled manner. Most scientific workflow-engines use centralized coordination as the choice of approach for executing workflows, requiring the coordinator (i.e., workflow-engine) to send and receive all input and output data of component services. Such indirect data communication between the component services increases the data-traffic of the coordinator and weakens the performance of the workflow. To optimize this, we propose an approach where data-flow is dynamically delegated from the coordinator to the component services, with direct transportation of data between the component services."
1180,"Web service composition is the art of combining multiple platform independent and modular pieces of software with varied configurations, to achieve an efficient solution for a complex business process. Though, web service composition has been an important area of research in the last decade, however the issue is expected to exacerbate in the 'Future Internet', as it is expected to house billions of services, with thousands offering the same functionality. In this context, maintaining an operational and solid set of Web service configurations will be a challenging task. Moreover, a centralized solution for Web service composition in the Future Internet scenario would be another issue. In this paper, we present a technique inspired from ElectroMagnetism in physics to create an environment which facilitates the selection of a service from a set of similar services. The proposed model achieves service composition in a decentralized environment without involving a centralized orchestrator. To validate the proposed technique in-silico experiments were conducted whose results demonstrate good performance in terms of completion time and load balancing. Further, the proposed technique is validated in-house (within our Institute's Intranet and with real users) by deploying real Web services on decentralized nodes. The results obtained via simulation are verified through a prototype based on the proposed model. We present, discuss and compare the effectiveness of the proposed work in the results section."
1181,"Web-services are supported by a complex software infrastructure that must ensure high performance and availability to the client applications. Web services industry holds a well established platform for performance benchmarking (e.g., TPC-App and SPEC jAppServer2004 benchmarks). In addition, several studies have been published recently by main vendors focusing web services performance. However, as peak performance evaluation has been the main focus, the characterization of the impact of faults in such systems has been largely disregarded. This paper proposes an approach for the evaluation and comparison of performance and recovery time in web services infrastructures. This approach is based on fault injection and is illustrated through a concrete example of benchmarking three alternative software solutions for web services deployment."
1182,Provides a listing of current committee members.
1183,"WSLAs can be viewed as describing the service aspect of Web services. By their nature, Web services are distributed. Therefore, integrating support code into a Web service application is potentially costly and error prone. Viewed from this AOP perspective, then, we present a method for integrating WSLAs into code generation using the AXpect weaver, the AOP technology for Infopipes. This helps to localize the code physically and therefore increase the eventual maintainability and enhance the reuse of the WSLA code. We then illustrate the weavers capability by using a WSLA document to codify constraints and metrics for a streaming image application that requires CPU resource monitoring."
1184,"The run-time monitoring of Web service compositions has been widely acknowledged as a significant and challenging problem. In this paper, we propose a novel solution to the problem of monitoring Web services implemented in BPEL. We devise an architecture that clearly separates the business logic of a Web service from its monitoring functionality. The architecture supports both ""instance monitors"" that deal with the execution of a single instance of BPEL process, as well as ""class monitors"" that report aggregated information about all the instances of a BPEL process. We also define a language for the specification of instance and class monitors. The language allows for specifying boolean, statistic, and time-related properties. Finally, we devise a technique for the automatic translation of all these kinds of monitors to Java programs"
1185,"The rapid growth of Web applications has prompted increasing interest in the area of composite Web services that involve several service providers. The potential for such composite Web services can be realized only if consumer privacy concerns are satisfactorily addressed. In this paper, we propose a framework that addresses consumer privacy concerns in the context of highly customizable composite Web services. Our approach involves service producers exchanging their terms-of-use with consumers in the form of ""models"". Our framework provides automated techniques for checking these models at the consumer site for compliance of consumer privacy policies. In the event of a policy violation, our framework supports automatic generation of ""obligations"" that the consumer generates for the composite service. These obligations are automatically enforced through a dynamic program analysis approach on the Web service composition code. We illustrate our approach with the implementation of two example services"
1186,"Efficient and accurate discovery of user desired Web services is a key component for achieving the full potential of service computing. However, service discovery is a non-trivial task considering the large and fast growing service space. Meanwhile, Web services are typically autonomous and a priori unknown. This further complicates the service discovery problem. We propose a service community learning algorithm that can generate homogeneous communities from the heterogeneous service space. This can greatly facilitate the service discovery process as the users only need to search within their desired service communities. A key ingredient of the community learning algorithm is a co-clustering scheme that leverages the duality relationship between services and operations. Experimental results on both synthetic and real Web services demonstrate the effectiveness of the proposed service community learning algorithm."
1187,"Nowadays, although there exists many ridesharing services and dynamic matching algorithms for passengers and drivers, there is no service or algorithm that can balance the benefit of passengers and drivers while taking their time and cost constraints into consideration. In this paper, we try to solve the dynamic ridesharing problem by considering all above factors for all the participants. To this end, we present URoad, an efficient algorithm for large-scale dynamic ridesharing service, where a new price cost model is carefully designed to make up for the shortcomings of existing algorithms, and in the meantime a corresponding efficient matching algorithm is proposed to satisfy both the time and cost constraints of passengers and drivers. Specifically, for a given passenger, URoad will find out the optimal driver who can satisfy all the constraints of the passenger and the driver with the minimum detour distance. We design a series of data structures to speed up URoad for large scale ridesharing service application, e.g., Time Index, Grid Index and Greedy Strategy. Through extensive experiments, we prove that URoad can find the optimal driver for a given passenger from more than one hundred thousand drivers within 0.5 second in average."
1188,"Recently, many efforts have been devoted to explore the integration of Internet of Things (IoT) and Service-Oriented Computing (SOC). These works allow the real-world devices to provide their functionality as web services. However, two important issues, unreliable service providing and resource constraints, make the modeling and analysis of service composition in IoT a big challenge. In this paper, we propose a probabilistic approach to formally describe and analyze the reliability and cost-related properties of the service composition in IoT. First, a service composition in IoT is modeled as a finite state machine (FSM) which focuses on the functional part. Then, we extend this FSM model to a Markov Decision Process (MDP), which can specify the reliability of service operations. Furthermore, we extend MDP with cost structure, which can represent the different service quality attributes for each operation, such as energy consumption, communication cost, etc. The desirable quality properties of the service composition are specified by a probabilistic extension of temporal logic PCTL. We adopt a well-established probabilistic model checker PRISM to verify and analyze those properties of our service composition models."
1189,"Web service compositions have attracted considerable efforts in the context of supporting enterprise application integrations. For a composite service, in addition to its functional requirements, QoS requirements are important and deserve a special attention. The central question to a QoS composition problem is how to compose a service from different subcomponent services so that its overall QoS can satisfy certain requirements. In this paper, we propose an agent-based method using fuzzy distributed constraint satisfaction problem (fuzzy DisCSP) techniques to solve this problem. We show that by using the composition structures, local constraints can be constructed and used with DisCSP. We also present an a new algorithm called the fuzzy constraint satisfaction algorithm for distributed environment (FADE) to solve the problem and discuss our experiment in building a prototypical system to prove the feasibility of our approach"
1190,"With the wide availability of products and services through popular e-commerce platforms and dozens of similar offerings to choose from, there is a need to accurately assess and evaluate the quality of offerings. Several studies have shown that consumer feedback is an important source of information. This paper presents: (a) consumer Rating as a Service (RaaS) -- a building block service that can be used to add the consumer feedback lifecycle feature in the development of e-commerce platforms, (b) an approach to evaluate the quality of composite offerings based on the aggregation of consumer ratings using the composition structure and component relationships. Benefits of the proposed service include reduced development effort, shorter delivery time and a fine-grained aggregation of consumer ratings for composite offerings even with limited ratings."
1191,"This paper outlines the design and implementation of WSMQ, which is a message-oriented middleware specifically designed to enhance the reliability of Web services. Highlights of this application feature fault tolerance of Web services communication, Quality of Services including authentication and prioritization, security enhancement and performance improvements in Web services over the existing architecture. The implementation of these features aims to address the existing issues surrounding Web services, and further its advancement towards a new standard for distributed application development."
1193,"Modeling and simulation permeate all areas of business, science and engineering. To promote the interoperability and reusability of simulation applications and link geographically dispersed simulation components, distributed simulation was introduced. While the high level architecture (HLA) is the IEEE standard for distributed simulation, a run time infrastructure (RTI) provides the actual implementation of the HLA. With increased size and complexity of simulation applications, large amounts of distributed computational and data resources are required. The Grid provides a flexible, secure and coordinated resource sharing environment which can facilitate distributed simulation execution. In this paper, we propose a service oriented HLA RTI (SOHR) framework which provides the functionalities of an RTI as Grid services and enables large scale distributed simulations to be conducted on a heterogeneous Grid environment. The various services in SOHR can be dynamically deployed, discovered and undeployed, leading to a scalable distributed simulation environment. While the communications between simulators are through Grid service invocations, the standard HLA interface is provided as a library to increase simulator reusability and interoperability. A subset of HLA specifications was implemented in a SOHR prototype based on GT4 and the experimental results have verified the feasibility of SOHR."
1194,"BPEL (Business Process Execution Language) has become the standard for specifying and executing workflow specifications for Web service composition invocation. A major weakness of BPEL is the lack of so-called ""human workflow"" support. The BPEL4People specification tries to amend this by adding human task support to BPEL. In this paper, we propose a formal model of BPEL4People using the CSP process algebra, and discuss some issues we found through analyzing the model. Although based on BPEL4People, this is a general work, and can also be viewed as a formal model of human workflow."
1195,"WSBPEL opens up the possibility of applying a range of formal techniques to the verification of Web service behaviors from two points of view: constraints between activities within the same process and dependencies between activities of different processes. In a previous work, we have described an approach for the verification of Web service compositions defined by a set of BPEL processes. The key aspect of such a verification task is the model adopted for representing the communications among the services participating to the composition. In this paper, we propose to extend this approach to handle dependencies between activities of different process orchestrations through message exchanges. Our aim is to enable supporting models of service choreography with multiple interacting Web services compositions, from the perspective of a collaborative distributed composition development environment. The process of behavior analysis moves from a single local process to that of modelling and analyzing the behavior of multiple processes across composition domains."
1196,"Increased trade and globalization has created an increasing need for the dynamic formulation and integration of cross-enterprise collaborative business processes (cBP's). However, current systems and methodologies, being static in nature, are unable to dynamically formulate cBP's based on business goals and selection criteria. Much of this stems from the current inability to bridge high level strategic business goals to low-level operational tasks, and the inability to dynamically decompose compound business process tasks into primitive operational tasks for direct Web service execution.In this paper, we demonstrate how the concepts from hierarchical task network (HTN) planning are feasible for dynamically creating cBP task sequences ideal for direct Web service execution. We also establish the rationale behind modeling business-to-business (B2B) collaboration tasks as hierarchical Web ontologies. To demonstrate the achievability of dynamic cBP formulation, we developed the genesis methodology, which consists of (1) business-OWL (BOWL) - a B2B hierarchical task Web ontology, and (2) the genesis algorithm - an extension of the hierarchical task network (HTN) planning algorithm to handle business criteria and control flows commonly found in business processes."
1197,"As a communication mean, personal portal which is an entrance where each user can acquire his/her interested information on the network, is often used. We have problems on the existing personal portal described as follows. First, the program to realize the personal portal which is located on a specific Web server, becomes large-scaled and complicated. Because Web pages as personal portal depend on that program, those Web pages can't always be useful for each user equally. Therefore, to solve these problems, we developed two kinds of new Web service which are realized on the network introducing DACS(destination addressing control system) scheme, which is the network management scheme by communication control of client computers. By use of these Web services, each user can create the customized personal portal, which displays the information by a user unit, for using in actual network easily and freely for oneself. In this paper, to deal with information not only by a user unit, but also by a group unit and by all users unit, DACS Web service is proposed. DACS Web service is realized by extending two kinds of functions of Web service respectively and integrating these functions. As the result, each user can create the customized Web page as personal portal, which will be often useful in actual network."
1198,"In recent years, the Web services composition has received much attention. By Web services composition, we mean taking advantage of currently existing Web services to provide a new service that does not exist on the repository. In this paper, we propose a new system called PSR for Web services composition search using a relational database. We also propose algorithms for pre-computing Web services composition using joins and indices. We demonstrated that our pre-computing Web services composition approach in RDBMS yields lower execution time and good scalability when handling a large number of Web services and user queries."
1199,"For a Web service composition to satisfy a user's needs, it must not only provide the desired functionality, but also have nonfunctional properties (e.g., reliability, availability, cost) that are acceptable to the user. In the recent past, several techniques have been developed and deployed to identify a composite service that conforms to the functional requirements and is also optimal with respect to the user-defined preferences over non-functional properties. However, these composition techniques are limited to using one formalism for specifying the required functionality, in short, the existing techniques cannot identify optimal (w.r.t. non-functional properties) composite services that are required to satisfy functional requirements described in multiple formalisms. We have previously proposed a meta-framework for service composition that involves decomposing the required functionality into a boolean combination of atomic requirements, which are expressed using different formalisms. This meta-framework supports the use of multiple formalisms and their corresponding composition algorithms within a single scenario. In this paper, we integrate support for unconditional preferences over nonfunctional requirements into this composition meta-framework. We show that for a large class of problems, local selection of preferred service(s) can yield the most preferred composite service that satisfies the desired functional requirements."
1200,"With the rapidly increasing number of services, there is an urgent demand for service recommendation algorithms that help to automatically create mashups. However, most traditional recommendation algorithms rely on the original service descriptions given by service providers. It is detrimental to the recommendation performance because original service descriptions often lack comprehensiveness and pertinence in describing possible application scenarios, let alone the possible language gap existing between service providers and mashup developers. To solve the above issues, a novel method of Targeted Reconstructing Service Descriptions (TRSD) for a specific mashup query is proposed, resorting to the valuable information hidden in mashup descriptions. TRSD aims at introducing mashup descriptions into service descriptions by analyzing the similarity between existing mashups and the specific query, while leveraging service system structure information. Benefit from this approach, missing application scenarios in original service descriptions, query-specific application scenario information, mashup developers' language habits, and service system structure information are all integrated into the reconstructed service descriptions. Based on the reconstructed service description by TRSD, a new service recommendation strategy is developed. Comprehensive experiments on the real-world data set from ProgrammableWeb.com show that the overall MAP of the proposed TRSD model is 6.5% better than the state-of-the-art methods."
1201,"In this paper, we present a novel deployment-time binding selection framework for Web services to improve the performance. Using the information about target environments, we determine the best binding based on the availability and the accessibility of a service, and the performance characteristics of the bindings in a target environment. We have implemented the proposed mechanism as part of Eclipse-based development tools. We present an extensive performance evaluation of our methodology using benchmarks that we have created following public Web service interfaces, and emulating several e-business applications including a large scale legacy transaction processing system that runs on a mainframe."
1202,"Increasingly, distributed systems are being constructed by composing a number of components, often legacy applications exposed using Web service interfaces. There are a number of architectural configurations or distribution patterns, which express how such systems are to be deployed. However, the amount of code required to realise these distribution patterns is considerable. Here, we propose a novel model driven architecture using UML 2.0, which takes existing Web service interfaces as its input and generates an executable Web service composition, based on a distribution pattern chosen by the software architect"
1203,"Composing business processes from individual services can be viewed as a planning problem in which a planner determines the execution orders of services in a process. Most existing Web service composition research considers connecting Web services into a business process. We argue that most existing Web services are informative Web services that are not the actual business services, but give the parameters of their correspondent business services. The planning problem is not only to select the proper business services, but also to determine the parameters of the business services which affect the ordering of the business services. Furthermore, it is not possible to extract all information from informative Web services through queries. The planner has to work with the problem space that is not fully enumerable. This paper presents a method to optimize planning results with incompletely observed problem space. Genetic algorithms (GA) help to navigate the incompletely observed problem space. At each loop of GA, Web service data are queried and a new sub problem space is built. The planner works with the sub problem space and calculates all feasible plans. The plans are evaluated by GA in fitness function and the best plans are kept for the next loop of GA. The fitness function of GA reflects domain-dependent user preferences. The selected final plan is an optimized feasible plan though global optimization is not guaranteed"
1204,"Dynamic VM memory management via the balloon driver is a common strategy to manage the memory resources of VMs under changing workloads. However, current approaches rely on kernel instrumentation to estimate the VM working set size, which usually result in high run-time overhead. Thus system administrators have to tradeoff between the estimation accuracy and the system performance. This paper presents iBalloon, a light-weight, accurate and transparent prediction based mechanism to enable more customizable and efficient ballooning policies for rebalancing memory resources among VMs. Experiment results from well known benchmarks such as Dacapo and SPECjvm show that iBalloon is able to quickly react to the VM memory demands, provide up to 54% performance speedup for memory intensive applications running in the VMs, while incurring less than 5% CPU overhead on the host machine as well as the VMs."
1205,"With the rapid proliferation of Web services as the medium of choice to securely publish application services beyond the firewall, the importance of accurate, yet flexible matchmaking of similar services gains importance both for the human user and for dynamic composition engines . In this paper, we present a novel approach that utilizes the case based reasoning methodology for modelling dynamic Web service discovery and matchmaking. Our framework considers Web services execution experiences in the decision making process and is highly adaptable to the service requester constraints. The framework also utilises OWL semantic descriptions extensively for implementing both the components of the CBR engine and the matchmaking profile of the Web services"
1206,"The popularity of smartphones and tablet computers in recent years makes mobile apps burst. Mobile apps have become the main consumers of the Internet-based services. Compared to traditional applications in the desktop computing era, mobile devices with their apps bring new opportunities and challenges to service computing community, in various aspects like service publication, discovery, interaction, composition, et al. In this paper, we propose a novel data-driven, content-based mobile apps composition approach, called MashDroid, by leveraging a novel In-App Search mechanism, i.e., Discovering relevant services for the data and content in apps. Rather than existing techniques that usually integrate fixed Web services, our approach relies on the dynamic service discovery and flexible data exchange between several apps. The unique feature of our approach is enabling the data communication channel between apps by the content index services provided by a leading Android appstore, Wandoujia, which now has over 1,000,000 apps and 200 million users. We employ the In-App Search mechanism to define a Restful-style app model and resource-oriented app description model. Based on the models, we design a framework for dynamically discovering relevant apps that could be composed with current app's contexts. We implement a prototype to demonstrate our approach."
1207,This paper explores the use of rationale for understanding the context of reputation information so as to facilitate the exchange and transfer of reputation information across distributed heterogeneous reputation systems for selection of the best service for a particular user's requirements.
1208,"Semantic Web services have been envisioned in the form of different conceptual models like OWL-S, WSMO and WSDL-S. Web services execution environment (WSMX) is our reference implementation for semantic Web service conceptual model, WSMO. In this paper, we have investigated the communication problems that semantic Web services systems are facing. We have proposed how to overcome these problems by using triple space computing as an extended space-based computing paradigm for semantic Web data. We have proposed the usage of triple space computing to improve the internal communication and coordination during the process of execution of semantic Web services. We also have presented and analyzed the experimental results to demonstrate the effectiveness of our solution."
1209,"Identity federation denotes a concept for the controlled sharing of user authentication and user attributes between independent trust domains. Using WS-Federation, service providers and identity providers can set up a Circle of Trust, a so called federation, in which each member is willing to trust on assertions made by another partner. However, if a member has to rely on information received from a foreign source, the need for assurance that the information is correct is a natural requirement prior to using it. Identity assurance frameworks exist that can be used to assess the trustworthiness of identity providers. The result of this assessment is a level of trust, that can be assigned to an identity provider. However, existing approaches for evaluating identity assurance do not allow to define trust levels for individual attributes. In our trust model, we consider both: (a) trust in an identity provider as the issuer of assertions and (b) trust in single attributes that an identity provider manages. In this paper, we show how our approach that we implemented in a logic-based framework can be used in web service scenarios to provide trust information on the level of identity attributes, especially about the verification process, and to match trust requirements of attributes during request processing."
1210,"Due to a rapidly increasing volume of information and autonomous services, domain experts spend considerable time and effort in correlating and integrating these resources into meaningful business processes. In this paper we present IRIS, a framework that effectively supports users in creating service compositions with the special focus on interoperability. We illustrate our work by means of a scientific business process from the area of life science informatics. For the sake of adaptability and reusability our approach relies on a component-based mediator model. The problem of mediator discovery and composition is addressed by an ontology-based registry approach."
1211,"Cloud applications usually face challenges from the dynamic changing environment, growing number of users and large amount of concurrency accesses etc. A self-management based on resource adjustment is emerging to deal with these problems. To achieve self-management, it is the key problem to make resource adjustment decisions adaptively. There are two types of decision making in self-management: static and dynamic. Static method is not suitable for cloud environments. And the existing dynamic methods (i.e. intelligence optimization algorithm-based method and off-line model-based method) are often inefficient or not adaptive to the variation of environment. In this paper, we propose a reinforcement learning-based approach to dynamic decision making in resource adjustment-based self-management. This approach enables a cloud application to guarantee its performance by learning the results of its behavior and by dynamically changing its plans based on the learning in the presence of environmental changes. The process of our decision making approach for the self-managed cloud applications is also presented in this paper. Experimental results using a prototype framework in the context of a SBS application demonstrate the effectiveness of this approach."
1212,"Service brokering has an increasingly prominent role in bridging the gap between business requirement and technology enablement. We propose the concept of service value brokering (SVB) to fulfil the possible missing linkages between business and technology layer. In this paper, we modeled a SVB Web service as an integration of two layers with the business interface (BIF) and the technical interface (TIF). With this distinction, Web service compositions can map to two layers of compositions at both BIF and TIF levels. We notice that any partial consideration on the consistency of either BIF or TIF layers would likely leave mismatching situations on the other layer. We employ SVB to solve these mismatching situations. With the help of SVB, we address the needs of coherent business planning and IT implementation in a model drivenmanner. Finally, we illustrate the feasibility of our approach in the development of a modern cloud-based tourism e-commerce platform."
1213,"Availability of several web services having a similar functionality has led to using quality of service (QoS) attributes to support services selection and management. To improve these operations and be performed proactively, time series ARIMA models have been used to forecast the future QoS values. However, the problem is that in this extremely dynamic context the observed QoS measures are characterized by a high volatility and time-varying variation to the extent that existing ARIMA models cannot guarantee accurate QoS forecasting where these models are based on a homogeneity (constant variation over time) assumption, which can introduce critical problems such as proactively selecting a wrong service and triggering unrequired adaptations and thus leading to follow-up failures and increased costs. To address this limitation, we propose a forecasting approach that integrates ARIMA and GARCH models to be able to capture the QoS attributes' volatility and provide accurate forecasts. Using QoS datasets of real-world web services we evaluate the accuracy and performance aspects of the proposed approach. Results show that the proposed approach outperforms the popular existing ARIMA models and improves the forecasting accuracy of QoS measures and violations by on average 28.7% and 15.3% respectively."
1214,"IPTV has emerged as the future standard of television and drawn enormous attention from both industry and research communities. Among different IPTV services, on-demand services are the most popular ones due to their convenience and rich content. However, supporting scalable and reliable on-demand IPTV services remains to be an important challenge. Existing IPTV architecture dedicates a centralized regional station to serve subscribers in the respective region regardless of temporal and spatial dynamics in service demand. As a result, it may cause significant imbalance of resource utilization and service provisioning delay at different stations, especially with increasing subscribers and video content. In this paper, we propose to allow IPTV stations of different regions to collaboratively serve user requests for delivering scalable and reliable IPTV services. One key challenge in achieving this station-wise collaboration is to route service requests to appropriate stations according to cost-effectiveness and load distribution in a fully distributed manner. We devise a novel request dispatching protocol which runs on each IPTV station, and yet forms a collaborative dispatching strategy that avoids hot spots and reduces service delivery cost at the same time. Our experiment results suggest that our service request dispatching algorithm significantly improves the scalability of on-demand IPTV services for the existing IPTV architecture."
1216,"As an industry-backed registry for Web Services, UDDI plays an important role in helping requesters find suitable services. Unfortunately, the current search functions in UDDI are limited in their support for making automatic service selection decisions. While some approaches have been suggested to enhance the search capabilities in UDDI using service semantics, they suffer from limitations. These approaches either prescribe performing semantic matching outside of UDDI leaving its search function unchanged or propose embedding a specific matching engine within UDDI thereby making the search function inflexible. In this work, we present a flexible mechanism to enhance UDDI search function. Using our approach, users can integrate multiple external matching services with a UDDI registry to support multiple external service description languages. The result is a UDDI registry with flexible and intelligent service search function that can be used for dynamic service selection."
1217,"On a crowd sourcing platform consisting of task publishers and workers, it is critical for a task publisher to select trustworthy workers to solve human intelligence tasks (HITs). Currently, the prevalent trust evaluation mechanism employs the overall approval rate of HITs, with which dishonest workers can easily succeed in pursuing the maximal profit by quickly giving plausible answers or counterfeiting HITs approval rates. In crowd sourcing environments, a worker's trustworthiness varies in contexts, i.e. It varies in different types of tasks and different reward amounts of tasks. Thus, we propose two classifications based on task types and task reward amount respectively. On the basis of the classifications, we propose a trust evaluation model, which consists of two types of context-aware trust: task type based trust (TaTrust) and reward amount based trust (RaTrust). Then, we model trustworthy worker selection as a multi-objective combinatorial optimization problem, which is NP-hard. For solving this challenging problem, we propose an evolutionary algorithm MOWS_GA based on NSGA-II. The results of experiments illustrate that our proposed trust evaluation model can effectively differentiate honest workers and dishonest workers when both of them have high overall HITs approval rates."
1218,"Service retrieval holds a central role during the development of Web services and Service-Based Applications (SBAs). The higher the number of available services, the more complex it becomes to locate the service closer to the developer needs. The complexity increases further with the number of available service versions that could also be suitable for this purpose. Existing approaches on service retrieval use a similarity measure between service interfaces to identify potentially relevant services. In this work we focus on introducing information about the compatibility of services while calculating their similarity as the means for providing more suitable results. For this purpose we update and extend an existing Web services matchmaker called UDDI Registry by Example (URBE)."
1219,"In recent times, automated business processes and web services have become ubiquitous in diverse application spaces. Efficient composition of web services in real time while providing necessary QoS guarantees is a computationally complex problem and several heuristic based approaches have been proposed to compose services optimally. In this paper, we present the design of a scalable but approximate QoS-aware service composition mechanism which balances the computational complexity of service composition with the QoS guarantees of the composed service and achieves scalability for dynamic service composition. We present experimental results to show the efficiency of our proposed mechanism."
1220,"Service-oriented architecture (SOA) has been widely employed in the field of software engineering, since it encourages attributes such as modularity and code reusability across different applications. Web service composition, where atomic services that accomplish simple tasks are combined into an application that fulfils a more complex function, is one popular application of SOA. Existing composition works focus on building functional and quality-optimised applications. A subset of these works use multi-objective evolutionary computing techniques to produce a Pareto front of compositions, though they assume that the basic structure of the composition workflow is already known. In our recent work, we removed this assumption by combining NSGA-II with a solution representation that allows for different workflow structures to be investigated. However, the multi-objective nature of the problem made it difficult to apply local search to further improve the results. In this paper we hybridise NSGA-II with MOEA/D, which allows the problem to be decomposed into multiple single-objective subproblems where a simple form of local search can be applied. Experiment results show that the use of local search improves the quality of the fronts produced by the hybrid approach for a number of composition tasks."
1222,"A service level agreement (SLA) is a service contract that includes the evaluation criteria for agreed service quality standards. Since agreeable specifications on the evaluation criteria cannot be limited in practice, competitive SLA management products must be extensible in terms of their support for contract-specific SLA compliance evaluations. While the need of running and managing those software products as services increases, we have found that developing a good solution for configuring them as per contractual terms is a challenging task. This paper presents the Fresco framework, which facilitates configuring extensible SLA management systems using Web services. An XML-based specification of SLA management related data called SCOL will also be presented to show how the framework supports contract-specific SLA terms and contract-specific extensions of the deployed SLA management software. The paper furthermore shows how the Fresco system uses a template-based approach to communicate with other Web services applications with support for various input and output formats. Our experience with implementing the Fresco framework for a leading commercial SLA management software product demonstrates that the framework facilitates the creation of effective and efficient solutions for configuring extensible SLA management systems."
1223,"Classifying Web services and labeling them based on their functional features have played a major role in several fundamental service management tasks, such as service discovery, selection, ranking, and recommendation. Existing approaches leverage text mining techniques and follow a supervised learning process, which involves building a classifier from a training set of services and applying the classifier to other services. This process requires intensive human effort on labeling services in the training set. In this paper, we propose to leverage the idea of pool-based active learning to realize a scalable service classification approach. Instead of manually labeling a large number of services to construct a complete training set, the approach starts with a base classifier with a small set of training set and iteratively asks for the labels of the most informative services outside of the initial training set. By doing this, the classifier can achieve comparable accuracy compared to traditional classification method with much smaller size of training set. We use SVM as the base classifier due to its effectiveness in text classification. We also incorporate probabilistic topic models to address the issues caused by sparse term vectors generated from service descriptions and reduce the dimensions to improve the efficiency. We conducted a comprehensive experimental study on real-world service data to demonstrate the effectiveness of the proposed approach."
1224,"Effective management of Web Services systems relies on accurate understanding of end-to-end transaction flows, which may change over time as the service composition evolves. This work takes a data mining approach to automatically recovering end-to-end transaction flows from (potentially obscure) monitoring events produced by monitoring tools. We classify the caller-callee relationships among monitoring events into three categories(identity, direct-invoke, and cascaded-invoke), and propose unsupervised learning algorithms to generate rules for each type of relationship. The key idea is to leverage the temporal information available in the monitoring data and extract patterns that have statistical significance. By piecing together the caller-callee relationships a teach step along the invocation path, we can recover the end-to-end flow for every executed transaction. Experiments demonstrate that our algorithms outperform human experts in terms of solution quality, scale well with the data size, and are robust against noises in monitoring data."
1225,"Service Oriented Architectures and web service technology are becoming popular in recent years. As more web services can be used over the Internet, the need to find efficient algorithms for web services composition that can deal with large amounts of services becomes important. These algorithms must deal with different issues like performance, semantics or user restrictions. In this paper we present an A* algorithm which solves the problem of semantic input-output message structure matching for web service composition. Given are quest, a service dependency graph with a subset of the original services from an external repository is dynamically generated. Then, the A* search algorithm is used to find a minimal composition that satisfies the user request. Moreover, in order to improve the performance, a set of dynamic optimization techniques has been implemented over the search process. A full experimental validation with eight different public repositories has been done showing a good performance as in all tests as the algorithm finds a valid solution with minimal number of services and execution path."
1226,"In this paper, we present a novel algorithm to compose Web services in the presence of semantic ambiguity by combining semantic matching and AI planning algorithms. Specifically, we use cues from domain-independent and domain-specific ontologies to compute an overall semantic similarity score between ambiguous terms. This semantic similarity score is used by AI planning algorithms to guide the searching process when composing services. Experimental results indicate that planning with semantic matching produces better results than planning or semantic matching alone. The solution is suitable for semi-automated composition tools or directory browsers"
1227,"Online Social Networks (OSNs) have been one of the most successful web-based communication models. In the recent years, a new category of OSNs namely anonymous social networks are becoming popular. Unlike traditional Online Social Networks, anonymous social networks allow users to communicate without exposing their identity. This paper presents a trusted anonymous social network service that can anonymize user identities during interaction even though the communication happens with the user's own trusted friends and contacts on the social network. A fundamental requirement of such a trusted anonymous social networks is to protect the user's identity under the guarantees of anonymity. However, in existing approaches, even though the user information is anonymized, by continuously aggregating the information from the messages posted by a user, it is possible to re-identify the user with high probability. In this paper, we propose SocialMix that anonymizes the users of a trusted social network such that the aggregation of messages can be prevented. We make three original contributions. First, we develop the SocialMix model for trusted anonymous social networks so that communication privacy can be protected by k-anonymization. Second, by considering the features of OSNs, we analyze the vulnerabilities of the naive methods that might be exploited to break the privacy. We develop new techniques to improve the attack-resilience of the SocialMix approach. Third, we propose intelligent mix node selection methods to significantly reduce the required number of social mix nodes while still keeping high anonymization rate. Our experiments shows that SocialMix provides high attack resilience and keeps high anonymization rate with few mix nodes under the trusted social network model."
1228,"Service oriented architectures (SOAs) are constantly gaining ground for the provision of business to business as well as user-centric services, mainly in the form of Web services technology. SOAs enable service providers to design and deploy new,composite service offerings out of existing component services. In order to match end-user expectations with respect to personalization and ease of use, these services should be designed in a manner that allows them to exhibit a certain level of context-awareness which is a basic element towards a richer end-user experience. However, in the majority of such services, context-handling is still tightly coupled with the core functionality of the service, resulting in a design which is difficult to implement and maintain. The paper proposes the decoupling of core service logic from context-related functionality by adopting a model-driven approach based on a modified version of the ContextUML metamodel. Core service logic and context handling are treated as separate concerns at the modeling level as well as in the resulting source code where aspect oriented programming (AOP) encapsulates context-dependent behavior in discrete code modules. The design of a restaurant finder service is used to portray the modified ContextUML metamodel and the service modeling process which is covered in full. Respective code snippets belonging to the executable version of the service (part of work in progress) are also provided, illustrating the transition from model to code and the resulting separation of concerns."
1230,"Grid computing has been widely spread in pragmatic areas and scientific research areas in order to achieve more powerful computation than ordinary computation by single computer. Globus Toolkit4 (GT4), which has been the de facto standard middleware in Grid, gives to us many chances of submitting jobs, especially using WSRF technologies. However, in case of cluster type grid using GT4, rules for job execution depend on a local scheduler (LS) which interacts with WS-GRAM in GT4, so that clients can not reflect their own rules to behaviors in each worker node. In this paper, we formulate those potential problems and solutions based on our proposed architecture in, whose main feature is to monitor resource property (RP) in WS-GRAM and notify its status to worker nodes. Then we describe how to implement a mechanism for monitoring RP in WS-GRAM to achieve the transparent delivery of client's rules to worker nodes. Our experimental results show that our proposed architecture can be incorporated in WS-GRAM without difficulty."
1231,"Recent advances in the distributed computing infrastructure like Web, grid, and pervasive computing environment accompany demands for a more powerful and autonomous service framework. We believe it is feasible to achieve service discovery and matching in an automated manner and perform the service execution not in restricted environments. For the purpose, we developed an autonomous service framework, called ASF, which allows autonomous service to be published, discovered and executed in distributed environments based on the autonomous service specification, extension of the Web service ontology (OWL-S) to incorporate physical/logical resources and resource policy."
1232,"Web service applications integrate explicit client-service interactions with notification-style programming. Such event-driven interactions are often called implicit invocations because the notification message is not in itself a business interaction. Rather, it is used to implicitly trigger such interactions as a means for reacting to events. Web Services specifications such as WS-Notification have been developed in order to standardize the way notifications are sent, but they only support the most basic form of event notification. In this paper, we discuss the architecture and the implementation of an expressive notification broker based on extension mechanisms presented by the WS-Notification specification. This broker improves the standard WSN broker in three ways. First, it supports event correlation and hence allows for composite event detection. Second, it integrates events and reactions into Event-Condition-Action (ECA) rules that can be registered directly at the broker. Third, it introduces event-driven lifecycle management for ECA rules, thus allowing to dynamically adapt these ECA rules to changing situations."
1233,"Today, more and more software are augmented with service-oriented packaging. At the same time, more and more business and government services are provided and offered in the form of software. However, there are debates on whether and how much service engineering has in common with software engineering. Can we design service engineering models and frameworks in a similar spirit as the way software has been engineered in the past two decades? Or should software be designed, engineered and offered in a way similar to services that existed even before computer age?"
1234,"In this paper, we develop a Web service for ontology comparison based on a novel senses refinement algorithm, which builds senses sets to represent the semantics of the input ontologies. The senses refinement algorithm converts the measurement of ontology difference into simple set operations based on set theory, thus ensures the efficiency and accuracy of the ontology comparison. We believe our Web service is the first available online measurement tool for ontology comparison."
1235,"The advent of and growing interest in service- oriented architectures (SOA) present business leaders with a number of problems. They promise to deliver hitherto unseen business process agility, but at the risk of making investment in existing systems obsolete. The established orthodoxy is that the maintenance problem presented by installed systems is about finding an acceptable balance between risk involved in evolving the system and benefits offered by the update. SOAs represent a ""paradigm-shift"" and, as such, present a more complicated problem: how to minimise the risk to their investment (existing software systems) and exploit the benefits of migrating to SOA. We provide a review of a number of approaches that may contribute to a pragmatic strategy for addressing the problem and outline the significant challenges that remain."
1236,"The results from using the current service recommendation algorithms are still unable to meet the dynamic and diverse demands of users. Therefore, a recommendation algorithm is proposed to take into account the dynamic and diverse demands of users. This algorithm extracts the user-implicit-demand-factors from the Latent Dirichlet Allocation model in the field of machine learning, and uses both explicit and implicit demand as the intermediary variable to generate a service recommendation list for the user. Experimental results on a real-world data set regarding service composition show that the proposed algorithm can represent a variety of user demands, and the performance of the proposed algorithm is better than the existing algorithms in terms of accuracy, novelty and timeliness."
1237,"XML has been widely adopted across a wide spectrum of applications. Its parsing efficiency, however, remains a concern, and can be a bottleneck. At the same time, with the trend towards multicore CPUs, parallelization to improve performance has become increasingly relevant. In previous work, we have investigated parallelizing DOM-style parsing and gained significant speedup. For streaming XML applications, however, SAX-style parsing is often required. In this paper, we present a technique and implementation of a parallel XML SAX parser. To handle inherent data dependencies in XML while still allowing reasonable scalability, we use a 4-stage software pipeline with a combination of strictly sequential stages and stages that can be further data-parallelized within the stage. We thus utilize a hybrid between pipelined parallelism and data parallelism. To demonstrate effectiveness, we test this approach on a Linux machine with two Intel Xeon L5320 CPUs for a total of 8 physical cores, and obtain good speedup up to about 8 CPUs."
1238,"With the fast development of Internet of Things (IoT), a large amount of services are being generated continuously by different business process applications hosted on edge devices. In order to facilitate seamless access and service life cycle management of large, distributed and heterogeneous IoT services, service computing and fog computing have been widely used as the promising technologies. However, an execution environment integrating IoT services into these two technologies is still an open research challenge. In this paper, we proposed a novel service-based fog execution environment to make the business process applications fit in the dynamic IoT service environment. The proposed IoT execution environment promises a full-life cycle management of the IoT services, a low latency response of the edge devices and a distributed execution of the business process applications. An actual running intelligent medical case is given to validate our proposed IoT execution environment."
1239,"Outsource encrypted data has attracted attentions from industry and academics for storing sensitive data in third party clouds. Many cloud applications need privacy preserving multiple keywords search services over encrypted data with dual capabilities. On one hand, they need to keep the query keywords and associated search operations private such that data hosting service providers cannot trace and infer sensitive data stored in the third party data hosting servers. On the other hand, they need to support multiple keywords search to significantly improve the search efficiency. However, current keyword search protocols for encrypted data are not practical with poor privacy and low efficiency. In this paper, we propose a new oblivious multiple keywords search (OMKS) service, which provides privacy for both users and cloud storage service provider and supports efficient multiple keywords search. Compared to previous oblivious keyword search (OKS) protocols, our protocols maintain strong privacy, i.e., database security and query privacy, and effectively support disjunctive and conjunctive keywords search. The analysis and experiments show that OMKS protocols significantly reduce the storage and communication overhead. Moreover, the computation overhead of conjunctive search is not increased with the number of query keywords such that it can performs highly efficient conjunctive keywords search."
1240,"Today, Web services are pervasive and omnipresent in the Internet and within enterprises. Even though there are massive Web services specifications development underway, early adoption by developers and tool vendors is becoming a need. The potential growth of this technology is highly predictable because of its universal acceptance and use among the developer community. This industry may expect to grow enormously based on the support from various communities that benefit from this technology. Researches are carried out in various standards bodies on various aspects of Web services such as definition, architecture, security, discovery, interoperability, etc. As we are committed to the success of this technology, we need to research on service oriented containers that makes Web services potential more constructive. This paper proposes a container for Web services which can manage and monitor the state and behavior of Web services, which may address the quality of service (QoS) factor for Web services."
1241,"Currently, workflow systems are either XML based or component based. Both paradigms have usability deficiencies. XML is not designed for procedural programming. Legacy code is difficult to adapt to component based systems. We propose a new paradigm by adding workflow keywords to an existing high-level language. This approach, called GroupSpeak, uses a procedural style of programming and allows for easy introduction of workflow patterns to legacy code. The programmer can leverage their existing knowledge of the high-level language to easily add workflow capabilities to their applications."
1242,"Summary form only given. Web services and Enterprise Java Beans are evolving technology that allows the development and publication of software components. These components can be integrated together in plug and play manner. It provides a generic component based framework on the Web for large and complex distributed application development. This tutorial provides a detailed explanation of how you can build different types of EJBs to implement your distributed application. It will also provide details of publishing, accessing and innovation of Web service functions. Moreover, it will give you some dos and don'ts for developing Web services and EJBs. This tutorial is valuable for those involved in designing and deploying distributed B2B solutions such as Web developers, architects, consultants, engineers and analysts and programmers. The basic programming experience is assumed."
1243,"Today, web services are widely used for data exchange. The format of individual messages exchanged among them is usually described with XML schemas in WSDL documents. It is a common practice that there is not only one but a whole family of formats each, for example, tailored for a specific consumer. In such environments, the design and maintenance of the web service interfaces and, in particular, the XML schemas describing the structure of messages is not a simple task. In our previous work we developed a method based on the principles of Model-Driven Development for evolution of a family of XML schemas. It automated a portion of design and maintenance tasks to be done when a change in user requirements or surrounding environment of the system influences more XML schemas in the family. We provided a formal model of possible evolution changes and their propagation mechanism. In this paper, we extend our method with inheritance modeling. We formally extend our conceptual model and we introduce new evolution changes and update the current ones so that they keep the model in a consistent state."
1244,"The most promising feature of the Web services platform is its ability to form new services by combining the capabilities of existing services, i.e., its compos ability. The existing services may themselves be composed of other services, leading to a hierarchical composition. In a hierarchical composition, providers vary in the visibility they have over the other providers in the composition. For example, a provider may not be aware of any providers in the hierarchy other than its parent and children. On the other hand, a provider may be aware of all other providers in the hierarchy. Towards this end, we introduce the notion of spheres of visibility (SoV) as an abstraction to capture the upward/downward visibility aspects of the providers in a hierarchical composition. The latter part of this paper deals with compensation. We outline a compensation mechanism for hierarchical compositions conforming to the visibility restrictions modeled as SoV."
1245,"A new trend involves Web services such as Twitter beginning to publish streaming Web APIs that enable partners and end users to retrieve streaming data. By combining such push-based Web services and existing pull-based Web services, it is now possible for us to understand the current status or trends of the world in a more real-time way, such as real-time tracking of infectious disease, real-time crime prediction, or real-time marketing, and so various innovative business services are possible. For a system architecture to implement such services, the services are normally built from the scratch, and the performance and scalability depend upon the engineers' skills. In this paper we propose a real-time Web monitoring system called ""StreamWeb"" on top of a stream computing system called System S developed by IBM Research. The Stream Web system allows developers to easily describe their analytical algorithms for a variety of kinds of Web streaming data without worrying about the performance and scalability, and provides real-time and scalable Web monitoring for massive amounts of data. As an experimental proof-of-concept application, we built an application that monitors a list of keywords in the Twitter streaming data, and that displays any messages including the specified keywords onto a map of the physical location (from Google) where the message was posted. Our system can handle nearly 30 thousand Twitter messages per second on a system with 8 computing nodes. This prototype application confirms that we can build real-time Web monitoring systems while satisfying the needs for high software productivity and for system scalability."
1246,"BPEL is emerging as an open-standards language for Web service composition. However, its procedural style can lead to inflexible and tangled code for managing a crosscutting aspect - synchronization constraints that define permissible sequences of execution for activities in a process. In this paper, we present DSCWeaver, a tool that enables a synchronization-aspect extension to BPEL. It uses DSCL, a synchronization expression language, to specify constraints. DSCL has the desirable features of declarative syntax, fine granularity, and validation support. A designer can use DSCL to describe and validate the synchronization behavior and rely on DSCWeaver to generate BPEL code. We demonstrate the advantages of our approach in a service deployment process and evaluate its performance using two metrics: lines of code (LoC) and places to visit (PtV). Evaluation results show that our approach can effectively reduce development effort of process designers while providing performance competitive to un-woven BPEL code"
1247,"Web service discovery on the web is not a trivial task as the number of available web service descriptions continuously increases, and global UDDI registries are no longer available. As discovery through conventional, general-purpose search engines does not yield satisfactory results, a more promising alternative should be explored through specialized search engines. This paper explores the design and implementation of such a framework, resulting in WESS, a search engine targeted to discovering and retrieving web service descriptions. The presented system features an adaptive web service description collection process, through specialized and directed crawling, as well as an enhanced indexing and retrieval mechanism, which handles description documents as semi-structured text, separating actual information from tags and annotations. The paper also presents experiments and use cases regarding different search scenarios, in addition to performance results."
1248,Provides a listing of current committee members.
1249,"In recent years, Web service technologies are becoming more and more important communication scheme for grid environment. WSRF-based resource management components, such as WS-GRAM in GT4, lay emphasis on how to make existing procedures such as local scheduler be invoked via HTTP, so that they inevitably have two problems described below. (1) If assumed environment is cluster type grid, each worker node does not have access scheme to any information included in resource property of master node. (2) As a result, the contents of information sent to clients is forced by the local scheduler and therefore clients can not control job status in accordance with their own policy. If those two problems are resolved, any client's own resource management policy can be reflected on worker nodes. In this paper, we propose a method for resolving those problems in order to realize the interactive communication between clients and worker nodes for WSRF-based cluster grid."
1250,"The semantic Web promises automated invocation, discovery, and composition of Web services by enhancing services with semantic descriptions. One such language used for creating semantic descriptions is the Web ontology language or OWL. An upper ontology for Web services called OWL-S has been created to provide a mechanism for describing service semantics in a standard, well-defined manner. Unfortunately, the learning curve for semantic-rich description languages such as OWL-S can be steep, especially with given the current state of tool support for the language. This paper describes an automated software tool that uses model-driven architecture (MDA) techniques to generate an OWL-S description of a Web service from a UML model. This allows' the developer to focus on creating a model of the Web service in a standard UML tool, leveraging existing knowledge."
1251,"Today, modern IT-systems are often an interplay of third-party web services. Developers in their role as requesters integrate existing services of different providers into new IT-systems. Providers use frameworks like Open API to create syntactic service specifications from which requesters generate code to integrate services. Proper service discovery is crucial to identify usable services in the growing plethora of third-party services. Most advanced service discovery approaches rely on semantic specifications, e.g., OWL-S. While semantic specifications are crucial for a precise discovery, syntactical specification are needed for service invocation. To close the gap between semantic and syntactic specifications, service grounding establishes links between the semantic and syntactic specifications. However, for a large number of web services still no semantic specification or grounding exists. In this paper, we present an approach that semi-automates the semantic specification of web services for service providers and additionally helps service requesters to leverage semantic web services. Our approach enables a higher degree of automation than other approaches. This includes the creation of semantic specifications and service groundings for service providers as well as the integration of services for requesters by using our code generator. As proof-of-concept, we provide a case study, where we derive a sophisticated semantic OWL-S specification from a syntactic Open API specification."
1252,"One of the fundamental pillars of the Web service vision is a brokerage system that enables services to be published to a searchable repository and later retrieved by potential users. This is the basic motivation for the UDDI standard, one of the three standards underpinning current Web service technology. However, this aspect of the technology has been the least successful, and the few Web sites that today attempt to provide a Web service brokerage facility do so using a simple cataloguing approach rather than UDDI. In this paper we analyze why the brokerage aspect of the Web service vision has proven so difficult to realize in practice and outline the technical difficulties involved in setting up and maintaining useful repositories of Web services. We then describe a pragmatic approach to web service brokerage based on automated indexing and discuss the required technological foundations. We also suggest some ideas for improving the existing standards to better support this approach and Web service searching in general."
1253,"In seeking more profits, many business service providers have turned their attention to the technologies that can enable the delivery and operations of the network-delivered business services more efficiently and highly automated. To that end, many service providers have adopted the shared service model where the service offerings are shared across multiple customers to reduce the costs associated with the services' delivery. One key challenge for service providers is how to configure various parameters and options of the services with highly automated and self-service modes to meet the customers' diverse requirements with minimized costs. Therefore, effective service configuration lifecycle management technologies and approaches have become the differentiations of successful service providers. In this paper, we propose a common service configuration framework as one of the infrastructure features of the service operation platform based on the shared service model to ease and unify the configuration lifecycle across service offerings. In this framework, we adopt the model-driven technology to automate the service configuration lifecycle. This novel approach combines the wizard technology and rule-based validation technology to enable the service customers to perform self-service configuration. Furthermore, the effectiveness of the framework is discussed at the end of this paper."
1254,Presents the welcome message from the conference proceedings.
1255,"The ability to compose web services from available services is one of the most crucial problems in the service-oriented computing paradigm. Conventional software engineering approaches and even standard languages compose web services as workflow models that control the business logic required to coordinate data over participating services. Such models would not apply to the design of multiagent-based web services, which offer high-level abstractions that support autonomy, business-level compliance, and flexible dynamic changes. In this paper, we model interactions among multiagent-based services by commitment modalities in the figure of contractual obligations and devote multiagent commitment protocols to regulate such interactions and engineer services composition. We develop and fully implement a symbolic model checking algorithm by enriching the MCMAS model checker with certain symbolic algorithms to verify the correctness of protocols, given properties expressed in a temporal commitment logic, suitably extended with actions. The time complexity and space complexity of the developed algorithm are P-complete for explicit models and for PSPACE-complete concurrent programs. Finally, we report the experimental results of two case studies, adopted to check the algorithm's efficiency."
1256,"The semantic web promises to bring automation to the areas of web service selection, discovery, composition, invocation. In this paper we introduce a means of facilitating automation of web service composition by exploiting semantic matchmaking between web service parameters (i.e., outputs and inputs) to enable their connections and interactions. The idea is that matchmaking functions are key components to find semantic compatibilities among independently web service descriptions. To this end, our approach extends existing methods (exact, plug-in, subsume, intersection and fail) with concept abduction to provide explanations of misconnections between web services. From this we generate web service compositions that realize the goal, discovering and satisfying semantic connections between Web services. Moreover a process of relaxing the hard constraints is introduced in case the composition process failed. Our system is implemented and interacting with web services dedicated on a France Telecom scenario."
1257,"In this paper, we report on a client-side framework that allows applications to consume Web services that adhere to the callback pattern in the context of network security schemes. This framework supports extensions of the callback pattern such as the one-request-multiple-response pattern, also allows the applications to consume the callback results in a flexible way."
1258,"An important problem in Web services composition process is optimal selection of services meeting the user functional requirements (tasks of a workflow) and ensuring a reliable execution of the composition. Therefore, non-functional properties of services such as their transactional behavior as well as their Quality of Service (QoS) must be considered. In this context, a challenging objective is to assist users in integrating on the fly the operations of services to realize their required tasks by further meeting their transactional and QoS preferences. Towards this purpose, we present SPLTQSSS, a Software Product Line based approach for Stateful (conversation-based) Service Selection problem with Transactional and QoS support. SPL-TQSSS considers the set of functionally-equivalent services as part of a service family by modeling their internal operations using Feature Models. Then, SPL-TQSSS chooses the best services, from the service families matching with every task of the workflow, which fit with the user transactional preference and satisfy QoS constraints."
1260,"In express delivery, couriers will generate a mass of trajectory logs when delivering shipments. To analyze these logs is of great value for the promotion of express delivery service. For any research based on trajectory data, map-matching plays an important role, so in this article, we design a map-matching service specially for courier trajectories. As far as we know, existing map-matching algorithms are designed mainly for cars or walks, or ignoring means of transportation. Although these methods can be applied to courier trajectory map-matching, the accuracy of them can hardly be guaranteed as they ignore the characteristics of courier trajectories. To solve this problem, we design a new map-matching service based on a map-matching algorithm called Courier Trajectory Based Map-Matching (CTB-Matching), which is specially used to deal with courier trajectories. Courier trajectories have some characteristics different from traditional trajectories. Firstly, couriers have to deliver shipments at different sites, so the trajectories seem more irregular, which is called as ""fragmentation"" problem. Secondly, unlike cars, couriers' positioning information is mainly generated by Wi-Fi location system, which is not precise as Global Position System (GPS), so the location deviation problem should be taken into account. What's more, couriers usually use electric bikes for delivery, which travel slower than cars, and are less likely to be influenced by traffic. Therefore, the speed or temporal analysis for cars is not suitable here. Based on the analysis of current algorithms and the problems stated above, this paper designs a map-matching service for courier trajectory data. The experiments verify that our service performs better when dealing with courier trajectory data. Besides, our service is efficient with low time complexity and space complexity, making our service responses with low latency."
1261,"As Web services start to be deployed for mission-critical applications and for e-business scenarios, higher quality of service (QoS) and continuous service delivery become a critical issue to ensure high availability and reliability in spite of the failure or unavailability of the participating services or networks. These challenges call for vast improvements in the Web services containers and the mediation infrastructure. To address these requirements, we propose Web services message bus (wsBus), a lightweight service-oriented middleware for reliable and fault tolerant Web services interactions. This paper first discusses wsBus architecture and features and then it reports some experimental results to illustrate the effectiveness of wsBus in adding reliable and uninterrupted services to a supply chain management system."
1263,"SPARQL is a standard query language for knowledge graphs (KGs). However, it is hard to find correct answer if KGs are incomplete or incorrect. Knowledge graph embedding (KGE) enables answering queries on such KGs by inferring unknown knowledge and removing incorrect knowledge. Hence, our long-term goal in this line of research is to propose a new framework that integrates KGE and SPARQL, which opens various research problems to be addressed. In this paper, we solve one of the most critical problems, that is, optimizing the performance of nearest neighbor (NN) search. In our evaluations, we demonstrate that the search time of state-of-the-art NN search algorithms is improved by 40% without sacrificing answer accuracy."
1264,"QoS-based service rating has made positive contributions to the area of service selection. Especially for Cloud service users, the right decision when choosing suitable Cloud services can help them improve user satisfaction and trading revenues. This work aims to address the issue of uncertainty in service requests, service descriptions, user and expert preferences, as well as evaluation criteria in a MCDM-based service selection procedure. A hybrid fuzzy framework for Cloud service selection is proposed, addressing the challenge using three approaches: a fuzzy-ontology-based approach for function matching and service filtering, a fuzzy AHP technique for informed criterion weighting, and, a fuzzy TOPSIS approach for service ranking."
1265,"Transition to Service-Oriented Architecture (SOA) is underway in the military and there have been many excellent examples of cutting-edge SOA and web service implementations. However, the process to build services for SOA frameworks is not well defined and therein poses a security risk. This research develops models, methodologies and specifications to help programmers integrate their work into selected SOA, including the installation of security appropriate to SOA. Research will establish baselines, models and specifications to help define, categorize, normalize, and weight security risks in distributed SOA. To be included are methodology to help manage security risk."
1266,"This work presents the use of security proposals in the Web Services architecture aiming to provide an environment that guarantees authentication and authorization transfer between different security domains. The model described facilitates the access of rights owners into an environment with different security technologies. This model is based on the federation Web concept, which allows scalable and flexible rights management solutions. This work illustrates the model properties through examples involving different security technologies."
1267,"Among the problems facing designers of complex multi-participant Web services-based applications is dealing with the consequences of the lack of suitable isolation mechanisms. This deficiency means that concurrent applications can interfere with each other, resulting in race conditions and lost updates. This paper considers a proposed solution to this problem based on 'promises' and shows that this model can be implemented in practice. We consider implementation issues that need to be handled in promise-based systems and discuss a proof of concept prototype that supports promise-based isolation without requiring changes to existing applications and resources."
1268,"With the rapid development of Internet, the explosive growth of information challenges people's capability on finding out items fitting to their own interests. The emergence of recommender system helps users to make decisions to a certain degree. So far, most of the studies pay much attention to designing or improving recommendation algorithms. However, few works consider the extraction of core users with whom recommender systems can generate satisfactory recommendation. In this paper, we propose new approaches to identifying core users based on trust relationships and interest similarity. The trust degree and interest similarity between all user pairs are calculated and sorted first, and two strategies based on frequency and weight of location are used to select core users. Experiments show the effectiveness of the extraction of core users and prove that 20% of core users enable recommender systems to achieve more than 90% of the accuracy of the top-N recommendation."
1269,"Workflow models have been used and refined for years to execute processes within organisations. To deal with collaborative processes (choreographies) these internal workflow models have to be aligned with the external behaviour advertised through Web service interfaces. However, traditional workflow management systems (WfMS) do not offer this functionality. Simply sharing and merging process models is often not possible, because workflow management lacks a widely accepted standard theory for workflow models.Multiple research and standardisation efforts to integrate different workflow theories have been proposed over the years. XPDL is the most widely used standard for process model interchange and supported by over 80 systems.However, XPDL also lacks the possibility to relate a workflow model to its possible choreography interface abstractions.To remedy this situation, we propose to abstract the XPDL model to a higher-level model, perform the integration and the compaction algorithms at that level and then ground it back to the desired choreography models. We develop and use an integrated ontology which is based on the XPDL standard for this purpose. To facilitate the abstraction and grounding, we present a mapping procedure to automatically translate XPDL and BPMN workflow models into this ontology. After translation, these models are annotated with a parameterised role model and other collaborative properties. We present a compaction procedure that automatically maps the annotated models into external choreography interfaces that expose only the relevant information for a particular partner collaboration. Our procedure is agnostic with respect to the target choreography model. We demonstrate our approach using WSMO choreographies which enables us to automatically generate interface models from any WfMSs that supports XPDL export."
1270,"Service flow in SOA systems need to detect quality of service (QoS) problems and to guarantee end-to-end performance. In previous work, we have proposed two faulty service identification methods: a dependency matrix based diagnosis and a Bayesian network based diagnosis. In this paper, we present a hybrid diagnosis to achieve high diagnosis accuracy and low diagnosis cost. The hybrid diagnosis reduces the problem size by applying dependency matrix based diagnosis result in Bayesian network and excluding services that are not critical to the end-to-end QoS from the diagnosis. Our experimental results show that the accuracy of the hybrid diagnosis is similar to the Bayesian network diagnosis yet reduces more than 90% of the diagnosis time."
1272,"Recent advances in the area of Web services have enabled inter-organization sharing of data and data-oriented software services. The challenges of developing software in a service-oriented development environment include search, retrieval, and integration of services with client applications. Such applications can be dynamic in nature and may vary depending on current availability of services or on the current relationship between client and service organizations. As such, applications must be able to quickly locate and integrate different potential service components. In this paper we describe an approach for automating the process of searching for Web services using signature matching and describe a new signature match criteria called the contains match."
1273,This paper proposes an approach for cross-domain connectivity that enables domain autonomy and that preserves across domains properties that are taken for granted by services within a domain.
1274,"Service interface structure is of primary importance in SOA to ensure best practice of third-party reuse. One of the key factors for deploying successful services is assuring an adequate interface structure. However, a common bad service design practice is to place semantically unrelated operations in a single interface. This poor design practice typically result in a system which is difficult to comprehend, maintain and evolve providing low performance and reusability. To address this problem, we present an automated approach, SIM, to support service developers improve the quality of their interface modularization. Our approach analyzes structural and semantic relationships among the operations exposed in a service interface to identify chains of strongly related operations. The identified operation chains are used to define new interfaces with higher cohesion and better usability. We empirically evaluate our approach on a benchmark of 22 realworld Web services, provided by Amazon and Yahoo. The obtained results show that the produced interfaces are (i) able to improve the service design quality, and (ii) recognized as 'useful' from developers point of view in improving their service design. Additionally, we found that SIM significantly outperforms a recent state-of-the-art approach."
1275,"Software engineering has greatly evolved in recent years. Today applications are deployed on heterogeneous distributed infra-structure from mobile devices to cloud computing. Service-oriented architectures, such as SOA and REST Web Services, have been widely used to efficiently design high-availability, scalable and reliable systems for dynamic business environments based on a distributed infra-structure. Despite the improvements these architectures have made to enhance the evolvability of systems, there are some challenges that still need to be overcome. More concretely, service-based systems and development teams are constantly under pressure from business stakeholders who continuously increase their demands for changes in systems. This paper describes a configuration-based approach that can empower adaptive mechanisms in order to overcome this challenge. It presents a solution based on a centralized application configuration repository service specially designed as a RESTful web service API to provide the benefits of configuration, such as adaptability, to high-availability, scalable and loosely coupled systems, allowing them to respond quickly to changes. The solution was successfully implemented in an evolutionary online advertising system used by the largest Brazilian web-portal, responsible for processing 5 billion ad requests per month. It allowed the design of a self-adaptive advertisement ranking mechanism that continuously evolves the system configuration, without human supervision. The adoption of this solution was responsible for a drastic increase in the amount of changes applied in this advertising environment. It also greatly reduced the time from conceiving a new change to having it working in the system. Moreover, the solution is available as open source and it has also being used by several other service-based systems."
1276,"The ultimate target of business to business (B2B) integration and deployment is complete business process automation within and between enterprises with no human intervention. From a business point of view, negotiation is the main mechanism that modern enterprises use to achieve targets that maximise their profits. This paper proposes an automated B2B negotiation solution: the implementation of semantic-enabled machine versus machine business negotiation as a Web service. We argue that the shift from human vs. human to machine vs. machine business negotiation is facilitated by using semantic Web technology implemented with Web services. The negotiation process in this work is designed for multiple machines (a minimum of 3 computers), using a negotiation algorithm that emulates the behaviour of human business negotiations. Possible development directions are brought forward in the paper's conclusion."
1279,"Since Representational State Transfer (REST) architecture was proposed by Fielding in early 1990s for distributed hypermedia systems, it has become a popular architectural style of choice in various computing environments. However, REST was not originally designed to support enterprise requirements, in particular the accountability requirements that are crucial for the business services offered through the Software as a Service (SaaS) and Cloud Computing environments. In this paper, we propose an Accountable State Transfer (AST) architecture to bridge the accountability gap in REST. With AST, service participants can be held accountable for each representational state transfer during service consumption. A formal service contract model with a hybrid reasoning mechanism and a novel accountable state transfer protocol are designed as the mechanisms underpinning the AST architecture. Moreover, we implement a Credit Check service prototype based on AST, demonstrating the practicality of such architecture. Inheriting REST's scalability, AST architecture provides the much needed accountability capabilities for the virtual service delivery environment."
1280,"We investigate the use of subjective metrics in social media to evaluate cloud service performance in the market. We first examine the subjective factors that drive cloud consumers to/from purchasing cloud services. These include the ability to achieve greater scalability, security concerns, etc. according to several industry surveys. We then analyse the correlation between the consumers' perception on those factors and the cloud market revenue growth. This paper identifies the unique subjective metrics that are indicative of cloud service performance from the market perspective. The cloud consumers' perception is sourced from several particular social media using sentiment analysis techniques. We focus on consumers' perception on a leading cloud provider that holds the majority of the cloud market share. We find that subjective metrics are empirically proved to be applicable in evaluating the performance of cloud services in the market."
1281,"Database-as-a-Service (DBaaS) has gain significant momentum with the prevailing usage of Cloud computing. Multi-tenancy is one of the key features of DBaaS offering, where a large volume of databases with different Service Level Agreement (SLA) requirements are co-located in one environment and sharing resources. As Cloud resources are elastic and resource demands of database requests are unpredictable, it is challenging to decide when and where to place databases in Cloud environment according to their resource requirements. In this paper, we propose a cost-efficient placement algorithm striving to produce placement solution that optimizes multiple objectives considering multi-resource constraints, user preferences and system preferences. The objective is to help DBaaS providers to achieve effective resource allocation among multiple databases, minimize the disturbance to the system caused by database migration, and maximize Cloud resource utilization. The demonstrated online placement technique can be used as decision making reference for DBaaS providers to make optimal resource planning. The effectiveness and efficiency of the algorithm have been verified by intensive simulation experiments and real-case study in IBM cloud platform."
1282,"Mashup is presenting new kind of application in web 2.0 world. Mashup is not simply about the AJAX technologies, rather, it is typically related to reuse the data and other services from other web side and web applications. There are many ways to build up the mashup. This half-day tutorial will focus on using XML and JSON format of data and service and will introduce the following to the participants."
1283,"The composition of network management information is a feature widely required but poorly supported in traditional management technologies. Recently, Web services for network management has been enabling the investigation of more sophisticate management solutions, even though some concerns related to the Web services performance have been initially exposed, but quickly disappeared after the first research results. In this paper we show that Web services technologies have more to offer to the network management discipline than just bridging established network management protocols and Web services protocols. Particularly we explore the possibility of using Web services composition applied to network management. If successful, Web services composition can bring to network management the solution for some key problems yet to be solved, such as retrieving the information from several different devices and yet being able to use a simple and fast interface at the manager side. We present Web services composition for network management considering two approaches: in the first one a single network device needs to be contacted and its information composed; in the second one, many devices need to be contacted and the information retrieved from them need to be composed. We show that using proper tools we can not only really use Web services composition for network management, but also that such use can be integrated with traditional management technologies that are unlike to be abandoned in short and mid terms"
1284,"Time is an important parameter in modeling and analyzing Web services. A Web service can be presented by its behavior which can be described by a business protocol representing the possible sequences of message exchanges. Automated analyses of timed Web services such as compatibility checking are very difficult and in some cases are not possible with the presence of implicit transitions (internal transitions) based on time constraints. The semantics of the implicit transitions is the source of this difficulty because most of well known modeling tools do not express this semantics (e.g., epsilon transition on the timed automata has a different semantics). This paper presents an approach for removing complex implicit transitions of the timed business protocols before performing the interoperability analysis without changing the semantics of the protocols."
1285,"There are large demands for re-engineering human-oriented Web application systems for use as machine-oriented Web application systems, which are called Web services. This paper describes a framework named H2W, which can be used for constructing Web service wrappers from existing, multi-paged Web applications. H2Ws contribution is mainly for service extraction, rather than for the widely studied problem of data extraction. For the framework, we propose a page-transition-based decomposition model and a page access abstraction model with context propagation. With the proposed decomposition and abstraction, developers can flexibly compose a Web service wrapper of their intent by describing a simple workflow program incorporating the advantages of previous work on Web data extraction. We show three successful wrapper application examples with H2W for real world Web applications"
1287,"As the increasing development of application technology and infrastructure, many kinds of application and resources can be encapsulated as Web service and its variations, and service-oriented computing become research hot. But, we believe service-oriented computing should establish on virtualized service rather than concrete service instance directly. In this paper, we present formalized model of virtualized service and service instance, and define the concept of service virtualization. Then, we propose the implementation solution of virtualized service-oriented application from perspective of global and local instantiation process."
1288,"Distributed systems increasingly span organizational boundaries and, with this, system and service management domains. Web services are the primary means of exposing services to clients, be it in electronic commerce, Software-as-a-Service (SaaS) or on cloud platforms and are being used and integrated with customer-managed applications as well as in complex mashups. Maturing cross-domain relationships and an increase in loose coupling and ad-hocness makes managing configuration changes, e.g., changes in interfaces or endpoints, increasingly relevant. Traditional service management processes within organizations, in particular change management, relies on a central configuration management database (CMDB) to assess the impact a change has on other components of the system. However, this approach does not work in a cross-domain environment, due to the lack of a central CMDB, centralized management processes, and knowledge by service providers which clients depends on their respective services. This paper proposes the Change 2.0 approach to cross-domain change management based on an inversion of responsibility for impact assessment and the facilitation of cross-domain service process integration. We present the requirements imposed by cross-domain change management, the Change 2.0 architecture, and a brief evaluation of its benefits."
1289,"WSDL-S and OWL-S are semantic Web services languages that both aim at enriching WSDL with semantic annotation. In this paper, we analyze the similarities and differences between the two languages aiming at showing how OWL-S annotations could take advantage of WSDL-S annotations. In the process, we discover and analyze representational trade-offs between the two languages"
1290,"The burst of Web-based Restful services brings us a number of facilities in our life and work. We are used to take smartphones to access these Web services, like location-based services, weather search, mapping, social networking, et al. On smartphones, we have two options of service consumers, a.k.a, Native apps and Web apps. Despite the platform-independence, Web apps are claimed to provide the same features and comparable user experiences with native apps. However, one fact is that more and more people prefer native apps rather than Web apps. In this paper, we make an empirical study on characterizing the performance disparity of native apps and Web apps. Given the same functionalities provided by the same service providers, we explore the Restful Web services that are used by native apps and Web apps. With HTTP-level trace analysis, we demystify the workflows on how native apps and Web apps use Web services and summarize different service usage patterns from architectural style perspective. Then we characterize the performance differences between native apps and Web apps on realizing Restful Web services including GET, DELETE, PUT &amp; POST, in terms of number of network connections, response time, and data drain, given the same functional features. Our observations reveal that Web apps do not always perform worse than native apps using Restful Web services under the same context. We further propose some implications to improve both native apps and Web apps on smartphones."
1291,"Business process mining is to extract process knowledge from a system's log in order to reconstruct workflow models. Existing approaches treat a log record as an instance of one workflow model. They do not deal with interleaved logs, where each log record is a mixture of multiple workflow traces. However, such an interleaved log is typical for many systems especially web-based ones where all the user-system interaction traces are recorded and maintained by a web server. Dealing with interleaved logs is challenging due to the lack of prior knowledge of workflow models and noises contained in the log data. In this paper, we propose a two-phase workflow learning process. During the first phase, we use a probabilistic approach to learn the links between operations and the hidden workflow models. We consider a workflow model as a probabilistic distributions over operations and derive it through likelihood maximization. This allows us to identify the membership of an operation to a workflow model, which can be used to unravel a log record and generate a set of workflow instances from it. During the second phase, the sequential patterns between operations within each workflow model are derived from all its instances. We have conducted a comprehensive experimental study, which indicates the effectiveness of the proposed solution."
1292,"The increasing amount of web services over the Internet enable users composing them to satisfy the users'needs efficiently. Such service composing is prone to errors. Automatically detecting incompatible web services interaction and correcting them will largely improve users' experience on service composing. When correcting the errors, two major issues need to be addressed: First, how to satisfy diverse correction requirements of different users, Second, how to find the corrections efficiently. This paper proposes an approach to discovering maximum diversity corrections to reduce the risk of unsatisfying different end users' needs when presenting correction plans to them. To solve the problem efficiently, this paper proposes an approximate algorithm to find diverse correction plans. Furthermore, two pruning strategies are adopted to reduce the runtime of the algorithm. Experiments illustrate that our approach outperforms the baseline on the diversity of correction plans, and the two pruning strategies reduce the runtime significantly."
1293,"The ASAPM project aims at developing new techniques, mechanisms and software solutions for enablement of flexible, dynamic and robust management of service-oriented application provision processes to ensure collective functionality, end-to-end QoS and stateful coordination of complex services"
1294,"This paper reports on an implementation for tool support of model-checking collaborating service compositions with deployment configurations under resource constraints.The implementation accepts UML Deployment Diagrams with an applied service deployment profile and one or more WS-BPEL orchestrations that are assigned to Web Servlets and servers in this deployment. Using model-checking techniques the tool can determine whether the configuration of deadlock-free service orchestration processes introduce deadlock scenarios when combined with resource constraints of a deployment environment. The implementation is built upon a tool suite, called WS-Engineer, which is aimed at assisting service engineers in constructing and testing various aspects of a service engineering approach, including orchestration, choreography and deployment artifacts. The tool integrates as a plug-in for Eclipse, alongside the IBM Rational software architect tool suite and others. A case study based upon a complex service grid solution, for analyzing chemical markup patterns, is used to demonstrate the accessible and practical nature of the solution."
1296,"The introduction of the semantic Web paradigm in service-oriented architectures enables explicit representation and reasoning about services, via a semantically rich description of their operations. We propose an approach towards service selection and composition based upon the interpretation of user requests expressed through an informal human-computer interaction interface that employs (restricted) natural language."
1297,"Web services have become the key technology in business processes management. Business processes can be self-contained or be composed from sub-processes; the latter category is typically specified using the Web services business process execution language (WS-BPEL) and executed by a Web services orchestrator (WSO). During the execution however of such a composite service, a number of faults stemming from the distributed nature of the SOA architecture, e.g. network or server failures may occur. WS-BPEL includes provisions for exception handling, which can be exploited for detecting such failures; once detected, a failure can be resolved by invoking alternate Web service implementations that perform the same business task as the failed one. However, the inclusion of such provisions is a tedious assignment for the business process designer, while additional effort would be required to maintain the BPEL scenarios in cases that some alternate WS implementations cease to exist or new ones are introduced. In our research we are developing a framework for automating handling of that kind of exceptions. The proposed solution employs a pre-processor that enhances BPEL scenarios with code that detects failures, discovers alternate WS implementations and invokes them, fully thus resolving the exception. Alternate WS implementation discovery is based on service relevance, which takes into account both functional and qualitative properties of Web services."
1298,"Existing service recommendation methods, that employ memory-based collaborative filtering (CF) techniques, compute the similarity between users or items using nonfunctional attribute values obtained at service invocation. However, using these nonfunctional attribute values from invoked services alone in similarity computation for personalized service recommendation is not sufficient. This is because two users may invoke the same service, but their personalized preferences on nonfunctional attributes that describe the service may be different. Thus, to accurately personalize service recommendation, it is necessary for CF-based recommendation systems to incorporate users personalized preferences on nonfunctional attributes when recommending services to an active user. This paper proposes a CF-based service recommendation method that considers users' personalized preference on nonfunctional attributes. We first compute the satisfaction of an active user's preference on nonfunctional attribute(s) and then use these satisfaction values to obtain their similarity measures. We then employ the top-k algorithm to identify neighbors of the active user and subsequently, use the weighted average with mean offset method to predict his/her nonfunctional attribute. We evaluate our method using real-world services and also conduct experiments to show that the proposed method improves recommendation accuracy significantly."
1299,"QoS-based service selection is one of the important requirements in Service Oriented Computing (SOC). A challenging task towards this purpose is the selection of the best combination of services that fulfils user's requirements while meeting quality of service (QoS) constraints. This challenge becomes more complex when dealing with time-dependent QoS values and temporal properties. Indeed, during the selection, mutual dependencies between the different temporal constraints may arise so that the selection of each service may influence or be influenced by the selection of other services. On other side, to find the best solution, all potential combinations must be compared. However, the number of these combinations may be very high, which can present a barrier for enabling effective service selection. In this paper, we present a heuristic based time-aware service selection approach to efficiently select a close-to-optimal combination of services. First, pruning techniques are adopted to reduce the search space. Second, a novel heuristic approach is proposed based on service clustering, constraints decomposition and local selection while considering both QoS and temporal constraints. Finally, experiments which confirm the feasibility and effectiveness of the proposed approach in terms of its timeliness and optimality, are conducted."
1300,"Due to the large and increasing number of web services, it is very helpful to provide a proactive feed on what is available to users, i.e., Recommending web services. As collaborative filtering (CF) is an effective recommendation method by capturing latent factors, it has been used for service recommendation as well. However, the majority of current CF-based service recommendation approaches predict users' interests through the historical usage data, but not the service description. This makes them suitable for making QoS-based recommendation, but not for functionality-based recommendation. In this paper, we propose to use machine learning approaches to recommend web services to users from both historical usage data and service descriptions. Considering the great popularity of Restful services, our approach is applicable to both structured and unstructured service description, i.e., Free text descriptions. We exploit the idea of collaborative topic regression, which combines both probabilistic matrix factorization and probabilistic topic modeling, to form user-related, service-related, and topic related latent factor models and use them to predict user interests. We extracted public web service data and developer invocation history from Programmable Web and conducted a comprehensive experiment study. The result indicates that this approach is effective and outperforms other representative recommendation methods."
1301,"QoS-aware Web service composition has recently become one of the most challenging research issues. Although much work has been investigated to solve the problem, they mainly focus on certain QoS of Web services, while QoS with uncertainty exposes the most important characteristic in a real and highly dynamic environment on the Internet. In this paper, with the consideration of uncertain service QoS, we model the issue of Web service composition with QoS uncertainty that is translated into a multi-objective optimization problem via uncertain interval number, which can be solved by our proposed approach via an non-deterministic multi-objective evolutionary algorithm using the strategy of decomposition. Large-scale empirical experiments have been conducted on our simulated datasets. The experimental results demonstrate that our proposed approach can effectively and efficiently find an optimum composite service solution set with satisfactory convergence."
1302,"We examine the lifecycle requirements of intermittently connected web-applications (ICWAs) and investigate whether such applications can be developed as an ""always connected"" web-application combined with middleware that address ICWA requirements. We show that this is difficult to do because ICWAs require application-specific logic that is not easily combined with a middleware API. We therefore propose the use of ""middleware components"" in the areas of data-provisioning and change-set propagation. Combined with application-specific logic, these components make it easier to develop an ICWA by reducing the amount of required developer code. We show how our prototype UNTETHER system implements these components and reduces the burden on the application developer."
1303,"A variety of extended transaction models have been proposed in distributed database community, and the development of such models has drawn more attentions since they are not adequate for long-running Web services. From a formal method perspective, a Flexible transaction model for Web services based on MPi-calculus is proposed in this work. MPi-calculus, without introducing any new operators, creates the dynamic association between the changes of transaction scope and their interactive actions in the original Pi-calculus. According to the flexible transaction dependency, a new weak transactional open bisimulation relationship is presented to characterize the transactional equivalence. All of the results can serve as the theoretical foundation to model flexible and powerful mechanisms for implementing long-running transactions."
1305,"On-demand computing has transformed enterprise software, lowering risk and cost while increasing user adoption and customer success. To be successful, an application must be designed for on-demand from the ground-up, including core architectural elements such as multi-tenancy, availability, performance, security, metadata-driven customization, integration via Web services, etc. As with any new paradigm, initial applications must design and implement all these core attributes, but ultimately platforms emerge that encapsulate core computing services, allowing application developers to focus on innovation and value, and not on reinventing the wheel. With AppExchange, salesforce.com has delivered the first on-demand platform, allowing developers to easily develop and deliver the next generation of on-demand applications. In this talk, Steve Fisher discusses the technical architecture of the AppExchange platform"
1306,"Service composition is nowadays mainly seen as a once-for-all activity. Supporting a dynamic service world, where both available services and needs may change, requires runtime adaptive features for service composition. In this paper we propose a repair technique for internal composition adaptation, as opposed to external adaptation. Moreover, setting up our proposal in the planning framework, we compare our repair technique with reference to re-composition, that is re-planning."
1307,"With the prevalence of SOA, an increasing number of Web services are created and composed to construct complex business processes. Selecting an appropriate service from a lot of independently developed services which have the same functionality but different QoS properties is essential for the effect of the composite service according to users' preference. Moreover, the efficiency and effect of the service selection algorithm also play an important role. In this paper, we propose a novel algorithm, named QSSAC, for service selection problem. This algorithm is based on the service clustering which can cluster a lot of atomic services of each task into a few classes according to their QoS properties. With the help of service clustering, our algorithm is able to reduce the execution time and guarantee the near-optimal result as well. Finally, three strategies are provided for re-selecting atomic services in dynamic environment. In experiment, we study the performance of QSSAC algorithm, and its feasibility has been demonstrated by simulation."
1308,"Web service recommendation plays an important role in building reliable service-oriented systems for both the service providers and the active users. However, with the proliferation of web services on the World Wide Web, traditional service recommendation is hard to accurately provide customized services to active users. In this paper, we propose a novel web service recommender model using collaborative filtering to improve the prediction of Quality-of-Services. Benefiting from the accuracy of hybrid recommenders, we extend the idea of optimized predicting order and design the Graph to describe the neighborhood. Furthermore, a new algorithm using adjusted topological sorting for Graph is proposed to generate the optimized order while predicting. Finally, we conduct extensive experiments to evaluate our proposed model, in which a real data set with 1.5 million invocation information is taken as input. The experiment results show that our model achieves higher prediction accuracy than other models."
1309,"Mash up has emerged as a promising way to compose web APIs and create value-added compositions. The increasing of APIs demands more accurate recommendation algorithms. However, service domain evolution, mash up-side cold-start and information evaporation are somehow overlooked by existing work. In this paper, by extending the collaborative topic regression (CTR) model, the procedure of service selection is modeled with a generative process, and the mash up-side cold-start problem that cannot be dealt with by naïve CTR is resolved. By learning the maximum a posteriori estimates of the whole generative process, both content information and historical usage are taken into consideration to extract service domains, thus the service domains can evolve with the evaluation of historical usage pattern. Meanwhile, information evaporation is also considered by giving time-related confidence levels to historical usage to track the evolution of service ecosystem. Experiments on the real-world Programmable Web data set show that compared with the state-of-the-art methods, our approach gains a 6.8% improvement in terms of recommendation accuracy."
1310,"Recommender systems can be used to assist groups of users to select services in Internet of Things (IoT)-enriched environments. However, aggregating the preferences of the individual users of a group, which is generally used in group recommendation, is not appropriate for IoT environments, where the user groups' preferences for IoT-based services differ significantly from those of individual users. In this paper, we propose a user-based collaborative filtering approach that considers member organization for a new user group. We select neighbor user groups that are similar to the new group based on combinations of member organization-based group similarity (MOGS) metrics such as the group size-based, common member-based, and member preference-based metrics. We conduct experiments to evaluate our approach using real-world datasets collected from practical IoT testbed environments. The results demonstrate that the proposed approach is effective in improving the performance and stability of service recommendations in IoT environments regardless of the locational characteristics."
1311,"Dynamic service compositions pose new verification and validation challenges such as uncertainty in service membership. Moreover, applying an entire test suite to loosely coupled services one after another in the same composition can be too rigid and restrictive. In this paper, we investigate the impact of service selection on service-centric testing techniques. Specifically, we propose to incorporate service selection in executing a test suite and develop a suite of metrics and test case prioritization techniques for the testing of location-aware services. A case study shows that a test case prioritization technique that incorporates service selection can outperform their traditional counterpart - the impact of service selection is noticeable on software engineering techniques in general and on test case prioritization techniques in particular. Further-more, we find that points-of-interest-aware techniques can be significantly more effective than input-guided techniques in terms of the number of invocations required to expose the first failure of a service composition."
1312,"Mobile devices are evolving as a new computing platform and a common means to provide access and process digital information. In the move to achieve ubiquitous computing, the role of mobile devices and web services cannot be understated. Mobile devices are predominantly in use for accessing web services. The same mobile devices are now becoming a feasible option for small and medium size enterprises to host and provide personalized web services to clients. The motivation being the minimal infrastructure requirements and configuration costs. Technology today enables the provision of web services over hand-held mobile devices, realizing a web based service-oriented architecture in a mobile environment. For this, an efficient service discovery mechanism is required. It is difficult to adapt traditional approaches of managing web service directory for mobile environments, this is mainly due to the dynamic arrivals/departures of mobile devices in network zones. In this paper, we propose a model for web services in mobile environments to maintain a service directory using the XMPP (eXtensible Messaging and Presence Protocol). The proposed model enables mobile devices to manage web service directory without requiring high-end computers and high management cost. This paper presents proposed architecture, design concept, system components and workflow of the framework. Moreover, a comparative study of the proposed approach and the traditional UDDI (Universal Description, Discovery, and Integration) registry is presented."
1313,"A satisfactory solution for data-intensive web service is required to overcome the problems of web service performance and scalability. We address these problems and propose service atomization, a new approach in atomizing information analysis that tackles large scale information. It uses a measure to ease the creation and collaboration of services. We call this measure service enthalpy, which comprises enthalpy of data intensification, functionality simplification, analysis shall owing and data uniformization."
1314,"Online trading takes place in a very complex environment full of uncertainty in which deceitful service providers or sellers may strategically change their behaviors to maximize their profits. The proliferation of deception cases makes it essential and challenging to model the dynamics of a service provider and predict the trustworthiness of the service provider in transactions. Recently, probabilistic trust models have been used to assist decision making in computing environments. Although the typical Hidden Markov Model (HMM) has been used to model a provider's behavior dynamics, existing approaches focus only on the outcomes or ignore the hidden characteristics of the HMM model. In this paper, we model the dynamic trust of service providers concerning a forthcoming transaction in light of as much information as we can consider, including the static features, such as the provider's reputation and item price, and the dynamic features, such as the latest profile changes of a service provider and price changes. Based on a service provider's historical transactions, we predict the trustworthiness of the service provider in a forthcoming transaction. In addition, the Mutual Information theories and the Principle Component Analysis method are leveraged to eliminate redundant information and combine essential features to form lower dimensional feature vectors. Furthermore, by adopting Vector Quantization techniques, we apply the discrete HMM in a more powerful way, in which all the features extracted from both contextual information and the rating of each transaction are treated as observations of HMM. We evaluate our approach empirically in order to study its performance. The experiment results illustrate that our approach significantly outperforms the state-of-the-art probabilistic trust methods in accuracy in the cases with complex changes."
1315,Most of the current semantic representation methods for service capabilities are usually based on top-down methodology. We aim to develop services characterization methods with statistical study on existing Web services and to improve services capability representation with bottom-up software services comprehension. Two services characterization methods are proposed in our work: quantitative statistical study which used for probing the distribution of main objects in Web services and relational statistical study which used for clustering actions or contents and measuring the similarity of Web services. A statistical relational model is proposed for mining and recognizing the patterns of services.
1316,This article presents a theoretical framework distinguishing four systematically different levels of Web service technology adoption. Based on Swanson's IS innovation theory the conjecture is made that each level of Web service technology adoption has important differences in adoption requirements and the potential outcomes. The framework is intended to be a conceptual basis for methods to evaluate Web service technology adoption and may consequently help devise strategies to effectively adopt Web service technology.
1318,"We suggest that the composition of Web services is an activity that needs to be managed, and that Web service composition management is distinct from the management of individual Web services. We describe a set of requirements to help make this distinction. The four main groups of these requirements are: service discovery, service selection and contract formation, composition verification, composition management. Then, we discuss architectural alternatives (centralized, federated, and peer-to-peer) for a Web service composition management framework."
1319,EAI and SOA are widely adopted in enterprise information systems with business processes being orchestrated by a process engine. An algorithm which schedules tasks of process instances to maximize overall customer satisfaction is proposed in this paper. This new algorithm maximizes the total value of all process instances by dynamically assigning different priority to each task based on the business value and time-left of the corresponding process instances. The value of each kind of process instance is modeled as a utility function of response time which reflects the inverse proportional relationship between the customer satisfaction and response time. Experiments show that the total value of utility function with the proposed algorithm is promoted a lot than traditional methods.
1320,"WS-BPEL processes can facilitate service discovery when the services have multiple interfaces in certain order. Current approaches derive the abstract WS-BPEL processes directly from the corresponding executable ones by hiding or omitting the internal activities. However, these simple approaches may prevent the services from being found by valuable potential partners at service discovery stage. To address this problem, we propose a novel approach to refactoring the executable and abstract WS-BPEL processes for service discovery. We show the application of our approach through a typical travel agency service."
1321,"HPC2-ARS supports a high performance cloud computing (HPC2) based streaming data analytic system, which ensures real-time response on unpredictable and fluctuating Big Data Streams by provisioning and scheduling computing resources autonomously. It focuses on parallel high-volume streaming applications, which have stringent real-time constraints and bring Big Data issues. It is a brand-new three-layered architecture, which solves three essential problems: (a) how many resources are needed for each application to achieve real-time analytic on streaming Big Data, (b) where to best place the allocated resources to minimize resource consumption, and (c) how to minimize response time for parallel applications. In summary, HPC2-ARS provides high performance streaming services."
1322,"This paper presents the design and implementation of Heterogeneous Event Management Middleware (HEMM) that integrates events across multiple siloed workflow management systems (WFMS) in enterprises. Specifically, we address the issue where a high-level change (e.g. change in business policy or rolling out of a new service) necessitates external events (generated from other enterprise system) to be handled by running workflow (WF) instances in the WFMS. Current solutions either require a change in the workflow process definition or change in the workflow engine - both being cost intensive solutions from an enterprise 's point of view since they compromise on enterprise backend downtime. HEMM circumvents this by introducing an overlay on top of existing WFMS and abstracting out unforeseen event handling from workflows. At the same time, it employs event transformation to map events to appropriate event handlers of running workflow instances, enabling them to adapt to new events that their process definitions do not handle. We demonstrate our prototype implementation with the example of rolling out a bundled service that is prevalent in telecom industry."
1323,"Information diffusion has been studied between and within biosphere, microblogs, social networks, citation networks and other domains, where the network structure is present. These studies have turned to be useful for acquiring intrinsic knowledge for strategic decision making in related areas, for example, planning online campaigns in case of microblogs and blogosphere. In the context of data-centric Web services, information exchange patterns will reveal practical heuristics for efficient Web services selection and composition. For example, by possible knowledge that there is information flow between Web services of \textit{Financial Analysis Services} and \textit{Enterprise Resource Planning Services}, as outlined by our experimental results, the potential applications, which interact with \textit{Financial Analysis} services, can be adjusted to take advantage of \textit{Enterprise Resource Planning} services as well. In this paper we present a method for analyzing information diffusion between categories of data-centric Web services. The method operates on a Web services network constructed by linking interface descriptions of categorized Web services. The proposed method is evaluated on a case study of global Web services. The results indicate high potential of the proposed model in understanding interactions between categories of Web services."
1324,"Nowadays, with the great diffusion of mobile technology, and ubiquitous systems, the context has become the ear and the eye of information systems. These systems are more and more based on the usage of Web services. The classical architecture of these services that allows an interoperable interaction between service users and providers does not take in account context adaptation. In this article, we aim to integrate context adaptation within the classical architecture of Web services, adding to it dedicated components that would return to nomadic users a list of Web services that are adapted not only to his profile but also to his context."
1325,Current approaches for service adaptation focus on either the message interface aspect or the control flow interface aspect separately. Our work recognizes that message adaptation may affect control flow adaptation and vice versa in complex ways. Hence an integrated approach is necessary. We propose a framework for integration and identify a set of extendible message adaptation patterns to solve typical message mismatches. In addition we give an algorithm for generating new message adapter on the fly so as to integrate control flow considerations into message adaptation. Finally we show how these individual patterns can be combined by another algorithm to create a complete adapter for two processes. The advantages of our method are illustrated with a case study. We present the design of a prototype and show XSLT code for implementing the message transformation.
1326,"With the development of web service technology, identifying and discovering the users with similar preferences have an important significance to service selection and service optimization in the service environment. In order to divide the users into groups based on their preference similarity in the process of service selection, a combination-based clustering algorithm, named AAK, is presented in this paper. The method combines the K-means algorithm with the Affinity Propagation (AP) algorithm to cluster the users with similar preferences. In the clustering process, the algorithm makes full use of the advantages of the two algorithms, including the high partition accuracy of K-means algorithm and the independence in the prior knowledge of AP algorithm, which breaks the limitation of using a single clustering algorithm. Then a parallel execution model of the algorithm is built and implemented by a high order MapReduce sequence linking technology. Finally AAK algorithm is compared with its serial model and the other combination-based clustering methods on Matlab platform and Hadoop platform. The experimental results show that AAK algorithm can be applied to distinguish user group with different preferences and has a good effectiveness and efficiency."
1327,"To leverage the strengths of different distributed systems, a user may want to develop an application that spans multiple frameworks. Currently, in such cases a user needs to use a different code generator for each one of the distributed frameworks that the application incorporates. Learning the details of the interface specification and code generation tool for each one of these distributed frameworks is tedious and error-prone Instead, it is desirable to present the user with a flexible code generation framework that leverages the wide variety of available XML-based tools and is capable of generating code for all distributed systems. Our work involves the design and implementation of an XML schema based toolkit that can serve as the universal code generation toolkit for distributed frameworks."
1328,"The semantic web promises to bring automation to the areas of web service discovery, composition and invocation. In order to realize these benefits, rich semantic descriptions of web services must be created by the software developer. A steep learning curve and lack of tool support for developing such descriptions thus far have created significant adoption barriers for semantic web service technologies. In this paper, we present a model-driven architecture based approach for specifying semantic web service compositions through the use of a UML profile that extends class and activity diagrams. This profile is used in transformations that facilitate automatic construction of OWLS specifications from UML diagrams. Conditions required by the composition, such as those on control constructs, are specified using OCL and transformed into SWRL during the construction process."
1329,Business artifacts allow to manage operations of business processes by capturing the key concepts and relevant information to guide their work flow. The Guard-Stage- Milestone (GSM) meta-model is a novel formalism for designing business artifacts that features declarative description of the intended behaviour without requiring an explicit specification of the control flow. Its concept of hierarchical structures of stages and explicit rules for the fulfilment of their guards and milestones supports the designing process but poses a challenge for formal verification. We show here how to approach the verification problem by developing a symbolic representation amenable to model checking. The feasibility of the approach is demonstrated by presenting a case study on the direct verification of a GSM model using a tool implementation.
1330,"With the rising popularity of Web services, both academia and industry have invested considerably in Web service description standards, discovery, and composition techniques. The standards based approach utilized by Web services has supported interoperability at the syntax level. However, issues of structural and semantic heterogeneity between messages exchanged by Web services are far more complex and crucial to interoperability. It is for these reasons that we recognize the value that schema/data mappings bring to Web service descriptions. In this paper, we examine challenges to interoperability; classify the types of heterogeneities that can occur between interacting services and present a possible solution for data mediation using the mapping support provided by WSDL-S, the extensibility features of WSDL and the popular SOAP engine, Axis 2"
1331,"The use of SOAP based communication protocol in complex Service-Oriented Architecture (SOA) environments can cause a significant strain on network resources thereby degrading application level performance. In this paper, we present software and hardware approaches for transparent line-rate encoding and decoding of SOAP traffic for alleviating network bandwidth bottlenecks. The proposed approaches are significantly lighter weight than gzip and binary XML encoding techniques while achieving better compression ratios. First we present a software-based scheme with a linear runtime that uses a finite set of pre-selected message templates for differential encoding of SOAP messages. We propose generating the templates by extracting the longest common subsequence from a representative set of SOAP messages encountered during the service operation. Next we present an architecture for line-rate implementation of data differencing functionality in hardware. As compared to the software-based scheme, the hardware design allows higher flexibility in the template selection and message encoding without compromising the encoder throughput. We test both schemes on a sample workload running on a SOAP based Business Intelligence software platform. Our results show that the proposed techniques can achieve an average of 10X reduction in the overall SOAP traffic. Furthermore, we show that the differentially encoded data can be further compressed by gzip to achieve a combined 15-17X reduction in the message sizes as compared to less than 5X compression obtained when using gzip by itself."
1332,"PROMCODE (PROject Management for COntracted DElivery) is an innovative research consortium to change the way of project management on the resource-oriented services platform. It was launched by Nanzan University and six major Japanese IT companies including Fujitsu, Hitachi, IBM, NEC, Nomura Research Institute, and NTT DATA in May, 2012. Since then, we have developed a common data model to exchange project management data across organizational boundaries along with software supply chains, and supporting service platform to facilitate the data exchange. This article reports on the resource-oriented service model based on a common data model. We also developed a resource-oriented platform for exchanging data based on the OSLC (Open Services for Lifecycle Collaboration), an open standard for tool chain compliant to Web standards, namely RDF/Linked Data and RESTful Web services. To study the feasibility of the developed technologies, we, six companies, conducted intensive experience with real project management data. We discuss the effectiveness of the developed technologies and lessons learned from the experiences."
1333,"Although EDI (electronic data interchange) have provided a useful method to the automated message interchanging between enterprises, this method costs a lot of money and resources, there is a new technology named ebXML (electronic business using eXtensible Markup Language) BPSS (business process specification schema) which provides a more easy way to integrate the whole business process. EbXML BPSS supports the specification of the set of elements required to configure a runtime system in order to execute a set of ebXML business transactions. EbXML BPSS describes which message should be interchanged and how to interchange, and provides a set of specification for the process automation. In this study, we propose a federated multi-agents system, which can implement the business process, and describe a detailed mechanism to apply the BPS to a prototype implementation."
1335,"Emerging Internet of Things(IoT) applications are moving from silo and small scale sensor data sharing to composite and large scale ones. With the rapid growth of application scale, IoT applications is going to leverage cloud infrastructure for scalable solutions and real-time services. Thus large volumes of heterogeneous and high frequency sensor data are fed into IoT cloud services for real-time actionable insight, which raises great challenges of performance and adaptability on cloud solutions. In this paper, we propose a streaming based processing infrastructure for high throughput and low latency IoT real-time analytics services. A data adaptive mechanism is also introduced for heterogeneous data stream integration, interpreting and processing with application logics, as well as context stream. We implemented the proposed mechanisms with spark streaming, and deployed real time IoT analytics service in cloud. Experiment results show that the service has a good scalability and high throughput for IoT data analytics."
1336,"Analyzing interactions among peers that interact via messages is a crucial problem due to increasingly distributed nature of current software systems, especially the ones built using the service oriented computing paradigm. In service oriented computing, interactions among peers participating to a composite service involve message exchanges across organizational boundaries in a distributed computing environment. In order to build such systems in a reliable manner, it is necessary to develop techniques for analysis and verification of interactions among services. Collaboration diagrams provide a convenient visual model for modeling service interactions. In this paper, we present a tool that (1) checks the realizability of interactions specified by the given collaboration diagram, (2) verifies the LTL properties of the interactions specified by the given collaboration diagram by automatically converting it to a state machine model, and (3) synthesizes peer state machines that realize the set of interactions specified by the given collaboration diagram."
1337,"In today's global economy, companies are challenged to become more efficient and agile in the face of increased transparency in their supply chains and rising compliance requirements. In addition, IT hardware continues to increase in price and performance resulting in more information storage, processing capabilities and network throughput. In fact, by the year 2010 we could see over 100 cores per server with multiple terabytes of memory."
1338,"Search results in technical forums are typically keyword based. The relevance of a link is usually gauged by closest content match. However, it has been shown in literature that users' click behavior is an integral part of deciding the relevance of a search result. Moreover, it is not just the number of clicks that matter, but time spent on a clicked link, order in which the links were clicked etc. also play an important role in the relevance decision. In this paper, we have developed a service that analyzes the click logs of searches performed in the technical forums and learns the new relevance scores for the search results with respect to a query. The computation model for relevance is an optimization problem, the constraints for which have been designed based on real user behavior study. We ingested StackOverflow data for few domains and designed a QA style search to carry out the study. We have developed heuristics to solve the optimization problem and have validated the relevance model using user behavior simulations. The relevance model is shown to yield efficient, robust and effective rank order using DCG (discounted cumulative gains) and stability metrics."
1339,"Business-to-business will be a considerable market in the near future of Internet e-business. In this future market, several providers need to be able to integrate or exchange information in providing a global service. The problem that we want to tackle in this paper is related to the existing information sources in the current Internet environment. That is how to integrate existing Web sites each other to become a new Internet service. The difficulty comes from a historical objective. Internet Web sites were developed for human users browsing and so, they do not support machine-understandable as well as interprovider interaction. To overcome this gap, we need a framework to systematically migrate the existing presentation-oriented Web sites to service-oriented one. Evidently, redeveloping all of them is an unacceptable solution. In this paper, we propose a mechanism of Web service gateway in which existing Web sites are wrapped by several Web service wrappers. Thus, without any efforts to duplicate the Web sites code, these services inherit all features from the sites while can be enriched with other Web Service features like UDDI publishing, semantic describing, etc. As a consequence, they can be easily integrated to each other in a business-to-business schema to provide a more valuable service for users. This Web service gateway was developed in Toshiba with Web Service Generator, allowing to automatically generate Web service wrappers. By using this system, several ""real"" Web services were generated and made available for use. The Web service gateway and these services are also presented and evaluated in this paper."
1340,"Composing a set of Web services as a service process is becoming a common practice, but it involves multiple service invocations over the network, which incurs a huge time cost. To accelerate its execution, we propose an engine-side block-based service process caching strategy (BPCS). It is based on, and derives its advantages from, three key ideas. First, the invocation of Web service embodies semantics which enables the application of semantic-based caching. Second, cachable blocks are identified from a service process and each block is equiped with a separate cache so that the time overhead of service invocation and caching can be minimized. Third, a replacement strategy is introduced taking into account time and space factors to manage the space allocation for a process with multiple caches. The algorithms and methods used in BPCS are introduced in detail. Finally, BPCS is validated with a detailed performance study on real service processes and Web services via comparison experiments, which shows considerable improvements of BPCS over other strategies."
1341,"A critical aspect for pervasive computing is the possibility to discover and use process knowledge at run time depending on the specific context. This can be achieved by using an underlying service-based application and exploiting its features in terms of dynamic service discovery, selection, and composition. Pervasive process fragments represent a service-based tool that allows to model incomplete and contextual knowledge. We provide a solution to automatically compose such fragments into complete processes, according to a specific context and specific goals. We compute the solution by encoding process knowledge, domain knowledge and goals into an AI planning problem. We evaluate our approach on different scenarios stress testing the main characteristics of pervasive process fragments."
1342,"We investigate scalable algorithms for automated composition (WSC) of Semantic Web Services. Our notion of WSC is very general: the composition semantics includes background knowledge and we use the most general notion of matching, partial matches, where several web services can cooperate, each covering only a part of a requirement. Unsurprisingly, automatic composition in this setting is very hard. We identify a special case with simpler semantics, which covers many relevant scenarios. We develop a composition tool for this special case. Our goal is to achieve scalability: we overcome large search spaces by guiding the search using heuristic techniques. The computed solutions are optimal up to a constant factor. We test our approach on a simple, yet powerful real world use-case; the initial results attest the potential of the approach."
1343,"This intense and unique tutorial provides a direct, step-by-step walkthrough of the key factors and considerations for migrating an organization from its current state to a service-oriented architecture. It discusses key areas of concern that should be addressed in this transformation process from current to future state. The identification, modeling and design of appropriate services at a proper level of granularity that are business aligned and leverage IT systems is a key issue addressed by a Service-oriented Analysis and Design. Thus, in the second part of this tutorial we provide a step-by-step introduction to Service-oriented Analysis and Design using the SOMA (service-oriented modeling and architecture) method [1]. Using this method in conjunction with key web services standards and tools, we present a set of concrete process steps that lead to the realization of an SOA."
1344,"The goal of this paper is to formalize various processing approaches to the process of verification of the web services integrity and to show its vulnerability to many attacks. After introducing the necessary terminology, several processing approaches are particularized and its security drawbacks concerning notably XML Signature W3C Recommendation are divided into several areas, analyzed, and the solutions are described and compared. If the solution of the particular problem does not exist, or is not yet fully standardized, it is marked as an open problem and the solution is proposed."
1345,"An important challenge in today's software development domain is to integrate diverse development tools. In this paper we present a method that combine various modeling languages which underline a set of steps for the purpose of specifying tool integration, and then establish tool chains above the Service cloud and provide these tools as Services. A case study from the embedded system development area is used to exemplify the method, highlighting the benefits and challenges of establishing tool chains above the service cloud."
1346,"Web services search and discovery systems have received a great deal of attention recently. However, little attention has been given to a Web services filtering system. In this paper, we propose a design and architecture of a semantic Web services filtering system called SWFilter. SWFilter uses Prufer sequences transformed from Web services and user queries. The filtering process is performed in a bottom-up manner. Ontology information and compositions of Web services are also considered in the filtering process."
1347,"Context-Aware Recommender System (CARS) aims to not only recommend services similar to those already rated with the highest score, but also provide opportunities for exploring the important role of temporal, spatial and social contexts for personalized web services recommendation. A key step for temporal-based CARS methods is to explore the time decay process of past invocation records to make the Quality of Services (QoS) prediction. However, it is a nontrivial task to model the temporal effects on web services recommendation, due to the dynamic features of contextual information in view of temporal spatial correlations. For instance, in location-aware services recommendation, the user's geographical position would change very frequently as time goes on. In this paper, we propose a Context-Aware Services Recommendation based on Temporal Effectiveness (CASR-TE) method. Inspired by existing time decay approaches, we first present an enhanced temporal decay model combining the time decay function with traditional similarity measurement methods. Then, we model temporal spatial correlations as well as their impacts on the user preference expansion. Finally, we evaluate the CASR-TE method on WS-Dream dataset by evaluation matrices of both RMSE and MAE. Experimental results show that our approach outperforms several benchmark methods with a significant margin."
1348,"Web service composition, i.e., WSC, has emerged as a promising way to integrate various distributed computing resources for complex application requirements. However, much computation time is needed to determine the optimal composite solution, which embarrasses the popularity of WSC in actual real time applications. In view of this challenge, in this paper, a heuristic service composition method, named LOEM (Local Optimization and Enumeration Method, LOEM), is proposed. It aims at filtering the candidates of each task to a small number of promising ones by local selection, and then enumerates all the composite solutions to pursue a near-to-optimal one. The experiment results demonstrate the feasibility of LOEM in dealing with the WSC problems."
1349,"In this paper, we characterize the problem of how to synthesize a reliable service mediator that can guarantee reliable interaction of the mediated Web services. We first present formal models of reliable protocols for Web services and service mediators. Then, we introduce an algorithm for synthesizing reliable service mediators."
1350,"Previews of web links are typically generated based on the metadata captured from the URL content. Sometimes, the preview sentences are extracted by means of content summarization. Such web link previews can be seen in different apps like the web browser, chat app, messaging or email apps etc. These previews are static in nature and do not change with respect to changing context. Therefore, they may not be particularly relevant to the receiver of the link. In this paper, we present a web service for generating intelligent previews in a chat application, which captures the local intent of the user from the chat content and uses it to display only relevant content extracted from the previewed URL. Since the user intent can change dynamically, our system generated previews are also dynamic, which change on the fly if it detects a change of topic being discussed in the current chat. We describe details of a prototype web service implementation, with three methods for preview generation based on TF-IDF and Word2Vec word embedding. We also present results of an evaluation using shared URLs from a private real-world chat group as well as a sample chat app with a few users to determine the accuracy of the preview generation system."
1351,"In this paper, we present a new SaaS (software as a service) design for employee job performance appraisals, SaaS-JPA. We use IoT and computer systems to collect data related to the daily works of employees. A semantic model is developed to guide the data collection process, facilitate data interpretation and interoperation, and enable big data analysis to make job performance appraisal decisions. We also propose two new performance assessment models: The similarity-based relative performance model and the revenue-based performance model. These performance models are enabled by the service technologies and big data analytics. Finally, we discuss the design of SaaS-JPA."
1352,"Reuse of service orchestrations or service compositions is extensively studied in the literature of process modeling. Sub-processes, process templates, process variants, and process reference models are employed as reusable elements for these purposes. The concept of process fragments has been previously introduced in order to capture parts of a process model and store them for later reuse. However, similar efforts on facilitating the reuse of processes that cross the boundaries of organizations expressed as service choreographies are not available yet. In this paper, we introduce the concept of choreography fragments as reusable elements for service choreography modeling. Choreography fragments can be extracted from choreography models, adapted, stored, and later inserted into new models. Based on a formal model for choreography fragments, we define methods and algorithms for the extraction and insertion of fragments from and into service choreographies. We then discuss an experimental and proof-of-concept evaluation of our proposal."
1353,"In this paper, we propose the concepts of virtual partner and inspector into the Web services composition. Virtual partner, as an IT level concept, is a Web service (pseudo Web service) using the same interface with the actual partner but different binding message. A virtual partner can be invoked directly by a business process described by BPEL, so that the BPEL programmer can test both application's functionality and non functionality performance early in the development cycle to avoid any problems in the final runtime, or test the selection of their partners in business level design. The IT virtual partners provide developers with a range of the techniques which let them explore every aspect of their program. Inspector is proposed when using the third-party process engine. An inspector itself is also a Web service. The programmer can register any required output information in it. The IT virtual partner and the inspector concepts have been integrated in our WSCE, a flexible Web Services Composition Environment for a business process. WSCE is a prototype of autonomic modeling and simulation environment. With the help of a third-party BPEL engine, it provides programmer with concepts and tools to facilitate business process programming."
1354,"This paper introduces the concept of temporal logic of actions (short for TLA), with which we can formally specify the behavior of a service, and compose Web services. The approach is demonstrated by an example. A services composition algorithm is presented."
1355,"The web service community has introduced many techniques to cope with the inability of WSDL to describe a service's behavior. Those techniques range from embedding more XML tags in WSDL, to generate formal behavioral models on top of WSDL. Apart from the efficiency of these techniques, a common problem is that they require manual efforts to model the behavior of a service, and often need informal documentation from service vendors to do so. In this paper, we propose a solution for the above problem by automatically extracting a service's behavior, directly from its WSDL document. Our approach is based on the utilization of particular WSDL elements, which are usually ignored by bottom-up approaches while generating a WSDL file. We illustrate our process in steps by taking a scenario from Amazon E-commerce Web Service. We also survey the issues with extracting the behavioral models, from the WSDLs of existing web services. Finally, we tested our automatically generated models by composing them together using our service composition framework."
1356,"WS-I's basic security profile (BSP) defines best practice guidelines for secure Web services communications, enabling interoperability between vendors. However it is difficult for developers to know if their SOA solutions are in fact compliant to these guidelines. In this paper, we discuss methods to assess compliance against BSP. We have implemented runtime validation of SOAP messages to check for compliance against BSP, a method implied by the BSP definition itself. Additionally, we have implemented a novel approach to statically validate WS security policies against BSP using Schematron. From our experiments dynamic validation for BSP compliance offers greater coverage but results in a significant overhead, while static validation is limited in its scope but extremely valuable since under reasonable assumptions it provides assurances about compliance prior to deployment. We conclude with a summation of our results and lessons for SOA practitioners."
1357,"Popularity of service-oriented computing makes more and more companies and organizations provide their services through Web Application Program Interfaces (Web APIs). The Web APIs are considered to offer a convenient way to integrate web services to client applications. However, the integration process is often challenging. For example, updated Web APIs may be no longer compatible with the current version of client applications, thus break the client applications. To help the integration process, it is of significant interest to understand the challenges that are encountered by client developers. Developer forums and Stack Overflow are commonly used by client developers to seek help from fellow peers. In this paper, we mine both developer forums and Stack Overflow to find the common challenges encountered by client developers. We perform an empirical study on 32 Web APIs with a total of 92,471 discussions. To extract topics from all discussions, we apply a topic modeling technique called Latent Dirichlet Allocation (LDA). The results show that on average five dominant topics can cover at least 50% of questions regarding each Web API. We further investigate how topics evolve across Web APIs, and find five patterns. As a summary, our findings highlight a list of dominant concerns and persistent concerns for each Web API that Web API providers should pay more attention to."
1358,"Mashup is a developer-centric technique which allows developers to compose existing Web services together to create innovative or consolidated web applications. However, the rapid growth in the number of services and the myriad of functionally similar services make it difficult for developers to select appropriate ones to develop new applications. Therefore, it is vital to recommend a set of suitable services for mashup creation based on functionalities of services and their relationships. To this end, we propose a service package recommendation approach for mashup development, which is based on mashup textual description mining to discover semantic relationships among services. Specifically, discourse analysis of computational linguistics is utilized to uncover the structures underneath mashups' functional specifications, then the semantic relationships between services can be learned from their appearances and the constructed structures in mashup specifications. Accordingly, we are able to recommend a package of services that can be used together with high compatibility for a new mashup to be developed. We evaluate our approach on a real-world dataset. Experimental results show that our approach achieves higher accuracy and outperforms other comparative ones."
1359,"In this paper we enhance the mediation approach for interoperability in dynamic manufacturing networks (DMN) by decreasing the number of iterations that should be performed in order to find the right adaptation function between an expected message and the incoming ones. Then, we define a deployment architecture for mediators in dynamic collaboration environments that better handles the changes that could occur at run-time. We then conduct a set of experiments that demonstrate the efficiency of the introduced approach and architecture."
1360,"Central to the notion of dynamic binding and loose coupling that underlie service-oriented architectures is dynamic service discovery. At the heart of most service discovery mechanisms is a matchmaking algorithm that matches a semantic query to a set of compatible web service advertisements. These advertisements also describe service semantics as a set of OWL-S terms. Most current matchmaking algorithms are based on semantic matching of input and output terms alone. However, a complete description of the service profile also includes preconditions and effects and in order to find a true match the matchmaker needs to match on these aspects of the advertisement as well. In this paper, we make the case for augmenting existing matchmaking algorithms with preconditions and effects in the context of Web Services. Further, we propose an algorithm for condition matching that is layered on the top of input-output term matching that overcomes the limitations of existing work. Although the problem of condition matching is NP-Complete, we can overcome this limitation by using a set of heuristics that gives us results in polynomial time. We also analyze complexity of the algorithm by comparing it with brute force approach of matching. We show that our algorithm yields results more efficiently than brute force matching but with the same accuracy."
1361,"Documentation of provenance, the process which was taken to create a particular data item, is critical within service oriented architectures where loose coupling between services and clients (actors) is employed. Such a process may involve interaction between multiple services, with each service being managed by a particular actor. The recording of actor state may provide critical contextual information regarding the state of a particular actor at a point during a client-service interaction, but is however typically left undocumented. We discuss the issues that are encountered with the automatic documentation of actor state and the types of resources which may be used to provide it. An architecture which allows monitoring tools to be related to user needs is presented and evaluated against a number of scenarios in a Web Services environment."
1362,"Provision of services within a virtual framework for resource sharing across institutional boundaries has become an active research area. Many such services encode access to computational and data resources, comprising single machines to computational clusters. Such services can also be informational, and integrate different resources within an institution. Consequently, we envision a service rich environment in the future, where service consumers are represented by intelligent agents. If interaction between agents is automated, it is necessary for these agents to be able to automatically discover services and choose between a set of equivalent (or similar) services. In such a scenario trust serves as a benchmark to differentiate between services. In this paper we introduce a novel framework for automated service discovery and selection of Web Services based on a user's trust policy. The framework is validated by a case study of data mining Web Services and is evaluated by an empirical experiment."
1363,"Object-level coherence in distributed applications and systems has been studied extensively. Object coherence in platform-specific and tightly-coupled systems is achieved with binary serialization protocols to ensure data structures and object graphs are safely transmitted, manipulated, and stored. On the opposite side of the spectrum are platform-neutral Web services that embrace XML as a serialization protocol for building loosely coupled systems. The advantages of XML to connect heterogeneous systems are plenty, but rendering programming-language specific data structures and object graphs in text form incurs a performance hit and presents challenges for systems that require object coherence. Achieving the latter goal poses difficulties by a phenomenon that is sometimes referred to as the ""impedance mismatch"" between programming language data types and XML schema types. This paper examines the problem, debunks the O/X-mismatch controversy, and presents a mix of static/dynamic algorithms for accurate XML serialization. Experimental results show that the implementation in C/C++ is efficient and competitive to binary protocols. Application of the approach to other programming languages, such as Java, is also discussed"
1364,"In this paper, we present WIP - Web service initiation protocol for multimedia and voice communication over IP. WIP is an entirely Web service based communication protocol, consisting of a set of Web service operations for initiating and establishing converged (e.g. multimedia, IM, voice, etc.) communication services over IP. It inherits the principle of separation signaling and media transmission of SIP (session initiation protocol); but it relies on a single Web service stack to provide a full featured communication signaling protocol. WIP opens a new paradigm of Web service based VoIP communication, which is extensible and can be easily integrated in end-to-end SOA solutions. The generic Web service approach used in WIP overcomes many limitations which would be otherwise difficult to achieve in non-Web service based communication methods used today. WIP is based on two-way, full duplex Web service interaction. The communication signaling establishment in WIP is through Web service interactions, and the media negotiation in WIP is modeled as a special Web service ""event"" subscription, which is fully extensible for various media needs. The signaling messages of WIP are encoded in the standard based SOAP message envelops which can be carried by multiple transport protocols, including HTTP. WIP supports both P2P (peer-to-peer) and B2B (back-to-back) broker mode communication services. A prototype research system has been implemented, and the results indicate that WIP, as a full Web service based communication protocol, is both feasible and advantageous"
1365,"As a result of recent trends in enhancing Service-Oriented Requirement Engineering (SORE) activities, a number of requirement specification methods have been proposed for fitting the reuse infrastructure in a Service-Oriented Architecture (SOA). The availability of different Requirement Engineering methods offers developers a range of options to choose from. However, most of existing research effort uses traditional Requirement Engineering methods in service-based application developments. During requirements specification, a reusable infrastructure of available web services is not considered at all. The risk is that atomic requirements do not always fit reusable services. As a result, the service composition is time-consuming and needs costly adaption. This paper therefore proposes a novel method by introducing service discovery in the early Requirement Engineering stages so as to guide the requirement decomposition process. Although several researchers have already recommended to involve service discovery in SORE, they do not focus on how to guide requirement decomposition. Our approach is implemented on top of the widely used goal-oriented approach. To this end, we leverage a semantic service discovery method as a means to act as a guide and sentinel in requirement elaboration. We demonstrate the requirement decomposition process by implementing a case study from the Business Traveling domain."
1366,"In the ocean dynamical environment real-time observing system of Taiwan Strait and adjacent maritime region, multiple observing data such as remote sensing data, structured data and so on are produced by the observing net from airspace, ocean surface, underwater space and ocean bottom. The data sharing and web service is a key part to influence the application efficiency of the whole system. According to the characteristics of the oceanic dynamical environment of Taiwan Strait, the construction scheme of the observing net is introduced in this paper firstly. Then the architecture of the observing data sharing and web service system is introduced, which includes five parts, i.e. the observing data acquiring module, the data integration module, the data processing and information production development module, and the data sharing and web service module. Next, the user classification system and service content classification are introduced. At last, the technology realization strategy and application in shipwreck salvation is introduced."
1367,"Cloud broker is an entity that manages the use, performance and delivery of cloud services and negotiates relationships between cloud providers and cloud consumers. In real life scenarios, automated cloud service brokering is often challenging because the service descriptions may involve complex constraints and require flexible semantic matching. Furthermore, cloud providers often use non-standard formats leading to semantic interoperability issues. In this paper, we formulate cloud service brokering under a service oriented framework, and propose a novel OWL-S based semantic cloud service discovery and selection system. The proposed system supports dynamic semantic matching of cloud services described with complex constraints. We consider a practical cloud service brokering scenario, and show with detailed illustration that our system is promising for real-life applications."
1368,"Semantic Web approaches are often used for Web service description, modeling, semantics discovery, capabilities matching, etc. However, as the primary querying tool for Semantic Web, SPARQL is yet to be deeply explored to support Semantic Web service composition. Therefore the description, modeling and composition of Semantic Web services are usually two-tier. This paper extends SPARQL to support path query, so that SPARQL is used in our service composition framework which finds the top-k shortest data flows that satisfy the user constraints. Experiment results on real-world Web service datasets exhibit its applicable performance compared with service compositions using other SPARQL engines/extensions."
1369,"In todays cloud market, providers are taking advantage of consumer reviews and ratings as a new marketing tool to establish their credibility. However, to achieve higher ratings, they need to enhance their service quality which comes with an additional cost. In this paper, we model this conflicting situation as a Stackelberg game between a typical service provider and multiple service users in a cloud environment. The strategy of the service provider is to adjust the price and IT capacity by predicting the users ratings as well as their demands variation in response to his given price, quality and rating. The game is solved through a backward induction procedure using Lagrange function and Kuhn-Tucker conditions. To evaluate the proposed model, we performed experiments on three real world service providers who have low, medium and high average of users' ratings, obtained from the Trust Feedback Dataset in the Cloud Armor project. The results show that improvement in ratings is mostly profitable for highly rated providers. The surprising point is that providers having low ratings do not get much benefit from increasing their average ratings, meanwhile, they can perform well when they lower the service price."
1370,"The increasing number of services available on the Web requires sophisticated mechanisms for the matchmaking, selection and composition of services based on business criteria. Such mechanisms require descriptions that address business terms, but existing approaches for the modeling of service properties are inadequate for expressing business conditions. In this paper, we propose a novel Service Description Language for Contract specification (SeDL-C). The proposal is based on a detailed analysis of business terms to identify the set of requirements a model and a language should fulfill. Formal syntax and semantics for SeDL-C are given."
1371,"Responding to social issues is very crucial because their impact can be significant to organizations or individuals. In this paper, we focus on proposing the method that identifies the personalized relevance of social issues to targets, such as individuals or organizations. To achieve this aim, we first collected trending social issues from Google Trends, microblog, and Internet news. Then, we obtained the well-structured document management system as a target domain that contains all activities regarding target objects. We applied the Term Frequency Inverse Document Frequency to obtain the personalized relevance weight of the social issue to a target."
1372,"Web services, with an emphasis on open standards and flexibility, can provide benefits over existing capital markets integration practices. However, Web services must first meet certain technical requirements including performance, security and so on. SOAP, based on Extensible Markup Language (XML), inherits not only the advantages of XML, but its relatively poor performance. This makes SOAP a poor choice for many high-performance Web services. In this paper, we propose a new approach to improve Web services performance. Focusing on avoiding traditional XML parsing and Java reflection at runtime, we create a service-specific SOAP processor to accelerate execution. Through our experiments in this paper, we observed that our approach obtained about a treble performance gain (maximum) by incorporating the SOAP processor into the SOAP engine"
1373,"Data intensive applications, e.g. in life sciences, pose new efficiency challenges to the service composition problem. Since today computing power is mainly increased by multiplication of CPU cores, algorithms have to be redesigned to benefit from this evolution. In this paper we present a framework for parallelizing service composition algorithms investigating how to partition the composition problem into multiple parallel threads. But in contrast to intuition, the straightforward parallelization techniques do not lead to superior performance as our baseline evaluation reveals. To harness the full power of multicore architectures, we propose two novel approaches to evenly distribute the workload in a sophisticated fashion. In fact, our extensive experiments on practical life science data resulted in an impressive speedup of over 300% using only 4 cores. Moreover, we show that our techniques can also benefit from all advanced pruning heuristics used in sequential algorithms."
1374,"Today, the choice for a particular programming language limits the alternative products that can be used to deploy the program. The purpose of this work is to break the strong ties between programming languages and runtime environments and thus make it possible to innovate at both ends independently. While this goal has been pursued in previous work, the specific focus of this work is on Web services and service-oriented architectures (SOAs); focusing on this domain and its particular properties makes it possible to achieve this goal with affordable efforts. The key idea is to introduce a Service Language Layer (SLL) which gives a high-level abstraction of a service-oriented program and which can easily and efficiently be executed on alternative Web services platforms."
1375,"Solving general real-life problems requires a set of appropriate services to be composed via planning, scheduled, and then executed. Web service composition is the most difficult aspect and is our focus. In this paper, we describe a new framework for intelligent semantic Web services that supports the planning and scheduling aspects by a combined HTN planner and CSP. The framework covers all of the procedures needed to deal with a user's request, including domain analysis of the request, task flow decisions and CSP creation by the planner, and solving the CSP by a distributed CSP solver"
1376,"Although reuse is the main goal of SOA, composing existing services to realize different user requirements is still a difficult and time-consuming task. Research on workflow templates and design patterns can facilitate reuse and assist with the service composition task. Nevertheless, workflow templates are too specific, whereas design patterns may be too abstract; their effectiveness in assisting the composition process may be limited. In this paper, we propose a comprehensive service pattern model that is more flexible than workflow/service templates while allowing systematic instantiation into the concrete workflows. It can help ease the composition process and enable flexible pattern-based reuse."
1377,"The challenge of optimally selecting services from a set of functionally appropriate ones under Quality of Service (QoS) constraints -- the Service Selection Problem -- has been extensively addressed in the literature based on deterministic parameters. In practice, however, Quality of Service QoS parameters rather follow a stochastic distribution. In the work at hand, we present an integrated approach which addresses the Service Selection Problem for complex workflows in conjunction with stochastic Quality of Service parameters. Accounting for penalty cost which accrue due to Quality of Service violations, our approach reduces the impact of stochastic QoS behavior on total cost significantly."
1378,"This paper presents a systematic literature review of end-user service composition. It reviews current activities performed by end users, and tools and approaches that enable them to compose and develop service systems from Web Services. The paper also highlights some key open research issues for the future."
1379,"This paper describes the graphical user interface design and implementation of LexOnt, a semi-automatic ontology creation tool. LexOnt is built as a Protégé plugin and uses the Programmable Web directory as its corpus. Current semi-automatic ontology creation systems are domain specific and rely on already structured text. LexOnt, on the other hand, is built specifically for those who are not experts within a domain, but want to understand the domain on a high-level and create an ontology that describes it. Using well-known NLP algorithms, LexOnt generates a list of top terms and phrases from the Programmable Web corpus to enable users to find high-level features that distinguish one Programmable Web service category from another. To also aid non-experts in a domain, LexOnt relies on outside sources such as Wikipedia and Wordnet to help the user identify the important terms within a service category. We describe the details of LexOnt's interactive interface that allows users to semi-automatically find top terms and phrases of a service domain and create ontology entities describing it with these features."
1380,"Ajax-based web applications are designed to mimic more traditional desktop applications and require quick response times from the underlying Web services. However, since availability and performance of Web services cannot be guaranteed, response time and overall performance of Ajax-based applications can vary. In this paper we describe a framework for developing autonomic self-healing Web service-based applications that rely on the notion of differentiated services (i.e., services that provide common behavior with variable quality of service) in order to maintain required performance characteristics. We present the expected impact of the framework through the use of a theoretical QN model, demonstrate the framework with an example, and provide an evaluation of the technique."
1381,"BPEL and similar languages have been provided a foundation for process-based implementation of composite Web services. These languages allow definition of orchestration processes without concrete binding information of involved partners. Although this approach facilitates customization of service discovery and selection before process execution, partner management during process execution has not been considered, such as rebinding of service providers upon events. In response to this problem, this study proposes a description model that facilitates runtime partner management in process-based services. The proposed model includes policy descriptions that allow insertion and customization of binding behavior according to surrounding environments and user preferences. The model also includes extensions in the standard process notation in order to allow process developers to give constraints to avoid adoption of inadequate policies as well as to define handlers to have common rebinding behavior reused. The proposed descriptions have been implemented and used in an agent framework for multimedia services."
1382,"Composition of Cloud services is necessary when a single component is unable to satisfy all the user's requirements. It is a complex task for Cloud managers which involves several operations such as discovery, compatibility checking, selection, and deployment. Similarly to a non Cloud environment, the service composition raises the need for design-time approaches to check the correct interaction between the different components of a composite service. However, for Cloud-based service composition, new specific constraints, such as resources management, elasticity and multi-tenancy have to be considered. In this work, we use Symbolic Observation Graphs (SOG) in order to abstract Cloud services and to check the correction of their composition with respect to event-and state-based LTL formulae (Hybrid LTL). The violation of such formulae can come either from the stakeholders' interaction or from the shared Cloud resources perspectives. In the former case, the involved services are considered as incompatible while, in the latter case, the problem can be solved by deploying additional resources. Using our approach, one can check then, if the resource provider service can supply sufficient Cloud resources w. r. t. the users' requests."
1383,"With the development of Web Services, a phenomenon of multiple users request for one service simultaneously has become more and more popular. To address the challenges of service selection for multi-users' requirements, we propose an interval number based service selection method (INSSM). We firstly propose a practical method for expression of users' requirements with multiple dimensional interval numbers. Secondly, we partition candidates into neighboring service sets based on hierarchical clustering. Thirdly, we compute matching degree between neighboring service sets and users' requirements. Finally, we select an appropriate service according to the matching degree results and load degree. Simulation results demonstrate that our method can not only make use of service resources more effectively but also avoid service overload more significantly compared with other existing methods."
1384,"Next-generation service offerings will integrate information from multiple interconnected servers. For the commercial success of these services, the ability to deliver good end-to-end quality-of-service (QoS) is crucial. Today, no mature solutions exist for the problem of realizing high and guaranteed end-to-end QoS for transaction-based services in multi-domain environments. Service level agreements (SLAs) are a well-recognized concept to obtain QoS guarantees, but currently no satisfactory solutions exist for SPs to determine the set of combinations of per-domain SLAs that they need to negotiate with the other domain owners to deliver the desired end-to-end QoS. To this end, in this paper we introduce the new concept called SLA negotiation space, i.e. the set of combinations of per-domain SLAs that SPs need to negotiate with other domain owners to realize desired end-to-end QoS levels. In addition, to identify the SLA negotiation space, we propose a modelling framework to quantify the complex relation between the per-domain SLA parameters and the end-to-end QoS. The practical usefulness of our results is demonstrated by a realistic example"
1385,"As more and more Web services are deployed, Web service's discovery mechanisms become essential. Similar services can have quite different QoS levels. For service selection and management purpose, it is necessary to explicitly, precisely, and unambiguously specify various constraints and QoS metrics for Web services descriptions. This paper provides a novel DAML-QoS ontology as a complement for DAML-S ontology to provide a better QoS metrics model. Three layers are defined together with clear role descriptions for developments. Cardinality constraints are utilized to describe the QoS property constraints. Basic profile is presented for general Web service's description and the speed startup of ontology definition. Matchmaking algorithm for QoS property constraints is presented and different matching degrees are described. When incorporated with DAML-S, multiple service levels can be described through attaching multiple QoS profiles to one service profile. Well-defined Metrics can be further utilized by measurement organizations to guarantee the promised service level."
1386,"Under current synchronous portal rendering mechanism, a user has to wait until all the portlets are ready for the next interaction. In the case of consuming remote portlets with Web services for remote portlets (WSRP) specification, the situation can be worse. To improve user experiences, an asynchronous mechanism is proposed in this paper leveraging popular Ajax technology. We also discussed several issues such as URL rewriting, cross-portlet interaction, security, etc. that have to be considered while applying asynchronous mechanism, and built up a prototype to prove that using Ajax in rendering remote portlets while providing cross-portlet interaction capability is feasible."
1387,"Recently, a new generation of adaptive process management technology has emerged, which enables dynamic changes of composite services and process models respectively. This, in turn, results in a large number of process variants derived from the same process model, but differing in structure due to the applied changes. Since such process variants are expensive to maintain, the process model should be evolved accordingly. In this context, we need to know which activities have been more often involved in process adaptations than others, such that we can focus on them when reconfiguring the process model. This paper provides two approaches for ranking activities according to their involvement in process adaptations. The first one allows to precisely rank the activities, but is expensive to perform since the algorithm is at NP level. We therefore provide as alternative an approximation ranking algorithm which computes in polynomial time. The performance of the approximation algorithm is evaluated and compared through a simulation of 3600 process models. Statistical significance tests indicate that the performance of the approximation ranking algorithm does not depend on the size of process models, i.e., our algorithm can scale up."
1388,"Service-oriented computing (SOC) suggests that many open, network-accessible services will be available over the Internet for organizations to incorporate into their own processes. Developing new software systems by composing an organization's local services and externally-available Web services is conceptually different from system development supported by traditional software engineering lifecycles. Consumer organizations typically have no control over the quality and/or consistency of the external services that they incorporate, thus top-down software development lifecycles are impractical. Software architects and designers will require agile, lightweight processes to evaluate tradeoffs in system design based on the ""estimated"" responsiveness of external services coupled with known performance of local services. We introduce a model-driven software engineering approach for designing systems (i.e. workflows of Web services) under these circumstances and a corresponding simulation-based evaluation tool"
1389,"The key objective of OPUCE system is to enable the participation of end-users in the management of their own services, by providing them with innovative tools which allow an easy creation and delivery of personalized communication and information services. This paper describes the OPUCE service and component repository, which extends the OMA OSPE service model storage approach XDM. By integrating an ebXML registry using the native notification mechanisms of XDM, the search capability of the repository is dramatically improved. Moreover, this repository also exploits semantic Web technology to provide an intuitive visualized browser for convenient service exploring."
1390,"Several approaches to web service selection and recommendation via collaborative filtering have been studied, but seldom have these studies considered the difference between web service recommendation and product recommendation used in e-commerce sites. In this paper, we present RegionKNN, a novel hybrid collaborative filtering algorithm that is designed for large scale web service recommendation. Different from other approaches, this method employs the characteristics of QoS by building an efficient region model. Based on this model, web service recommendations will be generated quickly by using modified memory-based collaborative filtering algorithm. Experimental results demonstrate that apart from being highly scalable, RegionKNN provides considerable improvement on the recommendation accuracy by comparing with other well-known collaborative filtering algorithms."
1391,"Quality of Service (QoS) characterizes the non-functional aspects of Web service, which is the key factor for satisfying service users and has to be carefully considered in service oriented architecture. There has been a lot of works on QoS description and service selection by QoS information. However, two issues have not been substantially explored in QoS management-how to collect QoS information from running services, and how to dynamically adapt service provisioning with run-time QoS information. Moreover, current monitoring approaches are focused on monitoring the service interaction in a single domain, while cross-domain monitoring has not been studied yet. In this paper, we present a policy-driven monitoring framework for collecting QoS information and adapting service provisioning for cross-domain service interaction. The monitor is built on our distributed QoS registry-Q-Peer. The monitor is user-centric, metric-oriented, feedback-enabled and loosely-coupled with service providers. Services are observed by capturing interaction messages, so the monitoring approach has no impact to service providers. The framework supports dynamic configuration of monitoring metrics for both single service and cross-domain composite service. It can be used as a reliable third-party QoS monitor for Web services."
1392,"For resource management purpose, administrators usually need to perform what-if analyses to predict the impact of any workload growths or planned changes on the performance of web services. A what-if analysis requires not only the design of system models, but also the workload models that represent the real-world user behavior. Existing methods of workload characterization based on probabilistic graphical models are quite complex if there are many web services provided by a system. Meanwhile, bandwidth resource is usually not taken into account in many related works, though it is a relatively expensive resource in cloud markets. In fact, it's very challenging to predict the network throughput of modern web services due to the factors of client-side caching, miscellaneous service responses and complex network transportation. In this paper we propose a methodology of what-if analysis named Log2Sim for the bandwidth management of web systems. We use a lightweight workload model to describe user behavior, an automated mining approach to obtain characteristics of workloads and responses from massive web logs, and traffic-aware simulations to predict the impact on the network throughput and the response time within changing contexts of user behavior. We also choose a real-life web system as use case to evaluate the effectiveness, accuracy and stability of this methodology."
1393,"In this paper, we overviewed self-healing issues of Web services using AOP. This type of programming supports separation of self-healing concerns from Web services code and promotes maintenance and reusability. Similar to any computing application, Web services are subject to failure and unavailability due to multiple reasons, such as Web service faulty-code and unreliable communication-infrastructure"
1394,"Dynamic service composition in wireless environment provides us with a promising approach to build complex applications based on the basic value-added services. In different network domains, multiple services may provide data with different security levels. In order to prevent from information leakage, information flow security is a major concern in composite services. However, the energy-limited nature of user terminal in mobile computing environments poses a significant challenge for the centralized information flow verification where the verification node need cost lots of computation and network resources. In this paper, we specify the security constraints for each service participant to secure the information flow in service chain based on the lattice model, and then present a decentralized information flow verification framework that cooperates different service participants to complete the verification process distributively with respect to their information flow policies. Through the experiments and evaluations, the results show it decreases the verification cost on single service node."
1395,"In web browsers with cloud assisted rendering such as Opera Mini and UC Mini, the rendering information in the HTML file on the cloud server is compressed to an intermediate lightweight format to save speed and bandwidth before sending to the client device. This causes a loss in rendering quality of content such as shapes, text and spacing since the JavaScript, CSS and animation information is lost while compressing the file. Also, the response to user gestures on the client web browser to resize the web content also results in reduced rendering quality. Thus the resultant rendering quality on the client device is not identical to the original web page when pixel by pixel comparison is performed. In this paper we present the architecture of a rendering engine that distributes the rendering process among the client and server in such a way that the pixel by pixel rendering is identical to the original web page. Our solution seeks to incorporate the best of server assisted and pure client side rendering, by saving on bandwidth, memory and power and also not compromising on the rendering quality. We present a couple of alternate methods in which the architecture can be implemented, and present the results of some tests to prove the latency and bandwidth saving. This system is useful in a variety of web browsers especially those embedded in home appliances."
1396,"Service composition is the process of automatically constructing a workflow from individual services so as to satisfy user requirements. When composing service workflows, it is important that both functional and non-functional requirements need to be considered. The so-called QoS-aware service composition is typically formulated analogous to the classical MMMKP optmization problem, and does not account for service and QoS dependencies which are often indispensable in real life. In this paper, we consider QoS-aware service composition in the presence of service-dependent QoS and propose a novel method that dynamically refines the composed workflow in light of QoS dependencies and user-provided topological and QoS constraints. Through evaluations, we demonstrate that our approach is capable of offering significant improvements in performance on real life scenarios with complex service and QoS dependencies."
1397,"In this paper, we present an architecture-centric assessment approach for model evaluation over reference architecture to quantitatively estimate architecture maturity and quality. Such assessment is essential to support design-level refinement for an enterprise solution. To achieve this analytic goal, we select a nine-layer SOA solution stack (S3) as reference architecture, and introduce the necessary mathematical definitions and formulation. The baseline for such assessment is a model template composed of S3 solution patterns. A template is the starting point of creating a design model. The selection of such template will largely determine the architecture properties of the final SOA solution.The maturity analysis is carried out at different granularity levels (architecture building block, architecture layer, and architecture model) to justify the 'completeness' of a design. The quality assessment is accomplished through a set of quality-indicators to justify the 'goodness' of an architecture based on the relationships of architecture building block instances. Finally, using UML 2.0 to capture the model of S3, we provide a real assessment prototype developed over IBM RSA platform."
1398,"Business process (BP) stakeholders want to enjoy the benefits of the cloud, but they are also reluctant to expose their BP models which express the know-how of their companies. To prevent such a know-how exposure, this paper proposes a design-time approach for transforming a BP model into BP fragments so that these BP fragments externalized in a multi-cloud context do not allow a cloud resource provider to understand a critical fragment of the company. While existing contributions on this topic remain at the level of principles, we propose an algorithm supporting automatically such a BP model transformation."
1399,"The increasing presence and adoption of Web services on the Web has promoted the significance of management of new service development for service developing sectors. The major challenge is that how to find missing but potentially valuable Web services to be developed. This problem can be divided into two sub-problems: finding missing Web services and measuring the added-value of the introduced services. This paper addresses a plausible solution to the first sub problem. Given a collection of Web services, we propose a framework for suggesting a set of candidate Web services that can be introduced to the collection. These suggested services are novel and do not present in the given collection. Our solution relies on the network structure of Web services for finding and recommending new Web services and utilizes the already observed properties of Web services networks for collective evaluation of the suggested services. The proposed solution is evaluated using 753 semantically annotated Web services. The experimental results shows that the proposed framework provides web service community with new network driven methods for finding and evaluation of new Web services."
1400,"We propose a new social-sensor cloud services trust model. We propose to represent social media data streams, i.e., images' meta-data and related posted information, as social-sensor cloud services. Images' meta-data and the related posted information are abstracted as the functional and non-functional aspects of the social-sensor cloud services. The trustworthiness of a social-sensor cloud service is measured based on the users' stance based trust model. We use the textual features of the social-sensor cloud services, i.e., comments and meta-data, e.g., spatio-temporal information to gather the trust-rate of the service. Analytical results are presented to show the performance of the proposed model with real datasets."
1401,"Summary form only given. This tutorial seeks to discuss the key concepts in collaborative business transaction management. Its intent is to explain the concept in business transaction and how it is different from traditional database transaction and workflow transaction management, to evaluate existing approaches, and to present existing techniques from other areas that can be adopted for business transactions and their limitations, and lastly to discuss a framework that addresses the challenges that are unique to business transaction management in the service oriented environment."
1402,"Software architectures of large-scale systems are perceptibly shifting towards employing open and distributed computing. Service Oriented Computing (SOC) is a typical example of such environment in which the quality of interactions amongst software agents is a critical concern. Agent-based web services in open and distributed architectures need to interact with each other to achieve their goals and fulfill complex user requests. Two common tasks are influenced by the quality of interactions among web services: the selection and composition. Thus, to ensure the maximum gain in both tasks, it is essential for each agent-based web service to maintain a model of its environment. This model then provides a means for a web service to predict the quality of future interactions with its peers. In this paper, we formulate this model as a machine learning problem which we analyze by modeling the trustworthiness of web services using probabilistic models. We propose two approaches for trust learning of single and composed services; Bayesian Networks and Mixture of Multinomial Dirichlet Distributions (MMDD). The effectiveness of our approaches is empirically assessed using a simulation study. Our results show that representing the quality of a web service by Multinomial Dirichlet Distribution (MDD) provides high flexibility and accuracy in modeling trust. They also show that using our approaches to estimate trust enhances web services selection and composition."
1403,"Large-scale graphs processing, which draws attentions of researchers, applies in a large range of domains. However, large-scale graphs processing on traditional platforms suffers from difficulties including computation and memory inefficiency. To enhance the computation-efficiency and energy-efficiency, in this paper, we exploit graph processing services on the energy-efficient hardware accelerator, called Domino. Domino adopts the asynchronous model to process graphs, which is efficient for many graph algorithms, such as Breadth-First Search, Depth-First Search, and Single Source Shortest Path. Domino also proposes a specific data structure based on row vectors to present graphs, named Batch Row Vector. Besides, our work employs naive update mechanism and bisect update mechanism to perform the asynchronous control. Ultimately, we implement Domino on an advanced Xilinx Virtex-7 board, and experimental results demonstrated that Domino has a significant performance and energy improvement. Case studies in Domino achieve 1.47x-7.84x and 0.47x-2.52x average speedup for small-diameter graphs(e.g., com-youtube, WikiTalk, and soc-LiveJournal), over GraphChi on the Intel Core2 and Core i7 processors, respectively. Besides, compared to Intel Core i7 processors, Domino also performs a significant energy-efficiency that is 2.03x-10.08x for three small-diameter graphs and 27.98x-134.50x for roadNet-CA which is a graph with relatively large diameter."
1404,"Several works address the problem of the automated composition of stateful services, e.g., specified in WS-BPEL. However, the key problem of their practical applicability in real composition scenarios is still open. Addressing this problem requires to provide an easy and affordable way to specify behaviors of component services and composition requirements, as well as composition techniques that are powerful enough to scale to scenarios of realistic size. In this paper we provide a first step towards addressing this problem by evaluating the feasibility and efficiency of a state-of-the art approach to automating the composition task on a real scenario that entails a high level of complexity: the Amazon e-commerce services and an e-payment service offered by an important Italian banking Group (Monte dei Paschi di Siena). We show the feasibility of the specification of composition requirements and of the automated generation of complex executable WS-BPEL processes, and analyze the possibility of reducing the effort in the composition task."
1405,"Software reuse through Application Programming Interfaces (APIs) is a common practice in software development. It remains a big challenge to bridge the semantic gap between user expectations and application functionality with the development of Web-based services. This paper proposes a service recommendation approach via extracting semantic relationship from natural language API descriptions and inferring. To validate our approach, large-scale experiments are conducted based on a real-world accessible service repository, ProgrammableWeb. The results show the effectiveness of our proposed approach."
1406,"There is a wealth of research on web service attack types and different techniques to mitigate them. However, there is little discussion on reusable methods for implementing these known techniques. In this paper, we introduce two handler architectures that can be reused to implement a broad set of known attack countermeasures. While structurally similar, the architectures differ in the information they require for attack detection, in the needed changes to or restructuring of the message and its content, and in their invocation order among other handlers deployed on the application server and used by the web service. We present the handler architecture designs and how they address the specific web service attack types. We discuss the benefits of their attachment to the Web service. Also, we cover their implementation and deployment details on a JBoss application server and provide a case study to document the results of test runs."
1407,"In service computing, the behavior of a service may evolve. When an organization develops a service-oriented application in which certain services are provided by external partners, the organization should address the problem of uninformed behavior evolution of external services. This paper proposes an adaptive framework that bars problematic external services to be used in the service-oriented application of an organization. We use dynamic WSDL information in public service registries to approximate a snapshot of a network of services, and apply link analysis on the snapshot to identify services that are popularly used by different service consumers at the moment. As such, service composition can be strategically formed using the highly referenced services. We evaluate our proposal through a simulation study. The results show that, in terms of the number of failures experienced by service consumers, our proposal significantly outperforms the random approach in selecting reliable services to form service compositions."
1408,"In the last few years, the field of Web services (WS) security has evolved rapidly producing an impressive number of WS-based security standards. This fact has caused that organizations are still reticent about adopting technologies based on this paradigm due to the learning curve necessary to integrate security into their practical deployments. In this paper, we present PWSSec (process for Web services security) as a process that enables the integration of a set of specific stages into the traditional phases of WS-based systems development providing them with security. PWSSec is composed of three stages, WSSecReq (Web services security requirements), WSSecArch (Web services security architecture) and WSSecTech (Web services security technologies) that allow the specification of WS-specific security requirements, the definition of the WS-based security architecture and the identification of the security standards that the security architecture must deploy, respectively"
1409,"A framework is proposed to improve Web service performance based on context-aware communication. Two key ideas are introduced to represent a client context: (1) available protocols that the client can handle, and (2) operation usage that shows how the client uses Web service operations. We call our context aware approach a situated Web service (SiWS). We implemented and evaluated the SiWS and found that the overall performance was improved if more than three Web services were executed between context changes"
1410,"Service composition refers to the aggregation of services to build complex applications to achieve client requirements. It is an important challenge to make it possible for users to construct complex workflows transparently and thereby insulating them from the complexity of interacting with numerous heterogeneous services. We present an extension to the Triana PSE to facilitate graphical Web service discovery, composition and invocation. Our framework has several novel features which distinguish it from other work in this area. First, users can graphically create complex service compositions. Second, Triana allows the user to share the composite service as a BPELAWS graph or expose it as a service in a one-click manner. Third, Triana allows the user to easily carry out ""what-if"" analysis by altering existing workflows. Fourth, Triana allows the user to record provenance data for a workflow. Finally, our framework allows the user to execute the composed graph on a Grid or P2P network. Triana is a part of the GridLab and GridOneD projects and is used in the GEO 600 project."
1411,"In this paper, we propose a time-sensitive probability skyline (TPS) approach to recommend services with uncertainty. We project services to n-dimensional data space and recommend services in TPS. Experimental evaluation on real data shows the great performance of TPS in service recommendation by comparing the experiment result with results of other approaches."
1412,"In this paper, we introduce the session initiation protocol (SIP) based multimedia conferencing on Internet, and mainly focus on the design and implementation for conferencing communication services model, such as SIP connection, session management, media control conferencing management, also we provide a prototype. Finally, we give the conclusions."
1413,"Web services are designed to standardize interactions between heterogeneous applications using Internet technologies. Within the framework of Internet search technologies, web services provide structured channels to access search engines and web-accessible databases. Our work involves research in methods to discover web service description language (WSDL) documents, which provide interface formats, expected data-types, supported protocols and precise service endpoints. This project extends current discovery research through use of the Google web service, UDDI category searching, and private registry querying with preliminary experiments resulting in a very high percentage of success. The goal is to find WSDL documents for a given domain name, parse the desired service document to obtain invocation formats, and automatically invoke the web service. Contributions of this research will support enhancements of HTML-dependent search tools by providing access to data inaccessible through surface HTML interfaces."
1414,"A configurable process model captures a family of similar processes. Such models can be configured to obtain a process variant according to specific requirements. With this aim, several approaches have been proposed for the configuration of process models. Nevertheless, an increasing attention is being paid to achieve this in a sound manner due to the complex inter-dependencies between the configuration decisions. In this work, we aim to guide the process analyst to easily configure process models while preserving soundness. To do so, we propose a formal approach for ensuring correctness of business process configurations while considering structural constraints they have to obey. Specifically, using the Event-B language, we formally define a configurable process model, its correctness-preserving conditions and its configuration constraints."
1415,"Different Web service standards, WS*, factorize Web service management tasks into different aspects, such as input/output, workflow, or security. WS* descriptions are exchangeable and developers may use different implementations for the same Web service description. Researchers investigating semantic Web services have clearly articulated these shortcomings of WS* standardizations and has been presenting interesting proposals to counter some of them. The kind of objectives that are to be approached are constrained by a costs trade-off between investing efforts for managing Web services and investing efforts for semantic modelling of Web Services. The objective of full automation by semantic modelling needs very fine-grained, detailed modelling of all aspects of Web services - essentially everything that an intelligent human agent must know. Thus, modelling costs skyrocket at the end of fine-grained modelling."
1416,"The rise of software-as-a-service has led to the development of Web 2.0 application. In many cases, the server's functionality is made publicly available as an instance of Web services. However, these services can't be invoked arbitrarily, and some behavior constraints must be obeyed. This paper explores an approach to generate data-centric properties automatically by mining execution logs."
1417,"In a distributed service-oriented multiagent system, agents have to cooperate with each other to complete decentralized service discovery tasks. Since system structure can influence the efficiency of service discovery, structural self-organization mechanism should be used to facilitate the decentralized service discovery in the system. During decentralized service discovery processes, different agents will present different intermediary utilities, and the intermediary utilities of agents imply the distribution information of system services in a certain extent. Therefore, the intermediary utility can be used to improve the efficiency of structural self-organization and service search. However, the existing self-organization based service discovery approaches ignore this issue. In this paper, we propose a novel self-organization based service discovery approach considering intermediary utility. The approach addresses the intermediary utilities of agents in both the structural self-organization mechanism and service search strategy. Besides, this paper considers maintaining the global connectivity of system structure during concurrent self-organization processes to guarantee the global accessibility of services. To the best of our knowledge, the proposed approach is the first self-organization based service discovery approach that can provably preserve the global connectivity of system structure. The experimental results show that the proposed approach outperforms the previous approaches and considerably improves the success rate and search efficiency of service discovery."
1420,"The promise of dynamic binding and the ability to dynamically and seamlessly move between service providers can only be realized through the path of semantic expressibility. Once we describe the semantics of a service in itpsilas advertisement, a semantic matchmaker can match a query with the set of advertisements that satisfy the query conditions. The state of the art today uses the IOPE (Inputs Outputs Preconditions and Effects) form of advertisements with languages such as OWL-S being used to represent the semantics. Inputs and outputs usually refer to concepts in an ontology. Many semantic matchmakers exist today and they focus on matching the IOPE form of the advertisement to the query. This may result in many matches and they return a set of matched advertisements without rank ordering them. We believe itpsilas important to rank the returned set of services in order to choose the best service. In this paper we present such a ranking algorithm that uses the IOPE information present in the each of the services relative to the query. Our algorithm can complement other approaches that use the history of issued queries and data mining. We have evaluated the effectiveness of the ranking algorithm against a benchmark ranking that is done manually and shown that our ranking scheme is close to the best ranking scheme possible."
1421,"A fundamental principle of service oriented architectures is the decoupling of service requesters and service providers to enable late binding of services at deployment time or even dynamic binding of services at runtime.This is important in enterprise settings, where different services that implement business functions in critical business processes are dynamically chosen based on availability or price. The same problem also applies to dynamic Grid environments where resources need to be dynamically chosen based on availability and other non-functional properties. The WS-Policy framework describes how policies for both providers and requesters are specified to allow the selection of services based on these policies. Existing approaches, using WS-Policy,have drawbacks by placing the burden of the service selection partially on the client. In this paper we present an extended enterprise service bus that allows service clients to submit policies to which service providers need to comply with together in one message with the service invocation request. We show how these policies are evaluated in the bus and how policies are defined for not only stateless services, but also stateful resources."
1422,Although WS-BPEL is a widely used language for modeling executable business processes in service oriented architectures it is almost impossible to dynamically bind services at runtime taking complex constraints and optimisation goals into account. The approach presented in this paper uses semantically annotated workflow templates and extensions to introduce adaptability which enables agile service oriented architectures. The technological solution will be validated in a critical infrastructure environment where resilience and security play an important role.
1423,"Web services composition has gained a considerable momentum as a means to create and streamline business-to-business collaborations within and across organizational boundaries. The composition approach proposed in this paper can enable user to generate a composite Web service with minimal execution price dynamically. After modeling Web services with rules and eliminating semantics conflicts among parameters of Web service models, we can compute a deduced network with inputs given by user. Based on the deduced network, a set of composition plans for every output can be obtained with a backward deduction approach. In order to get the composition plan which has the minimal execution price, all the output's composition plan sets should be joined by Cartesian product."
1424,"Web services are the main pillar of the service oriented computing (SOC) paradigm which enables the application integration within and across business organizations. One of the important features of the Web services is the choreography aspect which allows to capture collaborative processes involving multiple services. In this context, one of the important investigations is the choreography compatibility analysis. We mean by the choreography compatibility the capability of a set of Web services of actually interacting by exchanging messages in a proper manner. Whether a set of services are compatible depends not only on their sequences of messages but also on quantitative properties such as timed properties. In this paper, we investigate a model checking based approach that deals with checking the compatibility of a choreography in which the Web services support asynchronous timed communications. Particularly, in this paper we are using the model checker UPPAAL. We propose a set of required abstractions that allow to use the model checker UPPAAL to deal with timed asynchronous communicating services."
1425,"With the increasing popularity of Web services, a number of technologies have emerged that target the integration and composition of Web services as lightweight components. However, a number of problems have been identified in these approaches, for example regarding an overly static integration and lacking support for the modularization of crosscutting concerns. In this paper, we evaluate FuseJ, an architectural description language for unifying aspects and components, as an approach for the composition of Web services. We outline how FuseJ can be used to this end and present an evaluation that compares FuseJ to four other Web service composition approaches according to criteria such as the organization and flexibility of the composition and the support for aspect-oriented software development (AOSD). Although FuseJ does not allow describing complete business processes, we find that it excels at selective and dynamic composition and that it supports advanced separation of concerns without the need to introduce additional constructs."
1426,"Web service recommendation has become a critical problem as services become increasingly prevalent on the Internet. Some existing methods focus on content matching techniques such as keyword search and semantic matching while others are based on Quality of Service (QoS) prediction. However, services and their mashups are evolving over time with publishing, perishing and changing of interfaces. Therefore, a practical service recommendation approach should take into account the evolution of a service ecosystem. In this paper, we present a method to extract service evolution patterns by exploiting Latent Dirichlet Allocation (LDA) and time series prediction. A time-aware service recommendation framework for mashup creation is presented combing service evolution, collaborative filtering and content matching. Experiments on real-world ProgrammableWeb data set show that our approach leads to a higher precision than traditional collaborative filtering and content matching methods."
1427,"Web Services provide a standards-based open platform for integrating distributed service components. The development of large distributed XML Web Services is greatly simplified with XML data binding tools that automate XML parsing and serialization by binding XML to native data structures. This paper presents a schema slicing method to remove unused schema components from schemas, thereby significantly reducing the XML data binding code size of WSDL-based Web Services. Our results show that schema slicing applied to large Web Services, such as ONVIF, results in the removal of 70% of the schema components on average. Our method also obtains significant schema size reductions for several popular WSDL-based Web Services, such as eBay Web Services (10% reduction), PayPal Web Services (18% reduction), Microsoft Exchange Web Services (4% reduction), Amazon S3 Web Services (22% reduction) and ESRI ArcGIS Web Services (42% to 59% reduction). We implemented schema slicing in the popular gSOAP toolkit."
1429,"Web services and messaging, as application-to-application communication paradigms, have so far been considered separately, with independent programming models and supporting middleware. Different efforts are now introducing messaging notions such as asynchrony, greater consumer cardinality, and looser coupling between Web services. This trend will likely result in an extension of the Web services programming model. It is not clear, however, that this extension will adhere to a pre-planned approach. A coherent approach requires a thorough integration of the Web services and messaging paradigms. This paper proposes one such approach which, in addition to supporting the current style of Web services interactions, allows the incorporation of messaging-style interactions under a common programming model. These messaging-style interactions include asynchronous request-response, oneway multi-consumer interactions, and even multiple-choice point-to-point interactions, common in message queuing systems. This paper also elaborates on a model for oneway multi-consumer interactions that integrates the publish/subscribe mode of messaging into the Web services programming model. A primary motivation for our approach is to take advantage of key messaging features, while exerting as small an impact as possible on the Web services programming model"
1430,"The fast development of powerful mobile devices and rich Internet applications have boosted the production of Mobile Web applications designed to support end-users in their daily activities using smartphones. When these applications are the result of combining multiple heterogeneous data and services, the traditional one-size-fits-all development approach is not convenient since it does not consider the specificities of each potential user. New techniques and tools are then required to offer applications that better fit end-users constraints, preferences, and contexts while allowing them creating, consuming and sharing added-value services. We present in this article a novel mashup approach based on configuration theory and a visual tool that achieves this goal. As a proof of concept, we present an implementation scenario in the tourism domain."
1431,"The dynamic nature of the Grid environment brings challenges to the services provisioning technique. A major aspect of Grid is to match demands for services with resources. Since resources and demands are fluctuating, Grid services provisioning need to be adjusted accordingly in responding to changes. In this paper, we propose a novel approach, Grid services migration, to enabling services provisioning on demand. The major motivation is to achieve the desired Quality of Service (QoS) such as performance, reliability, and etc. of Grid services on resources assembled dynamically responding to dynamic demand. In this paper, the technical characteristics of Grid services migration and its taxonomy are introduced. The Grid service migration algorithms are also presented. The Grid services migration framework with MAPE (Monitor, Analyze, Plan, and Execute) loop is proposed based on Open Grid Services Architecture (OGSA) and the prototype named Services Ecosystem, a resilient infrastructure for services provisioning on demand in Grid is developed. Demonstrations and experiments are conducted and results are presented to illustrate the capabilities of Grid services migration."
1432,"Cloud computing has provided an attractive model for business service delivery. However, accountability aspects, mainly the monitoring of the execution of a service contract, liability assignment and dispute arbitration are still lacking. On the one hand, the traditional centralized monitoring and the trusted-third party (TTP) based arbitration solutions are not suitable for the distributed cloud environment. On the other hand, a decentralized solution also faces challenges in guaranteeing fairness, accuracy and sustainability. To address this issue, this paper firstly proposes an innovative service contract management scheme that facilitates the monitoring of the execution of a service contract in a peer-to-peer environment, inspired by the concept of blockchain in Bitcoin. Secondly, on top of the scheme it presents a novel dispute resolution protocol based on the Byzantine agreement and the commitment scheme. Thirdly, it identifies the optimal settings of the key parameters of the protocol through a set of experiments and scenario analysis, aiming to strike the balance of fairness, accuracy, incentive maximization for the honest arbiters and cost minimization for the overall arbitration process. With this approach, service participants can be held accountable in a truly distributed environment without the presence of a central authority, which increases businesses confidence on adopting cloud services."
1433,"Developers of situational applications - applications created by a small group of users/developers to satisfy the specific needs of the group - require two things from their software stack. First, they require support for their rapidly changing designs; second, they require semantics that are close to their own domain of expertise. For example, developers of situational applications prefer to use scripting languages because the ""duck typing"" style of programming language allows them to ignore issues related to class inheritance or interface implementation. In comparison to strongly-typed languages, developers can begin programming more quickly, and can modify their program more rapidly in response to changing requirements. In this paper we explore whether middleware services can similarly provide developers of situational applications with these desired software characteristics. Specifically, we present EDS, an Extensible Data-Service designed to support applications whose design changes rapidly and with semantics that are closer to the domain expertise of situational application developers. We present the features of EDS, contrast it to other data services and APIs, and discuss the EDS implementation."
1434,"A major goal of service-oriented architectures is to enable software interoperability in heterogeneous and dynamic environments. Web services standards and protocols aim to support this goal and middleware systems implementing these standards and protocols consequently are needed. Maintenance and administration of middleware is made difficult due to variations in standards and their constant evolution. In this paper, we introduce a new service-oriented middleware architecture for runtime Web services interoperability. Different from other middleware systems our approach applies service-oriented computing principles on the middleware layer, thereby establishing an on-demand model for middleware features. Clients can use middleware as services, dynamically discovering and using the services as interoperability requirements are determined. Further, middleware as services allows middleware to be provided and managed separately from its clients. We present the policy-based programming model, architecture, and details of our middleware, and discuss new challenges that arise in this context, such as distribution of middleware services. The approach is validated through a scenario integrating Web service transaction middleware"
1435,"Service network analysis is an essential aspect of web service discovery, search, mining and recommendation. Many popular web service networks are content-rich in terms of heterogeneous types of entities, attributes and links. A main challenge for ranking services is how to incorporate multiple complex and heterogeneous factors, such as service attributes, relationships between services, relationships between services and service providers or service consumers, into the design of service ranking functions. In this paper, we model services, attributes, and the associated entities, such as providers, consumers, by a heterogeneous service network. We propose a unified neighborhood random walk distance measure, which integrates various types of links and vertex attributes by a local optimal weight assignment. Based on this unified distance measure, a reinforcement algorithm, ServiceRank, is provided to tightly integrate ranking and clustering by mutually and simultaneously enhancing each other such that the performance of both can be improved. An additional clustering matching strategy is proposed to efficiently align clusters from different types of objects. Our extensive evaluation on both synthetic and real service networks demonstrates the effectiveness of ServiceRank in terms of the quality of both clustering and ranking among multiple types of entity, link and attribute similarities in a service network."
1436,"Decentralized execution settings are primordial for most processes where process participants are required to establish P2P interactions. This is particularly true for processes that subject the composition of services advertising different information flow policies. To provide a systematic approach to manage the information flow between composed services, we present a methodology that derives cooperating distributed processes of a centralized specification with respect to their information flow policies. Our contribution shows how service compositions, that cannot be realized with centralized orchestrations or inconsistent choreographies, can be realized with equivalent decentralized executions in an inexpensive and in a reasonable manner."
1437,"With the emerging technologies like cloud computing and Internet scale data centers, radically simplified deployment approaches are critical for the success of the even more complicated solutions. Although there have been many traditional solution deployment approaches, each of them mainly focuses on particular software type or product. In this paper, we propose the next generation solution deployment paradigm as hybrid solution deployment, which enables fast solution deployment from bare metal mode to production mode based on todaypsilas state-of-the-art provisioning tools. Solution provisioning automation (SPA) framework is our practice of such deployment paradigm. In SPA, we use solution template to capture the solution components and their dependencies, which exposes only the necessary parameters for users to customize. Provisioning Requests are placed through reservations and scheduled globally for execution. The SPA engine receives provisioning requests from scheduler, interprets the solution template, manages provisioning tasks and performs runtime logging. We also present a real world case to demonstrate the effectiveness of our approach."
1438,"Consumer ratings are widely used in online marketplaces-helping vendors in assessing the quality of offerings and consumers in discovery and purchase decisions. To build trust in a marketplace, which has a direct impact on sales, an accurate assessment of ratings is essential in determining the quality of offerings. This paper proposes novel extensions to consumer Rating as a Service (RaaS)-a rating management service providing consumer rating functionality to a marketplace using hierarchical aggregation, which is a rating aggregation mechanism using hierarchical relationships of components to evaluate composite offerings. Contributions include the optimization of RaaS design for Web-scale, the integration of consumer credibility in hierarchical aggregation, and the application of hierarchical aggregation to existing independent atomic offerings. Various experiments are conducted to demonstrate the practicality of RaaS and correctness of hierarchical aggregation using real ratings from Amazon.com."
1439,"It is a common experience while Web searching that one gets to see pages that are not of interest. Partly these are due to a word or words in the search query having different contexts, the user obviously expecting to find pages related to the context of interest. This paper proposes a method for disambiguating contexts in Web search results."
1440,"We present GHTraffic, a dataset of significant size comprising HTTP transactions extracted from GitHub data and augmented with synthetic transaction data. The dataset facilitates reproducible research on many aspects of service-oriented computing. This paper discusses use cases for such a dataset and extracts a set of requirements from these use cases. We then discuss the design of GHTraffic, and the methods and tool used to construct it. We conclude our contribution with some selective metrics that characterise GHTraffic."
1441,"In order to find suitable web services in a large market of web services, automatic support is needed to filter out web services semantically. Existing matchmaking approaches mainly consider only the types of the input and output parameters, which is not sufficient in practical scenarios. In this paper, we present formalisms for modeling functional and non-functional properties of web services and for specifying user goals. We show how expressive web service descriptions can be checked for satisfiability of the user goal."
1442,"Coordination is a central problem in distributed computing. The aim is towards flexible coordination, managed at run-time, in open, dynamic environments. This approach would benefit from an explicit common vocabulary for coordination and hence, in a previous paper, we modelled coordination in an ontology, describing the activities carried out and the interdependencies among these activities. The purpose of this paper is to show how such an ontology can be used alongside a set of rules to perform coordination by managing the interdependencies among activities. The ontology and rules can then be used to provide a general purpose coordination tool in the form of a Web service"
1444,"The constraints revealed during a logical composition of services are often too abstract for automatic service composition. The abstract constraints have to be transformed to concrete attributes. This research investigates semi-automatic transformation of intermediate constraints to concrete constraints for automatic service composition. It considers simultaneously a stack of composition attributes for QoS, preferences, and logic constraints."
1446,"The lack of effective trust establishment mechanisms for Web services impedes the deployment of trust models for online services. One important issue is the lack of privacy protection in trust establishment. Current Web service technology encourages a client to reveal all its attributes in a standard credential to the service provider for trust establishment. We propose a mechanism whereby the client formulates a single trust primitive by associating a subset of required attributes in a standard credential to negotiate a trust relationship. Client privacy is preserved because only those required attributes are revealed. After negotiation, a trust group element with dynamic validation is used to represent this trust relationship."
1447,"The Web services resource framework (WSRF) has redefined grid computing standards, making Web services more suitable for grid applications by adding persistent state management. However, it is still difficult to build, deploy, and evolve such services, since different Web and grid services containers require that the services they host be written in specific languages, to target particular internal interfaces for state management. Therefore, services must be built and deployed differently for each hosting environment. This paper describes uniform dynamic service code deployment on three Web services containers (namely, Tomcat, ASP.NET and a gSOAP based C++ container), and two grid services containers (namely, GT4 and WSRF.NET). Containers receive the code in an XML-based standard intermediate form, and then generate container-specific native code in different languages, without exposing these details to applications and grid services programmers. The dynamically deployed code can access state managed by the hosting container, can utilize functionalities exposed by statically deployed services, and can communicate with other dynamically deployed modules, running either in the same container or in different containers. A performance study shows that the newly deployed mobile code can run nearly as efficiently as it would if it had been deployed statically, through container-specific mechanisms."
1448,"There is a critical need to design and develop tools that abstract away the fundamental complexity of XML based Web services specifications and toolkits, and provide an elegant, intuitive, simple, and powerful query based invocation system to end users. Web services based tools and standards have been designed to facilitate seamless integration and development for application developers. As a result, current implementations require the end user to have intimate knowledge of Web services and related toolkits, and users often play an informed role in the overall Web services execution process We employ a set of algorithms and optimizations to match user queries with corresponding operations in Web services, invoke the operations with the correct set of parameters, and present the results to the end user. Our system uses the Semantic Web and Ontologies in the process of automating Web services invocation and execution."
1450,"It is well known that the fragmentation of Android ecosystem has caused severe compatibility issues. Therefore, for Android apps, cross-platform testing (the apps must be tested on a multitude of devices and operating system versions) is particularly important to assure their quality. Although lots of cross-platform testing techniques have been proposed, there are still some limitations: 1) it is time-consuming and error-prone to encode platform-agnostic tests manually, 2) test scripts generated by existing record/replay techniques are brittle and will break when replayed on different platforms, 3) Developers, and even test vendors have not equipped some special Android devices. As a result, apps have not been tested sufficiently, leading to many compatibility issues after releasing. To address these limitations, this paper proposes AppCheck, a crowdsourced testing service for Android apps. To generate tests that will explore different behavior of the app automatically, AppCheck crowdsources event trace collection over the Internet, and various touch events will be captured when real users interact with the app. The collected event traces are then transformed into platform-agnostic test scripts, and directly replayed on the devices of real users. During the replay, various data (e.g., screenshots and layout information) will be extracted to identify compatibility issues. Our empirical evaluation shows that AppCheck is effective and improves the state of the art."
1451,"With the dramatic growth of mobile application (app) markets, users can find various apps with any functionalities they desire in these markets. However, the huge amounts of apps make it quite a challenge for users to discover good apps efficiently. Previous studies recommend apps by considering all apps equal without capturing the specific interests of each individual user. To address this problem, we propose a model called Weight-based Matrix Factorization (WMF), which can capture user-specific interests and give a more accurate prediction on these apps. WMF views each user as a document and each app as a word, and calculates the weight of each app for target users. The weights are calculated by employing term frequency inverse document frequency (TF-IDF) algorithm, which are then introduced into matrix factorization to predict app ratings. Comprehensive experiments are conducted on a real-world datasets with 5057 users and 4496 apps. The experimental results show that WMF achieves a convincing performance and surpasses other existing prediction models."
1452,"Ensuring secure information flow is a critical task for service composition in multi-domain systems. Research in security-aware service composition provides some preliminary solutions to this problem, but there are still issues to be addressed. In this paper, we develop a service composition mechanism specifically focusing on the secure information flow control issues. We first introduce a general model for information flow control in service chains, considering the transformation factors of services and security classes of data resources in a service chain. Then, we develop general rules to guide service composition satisfying secure information flow requirements. Finally, to achieve efficient service composition, we develop a three-phase protocol to allow rapid filtering of candidate compositions that are unlikely to satisfy the information flow constraints and thorough evaluation of highly promising candidates. Our approach can achieve effective and efficient service composition considering secure information flow."
1453,"Today, the Business Process Execution Language (BPEL) is the most emerging specification for Web Service Composition, which is an important part of the SOA paradigm. Defining a stateful communication protocol, BPEL enables potential for new security vulnerabilities. In this paper, we present a severe Denial-of-Service attack on a leading BPEL engine, illustrating new threats on availability in the context of BPEL. Derived from our observations, we developed a protection concept and implemented an application level firewall fending these types of attacks."
1454,"Implementation and modification of non-functional properties related to monitoring quality of service (QoS) can be both a expensive and complex task. This paper presents a model-based approach to the implementation of QoS monitors by describing them as platform-independent models. Then, model-driven development (MDD) transformations are conducted to create both a system implementation and QoS constraints in the form of aspects according to the aspect-oriented programming (AOP) paradigm. Aspects are weaved within the system implementation."
1455,"Advances in the areas of embedded systems, computing, and networking are leading to an infrastructure composed of millions of heterogeneous devices. These devices will not simply convey information but process it in transit, connect peer to peer, and form advanced collaborations. This ""Internet of Things'' infrastructure will be strongly integrated with the environment, and its integration with the enterprise systems will not only further blur the line between business IT systems and the real world, but will change the way we design, deploy, and use services. New opportunities can emerge for businesses, which can now closely collaborate with the real world. The work presented here proposes an architecture for an effective integration of the Internet of Things in enterprise services."
1456,"REST APIs are being increasingly used in the industry including their application in safety-critical domain and in the IoT world. They offer basic CRUD (create, retrieve, update and delete) interfaces. However, REST APIs can be used to build services with more advanced scenarios. Developing such services with REST constraints requires rigorous approaches that are capable of creating services that can be trusted for their behavior. In this work, we present an approach based on formal verification technique for a development of REST services using Event-B. We focus on deriving a correct system architecture by refinement and consistency verification of service design models. We illustrate our approach on a Hotel Reservation System."
1457,"Selective regression testing involves retesting of software systems with a subset of the test suites to verify that modifications have not adversely impacted existing functions. Although this problem has been heavily researched, it has never been discussed in the context of SaaS (Software as a service). This paper presents the specific requirements, challenges and benefits in delivering regression test selection as a service (RTaaS). We will introduce how to design and implement a RTaaS platform. An implementation of RTaaS has been piloted and improved via several real projects in China market. The real customer cases illustrate that RTaaS is a cost-effective and easy way for software project teams to leap over technical barriers and tap into advanced regression testing selection technologies."
1458,"QoS identification for untrustworthy Web services is critical in QoS management in the service computing since the performance of untrustworthy Web services may result in QoS downgrade. The key issue is to intelligently learn the characteristics of trustworthy Web services from different QoS levels, then to identify the untrustworthy ones according to the characteristics of QoS metrics. As one of the intelligent identification approaches, deep neural network has emerged as a powerful technique in recent years. In this paper, we propose a novel two-phase neural network model to identify the untrustworthy Web services. In the first phase, Web services are collected from the published QoS dataset. Then, we design a feedforward neural network model to build the classifier for Web services with different QoS levels. In the second phase, we employ a probabilistic neural network (PNN) model to identify the untrustworthy Web services from each classification. The experimental results show the proposed approach has 90.5% identification ratio far higher than other competing approaches."
1459,"We are developing an Interoperable Distributed Simulation (IDSim) framework that builds upon the Open Grid Services Infrastructure (OGSI) to provide distributed simulation services to federated simulators. OGSI and Web technology standards provide novel opportunities for a domain-independent, distributed simulation framework to support extensibility by users of the system. IDSim services can be extended through inheritance to add or extend operations tailored to the needs of specific simulation domains. XML-based documentation of simulation models allows the framework to operate upon the contents of shared events (e.g., content-based distribution). Additionally, exposure of the underlying framework through Grid Web Services Description Language (GWSDL) allows for tool-based extensibility; that is, tools can be developed to create and manage complex federations. This paper presents the design and architecture of IDSim with a focus on extensibility; other design concepts are presented where relevant."
1460,"In dynamic cooperative architectures that are based on services (SOA), customers are not only interested in service functionalities, but also in their quality, such as performance, cost, reliability, security and so on. In this scenario, models, techniques and tools supporting the selection of the best service are needed. In this paper, we propose an evaluation framework that includes a flexible quality meta-model for formalising customer and provider views of quality, and a decisional model defining a systematic approach for comparing offered and requested quality of services. We also illustrate the applicability of the framework in a Web service (WS) scenario."
1461,"With software systems increasingly being employed in more complex and critical contexts, service-oriented system of systems has been paid more and more attention as a novel software system structure, which considers System as a Service. Under the loosely coupled SoS's dynamic and uncertain running environment, self-healing process, as the important safeguard mechanism of system running, pose a great threat to system quality analysis. Particularly, as the first step of self-healing, the research of quality failure prediction faces not only continual and immediate disturbance on service quality, but also complex users' preference on quality. In this paper, we propose a model based on Stochastic Differential Equations to analyze the disturbance more precisely and dynamically. And we adopt weighted conditional preference to consider different users' requirements. This model is testified in an empirical case study, in which the real data set is collected in real-time from the system platform of a Telecom in China. The experiments verify model's prediction abilities and evaluate the impact of the parameters on the prediction accuracy."
1462,"The emerging paradigm of Web services opens a new way of Web application design and development to quickly develop and deploy Web applications by integrating independently published Web services components to conduct new business transactions. As research aiming at facilitating Web services integration and verification, WS-Net is an executable architectural description language incorporating the semantics of colored Petri-net with the style and understandability of object-oriented concepts. WS-Net describes each Web services component in three layers: interface net declares the services that the component provides to other components; interconnection net specifies the services that the component acquires to accomplish its mission; and interoperation net describes the internal operational behaviors of the component. As an architectural model that formalizes the architectural topology and behaviors of each Web services component as well as the entire system, WS-Net facilitates the verification and monitoring of Web services integration."
1463,Online experimentation allows students from anywhere to operate remote instruments at any time. This promising e-learning application is well positioned to use Web services to conduct online experiment systems due to its interoperability and Internet compliance. We present a double client-server architecture for online experiment systems and the methodology to wrap the functions of instruments into Web services. We propose that the instrument Web services should be stateful services and we present the framework to manage the states of the instrument Web services. We benchmark the performance of this system when using SOAP as the wire format for communication and propose solutions to optimize performance.
1464,"The trust of sellers and transactions is a very important issue in e-commerce and e-service environments. At some e-commerce websites (such as eBay1), the trust management mechanism can compute a trust value of a seller, which is based on the ratings of past transactions given by buyers. This trust value, however, is static and can only reflect the general or global trust status of a seller, and it is not directly bound to a new transaction. As a result, a buyer maybe easily cheated by a malicious seller in a new transaction with the notorious value imbalance problem [5], i.e., the malicious seller can build up a good reputation by selling cheap products/services and then start to cheat buyers by selling expensive products/services. Instead of providing such a static trust value, in order to provide more objective trust result for anew potential transaction, a trust evaluation mechanism should based on the ratings of past transactions, the nature of both past transactions and the new transaction. In this paper, we propose a new contextual trust evaluation method. Our method compares the transaction context similarity between the new transaction and past transactions, from which the trust value of the new transaction can be determined. Our method can identify and prevent potentially malicious transactions with the value imbalance problem."
1465,"Future Web applications will be more collaborative and will use the standard and ubiquitous Internet protocols. We have previously developed system on mobile devices (SyD) middleware to rapidly develop and deploy collaborative applications over heterogeneous and possibly mobile devices hosting web objects. In this paper, we present the software engineering methodology for developing SyD-enabled Web applications and illustrate it through a case study on a system of calendar application, with implementation on iPAQs and its performance metrics study. SyD-enabled Web objects allow us to create a collaborative application rapidly with limited coding. In this case study, the modular software architecture allowed us to hide the inherent heterogeneity among devices, data stores, and networks by presenting a uniform and persistent object view of mobile calendar objects interacting through XML/SOAP requests and responses. The performance results we obtained show that the application scales well as we increase the group size and adapts well within the constraints of mobile devices."
1466,"Trust is a critical issue in e-commerce and e-service environments. In some applications (such as eBay), the trust management mechanisms have been introduced to provide valuable information to buyers prior to placing orders and making payments. Meanwhile, the trust issue is also actively studied in the research community. However, most studies and applications focus on approaches that result in a single trust value to represent the trust level of sellers or service providers. Such a simple trust evaluation method may not be able to depict the trust history exactly and may leave misleading information to service customers. In this paper, we present a novel approach of trust vector consisting of three values to reflect the trust level with more indications."
1467,"Optimizing the Quality-of-Service (QoS) levels of a service workflow is essential for the user satisfaction in Service-oriented Computing. For that purpose, QoS computation models are applied to reflect the actual QoS experienced by the user during service execution. Current QoS models ignore the possible dependencies of QoS attributes, such as the dependency on the time of the execution or on the input data supplied to the service. Apart from that, composition approaches consider only single workflows during service selection, narrowing the number of possible compositions. Thus, we introduce a novel QoS model that covers QoS dependencies and discuss how this model can be used to consider multiple workflows at the same time. Moreover, we adopt a multi-objective optimization approach to offer solutions varying in QoS such as finishing time and price, allowing the user to make fine-grained decisions."
1468,"Business processes involve interactions among autonomous partners. We propose that these interactions be specified modularly as protocols. Protocols can be published, enabling implementors to independently develop components that respect published protocols and yet serve diverse interests. A variety of business protocols would be needed to capture subtle business needs. We propose that the same kinds of conceptual abstractions be developed for protocols as for information models. Specifically, we consider (1) refinement: a subprotocol may satisfy the requirements of a superprotocol, but support additional properties; and (2) aggregation: a protocol may combine existing protocols. In support of the above, this paper develops a semantics of protocols and an operational characterization of them. This supports judgments about the potential subclass-superclass relations between protocols, which are a result of protocol refinement. It also enables protocol aggregation by splicing a protocol into another protocol."
1469,"The synchronized air power management (SAPM) initiative is a collaborative USAF, government, and industry effort. XML Web services were employed to integrate Air Force battle management command, control, and communications (BMC3) systems regardless of their operating environments (J2EE, Windows .NET) and hardware platforms (Unix, Microsoft Windows), and to integrate BMC3 systems with enterprise workflow engines. SAPM enhances visibility into the process of planning, scheduling, and executing missions, and streamlines BMC3 operations, demonstrating a drastic reduction in the time it takes to plan, evaluate, and execute decisions, as well as a reduction in associated manpower needs."
1470,"This panel brings together technology experts and business leaders and provide first hand insight to the evolution of cloud computing and IT as a service (ITaaS), from both technology and business model perspectives. The panel will discuss the disruptive nature of cloud computing and its business model, including the impact to the current enterprise IT industry, to the service provider industry, to the enterprise software industry, to the networking industry, and to the service industry. The panel will also discuss the confluence of SOA paradigm and SaaS paradigm and examine its implication to the enterprise IT architecture. The panel will also help audience understand the limitation and challenges for cloud computing and ITaaS. The audience of this panel is targeted at the technology leaders and business decision makers in enterprise IT, software industry, and networking industry."
1471,"Mashups are prevalent Service-Oriented Architecture (SOA) based applications consisting of multiple Web Application Programming Interfaces (APIs) and content. Tags have been extensively used to organize and index mashup services. However, people favor manual tags creation in the past. This approach demands user intervention, which is extremely time-consuming and probes to errors. In this paper we propose a novel Mashup-API-Tag model for automatic mashup tag recommendation. The model simultaneously incorporates the composition relationships between mashups and APIs as well as the annotation relationships between APIs and tags to discover the latent topics. Then the semantic similarity between Web APIs and mashups can be acquired. Subsequently, tags of chosen APIs are recommended to a mashup where the mashup and the APIs are most similar. In addition, we develop a tag filtering algorithm to select the most relevant tags for recommendation. The experimental results on a real world dataset prove that our approach outperforms other methods, including frequency-based methods and the methods that only consider the composition relationships and the annotation relationships separately."
1472,"In public clouds such as Amazon EC2, there are two main pricing models in purchasing Infrastructure-as-a-Service (IaaS) instances: the pay-as-you-go model and the subscription model. For these two options, users can dynamically combine them to provide services for demands to save their instance acquisition costs. Making optimal decisions toward the purchase of IaaS instances generally requires prior knowledge of future demands; however, it is difficult for users to predict all future workloads accurately. To deal with this problem, online reservation algorithms have been proposed to guide users in reserving instances. However, existing online algorithms do not conform to the pricing rules currently used in public cloud platforms. Therefore, we put forward a new online reserving algorithm for instance in accordance with the pricing policies used in most public IaaS offerings. Specifically, in this study, we use Amazon EC2 as an example to illustrate our algorithm. Through theoretical analysis, we prove that the cost of the proposed algorithm A
<sub>β</sub>
 in this paper is not greater than 2-1/β times of the optimal offline algorithm, where β&gt;1 is a critical point in the online reservation algorithm proposed in this paper. Via extensive experimental simulations using both synthetic and actual workload datasets, we demonstrated that the online algorithm A
<sub>β</sub>
 is much more cost effective for cloud users than always paying-as-you-go in public IaaS markets."
1473,"In scenarios such as search-and-rescue operations, it may be required to transmit information across multiple, heterogeneous networks, often experiencing unreliable connections and limited bandwidths. Under such conditions, Publish/subscribe-based communication, combined with store-and-forward capabilities in the network nodes, greatly improves the ability to transmit information. At the same time, it is desirable to use commercial, standards-based software as much as possible, in order to reduce both cost and development time, and to ease the interconnection of systems from different organizations. In this paper, we present our prototype middleware solution called the Delay and Disruption Tolerant SOAP Proxy (DSProxy) which adds Publish/Subscribe functionality to standard, unmodified Web services. Together with its ability to make Web services delay and disruption tolerant, the DSProxy enables SOA based on Web services in scenarios as described above. The DSProxy has been tested in field trials, with promising results."
1474,"As the Web services and grid community adopt semantic Web technology, we observe a shift towards higher-level workflow composition and service discovery practices. While this provides excellent functionality to non-expert users, more sophisticated middleware is required to hide the details of service invocation and service integration. An investigation of a common bioinformatics use case reveals that the execution of high-level workflow designs requires additional processing to harmonise syntactically incompatible service interfaces. In this paper, we present an architecture to support the automatic reconciliation of data formats in such Web service workflows. The mediation of data is driven by ontologies that encapsulate the information contained in heterogeneous data structures supplying a common, conceptual data representation. Data conversion is carried out by a configurable mediator component, consuming mappings between XML schemas and OWL ontologies. We describe our system and give examples of our mapping language against the background of a bioinformatics use case"
1475,"A composite Web service designed based on abstract Web services, which define only interfaces, allows an application developer to select services required for his application only by setting endpoints for the atomic Web services. In open environment, however, the composite Web service configured in this manner may fail due to unique behaviors of the selected services. It is difficult for the designer of the composite Web service to prevent the failure because he does not know which services are selected and how they behave. On the other hand, the application developer is not authorized to modify the composite Web service due to the need to protect intellectual rights. Our solution is Service Supervision, which monitors and controls execution of composite Web services. Service Supervision makes the followings possible. 1) An application developer can control the behavior of a composite Web service by changing the execution state, even if the he is not authorized to modify the composite Web services. 2) A control pattern for coordinating Web services can be applied to various composite Web services in order to reduce the load imposed by designing control processes. In order to realize Service Supervision, we introduce meta-level control of a composite Web service. Moreover we then use the choreography to define the interaction protocols for the controls. The proposed framework is based on existing standard languages, WS-BPEL and WS-CDL. Therefore we can exploit existing tools and expertise of SOA engineers."
1476,"With the increasing popularity of using Web services, Quality of Service (QoS) is becoming a significant concern for both service consumers and providers. Several models for considering functional and non-functional QoS requirements for the purpose of Web service discovery and selection have been proposed. We present in this paper a new Web services selection model based on fuzzy logic. In this model, the non-functional QoS requirements are taken into account for the appropriate selection of required services. Our model can handle consumers' imprecise preferences with fuzzy sets. Moreover, a new fuzzy ranking algorithm that is based on the dependencies between quality attributes is proposed. The experimental results obtained from real world Web service domains revealed better performance of our algorithm compared to Entropy-based ranking algorithms."
1477,"Fault management in Web services composed by individual services from multiple suppliers currently relies on a local analysis that does not span across individual services, thus limiting the effectiveness of recovery strategies. We propose to address this limitation of current standards for Web service composition by employing model-based diagnosis to enhance fault analysis. We propose to add diagnostic Web services to the set of Web services providing the overall service, acting as supervisors of their execution, by identifying anomalies and explaining them in terms of faults to be repaired. This approach poses the basis for the development of specialized recovery and compensation techniques aimed at addressing different problems, which could not be otherwise discriminated."
1478,"Complex adaptive systems are dynamically assembled systems characterized by multiple competing stakeholders, fluid requirements, emergent behavior, and susceptibility to external pressures that can cause change across the entire system. This paper shows how key principles of complex systems engineering can support achieving net centric operations (NCO) for the DoD. We discuss approaches to on-demand data, an IT infrastructure strategy based on Web service and semantic Web technologies, as well as guidance and initial observations on spiral development and management using Communities of Interest combined with a Developer's Environment."
1479,"Due to the increase in Web services, many recent studies have been addressing the service selection problem based on non-functional or quality aspects. Our study incorporates combinational use of functionally-equivalent services into the problem to compose an application of higher quality or with additional value. However, when such combinational use is introduced, computational cost for the service selection becomes much higher. In this work, we propose a set of methods that reduce the additional cost for the QoS (Quality of Service)-based service selection considering combinational use. This approach achieves low cost by considering only effective combinations. The experimental results show that it can reduce computational cost regardless of the number of services and whatever their QoS values are while keeping the effectiveness of combinational use."
1480,"Service matchmaking is an important process in the operation of Service-Oriented Architecture (SOA) based systems. In this process, information from both service providers and requestors are used. How to protect the privacy of participating parties during the matchmaking process imposes a challenge. In this paper, a privacy- preserving service matchmaking approach is presented to support semantic-based service matchmaking and avoid privacy leakages to untrusted parties. The approach uses situation-aware access control (SA-AC) mechanism to ensure the appropriate disclosure and use of private information by modeling, specifying and enforcing SA-AC policies. It provides an owner-centric mechanism for both service providers and requestors in SOA-based systems to protect their private information during service matchmaking."
1481,"This paper proposes a novel Web service discovery approach that depend on the mining the underlying semantic structures of interaction interface parameters, which can match interfaces with high precision when the parameters of those interfaces contain meaningful synonyms, abbreviations, and combinations of disordered fragments. Especially, we propose a conceptual Web services description model in which we include the type path for the interaction interface parameters in addition to the traditional text description. Then, based on this description model, we mine the underlying semantics of the interaction interface to create index libraries by clustering interaction interface names and fragments under the supervision of co-occurrence probability. Finally, we propose a Web service Operations Discovery algorithm (OpD) that support the “Single” operations and services with “Composite” operations discovery. The experimental shows that our approach performs better than other approaches in terms of both discovery time and precision."
1482,"Collaborative filtering approach based on rating is one of the most broadly used service recommendation approach. However, rating data is very sparse in most service recommender systems, which seriously impacts the precision of service recommendation. In view of this problem, a usage-based service recommendation approach is proposed in this paper. What is special about this approach is that usage information instead of rating data is recruited to infer user interest. Some experiments are implemented to verify the efficient of this approach."
1483,"In this paper, we propose a dynamic service discovery framework from Web services representation chain. The major components include Web service chains precategorizing module, service exploration engine, services container, chain change detection module with associated control parameters. From the working research prototype, our proposed mechanism enables businesses to easily retrieve up-to-date Web services linked and nested multi-level deep in the service description documents. In addition, a federated Web services discovery portal is illustrated to show how the research results are aggregated from hyperlinked WSIL documents as well as UDDl registries for achieving more accurate services exploration result."
1484,"Online social network services, such as Facebook and Twitter, have become increasingly popular recently. More and more users are accustomed to regularly reading the latest news feeds and interacting with friends on these social websites. However, when the number of friends increases to a large extent, users will receive hundreds of messages in a day and may be overwhelmed by the information overload. To alleviate this problem, we propose a novel visualization technique for social news feeds summarization on social web services. The proposed system SocFeedViewer can produce an egocentric network graph based on the news feeds generated in an arbitrary period of time. This graph provides an overview of those who have generated news feeds during this time period. To enhance the reading experience, we incorporate community detection, connectivity analysis, and importance analysis into our system to make users capable of preferentially surfing news feeds that are more significant and interesting."
1485,"The semantic Web technology and the Web services description language extensibility may be combined to describe services in an unambiguous and machine interpretable way, automating Web services discovery, selection and invocation. In this paper, we present an algorithm and a prototype for the automatic composition of Web services that implement workflows described in a high level language. Our approach has many advantages comparing to the manual creation of a simple program composition, such as smaller implementation time and cost, reliability with the generation of contingency plans, greater capacity to evolve with the dynamic service discovery, and faster execution time with the use of heuristics. We use the OWLS ontology to semantically describe Web services metadata and indexes to help selecting them. The proposed algorithm considers that equivalent services may have different interfaces and also respects preferences of the users."
1486,"The presence of multiple services sharing a common functional interface necessitates differentiating between them on the basis of their performance. However advertised quality of service (QoS) alone cannot paint the true picture of how the service has performed so far and how it will continue to function in the near future. This information is crucial for service selection. In this paper, we outline a method of monitoring and extrapolating service performance and using the same for automated service selection process."
1487,"The emergence of ubiquitous computing, and the wide adoption of smart phones over the past few years require many Web services to function in a context-aware manner. In such services, not only the functional attributes, but also the QoS attributes (e.g., Response time and availability) also depend on the context of the service. Trust (i.e., The degree of compliance of a service to its specification) and the QoS evaluation of a service and a system composed of such services should also consider these context dependencies. Our work proposes a model that uses such context-QoS dependency information of individual services and inter-service interaction patterns to get predictions for the QoS and trust of service compositions at the design phase. The predictions can be used to make better design and implementation decisions of composed systems in early phases of the software lifecycle thereby reducing cost, time and effort. The preliminary results show that the proposed framework provides more accurate predictions than the prevalent approaches."
1488,"Process measurement is the task of empirically and objectively assigning numbers to the attributes of processes in such a way as to describe them. We define process complexity as the degree to which a process is difficult to analyze, understand or explain. One way to analyze a process' complexity is to use a process control-flow complexity measure. This measure analyzes the control-flow of processes and can be applied to both Web processes and workflows. In this paper, we discuss how to evaluate the control-flow complexity measure to ensure that it can be qualify as a good and comprehensive one."
1490,An effort is underway to more fully integrate business/military process management (BPM) and grid computing with distributed robotics and automation capabilities. This work is a logical extension of BPM whereby process and workflow management are extended to include the configuration and reengineering of both human and software agents that collectively synchronize and coordinate both human and robotics driven activities.
1491,"Nowadays, more and more distributed digital TV and TV-related resources are published on the Web, such as Electronic Personal TV Guide (EPG) data. To enable applications to access these resources easily, the TV resource data is commonly provided by Web service technologies. The huge variety of data related to the TV domain and the wide range of services that provide it, raises the need to have a broker to discover, select and orchestrate services to satisfy the runtime requirements of applications that invoke these services. The variety of data and heterogeneous nature of the service capabilities makes it a challenging domain for automated web-service discovery and composition. To overcome these issues, we propose a two-stage service annotation approach, which is resolved by integrating Linked Services and IRS-III semantic web services framework, to complete the lifecycle of service annotating, publishing, deploying, discovering, orchestration and dynamic invocation. This approach satisfies both developer's and application's requirements to use Semantic Web Services (SWS) technologies manually and automatically."
1492,"Web services are very prevalent nowadays. Recommending Web services that users are interested in becomes an interesting and challenging research problem. In this paper, we present AWSR (Active Web Service Recommendation), an effective Web service recommendation system based on users' usage history to actively recommend Web services to users. AWSR extracts user's functional interests and QoS preferences from his/her usage history. Similarity between user's functional interests and a candidate Web service is calculated first. A hybrid new metric of similarity is developed to combine functional similarity measurement and nonfunctional similarity measurement based on comprehensive QoS of Web services. The AWSR ranks publicly available Web services based on values of the hybrid metric of similarity, so that a Top-K Web service recommendation list is created for a user. AWSR has been implemented and deployed on the Web. By conducting large-scale experiments based on a real-world Web services dataset, it is shown that our system effectively recommends Web services based on users functional interests and non-functional requirements with excellent performance."
1493,"Mining closed frequent itemset (CFI) plays an essential role in many real-world data mining applications. With the emergence of abundant large-scale data sets, it now turns to be a significant and challenging issue to mine CFI concurrently. This paper proposes a parallel balanced mining algorithm for CFI based on the MapReduce platform. The proposed algorithm adopts Greedy strategy to group items aiming to balance the computation burdens among all parallel tasks, which is consisted of three main steps: (1) Parallel Counting, (2) Global Construction of Frequent List (F_list) and Group Map (G_map), (3) Parallel Mining for Closed Frequent Itemset. Experimental results validate the method and show its effectiveness as satisfied speedup and scalability are both achieved in large-scale CFI mining tasks."
1494,"Web Services allow an enterprise to focus on its own business expertise and to employ information technology services offered by others, however, doing so exposes the enterprise to the risk of loss of service or data, if the service provider fails. The use of multiple service providers reduces the risk of failure, but it introduces the complication of incompatible service interfaces, data formats, and stored data. In this paper, we propose a conversion infrastructure that facilitates the use of multiple alternative Web Services at different service providers, to achieve high availability, even in the event of a failure or a catastrophe."
1495,"This paper addresses the issue of coordination mechanism selection in agent based service oriented computing systems. We aim to strengthen the current research on service selection and composition with a dynamic choice of the underlying coordination mechanisms. Indeed, since the environment in such systems might dynamically change, and the agents can have several protocols at their disposal, they need to select the most adequate one for service provision/request upon the prevailing circumstances. In this paper, we present a joint protocol selection mechanism, which enables agents to decide together the protocols they will use to coordinate each other during a service provision."
1496,"The turbulent modern business environments require firms to be more flexible and agile. This requires ability to quickly integrate and automate intra and inter-organizational business processes. An agile IT architecture is the key to achieving success in this environment. Service oriented architecture and Web services provide means for achieving the business agility. However, significant impediments to wide scale adoption and use of these technologies remain. We will focus on significant challenges in the areas of service identification and design, cataloging and searching for appropriate services, domain specific standardization of service interfaces, matching service functionality with requirements, and service selection based on non functional characteristics. We describe an integrated framework based on standard modeling, design and code generation tools to support development of highly agile application systems. This framework is a result of multi year research effort aimed at developing methodologies and tools for defining business components and Web services, develop schemes for describing components and services in business terms to allow efficient search of appropriate component/services and an environment for assembling application from reusable components and Web services. Specific approaches used to address some of the challenges described above will be discussed"
1497,"Convergence of information and telecommunication networks leading to integration of voice, data and video services will bring new opportunities for users as well as service providers. With a growing number of services available on the market, dynamic discovery of services, their composition, selection, mediation as well as execution will be required. In this paper we show how voice and data services can be seamlessly integrated using existing client as well as server VoIP systems based on a widely used SIP signaling protocol and newly emerging technologies in semantic Web services. Within particular use case scenario, we describe all phases of the call set-up between two call participants. We show how semantic Web services technology can facilitate dynamic and optimal integration of voice and data services with different characteristics while at the same time conforming to users' needs and preferences."
1498,"The successful progress of the Web services concept demands flexible ways for Web services to cooperate and to jointly fulfill a task that is requested by a client. In some applications, the execution of the task is not completely specified beforehand, but could rather be referred to as ""best effort"". One example is information searches. A flexible invocation of cooperating Web services might mean that their identities are not known beforehand. To achieve trust in the outcome of the task, it is essential that the identities of the cooperating Web services can be tracked in a secure way. It is also essential that the requesting client can decide to what degree the task has been executed. In this paper an approach to securely track identities of Web Services is introduced. The use of one-way messages creates a system where the state of a request is presented by the request itself."
1499,"Vehicular communication can enhance the safety and effectiveness of the autonomous vehicle (AV) control process. However, existing AV communication never considers the issues of when to communicate, who to communicate with, and what information to share. We show that the answer to the when, who, and what questions are situation dependent and attempt to define a semantic model to capture a relatively complete set of situations. Based on the semantic model, we define the communication protocols that are best suited to different situations. We then experimentally show that our protocol is efficient and effective."
1500,"The main memory management has been a critical issue to provide high performance in web cluster systems.To overcome the speed gap between processors and disks,many prefetch schemes have been proposed as memory management in web cluster systems. However, inefficient prefetch schemes can degrade the performance of the web cluster system. Dynamic access patterns due to the web cache mechanism in proxy servers increase mispredictions to waste the I/O bandwidth and available memory. Too aggressive prefetch schemes incur the shortage of available memory and performance degradation. Furthermore, modern web frameworks including persistent HTTP make the problem more challenging by reducing the available memory space with multiple connections from a client and web processes management in a prefork mode. Therefore, we attempt to design an adaptive web prefetch scheme by predicting memory status more accurately and dynamically. First, we design double prediction-by-partial-match scheme (DPS) that can be adapted to the modern web framework. Second, we propose adaptive rate controller(ARC) to determine the prefetch rate depending on the memory status dynamically. Finally, we suggest memory aware request distribution (MARD) that distributes requests based on the available web processes and memory.For evaluating the prefetch gain in a server node, we implement an Apache module in Linux. In addition, we build a simulator for verifying our scheme with cluster environments. Simulation results show 10% performance improvement on average in various workloads."
1501,"Over the last decade, an exponentially increasing number of REST services have been providing a simple and straightforward syntax for accessing rich data resources. To use these services, however, developers have to understand ""information-use contracts"" specified in natural language, and, to build applications that benefit from multiple existing services they have to map the underlying resource schemas in their code. This process is difficult and error-prone, especially as the number and overlap of the underlying services increases, and the mappings become opaque, difficult to maintain, and practically impossible to reuse. The more recent advent of the Linked Data formalisms can offer a solution to the challenge. In this paper, we propose a conceptual framework for REST-service integration based on Linked Data models. In this framework, the data exposed by REST services is mapped to Linked Data schemas, based on these descriptions, we have developed a middleware that can automatically compose API calls to respond to data queries (in SPARQL). Furthermore, we have developed a RDF model for characterizing the access-control protocols of these APIs and the quality of the data they expose, so that our middleware can develop ""legal"" compositions with desired qualities. We report our experience with the implementation of a prototype that demonstrates the usefulness of our framework in the context of a research-data management application."
1502,"Web services evolve frequently to meet new business demands and opportunities. However, service changes may affect service compositions that are currently consuming the services. Hence, audit testing (a form of regression testing in charge of checking for compatibility issues) is needed. As service compositions are often in continuous operation and the external services have limited (expensive) access when invoked for testing, audit testing has severe time and resources constraints, which make test prioritization a crucial technique (only the highest priority test cases will be executed).This paper presents a novel approach to the prioritization of audit test cases using information retrieval. This approach matches a service change description with the code portions exercised by the relevant test cases. So, test cases are prioritized based on their relevance to the service change. We evaluate the proposed approach on a system that composes services from eBay and Google."
1503,"With the rapid growth of Web 2.0 and its related technologies, more and more composite services have been created by end-users. In the traditional composite service development workflow, the composite service descriptions have always to be manually written by end-users as soon as the services are either created or modified. As a result, these handcrafted descriptions could be irrelevant or out-of-date to the corresponding composite services. Also, the users who speak other languages can hardly understand the languages used in these descriptions. Therefore, we present an approach that automatically generates context-aware descriptions for composite services to improve the workflow of composite service development as a means of overcoming the drawbacks exposed in traditional end-user service composition workflows."
1504,"Users of Web and grid services often must temporarily delegate some or all of their rights to a software entity to perform actions on their behalf. The problem with the typical grid services approach (X. 509 proxy certificates) is that commercial Web services tooling fails to recognize these certificates or process them properly. The security assertion markup language (SAML) is a standardized XML-based framework for exchanging authentication, authorization and attribute information. SAML has broadening commercial support but lacks delegation capabilities. To address this shortcoming, we exploit SAML's inherent extensibility to create a delegation framework for Web and grid services that supports both direct and indirect delegation. We develop a set of verification rules for delegation tokens that rely on WS-Security X.509 signatures, but do not force any trust relationship between the delegatee and the target service. We have implemented the framework on two common Web service hosting environments: Java/Tomcat and .NET. By leveraging existing Web services standards, we make it easier for Grid practitioners to build and consume Web and grid services without resorting to grid-specific protocols."
1505,"In the service-oriented paradigm, Web service interfaces are considered contracts between Web service subscribers and providers. The structure of service interfaces has an extremely important role to discover, understand, and reuse Web services. However, it has been shown that service developers tend to pay little care to the design of their interfaces. A common design issue that often appears in real-world Web services is that their interfaces lack cohesion, i.e., they expose several operations that are often semantically unrelated. Such a bad design practice may significantly complicate the comprehension and reuse of the services functionalities and lead to several maintenance and evolution problems. In this paper, we propose a new approach for Web service interface decomposition using a Formal Concept Analysis (FCA) framework. The proposed FCA-based approach aims at identifying the hidden relationships among service operations in order to improve the interface modularity and usability. The relationships between operations are based on cohesion measures including semantic, sequential and communicational cohesion. The identified groups of semantically related operations having common properties are used to define new cohesive and loosely coupled service interfaces. We conducted a quantitative and qualitative empirical study to evaluate our approach on a benchmark of 26 real world Web services provided by Amazon and Yahoo. The obtained results show that our approach can significantly improve Web service interface design quality compared to state-of-the-art approaches."
1507,"In this paper we investigate the problem of providing consistency, availability and durability for Web Service-transactions. We consider each transaction as a black box, with only the corresponding metadata, expressed as UML specifications, as transaction semantics. We refer to these WS transactions as coarse-grained WS-transactions. We propose an approach that guarantees the availability of the popular lazy replica update propagation method while increasing the durability and consistency. In our previous work, we proposed a replica update propagation method, called Buddy System, which required that updates are preserved synchronously in two replicas. In this paper we extend the Buddy System to handle course grained WS transactions, using UML stereotypes that allow scheduling semantics to be embedded into the design model. This design model is then exported and consumed by a service dispatcher to provide: 1.) High availability by distributing service requests across all available clusters. 2.) Consistency by performing the complete transaction on a single set of clusters. 3.) Durability by updating two clusters synchronously."
1508,"To catch up with today's fast changing business markets, web services has never slowed down its paces for evolution. As a consequence, service consumers have to employ in-time upgrades to guarantee continuous business integrity and avoid unnecessary runtime errors in their IT systems. However, a new web service release can involve hundreds of changes, and thus it is non-trivial for the service consumer to rapidly track and adopt relevant changes. This paper proposes a framework for facilitating the service consumer to keep synchronized with the web service evolution. On the service side, a Service Invocation Monitor is installed to monitor the interaction history of the client, a Service Delta Analyzer exports the service delta into well-formatted document, and a Release Note Customizeris configured to customize the full release note produced using the monitored interaction history and exported service delta. On the client side, the Consumer Code Customizer is used by the consumer developer to highlight the code fragments to be changed and provide facilitation linkages between the code fragments and the customized release note. An example is shown to demonstrate the usefulness of our proposal."
1509,"With the popularity of service-oriented computing, how to construct highly available service-oriented applications is becoming a hot topic in both the research and industry communities. As a fundamental problem in dynamic service selection, availability estimation is challenging because of the dynamic nature of Web services. To grasp the dynamic nature of Web services, we set up an experimental environment for collecting runtime information of Web services. Based on the collected runtime information, we identify several characteristics of service failures and successes, and further define three typical service runtime statuses. Based on these statuses, we propose a novel approach to dynamic availability estimation, which is called status identification based availability estimation for service selection (SIBE). To evaluate our approach, we compare SIBE with other approaches in an experiment of dynamic service selection on the Internet. Experimental results show that SIBE can efficiently improve the success rate of selecting available services."
1510,"The current state of the art of workflows over Web services employs a centralized composite process to coordinate the constituent Web services. Therefore, the coordinator process is complex, less scalable, and bulky. This paper introduces an architecture and a technique for distributing the centralized coordination logic of traditional workflows by (i) extending the stateless Web services into self-coordinating entities using coordinator proxy objects, and (ii) creating a workflow over these entities by interconnecting them into a distributed network of objects using Web bond primitives. Previously, we have developed Web bond primitives to enforce interdependencies among autonomous entities. We have designed and prototyped our BondFlow system, which provides a platform to configure such distributed workflows, producing coordination components with footprint small enough (around 150 KB) to be executed on a handheld"
1511,"We contribute performance functions to model the quality of service (QoS) of web services in horizontal composites. Performance functions are thereby used to formalize service level agreements (SLAs) to enable automated verification. Inter-Component rating is a model for gathering additional QoS information from web services that rate the output of their predecessor web services in composites. Furthermore, we contribute two approaches to verify if a web service adheres to its claimed QoS described with a performance function. For this verification, claimed performance is compared with reading points gathered from spot-testing the quality of the web service. The first approach we discuss is function interpolation, the second one applies methods from computational trust. The paper also discusses the advantages and drawbacks of both approaches as well as the influence of measuring errors."
1512,"To actually bridge the gap between business perspective and technical perspective, the prerequisite is to provide a comprehensive process modeling framework for business processes. Different from the previous traditional process methodologies, our work is neither only industrial process graphic-modeling nor pure theoretical studies. We mainly focus on the semantically-enhanced process description model. We propose the semantic modeling framework for business processes, i.e. BPMO: basically, we determine the description requirements for the whole business process lifecycle involving process discovery, composition and execution; furthermore, we refine the comprehensive semantic Web services conceptual model WSMO and make specific extensions to realize the BPMO modeling framework."
1513,"A multi-tenant software as a service (SaaS) system has to meet the needs of several tenant organizations, which connect to the system to utilize its services. To leverage economies of scale through re-use, a SaaS vendor would, in general, like to drive commonality amongst the requirements across tenants. However, many tenants will also come with some custom requirements that may be a pre-requisite for them to adopt the SaaS system. These requirements then need to be addressed by evolving the SaaS system in a controlled manner, while still supporting the requirements of existing tenants. In this paper, we focus on functional variability amongst tenants in a multi-tenant SaaS and develop a framework to help evolve such systems systematically. We adopt an intuitive formal model of services that is easily amenable to tenant requirement analysis and provides a robust way to support multiple tenant on boarding, which is modeled as a bi-objective optimization problem that attempts to maximize vendor profit and tenant functional commonality. We perform a substantial case study of a multi-tenant blog server to demonstrate the benefits of our proposed approach."
1514,"Web services are software systems designed to support interoperable machine-to-machine interaction over a network. WSDL descriptions, often characterized as IDLs for Web services, are a key for Web service interoperability. Therefore, special care should be taken in designing WSDL descriptions. We present an approach that provides UML-based support to design and validate WSDL descriptions. To promote Web service interoperability, WS-I organization provides a basic profile that defines clarifications, refinements, interpretations and amplifications of Web service specifications, including WSDL. We propose UML-based profiles to define structural rules of WSDL documents as well as basic profile recommendations for WSDL descriptions. These profiles can be used to guide the user in designing correct and basic profile compliant WSDL descriptions and to check the validity of existing WSDL descriptions. We propose a method and tools for such validity checking, and demonstrate the applicability of the approach with a case study."
1516,"Web services are a successful technology for enterprise information management, where they are used to expose legacy applications on the corporate intranet or in business-to-business scenarios. The technologies used to expose applications as Web services have matured, stabilized, and are defined as W3C standards. Now, the technology used to build applications based on Web services, a process known as orchestration, is also maturing around the Web Services Business Process Execution Language (WS-BPEL). WS-BPEL falls short on one feature though: as it is focused on orchestration of fully automatic Web-services, WS- BPEL does not provide means for specifying human interactions, even less their access-control requirements. Human interactions are nonetheless needed for flexible business processes. This lacking feature of WS-BPEL has been highlighted in a white paper issued jointly by IBM and SAP, which ""describes scenarios where users are involved in business processes, and defines appropriate extensions to WS-BPEL to address these."" These extensions, called BPEL4People, are well explained, but their implementation isn't. In this paper, we propose a language for specifying these extensions, as well as an architecture to support them. The salient advantage of our architecture is that it allows for the reuse of existing BPEL engines. In addition, our language allows for specifying these extensions within the main BPEL script, hence preserving a global view of the process. We illustrate our extensions by revisiting the classic loan approval BPEL example."
1517,"As REST (Representational State Transfer)-ful services are closely coupled to the HTTP (Hypertext Transfer Protocol), which eventually sits above the connection-based TCP (Transmission Control Protocol), it is common for RESTful services to experience latency and transfer inefficiencies especially in situations requiring the services to transfer large-scale data (i.e. above gigabytes of data) in RESTful workflows. Such inefficiencies are undesirable and impractical, and are compounded for RESTful service orchestrations in data-intensive industries such as Big Data analytics, cloud computing and life sciences. In this paper, we propose a non-invasive novel technique, Fast-Optimised-REST (FOREST), which enables RESTful services to overcome the traditional bottlenecks experienced during transfer of large sets of data. The initial experimental results show promise and demonstrated very significant reductions of up to 80% from original REST-ful data transfer times for extremely large data sets."
1519,"In view of the need for a highly distributed and federated architecture, a robust query expansion has great impact on the performance of information retrieval. We aim to determine ontology-driven query expansion terms using different weighting techniques. For this, we consider each individual ontology and user query keywords to determine the Basic Expansion Terms (BET) using a number of semantic measures including Betweenness Measure (BM) and Semantic Similarity Measure (SSM). We propose a Map/Reduce distributed algorithm for calculating all the shortest paths in ontology graph. Map/Reduce algorithm will improve considerably the efficiency of BET calculation for large ontologies."
1520,"There are a large number of Web sites serving valuable content that can be used by higher-level applications, Web services, Mashups, etc. Yet, due to various reasons (lack of computing resources, financial constraints etc.) they are unable to provide Web service APIs to access their data. In their desire to incorporate the latest and greatest technologies, as well as to adapt layouts that are more preferred by users, Web sites undergo changes over time. These changes can range from minor, e.g. function name changes, to major, e.g., shifting the Web platform to AJAX technologies. This paper addresses the problem of detecting layout changes for Web sites which are unable to provide any Web service to access their content, yet do not mind others harvesting said content."
1521,"This paper introduces a simple, yet effective, framework for clothes collocation by considering compatibility between items. In particular, we treat title sentences as the features of clothing items, instead of using clothing images. For feature transformation, the long-short term memory (LSTM) network is utilized for mapping title sentences into a low-dimensional space. Features of query and candidate items learned by the Siamese LSTMs are synthesized into a style space by a compatibility matrix. We evaluate our framework on two large-scale datasets compiled from Amazon and Taobao, respectively. Extensive experimental results show the effectiveness of our method in comparison to several state-of-the-art methods."
1522,"Service composition algorithms are used for realizing loosely coupled interactions in Service-Oriented Computing. Starting from an abstract workflow, concrete services are matched, based on their QoS, with the preferences and constraints of users. Current approaches usually only consider static QoS values and find a single solution consisting of one concrete service for each workflow task. In a business-to-business (B2B) environment, though, there are additional requirements for service compositions: 1) a high number of invocations, and 2) a high reliability. Thus, we introduce a probabilistic approach on the basis of a new QoS model to solve the composition problem for such long-term B2B service compositions. For each task and for every point in time, we determine the most appropriate services and backup services for a specific user. Thus, the selection depends on the actual response time and reliability, or recent invocation failures or timeouts. For that purpose, we propose an adaptive genetic algorithm that employs our QoS model and determines backup services dynamically based on the required reliability. Our evaluations show that our approach significantly increases the utility of long-term compositions compared with standard approaches in the envisioned B2B environments."
1523,"Web services simplify enterprise application integration by facilitating reuse of existing components for creating new services. In a dynamic environment, it is imperative to design a Web service composition and execution (WSCE) system that adapts to failure of component services or changes in their QoS offerings. In this paper, we motivate a staged approach for adaptive WSCE (A-WSCE) that cleanly separates the functional and non-functional requirements of a new service, and enables different environmental changes to be absorbed at different stages of composition and execution. We use Synthy, a prototype service creation environment, to implement our solution and demonstrate its effectiveness"
1524,"In recent years, we witnessed a range of innovations in the 'service' related technologies, such as Software as a Service, Platform as a Service and Infrastructure as a Service. Along with the Service Oriented Architecture, companies can wrap their technological product as a service, to collaborate with others. Facing the ever-escalating global competition, such collaboration is crucial. The viability of this paradigm highly depends on the compliance and therefore the trustworthiness of all collaborators. However, it is challenging to achieve trustworthiness in such a dynamic cross-domain environment, as each participator may deceit for individual benefits. As a solution, we have proposed to enforce strong accountability to enhance the trustworthiness. With this accountability, incompliance can always be determined in a provable and undeniable way. In this paper, we extend our work by proposing a novel modeling of the collaborative business process. Based on this modeling, we thoroughly analyze the evidence and proving procedure needed for different types of compliance, and evaluate the extent to which those compliance can be indeed proved. We have implemented a demonstrative system to show its effectiveness in real practice."
1525,"In service-oriented grid computing, great emphasis is placed on platform independence and cross-platform interoperability, at the price of a performance overhead incurred by the middleware and the high level programming languages typically utilized for developing software services. Reconfigurable hardware has been used in many areas of computing to improve the performance of applications by realizing performance critical parts in hardware. Typically, this is done in an application specific way, creating a custom solution for the project at hand for a specific reconfigurable hardware system. In this paper, we introduce a generic architecture in which grid services can be dynamically transformed and run on reconfigurable hardware in a dynamic environment in which different types of reconfigurable hardware systems are present. Three approaches - static design time integration, dynamic run time integration and transparent dynamic run time integration -are presented for integrating such on-demand ""hardware services"" into a service-oriented grid environment"
1526,"A common problem that mashup developers face is the discovery of APIs that suit their needs. This primary task becomes harder, tedious and time-consuming with the proliferation of new APIs. As humans, we learn by example, following community previous decisions when creating mashups. Most techniques do not consider at all reusing this social information. In this paper, we propose to combine current discovery techniques (exploration) with social information (exploitation). Our preliminary results show that by considering the reciprocal influence of both sources, the discovery process reveals APIs that would remain with low rank because the preferential attachment (popularity) and/or the lack of better descriptions (discovery techniques). We present a case study focusing on a public Web-based API registry."
1527,"We present in this paper a novel QoS prediction approach to tackle service recommendation, which is to recommend services with the best QoS to users. QoS prediction exploits available QoS information to estimate users' QoS experience from previously unknown services. In this regard, it can be modeled as a general matrix completion problem, which is to recover a large QoS matrix from a small subset of QoS entires. The infinite number of ways to complete an arbitrary QoS matrix makes the problem extremely ill posed. The highly sparse QoS data further complicates the challenges. Nonetheless, real-world QoS data exhibits two key features, which can be leveraged for accurate QoS predictions, leading to high-quality service recommendations. First, QoS delivery can be significantly affected by a small number dominant factors in the service environment (e.g., communication link and user-service distance). Hence, it is natural to assume that the QoS matrix has a low-rank or approximately low-rank structure. Second, users (or services) that share common environmental factors are expected to receive (or deliver) similar QoS and hence can be grouped together. The proposed approach seamlessly amalgamates these two features into a unified objective function and employs an effective iterative algorithm to approach the optimal completion of an arbitrary QoS matrix. We conduct a set of experiments on real-world QoS data to demonstrate the effectiveness of the proposed algorithm."
1528,"Non-Functional (NF) requirement is very important for the success of a software service. Considering that there could be multiple services implementing a same function, it is crucial for software providers to understand the real NF demands from consumers so that they can meet these demands and attract users. It is also crucial for consumers to know what is being offered so that they can pose realistic NF requests. We address both issues here by proposing a NF requirement analysis and recommendation system which works for both providers and consumers. NF requirements from various sources are first collected, and then we apply the factor analysis technique to identify those independent latent factors which contribute to those observable NF values. Finally we use cluster analysis to summarize the popular NF demands. Our experiment result shows the effectiveness of this approach."
1529,"Summary form only given. Services computing has become a foundational discipline of modernizing services and software industry. Services computing curriculum initiative (SCCI) is a community-driven professional activity which is sponsored by the IEEE computer society technical committee on services computing (TC-SVC). This session will present the latest advancements of SCCI in terms of newly formalized knowledge areas, case studies, and best practices of creating and delivering services computing courses. Several adoption approaches were introduced based on co-design and reuse principles for various degree programs. This paper also shares with you some long-term visions and latest lessons learnt from experienced professors and practitioners in the community."
1531,"Traditional Complex Event systems (CEP) did not consider the computation requirements of continuous dynamic behavior such as differential equations. In addition, the event composition rules were predefined before the CEP engine began working. The rule defining task is error-prone and cumbersome. In this paper, therefore, a hybrid complex event service is proposed, which deals with not only discrete events but also continuous behavior computation based on IoT (Internet of Things) resource models. In order to satisfy the real-time constraints of processing IoT events, a divide-and-conquer principle is adopted, where we give a combination theorem such that different events can be processed on different IoT resources and then these processed results can be combined to derive complex events. Based on the formal IoT resources and event knowledge, we define interest goals to direct event composition without enumerating event relation to define event composition rules. We finally present event composition algorithms and evaluation to show our idea."
1532,"With more and more Web services flooded on the Internet, the scale of Web services and complexity of connections among them are growing rapidly. This phenomenon has brought great challenges to service selection. Due to the huge search space, existing research approaches are hardly feasible in dynamic real-time scenarios under a stringent time limit with a large number of potential Web services involved. In order to deal with this problem, the focus of this paper is to improve the efficiency of QoS-aware web service selection in real-time considering a priori knowledge from historical log, which can reduce the search space effectively. We first analyse and discover the distribution of customer requests to identify request clusters, and we mine valuable fragments or service patterns from historical service solutions. Then, we build a probability matrix to improve the efficiency of service selection algorithm, which contains the request-solution mapping relationships between request clusters and service patterns based on statistical method. A series of experiments using both real and synthetic data demonstrate that our approach improves Global Planning optimisation algorithm (GP) and Artificial Bee Colony algorithm (ABC) by 36.20% and 41.98% respectively."
1533,"Software-defined networking (SDN) is a promising future network architecture which introduces new dimensionsin flexibility and adaptability to cope with different Quality of service (QoS) metrics, such as latency constraint, loss rate, etc. In this paper, we provide differentiated Services for flows of a topic-oriented publish/subscribe system to address some of the QoS guarantee issues, by leveraging multi-priority queue functions of SDN. Flows are bifurcated into multi-priority queues in terms of packet properties such as delay and reliability requirements. An active feedback-based queue management mechanism is responsible for ensuring bounded queueing delays for those higher priority queues, and a mapping method accounts for mapping time-constrained flows to different priority queues. The Experiments verify the effectiveness of the queue mechanism and flow scheduling."
1534,"Automated web service composition has been tackled from different directions and to different purposes. In addition, most of the approaches address the composition problem with under specified requirements, returning compositions models that do not necessarily satisfy and fulfill end-users objectives. Satisfying the latter objectives is a difficult problem, especially from scratch, which requires stronger requirements and a further step of integration with service-based components in order to make service oriented computing and service composition a reality. In this work, we address this issue by presenting an innovative and integrated approach to service composition which consists of i) an automatic template process generator, that is able to generate abstract process templates and their hierarchy from past executions; ii) a novel and scalable approach to AI parametric-design techniques using a multi agent approach to configure and adapt services processes, heavily relying on the latter set of abstract process templates; iii) an optimization process that maximizes the overall quality of final compositions. Finally, we compare the scalability of these components with some experiments."
1535,"Asynchronous interactions are becoming more and more important in the implementation of complex B2B Web applications. This paper addresses correlation and coordination issues involved with asynchronous Web services, by studying different mechanisms and metadata structures for supporting them; in addition, several interaction patterns for building asynchronous computations are discussed, and the trade-offs between the various patterns are shown. In conclusion, we illustrate the use of asynchronous Web services in the context of some concrete B2B applications."
1536,"In general, provenance of electronic data represents an important issue in information systems. So far, service-oriented computing research has mainly focused on provenance of data. However, service provenance also plays a central role since service providers and consumers want to be aware of the service's origin and history. In this paper, we present an approach for service provenance that builds on service metadata and various service runtime events. In addition, access control mechanisms are implemented to restrict access to this information. Besides being able to query and subscribe to provenance information, provenance graphs can be used to illustrate the history of services. We give some usage examples of service provenance and show how our approach was integrated into the VRESCo Web service runtime environment."
1537,"This paper presents a short survey on the quality evaluation of web services. The most popular metrics for estimating such quality and user perception of web services are Quality of Service (QoS) and Quality of Experience (QoE), which represent objective and subjective assessments correspondingly. For different types of web services, the values of QoS and QoE are measured in different ways. In this paper, we consider various definitions of QoS based on web service parameters and describe several methods for evaluating QoS and QoE. We start with experimental evaluation of QoS based on network traffic analysis and further turn to model based methods for QoS estimating. Existing relationships between different kinds of service quality evaluation are also discussed."
1538,"This paper describes Easy SOA. Easy SOA is a rapid prototyping model for SOA based on ad hoc development and integration tool for end users (ADIEU). With ADIEU, end users can prototype their Web applications and Web services rapidly by putting 'cards' into a 'sheet' constructed on a Web browser. Easy SOA realizes a prototype development model for service oriented architecture (SOA) as well as for Web applications and Web services using ADIEU"
1539,"Web services are supported by almost all major software vendors, but nevertheless there is still a certain barrier that prevents a broader user community to actually use them. The barrier is the lack of appropriate clients offered in conjunction with the services. This paper presents a Web Service Browser that automatically generates a dynamic user interface when the user browses to the location of the service description and additionally handles the invocation of the service. To ease the use of the service, the browser takes care of data management by using an implementation of the Flex-SwA architecture. Results are presented to the user in a human-readable manner. When the result contains multimedia data, an audio or video player is used to present the result. Use cases demonstrate the benefits of the browser. With the Web Service Browser, web services simply become a usable component offered in the WWW."
1540,"Web Services are a technology based on the Service Oriented Architecture that enables communication between applications through the Internet. Using Web Services, it is possible to send any type of information in any form of encryption. In this context, different techniques have been used to attach binary files in SOAP messages. However, there is no consensus on which technique has the best performance. This paper presents a performance evaluation study with three techniques for Web Service attachments: Pure Binary, MTOM and SwA. A testing environment was configured to verify the influence of the network and the size of files. Also, we present a tool called WSATPerf that supports the execution of the performance evaluation."
1541,"Large-scale distributed system, such as educational system, are difficult to develop due to their complex and decentralized nature. service oriented architecture (SOA) is a new form of distributed software architecture. The service oriented architecture facilitates the development of such systems by supporting modular design, application integration and interoperation, and software reuse. With open standards, such as XML, SOAP, WSDL and UDDI, the service oriented architecture supports interoperability between services operating on different platforms and between applications implemented in different programming languages. In this paper, In this paper, we propose software architecture for design and develop for a distributed e-education system in the way of service oriented architecture. We will explain what this e-education system concerns, how it is developed, and what services it provides."
1542,"Web services are becoming widely deployed to implement the automation of business processes such as supply chain management, inventory tracking, and healthcare management, just to name a few. A Web service is a new breed of web application that supports interoperable application-to-application interaction over a network based on a set of XML standards. This new architecture and new set of protocols brings a new set of security challenges such as confidentiality, integrity, anonymity, authentication, authorization and availability. As security has become an essential component for all information systems, several security solutions for Web services data have been proposed such as WS-Security, SAML and XACML. To enable privacy protection for Web service consumers across multiple domains and services, the World Wide Web Consortium (W3C) published a document called ""Web Services Architecture (WSA) Requirements"" that defines some specific privacy requirements for Web services as a future research topic."
1543,"SAP Netweaver Application Server (AS) provides two communication protocols to connect SAP ABAP (Advanced Business Application Programming) systems with external systems: Simple Object Access Protocol (SOAP) and Remote Function Call (RFC). RFC is the standard SAP interface for communication between SAP systems. SOAP is widely used in web service solutions for its nature cross-platform characteristic. It uses platform independent eXtensible Markup Language (XML) but it introduces more latency tags to represent the data. Thus, SOAP might need more memory and transferring time. This paper studies and compares their performance characteristic with introducing an evaluation model and then evaluates them for different use cases. This work gives a practical conclusion for protocol selection in special case."
1544,"Recommendation is an important issue in e-commerce systems. Conventional recommender algorithms, like the collaborative filtering recommendation algorithms, have been extensively studied and developed into a very mature stage, in which how to further promote user's favorite items and alleviate sparsity and cold start problem become increasingly important. In this paper, traditional recommender algorithms are promoted by exploring group's preference to mitigate the issues above and improve the predicting accuracy in both cases. Our work is based on the following observation. Users in the same group share common interests, given a group there are items that the group is most interested in, on the other hand, given some specific items there is the first-rate group which shows most preference compared to other groups. This leads to our proposed PromoRec algorithm which focuses on promoting items that users are most likely to prefer with sparse insensitivity since group enriches user's data largely. In a multi-dimensional space, we show how to efficiently compute (a) the most popular items for a target group and (b) the group which shows most interests in specific items. In addition, without the needs of available user group information, we propose an automatic classification algorithm based on users' similar interests. To improve the recommendation accuracy, we use additional item classification information to help determine the similarity between users. The experiment results confirm that our method significantly enhanced the traditional item recommendation algorithms especially while predicting ratings of promoted items for sparse users."
1545,Presents the welcome message from the conference proceedings.
1546,"There are different and conflicting views made by analysts and some vendors about the relative positioning of SOA and EDA. The panel is intended to investigate the relative positioning and determine pros and cons for a unified SOA-EDA solution. The panel questions will concentrate on: (1) what is the difference between the service-oriented approach to the event-oriented approach? (2) What are common between the two approaches? (3) Is combination of the two (""SOA 2.0"") makes sense. (4) What are its value add to customers?"
1549,"Service-oriented computing promises to create flexible business processes and applications on demand by dynamically assembling loosely coupled services within and across organizations. Quality requirements play a central role in service sourcing and, together with Service Level Agreements, facilitate service selection and measurement of service delivery effectiveness. This empowers customers to make better decisions when faced with multiple service offerings and varying service costs. However, existing business process modeling languages provide little support for quality requirements annotation and specification. This paper argues that quality requirements are a central aspect of business process modeling specification, and thus proposes to incorporate time, cost and reliability quality requirements as extensions to the Business Process Modeling Notation (BPMN). These quality requirements are evaluated based on analytical model using reduction rules. An example of online purchasing business process is illustrated to demonstrate the applicability of the proposed approach."
1550,"We present a tool to automatically derive choreography-conforming web services systems. The user provides a specification that describes peer-to-peer collaborations of the observable behavior of parties from a global viewpoint, in our case WS-CDL documents, and the tool automatically extracts the particular behavior of each participant, more concretely, WS-BPEL documents defining the behavior from a local viewpoint. We implement two automatic methods(centralized and decentralized) that derive conforming systems even in cases where projecting the choreography into each service would lead to a non-conforming system. This issue is addressed by adding some control messages that make services interact as required by the choreography. Experiments where the number of exchanged messages is measured are presented, and strategies to reduce the number of these messages are discussed."
1551,"Programmable Logic Controllers (PLC) are widely used in process automation due to their fast and guarantee response time. However their programming and interfacing capacities are limited. In this paper we present a Surveillance software aimed to extend the range of actions available to deal with anomalous situations that are not critical in terms of time and to complement any actions implemented by other units in critical situations. The system combines the possibilities offered by standard OPC XML-DA, the integration capabilities provided by web services and rule-based engines with state-of-the-art technologies for communication such as e-mail, SMS, instant messaging or VoIP. Variables values are monitored by the component using OPC XML-DA and a set of previously defined rules that can trigger actions via web service invocations. These two characteristics allow creating a dynamic behaviour able to handle exceptions. In summary, this component extends control capacities and facilitates exception processing on PLC and NC."
1552,"Web service based applications are expected to live in dynamically evolving settings. At run-time, services may undergo changes that could modify their expected behavior. Because of such intrinsic dynamic nature, applications should be designed by adhering to the principles of design- by-contract. Run-time monitoring is needed to check that the contract between service providers and service users is fulfilled while the collaboration is in place. We describe a language to specify the expected functional and non-functional requirements that a service provider should fulfill. The language (timed WSCoL) is a temporal extension of a previous proposal (WSCoL). We also illustrate the architecture of a run-time analyzer that checks timed WSCoL properties. Should such properties be disproved during execution, appropriate recovery and reconfiguration actions may be put in place."
1553,"With the growing use of cluster systems in Web servers, file distribution and database transactions, power conservation and efficiency have been identified as critical issues in the design of cluster systems. Widely adopted, distributor-based systems forward client requests to a balanced set of backend servers in complete transparency to the clients. In this paper, we use power and locality-based request distribution at the distributor to provide optimum power conservation, while maintaining the required QoS of the system. The distribution scheme uses a simple memory management technique using pinned memory on the backend servers and proactive distribution, with the aid of data organization of the Web site, to improve the locality of the files. A simple on-off based power management scheme is applied to conserve power. Our scheme provides reduced response time to the clients and improved power conservation at the backend server cluster without compromising performance. Simulations involving real-time Web traces and latest Web technologies witness performance boost of 15-23% and power conservation of 15-48% over the existing policies."
1554,"Distributed denial-of-service (DDoS) attacks are increasingly mounted by cyber-criminal gangs to extort money from online businesses. This kind of attacks is normally targeted at a particular service provider to exhaust the network and system resources of the provider. Since the scale of the attack is limited, the ISP operators normally cannot observe this type of attacks. As a result, the victim of the attack is left to deal with the attack on its own accord. This paper proposes a SOA approach to build a system against DDoS attacks targeting online businesses. The system is built on Web services. It can be constructed and reconfigured easily by an attack victim. Experiments were also carried out to measure the overheads and the effectiveness of the proposed approach."
1555,"A key challenge posed by the Internet of Services is that applications need to cope with a continuously changing environment, both in terms of the context in which they operate and of the services, users and providers involved. Differently from applications where traditional change detection and adaptation mechanisms can be used, the Internet of Services requires systems that are adaptive by design. In this paper we present a design for adaptation framework supporting the modeling, development and operation of service based applications operating in highly dynamic environments. The approach exploits advanced techniques for dynamic and incremental service composition, allowing to effectively deal with changes occurring at different levels in the system. An implementation of the proposed solution is presented and evaluated on a real-world scenario from the Smart Urban Mobility domain."
1556,"The grid-based infrastructure enables large-scale scientific applications to be run on distributed resources and coupled in innovative ways. However, in practice, grid resources are not very easy to use for the end-users who have to learn how to generate security credentials, stage inputs and outputs, access grid-based schedulers, and install complex client software. There is an imminent need to provide transparent access to these resources so that the end-users are shielded from the complicated details, and free to concentrate on their domain science. Scientific applications wrapped as Web services alleviate some of these problems by hiding the complexities of the back-end security and computational infrastructure, only exposing a simple SOAP API that can be accessed programmatically by application-specific user interfaces. However, writing the application services that access grid resources can be quite complicated, especially if it has to be replicated for every application. In this paper, we present Opal which is a toolkit for wrapping scientific applications as Web services in a matter of hours, providing features such as scheduling, standards-based grid security and data management in an easy-to-use and configurable manner"
1557,"There is a growing demand for provisioning of proportional responsiveness differentiation to various clients on scalable Web servers to meet changing resource availability, and to satisfy different client requirements. Theoretically, a queueing-based processing rate allocation scheme is able to achieve the objective by providing different processing rates to requests of different client classes. However, we find that an implementation of the queueing-theoretical scheme shows weak proportionality with large variance because it does not have fine-grained control over the resources that the kernel consumes and hence the processing rate is not strictly proportional to the number of processes allocated. We design a feedback controller and integrate it with the queueing-theoretical scheme. The integrated application-level approach allocates a certain number of processes to handle requests of different client classes according to the queueing-theoretical scheme. The process allocations are then adjusted according to the difference between target response time and the achieved response time by using proportional integral derivative control. Results demonstrate that this integrated approach can enable Web servers to provide fine-grained response time differentiation. The approach is robust and can be practically deployed on Apache Web servers."
1558,"Web services are now widely used in web-based applications. To protect the information in web services, many security specifications have been proposed. Attribute-based Encryption (ABE) provides us a brand new cryptographic primitive for access control. This paper sets out to examine an unexplored area to date - how attribute-based encryption might be used to provide privacy and security for web services. We try to implement ABE in web services. The implementation and performance evaluation demonstrate that ABE is efficient and feasible in web services."
1559,"With the popularity of service-oriented architecture, many web systems have been developed in form of composite services. Since the performance of these composite services highly depends on Quality of Service (QoS) of employed atomic web services, it is important to predict the QoS values of atomic web services with high accuracy. Although collaborative filtering based approaches have recently been proposed to predict the web service QoS values, they mostly face a cold start problem which causes unreliable prediction due to the highly sparse historical data, newly introduced users and web services. Furthermore, existing work only considers the case of newly introduced users. In this paper, we propose a Location-based Matrix Factorization technique via Preference Propagation (LMF-PP) to improve the cold start problem in web service QoS prediction domain. LMF-PP exploits the location information of entities (i.e., Users and web services) and employs the preference propagation to make the accurate QoS prediction even for the newly introduced entities and in the small amount of data (i.e., Highly sparse matrix). The performance of LMF-PP is compared with that of existing approaches on a real world dataset. The experimental results show that LMF-PP can outperform the existing approaches in not only a cold start environment but also a warm start environment."
1560,"Several activities in service oriented computing can benefit from knowing properties of a given service composition ahead of time. We will focus here on properties related to computational cost and resource usage, in a wide sense, as they can be linked to QoS characteristics. In order to attain more accuracy, we formulate computational cost / resource usage as functions on input data (or appropriate abstractions thereof) and show how these functions can be used to make more informed decisions when performing composition, proactive adaptation, and predictive monitoring. We present an approach to, on one hand, automatically synthesize these functions from orchestrations and, on the other hand, to effectively use them to increase the quality of non-trivial service-based systems with data-dependent behavior. We validate our approach by means of simulations with runtime selection of services and adaptation due to service failure."
1561,"In this paper, we first analyze the urgent needs of training service skills in a global environment. We then analyze the different patterns that 3D virtual world technologies can help learning activities. Instead of using the well known MVC model for Web application development, we propose a new design pattern for designing 3D virtual world learning applications named PWI model based on our own accumulated experience."
1562,"Recent technologies including Web services, business process modeling tools, and grid computing, are key to facilitating business agility. Web services standardize application interfaces to ease inter-application communication, and facilitate standards based business process management. Grid computing provides a way for virtualizing heterogeneous resources to collectively yield a computationally powerful IT environment with tremendous promise in improving utilization levels of individual IT resources. By leveraging Web services, a standards based grid is being envisaged in the OGSA model. In this paper, we explore the complementarities of Web services and business process management (BPM) technologies. We also lay the foundation for using Web services based BPM in solving orchestration and workflow issues in grid computing."
1563,"Test coverage is popularly applied in software companies to measure test efforts and create regression test suite. Many service providers actively provide test as a service to software companies to reduce the cost of software test. However, it is a big challenge to store increasing massive test coverage data, such as software build, test traces, etc. In addition, it is also a data-intensive processing for coverage statistics summarization and regression test generation. Hence, data availability and scalability becomes two obstacles for the traditional solutions to be deployed as a service. To overcome these obstacles, in this paper, we propose to leverage the cloud techniques including BigTable and MapReduce to deploy test coverage service."
1564,"Often there are several services providing similar functionality, moving the problem of selecting the most suitable to the forefront of interest. In this paper we consider the selection of services in a dynamic environment with changing requirements. In previous work we considered selecting services in isolation, here we present an enhancement to select services in their relation to each other to gain a global optimal solution which nevertheless respects local criteria. Novel contributions are the definition of a composition context and the global multi-criteria optimization mechanism."
1565,"With the development of Service-Oriented technologies, the amount of Web services grows rapidly. QoS-Aware Web service recommendation can help service users to design more efficient service-oriented systems. However, existing methods assume the QoS information for service users are all known and accurate, but in real case, there are always many missing QoS values in history records, which increase the difficulty of the missing QoS value prediction. By considering the user-service-time three dimension context information, we study a Temporal QoS-Aware Web Service Prediction Framework which aims to recommend best candidates to service user's requirements and meanwhile improve the QoS prediction accuracy. One major challenge is that how to deal with the high dimension, sparse QoS value data. Tensor which is known as multi-way array provides a natural representation for such QoS value data. Therefore, we formalize this problem as a tensor factorization model and propose a Tucker Decomposition (TD) algorithm which is able to deal with the triadic relations of user-service-time model. Extensive experiments are conducted based on our real-world QoS dataset collected on Planet-Lab, comprised of service invocation response-time values from 408 users on 5,473 Web services at 56 time periods. Comprehensive empirical studies demonstrate that our approach is more accuracy than other approaches and achieves 100X to 1000X memory space reduction."
1566,"In this paper, we first introduce some issues that are encountered in building a service debugger and briefly describe our approach to addressing them. Next, we outline some debugging modes and components of a simple composite debugger. Then, we mainly describe its message-based front-end and back-end, which are a co-existing, self-identifying, and non- intrusive. Finally, we preset some experimental results of our latest prototype."
1567,"This paper presents DynaBot, a domain-specific Web service discovery system. The core idea of the DynaBot service discovery system is to use domain-specific service class descriptions powered by an intelligent Deep Web crawler. In contrast to current registry-based service discovery systems -like the several available UDDI registries - DynaBot promotes focused crawling of the Deep Web of services and discovers candidate services that are relevant to the domain of interest. It uses intelligent filtering algorithms to match services found by focused crawling with the domain-specific service class descriptions. We demonstrate the capability of DynaBot through the BLAST scenario and describe our initial experience with DynaBot."
1569,"Outsource encrypted data is a popular trend for storing sensitive data in third party clouds. Many cloud applications need privacy preserving data encryption services with two capabilities: On one hand, they need querying over encrypted data in Web based data hosting services. On the other hand, they also need to keep the query keywords and associated search operations private such that data hosting service providers cannot gain access to unauthorized content or trace and infer sensitive data stored in the third party data hosting servers. In this paper we present a novel service oriented framework for verifiable searchable asymmetric encryption, called PVSAE. PVSAE offers strong support for outsourced encrypted data with two formal security properties in terms of IND-CKA security and search pattern privacy. Our framework supports two concrete PVSAE schemes. The first scheme l-PVSAE is based on the l-dimensional vectors and achieves strong security notions, namely statistical IND-CKA security and statistical search pattern privacy. The second scheme 3-PVSAE is a light-weight version based on 3-dimensional vectors. 3-PVSAE maintains the strong security properties and offers higher efficiency for search over encrypted data compared with existing verifiable searchable asymmetric encryption schemes. We experimentally evaluate the proposed PVSAE schemes and show that they not only offer strong security but also are practical and deployable."
1570,"Architectural mismatch; the semantic and syntactic gap preventing component composition, can be partially alleviated through the ubiquity of the protocols for service composition (syntax). And yet, the semantics of service or component composition have remained elusive. The contribution of this paper builds on the notions introduced by grammar-oriented object design (GOOD) using the notion of manners to specify the semantics of services and drive their automated composition through dynamically reconfigure architectures enabled through GOOD. This leads to the use of manners for explicit service semantics representation in a re-configurable architectural style. We describe the base requirements for automated component and services composition and show how automated assembly of components and services can be accomplished through a dynamically reconfigurable (DyRec) architectural style. In addition, we show how this vision of dynamic reconfiguration is naturally supported by the Web services framework, because it derives from some of the core requirements of the service oriented architectures (SOA) style embodied by Web services. Further, we discuss how such a DyRec architectural style can be implemented using grammar-oriented object design (GOOD) and describe a project that led to a tool called the Business Compiler, to accomplish dynamic composition and collaboration."
1571,"In this paper, we present a framework for evaluating the QoE of a service that includes functional and non-functional service requirements. Non-functional requirements are classified into objective, subjective, and business parameters that affect Quality of Service (QoS), Quality of Experience (QoE), and Quality of Business (QoBiz), correspondingly. As those metrics have a strong dependency between each other, we discuss how the QoE of a web-based Over-The-Top service (OTT) can be evaluated taking into account subjective, objective and business parameters. The functional service behavior is described by an Extended Finite State Machine (EFSM) in which non-functional objective, subjective and business-related parameters are tracked using context variables and corresponding updating functions. These parameters are used to evaluate the QoE of the service. We show that the corresponding model allows to keep a track of a user-service interaction. Moreover, the model of the service integrates subjective, objective and business parameters, and thus, can be applied to the QoE evaluation of any OTT service."
1572,"The increasing number of services available in the cloud market make them plausible and attractive for building Cloud Service Compositions (CSC). However, performance instability is common in the cloud environment due to changes in supply and demand of shared computational infrastructure and resources. Candidate compositions are vulnerable to such instability. We propose a novel approach to improve performance stability by leveraging on the principles of design diversity in service composition(s). The approach uses portfolio theory to construct a diversified composition of candidate services that share lowest possible correlation for their performances. We use an exemplar to illustrate the applicability of the approach. Controlled experiments are used to test the approach effectiveness in improving the performance stability of CSC. While the scalability of our approach is evaluated, we report on its sensitivity and effectiveness under multiple correlation settings."
1573,"Service differentiation is a practical approach for service provider to deliver ldquogoodrdquo service quality to different customers or customer segments under limited computing resources. In this paper, we address the problem of differentiating business process services by effectively scheduling tasks inside business processes, where dynamic value of service request, process instance execution status and workload of service components are all taken into consideration. Corresponding framework architecture and a scheduling algorithm are purposed for this purpose."
1574,"Composing adaptive and self-managing Web services needs plug-and-play architecture so that the deployment of control components does not require changes made to the Web services and the host middleware platforms. This is especially challenging for Web services running on COTS middleware platforms, such as Microsoft.Net. In this paper, we propose an architectural solution that introduces a management proxy between adaptive control components and Web services. The management proxy can be customized and seamlessly integrated with a COTS middleware platform by leveraging the existing middleware mechanisms. This solution enables dynamically composing adaptive Web services on COTS middleware without stopping its services. We demonstrate this architecture by a realistic Web service application built on .Net Windows Communication Foundation (WCF). The performance overhead incurred by this architecture is measured, and the results validate that our solution is efficient in terms of performance and flexibility."
1575,"It is still a great challenge to make composite services that fulfill massive requirements from large community of customers. Service network (SN) approach has been put forward to deal with this issue in a cost-effective and agile way, i.e., a large number of services are connected as a customizable SN in terms of underlying semantics correlations, and when a new requirement arrives, the SN is customized to generate a composite service. It has been observed that there are similarities among requirements, and so are there among composite solutions of these requirements. In this paper, we identify bilateral patterns (i.e., requirement patterns and service patterns) from historical service composition records, and then establish the probabilistic mappings between them, consequently, a bilateral pattern based service network customization algorithm named BPSC is put forward to take full advantage of such priori knowledge to speed up the customization process. From experiments we find that, compared with traditional SN customization approaches such as WSPR, our BPSC can significantly improve the efficiency in most instances."
1576,"SOA and cloud computing have brought new opportunities for the long expected agility, reuse and the adaptive capability of IT to the ever changing business requirements and environments. But due to the immature nature of the rapidly evolving technologies, especially in the areas of security, service or information integrity, privacy, quality of service and their possible detrimental consequences, many enterprises have been hesitating to make the shift. This paper adopts the concept of insurance and establishes a framework and the supporting reference model for cloud computing. We utilize the value-at-risk (VAR) approach to establish several appropriate mechanisms, and use a set of measurable metrics. Those quantitative or qualitative metrics can be applied as the basis for the business value and risk assessment, and eventually for insurance premium and compensation calculation for the failures of the services offered in Cloud environment. This model can also establish a potential new innovative market branch for the insurance industry."
1577,"Redundancy-based fault tolerance strategies are proposed for building reliable Service-Oriented Architectures/Applications (SOA), which are usually developed on the unpredictable remote Web services. This paper proposes and implements a distributed replication strategy evaluation and selection framework for fault tolerant Web services. Based on this framework, we provide a systematic comparison of various replication strategies by theoretical formula and real-world experiments. Moreover, a user participated strategy selection algorithm is designed and verified. Experiments are conducted to illustrate the advantage of this framework. In these experiments, users from six different locations all over the world perform evaluation of Web services distributed in six countries. Over 1,000,000 test cases are executed in a collaborative manner and detailed results are also provided."
1578,"Over the last years agents have proved to be a powerful mechanism for solving a range of problem sets. In order to fully leverage their power in a multi-agent environment with heterogeneous participants, the establishment of barrier-free communication between different parties is of prime concern. In particular, such an environment may consist of numerous entities, based on different implementations and with different nonfunctional requirements (NFRs), e.g. encryption key length or latency. Currently, interoperability between the involved parties in multi-agent systems is limited as there is no established way of elaborating a common set of NFRs. In this paper, we outline a Web services-based architecture addressing this issue of interoperability in multi-agent systems. Furthermore, we introduce a soft constraints-based approach for modeling and reconciling the NFRs. In particular, Web services serve as a tool for establishing an open system providing reliable communication between a variety of agent implementations and possible non-agent based entities while lacking the limitations of Agent Communication Languages (ACLs), which are asynchronous in nature and closed in implementation. In this way, Web services enable the desired level of openness and robustness needed for maximum interoperability. Adding a soft constraints-based approach to the Web services agent layer provides an extensible and flexible mechanism for the reconciliation of interdependent NFRs. We present a prototype implementation of an open, Web services-based framework optimizing unary soft constraints in a pareto optimal way and therefore enabling barrier-free communication in a multi-agent scenario. Due to its open nature, the framework is applicable to any heterogeneous environment in need of reconciling NFRs. To illustrate its functioning, we briefly demonstrate the framework in the potential use case of a Demand Driven Supply Network (DDSN)."
1579,The conference offers a note of thanks and lists its reviewers.
1580,"A service-oriented System of System (SoS) integrates component services into a value-added and more complex system to satisfy the complex requirements of users. Due to a dynamic running environment, online reliability prediction for the loosely coupled component systems that ensures the runtime quality poses a major challenge and attracts growing attention. To guarantee the stable and continuous operation of systems, we propose a online reliability time series prediction method basing on long short term memory (LSTM), which is a modified Recurrent Neural Networks trained with historical reliability time series to predict the reliability of component systems in the near future. We conduct a series of experiments on a dataset composed of real web services and compare with other competitive approaches. Experimental results have demonstrated the effectiveness of our approach."
1581,"Federated cloud enables a workflow to be deployed in multiple private and public clouds. By facilitating external cloud-based services to execute sub-tasks of the workflow, service workflow owners can reduce the cost of executing the workflow, while meeting a performance requirement, since those cloud-based services can be more cost efficient and have better performance than internal ones. However, due to inter-dependencies between sub-tasks, the complexity of the workflow, and the heterogeneity of clouds, it is a challenge to achieve the optimal tradeoff between cost and performance. This paper presents a novel workflow scheduler designed to achieve the optimal end-to-end execution time and cost when deploying such complex workflows in heterogeneous computing nodes in clouds. Specifically, our scheduling algorithm addresses the tradeoff between the execution cost, the computing time, and the data transfer delay between sub-tasks. Our scheduler can handle complex workflows that contain recursively paralleled sub-flows caused by branch and merging sub-tasks. Experiments indicate that our scheduler can efficiently compute the near optimal deployment compared with greedy and evolutionary algorithms for both end-to-end execution time and corresponding cost."
1582,"The promise of Web services is to enable the composition of new distributed applications/solutions: when no available service can satisfy a client request, (parts of) available services can be composed and orchestrated in order to satisfy such a request. Service composition involves two different issues: the synthesis, in order to synthesize, either manually or automatically, a specification of how coordinating the component services to fulfill the client request, and the orchestration, i.e., how executing the previous obtained specification by suitably supervising and monitoring both the control flow and the data flow among the involved services. In this work, we address the automatic composition synthesis when the behavior of the available services is non-deterministic, and hence is not fully controllable by the orchestrator. The service behavior is modeled by the possible conversations the service can have with its clients. The presence of nondeterministic conversations stems naturally when modeling services in which the result of each interaction with its client on the state of the service can not be foreseen"
1583,"Web services are finding their way into mobile devices in several disparate islands: We can find WS proxies connected via proprietary wireless connectors to mobile devices (Blackberry MDS), the beginnings of a Web service consumer stack in mobile Java (JSR172), Web service identity federation stacks built into smartphone operating systems (Series 60, Windows Live for Mobile), Web services ""Lite"" in the form of Ajax (Opera) on browsers and widgets for mobile devices). However, challenging issues abound: (1) Where should we terminate a Web service - at a proxy or on the mobile device? (2) Are there any compelling reasons to make a mobile device a Web service consumer or provider itself? (3) Can mobile Web services realize the potential of multivendor interoperability? (4) When will we see seamless interoperability between enterprise Web service platforms (for example J2EE and Vista WCF and their mobile counterparts J2ME and Windows Mobile)? (5) What are the mobility versions of the Web 2.0 scenarios of social networking and collaboration? This panel of experts from organizations (Microsoft, Sun, RIM, Opera, Motorola) leading these developments will debate their visions of the mobile Web services future."
1584,"In spite of the success of many commercial cloud service e-marketplaces, the search results from these platforms are usually presented as an unordered list of icons representing the services that best fit users' keyword-based queries. The drawback of such presentation mechanisms is that users are not able to immediately discriminate among the cloud services for easy decision making. A number of cloud service selection frameworks have been proposed, however, some of these frameworks do not enable users to make comparisons among services. In this paper, we introduce a visualization framework for cloud service selection. Our framework takes into cognizance the set of cloud services that matches a user's request and based on QoS attributes, users can interact with the results via bubble graph visualization to compare and contrast the search results to ascertain the best alternative. The bubble graph enables the exploration of services in a unified view of the QoS space, exhibiting both high object coherence and correlation. The result from our experiments shows that our framework simplifies decision making as users can identify services that best fit their requirements quicker and easier compared to tabular formats."
1586,"One benefit of Software-as-a-Service (SaaS) is the ability to rapidly deploy iterative improvements without requiring users to upgrade the application on their machine. However, the need to rapidly ""develop and test"" different versions of an application implies that developers need branch isolation to protect the system from local changes to both data and meta-data in the same way that they traditionally use branch-isolation to protect the system from source-code changes. Providing branch-isolation for source-code changes has well-known solutions, but these solutions do not extend well to providing isolation for changes to data and meta-data. EVERETT provides developers the ability to safely - and concurrently - change database values with new business logic or evolve data schema in various ways while sharing the same database."
1587,"This paper addresses one of the major problems of SOA software development: the lack of support for testing complex service-oriented systems. The research community has developed various means for checking individual Web services but has not come up with satisfactory solutions for testing systems that operate in service-based environments and, therefore, need realistic testbeds for evaluating their quality. We regard this as an unnecessary burden for SOA engineers. As a proposed solution for this issue, we present the Genesis2 testbed generator framework. Genesis2 supports engineers in modeling testbeds and programming their behavior. Out of these models it generates running instances of Web services, clients, registries, and other entities in order to emulate realistic SOA environments. By generating real testbeds, our approach assists engineers in performing runtime tests of their systems and particular focus has been put on the framework's extensibility to allow the emulation of arbitrarily complex environments. Furthermore, by exploiting the advantages of the Groovy language, Genesis2 provides an intuitive yet powerful scripting interface for testbed control."
1588,"In environments with limited network bandwidth, current web service execution mode will face challenges for its enormous demand for network bandwidth. The transmitted message which contains real payload and XML-based format data seems to be verbose. The response time and fail risk will be brought to increase as well. In this paper we address this issue and propose an agent-based compression method. A software agent is deployed between the requesters and invokers to execute services and return results to the requesters. The transmitted messages size is reduced by stripping off redundant data and adaptive data compression. We conduct a detailed evaluation of compression effectiveness and provide the results of execution time measurements. The effectiveness of the approach is proved by the experiment results and the conclusion is given."
1589,"We introduce BlueInfo, an open architecture for deploying Web services in WPAN hotspots for cost-free context-aware mobile access over Bluetooth. A BlueInfo hotspot either pushes subscribed services at desired intervals to registered devices (BlueInfo Push) or alternatively the user invokes a particular service by sending a simple keyword query to the hotspot (BlueInfo Pull). The BlueInfo hotspot requests the service from the origin server in the Internet and relays the response to the mobile device, possibly after adaptation for mobile viewing. The usability of BlueInfo Pull in comparison to a mobile phone browser is demonstrated with an empirical user evaluation conducted in a laboratory setting."
1590,"Web service recommendation has recently drawn much attention with the growing amount of Web services. Previous work usually exploits the collaborative filtering techniques for Web service recommendation, but suffers from the data sparsity problem that leads to inaccurate results. Our analysis on a real-world Quality of Service (QoS) dataset shows that there is a hidden correlation among users and services. We define such hidden correlation with an asymmetric matrix (namely asymmetric correlation), in which each entry presents the hidden correlation between a user pair or between a service pair. The goal of this work is to employ such asymmetric correlation among users and services to alleviate the data sparsity problem and further enhance the prediction accuracy in service recommendation. Specifically, we propose an asymmetric correlation regularized matrix factorization (MF) framework, in which asymmetric correlation and asymmetric correlation propagation have been naturally integrated. Finally, experimental results on a well-known real-world QoS dataset validate that the use of asymmetric correlation among users and services is effective in improving prediction accuracy for Web service recommendation."
1591,"Automatic service discovery and selection is the key aspect for composing Web services dynamically in service-oriented computing (SOC). Current approaches to automating discovery and selection make use of only structural and functional aspects of the Web services. We believe that behavioral selection of Web services should be used to provide more precise results. Service behavior is difficult to specify prior to service execution and instead is better described based on experience with the service execution. In this paper, we propose a novel approach to service selection and maintenance-inspired by agile software development techniques-that is based on behavioral queries specified as test cases. Behavior is evaluated through the analysis of execution values of functional and non-functional parameters."
1592,"The service-oriented architecture paradigm prescribes the development of systems through the composition of services, i.e., network-accessible components, specified by (and invoked through) their WSDL interface descriptions. Systems thus developed need to be aware of changes in, and evolve with, their constituent services. Therefore, accurate recognition of changes in the WSDL specification of a service is an essential functionality in the context of the software lifecycle of service-oriented systems. In this work, we present the results of an empirical study on WSDL evolution analysis. In the first part, we empirically study whether VTracker, our algorithm for XML differencing, can precisely recognize changes in WSDL documents by applying it to the task of comparing 18 versions of the Amazon EC2 web service. Second, we analyze the changes that occurred between the subsequent versions of various web-services and discuss their potential effects on the maintainability of service systems relying on them."
1593,"The problem of selecting services from a set of functionally appropriate ones under Quality of Service constraints - the Service Selection Problem - is well-recognized in the literature based on deterministic parameters. However, Quality of Service may rather follow a stochastic distribution and, thus, may change at runtime. In order to cope with differing Quality of Service, we present a heuristic approach for efficiently addressing the Service Selection Problem in conjunction with stochastic Quality of Service attributes. Accounting for penalty cost which accrue due to Quality of Service violations, our approach reduces the impact of stochastic Quality of Service behavior on total cost significantly."
1594,"Service dependency graph (SDG) is an AND/OR graph showing input output dependencies among service operations. As dependencies in an SDG are indirectly expressed by reasoning on data models used by service interface definitions, their re-usability and expressiveness are limited. In this paper, we propose an enhanced version of service dependency graph, namely SDG+. SDG+ enhances SDG with explicit dependency declaration, which expresses dependencies directly with static explicit declarations. Based on SDG+, we developed our automatic service composition algorithm for WS-Challenge 2007, which wins the championship of composition efficiency in the competition."
1595,"We propose a dynamic mechanism, thread count adaptation, that adjusts the thread counts that are allocated to services for adapting to CPU requirement variations in SMP environments. Our goal is to increase the maximum throughput available on a system that has multiple dynamic content services while meeting different service time criteria for these services in dynamic workloads. Our challenge is to significantly improve response times for dynamic content on a busy well-tuned thread-pool-based system without prioritizing any specific services. Our experiments demonstrate that a prototype using our approach on J2EE middleware quickly (around every 20 ms) adjusted the thread counts for the services and that it improved the average 90th-percentile response times by up to 27% (and 22% on average) for the SPECjAppServer2004 benchmark."
1596,"The potential of a large scale growth of private and semi-private registries is creating the need for an infrastructure which can support discovery and publication over a group of autonomous registries. Recent versions of UDDI have made changes to accommodate interactions between distributed registries. In this paper, we discuss METEOR-S Web service Discovery Infrastructure, which provides an ontology-based infrastructure to access a group of registries that are divided based on business domains and grouped into federations. We also discuss how Web service discovery is carried out within a federation."
1597,"We propose an approach to leverage the development life cycle of a semantic portal through the use of Object Management Group's model driven architecture (MDA). In our proposed approach, developers can use their prior knowledge about system modeling and UML to create semantic applications, without worrying much about various ontologic intricacies. We believe that our proposed combination of the semantic technologies and MDA-centric standardizations makes an ideal contrivance for the integration and development of standardized knowledge-based applications."
1598,"Web services are becoming the prominent paradigm for distributing, computing, and electronic business, while there is an increasing surge to provide online business- to-business collaborations. The Web services choreography description language (WS-CDL) is a Web service specification developed by W3C, in order to provide peer-to-peer collaborations for participants from different parties. Despite the great research interests it has received during recent years, no practical or even prototype execution engine has been built for WS-CDL, which is, however, essential to test and evaluate the properties of WS-CDL when doing research on it, and promote its application fields in business. This paper implements an execution engine of WS-CDL, which has never been built before, and experiments on the functionalities and performance of the engine. We also address the extensions toward WS-CDL, namely WS- CDL+, which are built into our execution engine. Finally, the whole paper is concluded, addressing the application perspectives of WS-CDL/WS-CDL+."
1599,"Online periodic prediction is very common in practical problems like stock index analysis or traffic status estimation, where a prediction service is invoked at every time point. However, due to the potential communication cost, service overheads or the expenditure incurred by every prediction service invocation, it is valuable to reduce the number of invocation times without sacrificing too much precision. Therefore, we present an approach based on reinforcement learning to select the timing for prediction service invocations, which still guarantees high quality in capturing the regularity in a future time series. Extensive experiments are conducted on a real-world airfare search volume dataset of multiple routes. The experiment results prove that our algorithms show superiority over other solutions."
1600,"As Web services increasingly become important in distributed computing, some of the flaws and limitations of this technology become more and more obvious. One of this flaws is the discovery of Web services through common methods. Research has been pursued in the field of ""semantic Web services"". This research is driven by the idea, to describe the functionality of Web services as accurately as possible and to create programs automatically out of already existing Web services. In this paper we discuss a new method for discovery and analysis of Web services. Our approach uses a vector space search engine to index descriptions of already composed services. Rather than generating or automatically composing applications, this approach provides developers with a valuable utility to browse repositories based on already existing information. Furthermore, we propose some additional modifications to extract the maximum amount of semantics from existing service definition repositories."
1601,"Service Oriented Architecture (SOA) has become a major application development paradigm. As a basic unit of SOA applications, Web services significantly affect the quality of the applications constructed from them. Since the development and consumption of Web services are completely separated under SOA environment, the consumers are normally provided with limited knowledge of the services and thus have little information about test oracles. The lack of source code and the restricted control of Web services limit the testability of Web services. To address the prominent oracle problem when testing Web services, we propose a metamorphic testing framework for Web services taking into account the unique features of SOA. We conduct a case study where the new metamorphic testing framework is employed to test a Web service that implements the electronic payment. The results of case study show the feasibility of the framework for web services, and also the efficiency of metamorphic testing. The work presented in the paper alleviates the test oracle problem when testing Web services under SOA."
1602,"Enterprise information systems are currently developed as Web-based applications in the service-oriented architecture style, which is implied by the stack of Web services standards. One of the most critical aspects in enterprise information systems is to maintain their continuous operation as long as possible. Conventional service invocation mechanism identifies Web services with similar signatures found by a service discovery as distinctively different ones, resulting in the suspension of a service requester and the modification of its invocation procedures. In this paper, we propose a reasonable solution to realize runtime invocation of Web services, regarding structural similarities in service signatures as subsumption relations of XML scheme. Given Web services with subsumed signatures, this solution enables a service requester to invoke the Web services by translating the scheme types of the invocation parameters."
1603,"It is a challenge to ensure data integrity for cloud-based Internet of Things (IoT) applications because of the inherently dynamic nature of IoT data. The available frameworks of data integrity verification with public auditability cannot avoid the Third Party Auditors (TPAs). However, in a dynamic environment, such as the IoT, the reliability of the TPA-based frameworks is far from being satisfactory. In this paper, we propose a blockchain-based framework for Data Integrity Service. Under such framework, a more reliable data integrity verification can be provided for both the Data Owners and the Data Consumers, without relying on any Third Party Auditor (TPA). In this paper, the relevant protocols and a subsequent prototype system, which is implemented to evaluate the feasibility of our proposals, are presented. The performance evaluation of the implemented prototype system is conducted, and the test results are discussed. The work lays a foundation for our future work on dynamic data integrity verification in a fully decentralized environment."
1604,"In the Web services environment, RBAC (role-based access control) model is widely accepted as an efficient approach to manage the access control. By defining the authorization relationship between subject roles and object roles in the RBAC, authorization policies are utilized to simplify the authorization management on different Web services. But the scalability and complexity of composite Web services may cause authorization policy conflict. A new authorization policy added to the system may conflict with existing ones and result in authorization chaos and authorization leaking. And when implemented in the composite Web services, policy conflict detection would be of high cost with manually checking. That makes automatic policy conflict detection important to ensure the security of authorizations in the composited Web services. This paper analyzes the features of the authorization policy in the CWS-RBAC (RBAC for composite Web services) and presents methods of detecting policy conflict including subject role propagation conflict, object role composition conflict and context conflict. The experiment designed is to validate the efficiency of each conflict detection method."
1605,"Web service composition lets users create value-added composite Web services on existent services, where top-k composite services are helpful for users to find a satisfying composite service efficiently. However, with an increasing number of Web services and users' various composition preferences, computing top-k composite services dynamically for different users is difficult. In view of this challenge, a top-k composite services selection method is proposed, based on a preference-aware service dominance relationship. Concretely speaking, firstly, user preferences are modeled with the preference-aware service dominance, and then, in local service selection, a multi-index based algorithm is proposed, named Multi-Index, for computing candidate services of each task dynamically. Then, in global optimization, combined with a service lattice, top-k composite services are selected under a dominant number-aware service ranking. At last, an experiment is presented to verify our method."
1606,"To avoid the lock-in problem in service-oriented software, existing interface-decoupling mechanisms focus on identifying high-level service mappings, which are not necessarily applicable on translating actual data. Based on the fundamental data-translation process, its successful outcome is guaranteed if mappings are low-level, i.e. they satisfy schema constraints. The problem is that if similar services have been identified based on high-level mappings, then schema constraints may be violated. To overcome this problem, we propose a proactive approach to identify similar services that satisfy schema constraints. In particular, our approach follows a composite workflow (instead of existing hybrid workflows), which determines schema constraints (service documents do not specify them) and estimates service-translation cost (actual cost is not available) as a function of ensured schema-constraints. We compare our approach against a state-of-the-art service-similarity approach on benchmark services and the results show that high service-similarity does not necessarily imply low service-translation cost, the bidirectional nature of service similarity can be misleading, ensured schema-constraints improve service similarity, and estimated translation-cost is very close to actual cost."
1607,"Recently, a new generation of adaptive Process-Aware Information Systems (PAIS) has emerged, which allows for dynamic process and service changes (e.g., to insert, delete, and move activities and service executions in a running process). This, in turn, has led to a large number of process variants derived from the same model, but differing in structure due to the applied changes. Generally, such process variants are expensive to configure and difficult to maintain. This paper provides a sophisticated approach which fosters learning from past process changes and allows for mining process variants. As a result we obtain a generic process model for which the average distance between this model and the respective process variants becomes minimal. By adopting this generic model in the PAIS, need for future process configuration and adaptation decreases. We have validated the proposed mining method and implemented it in a powerful proof-of-concept prototype."
1608,"As more and more reusable web services are published on the Internet, how to help users quickly identify appropriate candidate services has become an increasingly critical challenge. Most of the current research efforts on service discovery rely on syntax and semantics-based service matchmaking. In contrast, this paper presents a novel way of applying network routing mechanism to facilitate service discovery, featuring scalability and performance. Services annotated by Web Ontology Language for Services (OWL-S) are organized into a network based on semantic clustering. Virtual routers are created representing clusters, and Bloom Filters are generated for service routing. A service search request is thus transformed into a network routing problem to quickly locate semantic service cluster and in turn to candidate services. In addition, the deterministic annealing technique is applied to facilitate service classification in the network construction. Dynamic network adjustment is operated to ensure the search performance in the network. Empirical study over common testbed annotated in OWL-S is reported."
1609,"The number of systems communicating using web services has grown exponentially over the years thanks to the service-oriented architecture they enforce. The paradigm shift encouraged system owners to re-use existing services rather than implementing them again as well as consuming third party ones cutting the development time considerably. Since the number of consumers can grow radically it is important to ensure that the exchanged data is valid. In this article we will demonstrate a way to validate and potentially correct both the request and response of web services at run-time using a semantic rule oriented approach. The solution also provides a validation method for black-box systems where the original source code is no longer available or cannot be modified. By combining validation, proxy and correction our approach provides an all-in-one solution that can be leveraged in most scenarios."
1610,"A Service Oriented Enterprise (SOE) provides an efficient and flexible platform where multiple Web services can cooperate together to provide a value-added service. Change management is one of the fundamental issues in enabling SOEs. In this paper, we propose a framework that facilitates in automatically managing top-down changes in SOEs. We start with formalizing a SOE's schema since it is a central concept for specifying and managing top-down changes in SOEs. We then propose a change model as a guide to react to changes. Algorithms are proposed to implement changes by refining a SOE's behavior."
1611,"Web services are gaining acceptance as a standards-based approach for integrating loosely coupled services often distributed over a network. Hence, achieving high levels of reliability and availability in spite of service or infrastructure failures poses unique set of challenges. However, Web services middleware provide limited constructs for specifying faults detection and recovery actions. Additionally, faults-handling logic often gets scattered and tangled with the service logic. Consequently, this negatively impacts maintainability and adaptability. To address these requirements for reliable and fault tolerant Web services execution, we propose set extensible recovery policies to declaratively specify how to handle and recover from typical faults in Web services composition. The identified constructs were incorporated into a lightweight service management middleware named MASC (manageable and adaptive service composition) to transparently enact the fault management policies and facilitate the monitoring, configuration and control of managed services. Several experimental results with a service based supply chain management system illustrate the effectiveness of our approach to providing reliable and uninterrupted services"
1612,"This paper presents a middleware for building context-aware applications. One of the main components, Device Information Access (DIA), is discussed in detail. Since many kinds of devices (e.g., RFID, GPS, Bluetooth, etc.) can be used to collect the context information, the middleware defines the Device Information Access component to communicate with different devices. A set of interfaces are devised in DIA, and the common functions such as getting and setting a data element are defined in the interfaces. For each device, we shall provide an implementation of the interfaces to communicate with the corresponding servers or software agents. DIA can communicate with the software agents or servers using various protocols such as RMI, Web Services, and REST. In this way the access to the hardware are encapsulated by the middleware and virtualized to the end-point applications. The architecture of the middleware and the functions of DIA are discussed, and an empirical application is also developed to validate our design."
1613,"Service composition is complex. It has to reach a set of pre-defined non-functional qualities, like security for instance, which requires the production of complicated code.This code, often distributed between client and server sides, is highly error-prone and difficult to maintain. In this paper, we present a generative environment for the orchestration of abstract services and the separate specification of non-functional properties. This environment has been built within the European SODA project and validated on several industrial use cases. In this paper, we focus on an alarm management scenario with stringent security requirements."
1614,"In this paper, we present SAWSDL-MX2, a hybrid semantic Web service matchmaker for SAWSDL services. Building on our initial work, we adopt logic-based as well as text similarity service selection for model references and add a structural approach, which operates on the pure syntactic description of WSDL elements. The integration of these matching variants is accomplished using a Support Vector Machine (SVM) with non-linear kernel, thus automatically adapting an aggregation function based on previously experienced training data. Results of our performance evaluation based on the standard measures recall and precision over the SAWSDL-TC1 test collection as well as an exhaustive example for all basic matching variants are also given."
1615,"The ability to automatically answer a request that requires the composition of a set of web services has received much interest in the last decade, as it supports B2B applications. Planning techniques are used widely in the literature to describe the web services composition problem but they don't scale up well. This weakness is due to the search space explosion caused by the large ranges of data exchanged among services. In addition, it is more interesting to use a decentralised planner because the nature of the problem is distributed. In this paper, we consider a set of web service agents where each agent has a set of services organised in a graph. To respond to a request, agents propose their best local partial plans which are partial paths in the graph. They then coordinate their partial plans to provide the global plan for the submitted request using an algorithm based on a distributed heuristic function. This function ensures the optimality and the completeness of the algorithm. Indeed, it is based not only on the agent capabilities to respond to a request, but also taking into account the plans proposed by other agents. The complexity of the algorithm is polynomial. The experiments show the ability of our approach to find the optimal solutions for automated web services composition taking into account the dependencies betwen the agents."
1616,"Testing is useful to establish trust between service providers and clients. To test the service-oriented applications, automated and specification-based test generation and test collaboration are necessary. The paper proposes an ontology-based approach for Web services (WS) testing. A test ontology model (TOM) is defined to specify the test concepts, relationships, and semantics from two aspects: test design (such as test data, test behavior, and test cases) and test execution (such as test plan, schedule and configuration). The TOM specification using OWL (Web ontology language) can serve as test contracts among test components. Based on the WS semantic specification in OWL-S, the paper discusses the techniques to generate the sub-domains for input partition testing. Data pools are established for each parameter of the specified service. Data partitions are derived by class property and relationship analysis. Completeness and consistency (C&amp;C) checking can be performed on the data partitions and data values, both within the TOM and against the OWL-S, by ontology class computation and reasoning. A prototype tool is implemented to support OWL-S analysis, test ontology generation and C&amp;C checking."
1617,"Context-awareness and adaptability are important and desirable properties of service-based processes designed to provide personalized services. Most of the existing approaches focus on the adaptation at the process instance level which involves extending the standard Business Process Execution Language (BPEL) and its engine or creating their own process languages (e.g. However, the approach proposed here aims to apply an adaptation to processes modeled or developed without any adaptation possibility in mind and independently of specific usage contexts. In addition, most of the existing approaches tackle the adaptation on the process instance or definition levels by explicitly specifying some form of variation points. This, however, leads to a contradiction between how the architect logically views and interprets differences in the process family and the actual modeling constructs through which the logical differences must be expressed. We introduce the notion of an evolution fragment and evolution primitive to capture the variability in a more logical and independent way. Finally, the proposed approach intends to support the viewpoint of context-aware adaptation as a crosscutting concern with respect to the core “business logic” of the process. In this way, the design of the process core can be decoupled from the design of the adaptation logic. To this end, we leverage ideas from the domain of model-driven development (MDD) and generative programming."
1618,"We recently developed ""system on mobile devices"" (SyD) middleware for rapidly developing and deploying collaborative distributed applications over a collection of autonomous Web objects and data-stores, independent of the underlying device, data, or network. SyDListener is a key component of SyD middleware. SyDListener provides a set of interfaces and classes that allows distributed SyD-based application components to communicate seamlessly in mobile environments. SyDListener provides a uniform object view of the underlying server application and enables client applications to remotely invoke those methods using XML messages. SyDListener is implemented as a multi-threaded wrapper with simple persistence management and asynchronous invocation functionality for J2ME mobile information device profile (MIDP) on connected limited device configuration (CLDL) devices. We discuss the functionality, architecture, implementation, and performance of SyDListener. We believe it is the first comprehensive working prototype of its kind for Java-enabled handhelds with a small footprint of 10 KB."
1619,"Process retrieval is critical for workflow repository management. Structural similarity metric based on graph matching could achieve highest retrieval quality. Nowadays, researchers mainly adopt graph edit distance (GED) as the approach for comparing process models. However, the computation complexity of GED based methods are high and their cost functions depend heavily on the application domain. To overcome these shortcomings, we use the maximal common subgraph (MCS) approach instead and propose a depth-first search (DFS) code based method to implement the MCS. The minimum DFS codes are used to canonically label the process models and their fragments. By comparing the minimum DFS codes of the fragments, the maximal common subgraphs between the search model (i.e., a given process model or fragment) and the processes in the repository could be found. The experimental evaluations show that our method is feasible for real applications."
1620,"The use of the big data analytics (BDA) platform is increasingly becoming prevalent in the data sciences. However, BDA processes consume resources and time excessively. Automating BDA processes is a cognitive approach to the BDA domain, which is most impaired by its heavy consumption of time and resources. However, the BDA workflow is highly dependent on diversified constraints because of the high variability, veracity, and volume of data processing to accomplish highly influential and sophisticated requirements. The workflow has to pass rigorous and diverse data mining steps, each step contains several tasks and these tasks made by many sub tasks, that it must be accomplished to progress to the next step in the workflow. This increases the available solution space for BDA processes. The intelligent heuristic approach is needed to address the domain-specific concerns which are large solution space, and awareness of constraints are caused the BDA planning. Therefore, we propose to use GraphPlan-based dynamic workflow generation for the BDA domain. Our empirical studies prove that the proposed sophisticated method satisfied planning requirements and outperformed a related planning technique."
1623,"Edge computing may improve the processing quality of big IoT stream data and reduce network operational cost by moving computation onto the edge. However, there are two challenges in integrating cloud and edge computing for big stream data. Firstly, edge equipment usually has very limited computing power as well as storage ability, and apparently cannot support all the processing of big and real-time stream data. A flexible division of such services between edge and cloud is needed. Secondly, edge-end collaboration continuously changes due to some intrinsic interaction of data stream. In this paper, we propose a service-based approach to seamlessly integrating cloud and edge equipment. Based on our service model, we split a cloud service into two parts running on cloud and edge respectively. Also, we propose a dynamic service scheduling mechanism based on the improved bipartite graphs. We can deploy a cloud service to the edge at the right time when a key node emerges. The effectiveness of the proposed approach is demonstrated by examining real cases of China's State Power Grid. Experimental results verify the effectiveness and efficiency of our approach."
1624,"With the development of Web technology and distributed systems, online collaborations are becoming more common and more demanding. Web services now provide standard mechanisms to enable online interactions. Yet security, privacy and trust-related protection mechanisms for Web services need additional development. In an interconnected network environment, physical connections with proper security protections are required for bridging two autonomous networks. Likewise, collaborating organizations need mechanisms for bridging extant relationships among cooperating parties that provide proper protection for privacy and trust. A trust establishment mechanism for Web services must therefore ensure privacy and owner control at all times due to the subjectivity of trust relationships. This paper describes an indirect trust establishment mechanism to bridge and build new trust relationships from extant trust relationships providing privacy protection and owner control simultaneously"
1625,"We propose a methodology for designing Web services. The methodology is founded on Tropos (Perini et al., 2001 and Castro, et al., 2002), an agent-oriented software development technique, and supports early and late requirements analysis, as well as architectural and detailed design. An online retailer example is used for illustration of the proposed methodology. We also compare the generated design with a sample design presented in [BPT01]."
1626,"In modern service economies, service provisioning needs to be regulated by complex SLA hierarchies among providers of heterogeneous services, defined at the business, software, and infrastructure layers. Starting from the SLA Management framework defined in the SLA@SOI EU FP7 Integrated Project, we focus on the relationship between establishment and monitoring of such SLAs, showing how the two processes become tightly interleaved in order to provide meaningful mechanisms for SLA management. We first describe the process for SLA establishment adopted within the framework; then,we propose an architecture for monitoring SLAs, which satisfies the two main requirements introduced by SLA establishment: the availability of historical data for evaluating SLA offers and the assessment of the capability to monitor the terms in a SLA offer."
1627,"The potential benefits of Web services composition heavily rely on semantic interoperability, i.e., the ability to exchange data meaningfully amongst Web services. Context heterogeneity, which refers to different implicit assumptions about interpreting the exchanged data, hampers the automatic composition of Web services. However, existing initiatives of semantic Web services (SWSs) often ignore context heterogeneity. In this paper, we introduce an approach to address this issue. The contexts of the involved Web services are defined in a lightweight ontology and their WSDL descriptions are annotated by an extension of a W3C standard, i.e., semantic annotation for WSDL and XML schema (SAWSDL). The composition of Web services is described using BPEL specification. Given a BPEL file that ignores context heterogeneity, the approach automatically detects all context differences among the involved services, and reconciles them by producing a mediated BPEL file that incorporates necessary conversions using Xpath functions and/or Web services."
1628,"Today, information systems rely largely on web services and XML as their payloads. Every information system also depends on checking and preserving integrity constraints. In the world of conceptual modeling, they can be expressed using Object Constraint Language (OCL) expressions over class diagrams. However, the expressions defined in the conceptual model of the system must be rewritten to the form of XML schemas to be used for XML and web services. This is an error-prone task for a system designer who, in addition, must be skilled both in OCL and XPath and other XML languages. In this work, we show how OCL integrity constraints can be translated to Schematron schemas automatically and verified in XML documents thus reducing the effort of the initial translation and maintenance."
1629,"This paper sets out key issues of exception handling that relate to identifying, analyzing and dealing with an exception in service-oriented systems. Then required concepts, basic structures and algorithms for the Ml-Eh are discussed. The new concepts include the super-space of exception definitions, the meta-definition mode, and components in an exception message. The main characteristics of the Ml-Eh are a uniform definition mode, dynamical extensibility, and abilities of configurable analyses and encapsulations, and a message-level combination. Finally, implementation and evaluation of the mechanism is given"
1630,"Adaptive SOAP (A-SOAP,) is a practical approach to SOAP message compression. A-SOAP separates mechanisms from policies and allows for incremental deployment. This paper focuses on its three underlying mechanisms: accelerating message composition, reducing parsing overheads, and compressing messages, which leverages the previous two mechanisms. In contrast to existing dictionary-based compression techniques, A- SOAP does not require dictionaries to be exchanged between the two endpoints in advance; its dictionaries are built incrementally, as the communication progresses. A- SOAP endpoints agree on the dictionary management policy using a mechanism similar to HTTP content negotiation, possibly using a dedicated HTTP header field. In experiments with short messages and a simple policy, an A-SOAP prototype reduces processing overheads by half and message sizes by an order of magnitude without increasing message latencies."
1631,"Service-oriented workflows are typically executed using a centralised orchestration approach that presents significant scalability challenges. These challenges include the consumption of network bandwidth, degradation of performance, and single-points of failure. We provide a decentralised orchestration architecture that attempts to address these challenges. Our architecture adopts a design model that permits the computation to be moved ""closer"" to services in a workflow. This is achieved by partitioning workflows specified using our simple dataflow language into smaller fragments, which may be sent to remote locations for execution."
1632,"A significant obstacle to building usable, web-based interfaces for computational science in a Grid environment is how to deploy scientific applications on computational resources and expose these applications as web services. To streamline the development of these interfaces, we propose a new application framework that can deliver user-defined scientific workflows as both web services and OpenSocial gadgets. Through this application framework, scientists can focus on defining computational workflows using domain-specific applications and can use the software tools in the framework to quickly generate gadgets for running the applications and visualizing the output from workflow executions. By assembling these domain-specific gadgets and some common gadgets predefined in the framework for workflow management, scientists can easily set up a customized computational workspace to meet their requirements."
1633,"This paper presents QoS explorer, an interactive tool we have developed which predicts quality of service (QoS) of a workflow from the QoS characteristics of its constituents, even when the relationships involved are complex. This facilitates design and instantiation of workflows to satisfy QoS constraints, as it enables the user to discover and focus effort on the aspects of a workflow which most affect their primary QoS concerns, thus improving efficiency of workflow development. Further, the underlying model we use is more sophisticated than those of similar recent work (Jaeger et al., 2005; Ardagna and Pernici, 2005; Menasce, 2004), and includes processing of entire statistical distributions and probabilistic states (instead of the simple numeric constants used elsewhere) to model such non-constant variables as execution time"
1634,We propose a skyline computation approach that enables service users to optimally access sets of services as an integrated service package. We first present a one pass algorithm based on the observation that a multi-service skyline is completely determined by single service skylines. The skyline is returned after an enumeration on a significantly reduced candidate space. We then develop a dual progressive algorithm that is able to progressively report the skyline. We conduct an experimental study to assess the performance of the skyline computation approaches.
1635,"APIs are increasingly important for companies to enable partners and consumers to access their services and resources. API ecosystems deal with related challenges like publication, promotion and provision of APIs by providers and identification, selection and consumption of APIs by consumers. To address these challenges, to match consumers with relevant APIs, and to support API providers and thus ultimately the ecosystem to evolve, API ecosystems rely on information about APIs, their usage and characteristics, and the social environment around them. We present an extensible, graph-based data model to capture the entities in an API ecosystem and their relations. The data model includes temporal information to capture the evolution of API ecosystems. Analysis operations on top of the data model provide insights for consumers, providers and the ecosystem provider to address the introduced challenges. We present a system implementing the conceptualized data model. We integrate this system with an API ecosystem used in the context of a hackathon event to continuously collect data. We furthermore show the data model's capabilities to represent a well-known dataset about ProgrammableWeb and to drive analysis operations on both datasets."
1637,"One of the interesting aspects of the Web 2.0 'evolution' is the wide-availability of various Web applications as APIs or Web services. These APIs expose informational services on the Web and take many form of remote invocation of functions using standard Web protocols and XML for data representations, e.g., REST, SOAP/WSDL, XML-RPC, and other approaches. The services (or APIs) are also usually accompanied by user facing Web applications for human-consumption. Canonical examples are Google Maps, Yahoo! Flykr and del.icio.us, EVDB's Eventful's application and API, Amazon.com's S3, ECS, Alexa, and many others. The Ruby programming language and its Rails framework are ideal for programming Web applications and services in the Web 2.0. Ruby's modern and dynamic features make it an excellent language for rapid prototyping and integration of various Web services. Rails' superb support for rapid Web application development, database access, and AJAX, make it well suited for creating front-ends and back-ends to the next generation of Web applications and services. In this tutorial we will take a hands-on deep-dive into the Ruby and Rails platform and learn how they can be used to: (1) create Web applications backed by a relational database, (2) consume Web services, (3) create and deploy APIs or Web services, and (4) mashup of existing Web services and applications. No a priori knowledge of Ruby or Rails is required - although some programming in a modern OO language and Web application development are definite plus"
1638,"The support for non-functional concerns (NFC) is essential for the success and adoption of web services. However, state of the art works offer only a limited support for these concerns especially when it comes to the composition of multiple non-functional concerns with composite web services. In this paper, we focus on the composition of non-functional actions (NFA) with composite web services whose composition logic is made explicit using languages such as BPMN2 or WS-BPEL (gray box view). In contrast to the black box view where only the interface of the service is visible the gray box view reveals additional information on control flow, data flow, composed services, etc. NFAs can also be composite and constitute complex processes such as secure conversations or transactions which have to be integrated with composite web services. Additionally, the execution order of multiple superimposing NFA has to be declared. In our approach we introduce a model-driven approach for the integrated specification and realization of the different types of NFC composition (NFAs with composite services, execution order of superimposing NFAs and composite NFAs) as well as a set of editors and code generators supporting this approach."
1639,"Monitoring is the cornerstone for cloud service management, so it is significant for a cloud monitoring tool to support customization for specific monitoring requirements, to indicate the correlation between target services, and to guide adaptive service management. Unfortunately, traditional monitoring tools are always developed independently with service management platforms and are provided as self-contained softwares, which limit their capabilities on addressing these requirements. In this paper, we propose MonValley, an unified monitoring and management framework for cloud services. It consists of four components: 1) a high-level language for practitioners to express monitoring specifications on the services from all three layers, 2) a compiler to translate the expressive program into an executable program, 3) an execution engine to execute the executable program, 4) a runtime to provide supports on basic functionalities, such as data transmission. MonValley provides an innovative approach to facilitate the integrated monitoring and adaptive management for cloud services."
1640,"This panel is devoted to the topic of Services Architectures, which play a significant role in the effective operations and delivery of services businesses today."
1642,"With more and more requirements of navigation for complex indoor environments, the indoor location service has become the hotspot in the field of mobile computing. However, with only one single type of wireless signal, it is difficult to achieve ideal accuracy of positioning in the indoor environments full of indoor noises. In order to improve the performance of indoor location service, we propose a novel indoor localization mechanism, which realizes an effective data fusion of Wi-Fi and RFID signals via on-demand deployment of Wi-Fi access points and RFID tags. This mechanism can eliminate the blind areas of location so as to realize the low-cost and high-accurate indoor localization. In order to further improve the location performance, we put forward the Kalman filter algorithm based on singular value judgment (KFASVJ) and KFASVJ-based indoor localization algorithm (KILA). The KILA is adopted to judge the maximum singular value of Wi-Fi signal wave, so as to optimize the wireless signal wave. KILA can reduce the indoor noise interference with Wi-Fi signals, so to realize a high accuracy of positioning and real-time positioning stability in complex indoor environments. The experimental results and the performance analysis show that KILA achieves a better accuracy of positioning than typical Kalman-filter-based localization algorithm (KFLA), about 13% to 28% accuracy of positioning improvement in the indoor environments with the [35dB, 65dB] indoor noise. KILA has the lower time complexity, the higher location speed and the better stability, and it can maintain a good localization performance even in the indoor environment with the indoor noises changing dynamicly."
1643,"We present an experiment relative to the use of Bluetooth wireless technology to provide network support for midlet applications accessing Web services. We refer to the most common architecture used to invoke Web services, where a client and a server exchange SOAP messages using HTTP as the transport protocol. To the best of our knowledge, there is no implemented support for executing a HTTP POST operation over a Bluetooth channel. Therefore, to guarantee the independence of the application from the type of communication channel used, in this paper, we deal with the problem of designing a framework allowing a Java application programmer to directly interface Web services from a mobile device using a Bluetooth connection. This paper presents a proof of concept of how Bluetooth technology can be used to design, develop, and deploy Web services-based applications. According to our experiments, programming interfaces like Blue Cove and kSOAP, despite being still under development, are mature enough to be used as the underlying technologies for Web services invocation over Bluetooth in a real world application"
1644,"Current Web service discovery methods are based on centralized approaches where Web services are identified based on service functionality. Examples of service functionality include car rental, hotel booking and book selling. Since higher level Web services are increasingly composed in terms of lower level Web services, it is important that service discovery not only be based on service functionality but also be based on process behavior, i.e., how a service functionality is served. Furthermore, centralized approaches to service discovery suffer from problems such as high operational and maintenance cost, single point of failure, and scalability. Another issue that has not been considered in current Web service discovery paradigms is the issue of trust and quality of service of the service provider. We, therefore, propose a structured peer-to-peer framework for Web service discovery in which Web services are located based on both service functionality and process behavior. In addition, we integrate a scalable reputation model in this distributed peer-to-peer framework to rank Web services based on both trust and service quality."
1645,"Today, more and more software are augmented with service-oriented packaging. At the same time, more and more business and government services are provided and offered in the form of software. However, there are debates on whether and how much service engineering has in common with software engineering."
1647,"Web service orchestration languages describe executable business processes composed of Web services. A business process can fail for many reasons, such as faulty Web services or mismatching messages. It is important to find out which Web services are responsible for a failed business process because we could penalize these Web services and exclude them from the business process in the future. In this paper, we propose a model-based approach to diagnose orchestrated Web service process. We convert the Web service orchestration language, BPEL4WS, into synchronized automata, so that we have a formal description of the topology and variable dependency of the business process. After an exception is thrown, the diagnoser can calculate the business process execution trajectory based on the formal model and the observed evolution of the business process. The faulty Web services are deduced from the variable dependency on the execution trajectory. We demonstrate our diagnosis technique with an example."
1648,"Accounting for quality correlations among web services when performing service composition is essential to obtain more accurate quality estimations of service combinations, thus providing users with better composite solutions. Yet, most current composition approaches fail to address such correlations by assuming independence between services regarding their quality values. In response, this paper presents a correlation-aware composition approach, where quality dependencies among services are modelled and considered during composite service selection. Moreover, to improve selection efficiency, correlation-aware search space reduction techniques are introduced, which prune out uninteresting service compositions prior to selection. The effectiveness of the approach, in terms of time and optimality, is demonstrated via experimental results."
1649,"The aim of this paper is to reveal some intrinsic disadvantages of the current version of UDDI standard, which create problems in using it as a standard for private, in-house storage of enterprise services. Examples include: access control mechanisms in UDDI, limited rich queries capability, inappropriate mapping of Web service artifacts into UDDI entities, impossibility of managing classification system values, etc. For each disadvantage we consider, we give an illustrative example of its impact in an enterprise service environment. To overcome these disadvantages, some service registry implementations, based on UDDI introduce proprietary extensions to the standard, or embed additional programmatic logic in their client modules of UDDI, which decreases interoperability between them."
1650,"QoS-aware service composition problem has been drawn great attentions in recent years. As an NP-hard problem, high time complexity is inevitable if global optimization algorithms (such as integer programming) were adopted. Researchers applied various evolutionary algorithms to decrease the time complexity by looking for near optimum solution. However, each evolutionary algorithm has two or more parameters the value of which is to be assigned by algorithm designers and likely has impacts on the optimization results (primarily time complexity and optimality). Our experiments show that there are some dependencies between the features of service composition problems, the value of the evolutionary algorithm's parameters, and the optimization results. In this paper, we use a popular evolutionary algorithm Artificial Bee Colony (ABC) to solve service composition problem and focus on the ABC's parameter turning issue. The objective is to identify the potential dependency to help service composition algorithm designers easily set up the values of ABC parameters to obtain preferable composition solution without many times of tedious attempts. Five features of service composition problem, three ABC parameters and two metrics of the final solution are identified. Based on a large volume of experiment data, ABC parameter tuning for a given service composition problem is conducted using C4.5 algorithm and the dependency between problem features and ABC parameters are established using multiple linear regression method. An experiment on a validation dataset shows the feasibility of our approach."
1651,"Web services are widely used to enable remote access to heterogeneous resources through standard interfaces and build complex applications by reusing existing component services. However, massive commodity computers, storage, network devices and complex management tasks running behind web services make them subject to outage and unable to provide continuously reliable services. To address this issue, we present a Paxos-based replication framework for building consistent and reliable web services. The framework mainly consists of a replication protocol and a set of failure tackling algorithms. First, in the replication protocol, besides keeping consistency of service replicas we introduce pipeline concurrency and RDG (Request Dependency Graph) to traditional Paxos so as to improve its performance. Second, we design failure recovery algorithms to recover the failed nodes, which guarantee that each web service has enough available replicas and thus can deliver expected reliability. Third, through an extensive set of experiments, we show that our method is effective in terms of keeping consistency and reliability of web services and it outperforms other replication methods."
1652,"Web is becoming a programmable platform, with countless services blooming everyday in various forms like Feeds, REST APIs and Widgets, etc. Although the existing technologies, such as Mashups, have reduced the challenges to build new applications by composing these services, it's still far from enabling the non-technical users to solve their situational problems by correlating and consuming these services. In this paper, we present our HyperService technology, which empowers a much more flexible way to link and explore existing services for solving various situational problems. In HyperService, the service metadata, service linkages and user behaviors are indexed and managed; Based on the user's input keywords and navigation context, a group of relevant services are dynamically searched, ranked and recommended for facilitating future navigations; the service navigation is smoothed by a web2.0 style exploratory user interface. A prototype system is also presented to demonstrate the effectiveness of our HyperService research work."
1653,"Detecting runtime anomalies is very important to monitoring and maintenance of distributed services. People often use execution logs for troubleshooting and problem diagnosis manually, which is time consuming and error-prone. In this paper, we propose an approach for automatic anomaly detection based on logs. We first mine a hybrid graph model that captures normal execution flows inter and intra services, and then raise anomaly alerts on observing deviations from the hybrid model. We evaluate the effectiveness of our approach by leveraging logs from an IBM public cloud production platform and two simulated systems in the lab environment. Evaluation results show that our hybrid graph model mining performs over 80% precision and 70% recall and anomaly detection performs nearly 90% precision and 80% recall on average."
1654,"The Service Oriented Computing Paradigm proposes the construction of applications by integrating preexistent services. In recent times, automated business processes and web services have become ubiquitous. In this paper, we propose the automation of service composition that takes the abstract specification of a composition and the definition of concrete services. We make a reduction from this rewriting problem to exact cover problem, represented by a bipartite graph. We present a program analysis and experimental results to show the efficiency of our proposed mechanism."
1656,This talk will share with the technical community on some directions of Services Research at IBM.
1657,"The advent of Web services has made automated workflow composition relevant to Web based applications. One technique, that has received some attention, for automatically composing workflows is AI-based classical planning. However, classical planning suffers from the paradox of first assuming deterministic behavior of Web services, then requiring the additional overhead of execution monitoring to recover from unexpected behavior of services. To address these concerns, we propose using Markov decision processes (MDPs), to model workflow composition. Our method models both, the inherent stochastic nature of Web services, and the dynamic nature of the environment. The resulting workflows are robust to nondeterministic behaviors of Web services and adaptive to a changing environment. Using an example scenario, we demonstrate our method and provide empirical results in its support."
1658,"Existing Web services standards consider data primarily on the level of inputs and outputs specifications, with the major focus on functional aspects of interactions. Majority of applications rely on data sources, but such data sources are not part of the Web service specifications and cannot be accessed directly by clients. The fact that data are treated independently or as second-class citizens severely limits re-use, flexibility, customization and integration options of current Web services. In this paper we suggest to extend the WS specifications by introducing a data-centric Web services model that integrates functional and data perspectives in one coherent framework. The approach is based on Business Artifacts and in particular on the declarative modular Guard-Stage-Milestone (GSM) model. We introduce a Web Data- and Artifact- centric Service (W-DAS) model using GSM in its core which in addition to usual application specific WS operations defines a set of data access interfaces including CRUD operations, artifacts retrieval interface for querying, filtering and sorting data, and operations for arbitrary custom defined ad hoc run-time queries. We discuss W-DAS publish-subscribe mechanisms and implementation."
1659,"Dynamic selection of the best services to execute abstract tasks of business processes is very important. Indeed, it enables to cope with complex user's requirements that require the collaboration of several more elementary services. However, with the increasing amount of candidate services of each business task that offer different QoS (Quality of Service) values, the selection of the optimal combination of services becomes a very hard task. This problem is more complex when dealing with temporal properties of business processes associated with time-dependent QoS parameters that can change according to the execution time. Unlike static QoS which have been deeply studied in the existing service selection approaches, time-dependent QoS are insufficiently taken into consideration. In this paper, we are interested in the problem of service selection to satisfy a given business process while considering temporal properties associated to time-dependent QoS. The selection approach that we propose relies on a new service pruning approach that is applied prior to our selection algorithm to reduce the number of candidate services while guaranteeing that the optimal solution still be found."
1660,"This paper introduces a new P2P Electronic Cash system called Netcoin. The purpose of Net coin is to facilitate inexpensive peer-to-peer monetary transactions on the Web. Its salient features are that it is a traceable system with an efficient mechanism for verifying transactions. Net coins are reusable and can be easily passed from one user to another. The issuing of virtual currency and verification of transactions are performed by trusted mints, which act as the gateway between the fiat and virtual currency worlds. There is no need to maintain a public ledger, which would inhibit use on a global scale because of rapidly increasing memory and bandwidth requirements. The system is neither inflationary nor deflationary in nature and does not purport a new economic model. As a fiat-backed currency it should not suffer the volatility issues associated with Bit coin. In this paper the two most prominent electronic payment systems of the last forty years, namely Ecash and Bit coin, are examined. Net coin is then introduced in detail and is designed to address the shortcomings of these payment systems."
1661,"Reliability is a critical concern in the provision and placement of web services. A breakdown of service would seriously reduce customers' satisfaction, and thus harm the revenue of service providers. To maintain a high reliability, the common approach is deploying multiple service instances across different physical servers. This would inevitably raise another concern of energy consumption. Thus, greening web services also becomes an important issue. In this paper, we study the fundamental tradeoff between reliability and energy consumption, and propose an optimization framework that considers both factors. In specific, we build a continuous-time Markov model to analyze the steady-state reliability and mean time to failure (MTTF) from a service-oriented perspective, and obtain the minimum number of service instances to meet the given reliability requirement. Then, we show that deploying these instances in the server cluster to minimize energy consumption is NP-hard. To this end, we propose a heuristic algorithm to approximate the result. The analytical and experimental results show the effectiveness, and the approximation ratio is less than 1.25 for 90% of the data sets we use."
1662,"Dynamic Service composition is the art of composing a composite application from a set of service offerings from various providers at run time. Service composition is a complex task and the problem is with the construction of hierarchies of service providers where each service provider processes only a part of the application. This is further exacerbated by the fact that for every part of the application there are several potential service providers. Choosing the most appropriate service from functionally equivalent services is the issue being addressed here. In this paper, we focus on the waiting time of a service request as the criterion for selection. We present a technique borrowed and customized from the concept of friction in physics to select the best potential candidate service from a set of functionally equivalent ones, so that the waiting time for each service request is minimized. The technique not only facilitates the selection of the most appropriate service but also minimizes the waiting time of service requests. The model is validated through simulation and is shown to be able to equitably balance the load among similar service offering nodes."
1663,"This paper investigates the issues when several complementary geospatial Web services are chained to be a customized application process. This paper proposes a conceptual framework and a data structure, called service chain graph (SCG), to describe the geospatial Web service integration and the dynamic construction of service chaining process. A set of dynamic structural transformation rules is given to model the dynamic process of service chaining. Implementation issues are discussed and a prototype system is briefly presented for integration of several distributed, heterogeneous geospatial data services based on the model proposed by this paper."
1664,"The Intrusion Detection Systems (IDS) is becoming important and quite timing/space consuming due to the increasing volume of explosive data flood. During the past decades, there have been plenty of studies proposing software mechanisms to exploit the temporal locality in the IDS systems. However, it requires considerable memory blocks to store the redundancy table, therefore, the performance as well as the memory utilization is still worth pursuing. To tackle the above weakness, in this paper, we present xFilter, which explores the temporal locality to capture the redundancy, and propose a novel architecture to store and operate the redundancy table on FPGA. To demonstrate the performance of the xFilter structure, we designed a high efficient accelerator for Aho-Corasick (AC) algorithm used in Snort to detect the attack strings. To show the performance of xFilter, we implement a hardware prototype using Xilinx Zynq FPGA platform. Experimental results show that the xFilter accelerator can achieve 5.1x speedup against software implementation with insignificant hardware cost. Furthermore, the proposed hardware redundancy table mechanism can achieve 1.6x speedup against the traditional hardware accelerator."
1665,"PHP is well known as a programming language in the Web 2.0 era enabling agile server-side software development. It has officially supported SOAP messaging since version 5 through a C-based built-in library. In this paper we perform a thorough study of the capability of PHP as a Web service engine in both qualitative and quantitative aspects while comparing it with other Web service engines implemented in Java and C. We used Axis2 for this purpose as it is an open source web service engine whose implementation is available both in Java and C. We report that PHP as a web service engine performs competitively with Axis2 Java for Web services involving small payloads, and greatly outperforms it for larger payloads by 5-17 times. As the authors expected, Axis2 C performs best, but the experimental results demonstrate that PHP performance is closer to Axis2 C with larger payloads. This performance difference comes from the fact that the SOAP engine within the PHP runtime is implemented in C with a monolithic architecture, whereas Axis2 uses a more modular architecture for the flexible insertation of handlers for an assorted set of WS-* standards, and also that Axis2 uses a different data binding mechanism known as ADB (Axis2 Data binding). This paper is the first attempt to compare Web services engines implemented in PHP, Java and C, and the authors believe that this boosts the development of SOAP-based Web services in PHP by letting people know its decent performance score and high productivity characteristics."
1666,"This paper proposes a feature-oriented approach for Web services customization that can address three key challenges: reducing complexity, automated validation and dynamic deployment. We exploit techniques from Software Product Line (SPL) domain, particularly feature modeling techniques, to design our framework. Feature modeling allows to abstract variability in consumers' requirements to simplify customization processes, while automated analysis on feature models helps to validate customizations. The use of feature models at runtime enables dynamic deployment of a business process as a result of a service customization. We propose the use of feature models as service description artifacts to facilitate service customizations. The framework builds on a Model-Driven Development (MDD) approach to automate large parts of its operation."
1667,"Performing functionality testing in service-oriented architectures is not a trivial task. The difficulty is especially the large number of components that may be present in a SOA such as brokers, providers, service registries, clients, monitoring tools, data storage tools, etc. Thus, in order to facilitate the process of conducting functional testing and capacity planning in service-oriented systems, we present PEESOS. This first version is a functional prototype that offers facilities to assist researchers and industry to test their new applications, allowing collaborations that can be done between the participants to achieve an appropriate objective when developing a new application. The first results show that it is possible to make a planning environment easier to operate and to readily obtain results for performance evaluation of a target architecture. Since this is a first version of the prototype, it has interface and scalability limitations as well as needing improvements in performance of the logs repository and also in a core engine. We hope that such limitations can be corrected in the near future, including gathering information from the scientific community to make the prototype a useful and accessible tool. PEESOS is on-line and available at http://peesos.wsarch.lasdpc.icmc.usp.br."
1668,"In this paper, we propose an adaptive quality recommendation mechanism to help software service providers understand the dynamism of quality demand from the majority of requesters accurately. The unique feature of our approach is that based on the intra-cluster proximity index (called the icp-index) that we propose, the granularity of service clustering can be adjusted dynamically to meet the wide variation of service providers who want to target their services to different groups of clients. Experiments show that our approach is more accurate and flexible to identify the need of service quality of requesters than existing solutions such as simple averaging, minimum-maximum-mean, or traditional interval-range data clustering."
1669,"Service composition has received significant attention in the research community, and the focus has been on service semantics and composition algorithms. Surprisingly, the problem of representation of the composition outcome has been largely ignored. Ad-hoc workflows are often employed, which typically sacrifice alternative paths and parallelism for the sake of simple representation. In this paper, we show how theory of regions, which was originally developed to derive Petri nets from finite state automata, can be applied to find the optimal representation of composition. To apply the theory, we first propose an automaton-based composition framework that incorporates most existing composition techniques without changing the service semantics or its description language. Then based on the special requirements of the composition representation, we develop our own Petri net synthesis algorithm that combines the benefits of two well known algorithms from the theory of regions. We demonstrate that AND/OR workflow nets can limit the concurrency even for simple input/output based service composition, while our Petri net representation is optimal in terms of flexibility and parallelism. Our experimental evaluations include a case study on composing Google Checkout Service, and the study on Oracle BPEL samples, for which our algorithm obtains better concurrent representations for almost all non-trivial cases."
1670,"Enterprise digital dashboard (EDD) is an effective tool for executives to get a top level view of their enterprise as well as closely linked partners. Decision makers need to have easy access to data such as total sales per month, inventory status and a number of other key performance indicators (KPI). These business needs are not in sync with the technological challenges such as the presence of disparate enterprise systems (e.g. ERP, SCM, CRM, etc.). Web service is an effective technology to link disparate systems. This paper discusses a solution based on Web services in the context of EDD. We explain the solution with respect to a business scenario and propose a reference architecture for the same."
1671,"The service composition execution engine- WebJetFlow with dual feedback control loops, which separates process executor and service proxy to decouple process execution and service invocation, is designed and implemented. The experiment results show that the feedback control mechanism enables WebJetFlow to provide response time guarantee for differentiated processes even when workload varies significantly, and the decoupling method increases the throughput of WebJetFlow."
1673,"In this paper, we introduce a theoretical framework for two-way Web service (WS) interaction and interface design based on the concepts of operation reversal and XML type generalization. Under this theoretical framework, two generic types of interface solutions in two-way WS interaction, i.e. tightly coupled (TC) interface and loosely coupled (LC) interface, are formally defined and studied. The proposed theoretical framework is used to design, derive and verify WS interface for full duplex two-way WS interaction. An algorithm of interface verification is described that allows effective WS interface design and verification to support asynchronous two-way WS operation and event notification. The interface verification algorithm in our approach utilizes verification by derivation and verification by content to validate the client interface in two-way WS interaction. Use case studies are performed for both TC and LC interface solutions. A generic infrastructure based on two-way Web service application proxy (2SAP) is described and implemented. The proposed approach is applied to applications of realizing ECMA-348 for WS enablement of telecommunication."
1675,"This paper shows the effectiveness of dynamic over of service components for ubiquitous service composition. Although context-aware services are expected in the forthcoming ubiquitous computing era, conventional service coordination techniques are insufficient because of rigid interface designing. To cope with this problem, we have established a flexible service composition framework, where a semantic-level service scenario is translated and its components are dynamically found, selected, and bound. User situations change with time, so service components should be changed in accordance with context change. We propose a new method that reselects and rebinds service components based on context change. Through a performance measurement, we show the effectiveness and practicality of the new method."
1676,"We report on a novel approach to (semi-)automatically compile and verify contract-regulated service compositions. We specify Web services and the contracts governing them as WSBPEL behaviours. We compile WSBPEL behaviours into the specialised system description language ISPL, to be used with the model checker MCMAS to verify behaviours automatically. We use the formalism of temporal-epistemic logic suitably extended to deal with compliance/violations of contracts. We illustrate these concepts using a motivating example whose state space is approximately 10
<sup>6</sup>
 and discuss experimental results."
1677,"Automation of Web service composition is one of the most interesting challenges facing the Service Oriented Computing today. From this challenge, many issues such as control flow, data flow, verification, execution monitoring, or recovery actions (e.g., compensation) follows. In this paper we focus on automated data flow in Web service composition. The semantic Web, as an evolving extension of the current Web, seems a key initiative to overcome the latter issue. However, even if some approaches focus on discovering potential semantic connections between Web services, few or none of these tackle implementations issues related to XML messages management at syntactic level. In this direction we present an approach for performing automated data flow in Web service composition by i) exploiting semantic matchmaking between Web service parameters (i.e., outputs and inputs) to enable their connection and interaction, and ii) adapting XML database solutions, specifically XML Schema mapping, to perform syntactic data transformation and integration of exchanged messages. Our system is implemented and interacting with Web services dedicated on a Telecom scenario. The preliminary evaluation results showed not only high efficiency and effectiveness of the proposed approach but also complementarity of the semantic matchmaking and syntactic mapping to achieving data flow in Web service composition."
1678,"In order for e-government to be successful there is a strong requirement for approaches that are based on widely accepted technical standards and formal design methods. In this paper, we describe the architecture of the eMayor platform, a Web services based platform that is built as a holistic service framework for the deployment and delivery of e-government enterprise services for European municipalities. The design of the platform is based on the ISO/RM-ODP standard. The goal of the paper is to present both a general architectural overview of the platform and its services, along with the engineering and technology aspects of the architecture as instances of the engineering and technology viewpoints of the RM-ODP standard."
1679,We present an approach to the inference of automata models of Web-based business applications using only execution traces recording the externally observable behavior of such applications. The proposed approach yields behavioral models representing both the control flow of an application and the data variations corresponding to different types of users. We also describe how the obtained models allow the use of verification techniques like model checking in the validation phase using a case study featuring a travel reservation agency.
1680,"As various types of Internets of Things (IoT) are deployed in a wide range of areas, the need arises to utilize various IoT resources dynamically to accomplish user tasks. We call this environment an urban-scale IoT environment, where various IoT resources that are necessary to accomplish user tasks are directly connected to each other via users' mobile devices, such as their smart phones. IoT resources are utilized as resources with which to run a composite service that supports user tasks. In this urban-scale IoT environment, it is essential to create efficient binding between a service and an IoT resource so as to execute a composite service for a task successfully. In this paper, we propose a service resource allocation approach which minimizes data transmissions between users' mobile devices and which effectively deal with the constraints of these types of environments. We transformed the resource allocation problem into a variant of the degree-constrained minimum spanning tree problem and applied a genetic algorithm to reduce the time needed to produce a near-optimal solution. We also defined a fitness function and an encoding scheme to apply the genetic algorithm in an efficient manner. The proposed approach shows a 97% success rate on average when used to find near-optimal solutions. In addition, it takes significantly less time than the brute force approach."
1681,"With fast paced growth of digital data, keyword based search has become a critical enterprise application. Research has shown that nearly 85% of enterprise data lies in flat filesystems [10] that allow multiple users with different access privileges. Any search tool for such systems needs to be efficient and yet cognizant of access control semantics imposed by the underlying filesystem. Current enterprise search techniques use two disjoint search and access- control components by creating a single system-wide index and filtering search results for access control. This approach is ineffective as index and query statistics subtly leak private information. The other approach of using separate indices for each user is undesirable as it not only increases disk consumption due to shared files, but also increases overheads of updating indices whenever a file changes. We propose a distributed approach that couples search and access-control into a unified framework and pro vides secure multiuser search. Our scheme (logically) divides data into independent access-privileges based chunks, called access-control barrels (ACB). ACBs not only manage security but also improve overall efficiency as they can be indexed and searched in parallel by distributing them to multiple enterprise machines. We describe the architecture of ACBs based search and propose an optimization that ensures the scalability of our approach. We validate our design with a detailed evaluation using industry benchmarks and datasets. Our initial experiments show secure search with 38% improved indexing efficiency and low overheads for ACB processing."
1682,"Next-generation government information systems will integrate large amounts of heterogeneous data sources located on distributed networks like the Internet. We present Net Traveler which is a framework for Web services collaboration, orchestration and choreography in peer-to-peer autonomic environments. The main feature of our new approach is the elimination of a central coordination site running the queries and the autonomic query execution. Moreover, control information is embedded with the request for data, and also with the partial results in an XML document. This XML document indicates the next service to be requested, the target site (if known) and how should it process any partial results. We present the architecture, the ideas behind it, a prototype, and a performance study to validate this framework. This study shows this framework to be flexible, scalable and efficient for the Web services collaboration in electronic government applications."
1683,"Web services allow organizations to capture their human and software-based capabilities as modular software components that are called remotely over a network. In such service-oriented settings, it is important to establish an agreement that sets the obligations of the service provider and the expectations of the service consumer. Since traditional approaches such as Service Level Agreements (SLAs) are loosely defined with respect to data integrity aspects, in previous work, we proposed a formal model for specifying data-centric Web services. The goal is to formally and unambiguously specify the service behavior in terms of its underlying data model and data interactions. In this paper, we present our model and our effort in specifying and verifying data-centric Web services using three state-of-the-art specification languages: JML, Dafny and RESOLVE. Our goal is to study the feasibility of our proposed model and also to pinpoint the challenges and limitations of current specification and verification tools."
1684,"We propose an integrated framework that extracts semantics from WSDL descriptions and constructs operation-level ontologies for Web services. The semantics mainly focus on the functional features of Web services, which facilitates the efficient usage of Web services, such as service discovery and service composition. We use service operations as the first class objects to define service functionalities. We first create service ontologies by measuring the relevance between service operations and clustering operations into functionally relevant groups. We then construct the structure of the service ontologies through a hierarchical clustering algorithm."
1685,"In the modern service industry, both service processes and data structures are becoming increasingly diverse and complex. In addition, interdependences exist among data, such that the use of ""shoe size"" data must be based on the ""type of goods"" data returning ""shoe"". This is also observed for the functions and interfaces in a system, as one can use the function ""order payment"" only after the function ""order generation"". This kind of phenomenon is rather common in service systems nowadays, especially when the service is a transboundary service such as the new retail proposed by Jack Ma. Traditional modeling methods have difficulties in handling such scenarios. There have been studies on service modeling over the past several years, and they have focused mainly on the service processes and interactions among services. In this work, we construct MeCo-TSM based on three sub-models to handle multi-entity complex service process. We verify our model in the real processes of our cooperation company and compare it with related works. MeCo-TSM supports the service better in our cases and shows satisfactory efficiency, effectiveness and reusability."
1686,"When two Web services work together, they exchange messages in a predefined interface process. Two interface processes should be compatible when they can work properly. Our idea to fix incompatibility problem in service processes is to change an incompatible process so that the new process can simulate a compatible process. We consider not only the control flow but also the data flow in modeling the processes into FSMs. We present a technique that not only detects the incompatibility, but also provides resolution strategies to generate the new process."
1687,"The many conflicting technical, organizational, legal and domain-level constraints make the implementation of secure, inter-organizational workflows a very complex task, which is bound to low-level technical knowledge and error prone. The SECTINO project provides a framework for the realization and the high-level management of security-critical workflows based on the paradigm of model driven security. In our case the models are translated into runtime artefacts that configure a target reference architecture based on Web services technologies. In this paper we focus on the global workflow model, which captures the message exchange protocol between partners cooperating in a distributed environments well as basic security patterns. We show how the model maps to workflow and security components of the hosting environments at the partner nodes."
1688,"Existing approaches to recommend services using natural language queries are supervised or unsupervised. Supervised approaches rely on a dataset with natural language queries annotated with categorizing labels. As the annotation process is manual and requires deep domain knowledge, these approaches are not readily applicable on new datasets. On the other hand, unsupervised approaches overcome the limitation. To date, unsupervised approaches are primarily based on matching keywords, entity relationships, topics and clusters. Keywords and entity relationships ignore the semantic similarity between a query and services. Topics and clusters capture the semantic similarity, but rely on mashups that explicitly capture relationships between services. Again, for new services, the information are not readily available. We propose NL2API, a framework that relies solely on service descriptions for recommending services. NL2API has the benefit of being immediately applicable as a bootstrap recommender for new datasets. To capture relationships among services, NL2API provides different approaches to construct communities where a community represents an abstraction over a group of services. Based on the communities and users' queries, NL2API applies a query matching approach to recommend top-k services. We evaluate NL2API on datasets collected from Programmable Web and API Harmony. Our evaluation shows that for sizable datasets such as Programmable Web NL2API outperforms baseline approaches."
1689,"Service process usually needs to be changed during reuse because of the change of context. These change mapping relations provide additional information to help users customize service processes. In this paper, we present a mining approach to mining process change sequences based on different context and finding the best sequence to tailor the base process."
1690,"Web services are developed independently and deployed in a distributed environment, new service can be obtained by composing existing ones. The rapid introduction of new services also results in undesirable interactions between services. These conflicts are not mismatches of interfaces, but are usually based on the data in the executing instance and therefore runtime management of conflicts in Web services should be considered. We study the problem from the perspective of user's revenue, and propose an online approach to resolve conflicts is proposed."
1691,"The increasing ability for the Earth sciences to sense the world around us is resulting in a growing need for data-driven applications that are under the control of data-centric workflows composed of grid- and Web-services. The focus of our work is on provenance collection/or these workflows, necessary to validate the workflow and to determine quality of generated data products. The challenge we address is to record uniform and usable provenance metadata that meets the domain needs while minimizing the modification burden on the service authors and the performance overhead on the workflow engine and the services. The framework, based on a loosely-coupled publish-subscribe architecture for propagating provenance activities, satisfies the needs of detailed provenance collection while a performance evaluation of a prototype finds a minimal performance overhead (in the range of 1% for an eight service workflow using 271 data products)"
1692,"As the abundance of Web services on the World Wide Web increase, designing effective approaches for Web service selection and recommendation has become more and more important. In this paper, we present WSRec, a Web service recommender system, to attack this crucial problem. WSRec includes a user-contribution mechanism for Web service QoS information collection and an effective and novel hybrid collaborative filtering algorithm for Web service QoS value prediction. WSRec is implemented by Java language and deployed to the real-world environment. To study the prediction performance, a total of 21,197 public Web services are obtained from the Internet and a large-scale real-world experiment is conducted, where more than 1.5 millions test results are collected from 150 service users in different countries on 100 publicly available Web services located all over the world. The comprehensive experimental analysis shows that WSRec achieves better prediction accuracy than other approaches."
1693,"We argue that essential facets of Web services, and especially those useful to understand their interaction, can be described using process-algebraic notations. Web service description and execution languages such as BPEL are essentially process description languages; they are based on primitives for behaviour description and message exchange which can also be found in more abstract process algebras. One legitimate question is therefore whether the formal approach and the sophisticated tools introduced for process algebra can be used to improve the effectiveness and the reliability of Web service development. Our investigations suggest a positive answer, and we claim that process algebras provide a very complete and satisfactory assistance to the whole process of Web service development. We show on a case study that readily available tools based on process algebra are effective at verifying that Web services conform to their requirements and respect properties. We advocate their use both at the design stage and for reverse engineering issues. More prospectively, we discuss how they can be helpful to tackle choreography issues."
1694,"Web services are conveniently advertised and published based on (stateless) functional descriptions, while they are usually realized as (stateful) processes. Therefore, the automated enactment of complex Web services on the basis of pre-existing ones requires the ability to handle services described at very different abstraction levels. This is the main reason behind the current lack of approaches capable to perform automated end-to-end composition, starting from semantic requirements to obtain executable orchestrations of stateful processes. In this paper we achieve such a challenging goal, by modularly integrating a range of incrementally more complex techniques that cover the necessary discovery and composition phases. By gradually bridging the gap between the high-level requirements and the concrete realization of services, our architecture manages sensibly the complexity of the problem: incrementally more complex techniques are provided with incrementally more focused input. The tests of our architecture on a deployed scenario witness the functionality of the platform and its integrability with standard service engines."
1695,"IoT service and service composition provide an effective means to develop IoT applications based on correlating multiple sensor data. The change of specific sensor data can cause others' changes under uncertain situations. It makes difficult for defining service composition plan in advance to build IoT application. This paper proposes a data-driven service composition method based on our previous proactive data service model. We regard service events frequently happen together with given service event as its situation, and the service events happen next as reacted actions under the situation. We analyze two kinds of correlation among service events via an improved FP-tree algorithm, and realize the service composition at runtime based on the real-time service events. Based on the real sensor data set in a coal-fired power plant, a series of experiments demonstrate that our method can effectively detect new service events based on current service events."
1696,"To guarantee privacy in service oriented environments, it is essential to check for compatibility between a client's privacy requirements and a Web service privacy policies before invoking the Web service operation. In this paper, we focus on privacy at the Web service operation level. We present an approach that integrates k-Anonymity into a privacy management framework using Web Services Conversation Language (WSCL) definitions. In particular, we use the notion of k-Anonymity to determine the extent to which the invocation of an operation can be inferred if one knows that a downstream operation was invoked. We provide both a formal definition as well as an implementation of the proposed approach."
1697,"This paper presents a model which combines the processing power of parallel computation with the ease of Web service usage. In this model, parallel programming environment can be embedded in a visual environment. Parallelization of Web services is provided by using multithreading technology with dataset parameters. This work also provides parallel usage of computers located in different places via a wide area network such as Internet."
1699,"The rapid growth in the number and diversity of Mashup services, coupled with the myriad of functionally similar Mashup services, makes it difficult to find suitable Mashup services to develop Mashup-based software applications due to an unprecedentedly large number of choices of Mashup services. Even if the existing latent factor based methods show significant improvements in Mashup service clustering and discovery, it is still challenging to find Mashup services with high accuracy due to overlooking of relationships among Mashup services. The relationships among Mashup services actually can be exploited in mining latent functional factors to improve the accuracy of clustering and discovery. In this paper, we propose a Mashup service clustering method based on an integration of service content and network via exploiting a two-level topic model. This method, firstly designs a two-level topic model to mine latent topics for representing functional features of Mashup services. Secondly, it uses two different random walk processes to derive and incorporate the topic distribution of Mashup services at service network level into the topic distribution of Mashup services at the service content level. Thirdly, K-means and Agnes algorithm are used to perform Mashup service clustering based on latent topics' similarity. Finally, we conduct a comprehensive evaluation to measure performance of our method. Compared with other existing clustering approaches, experimental results show that our approach achieves a significant improvement in terms of precision, recall, purity and entropy."
1700,"Nowadays web users have clearly expressed their wishes to receive and interact with personalized services directly. However, existing approaches, largely syntactic content-based, fail to provide robust, accurate and useful personalized services to its users. Towards such an issue, the semantic web provides technologies to annotate and match services' descriptions with users' features, interests and preferences, thus allowing for more efficient access to services and more generally information. The aim of our work, part of service personalization, is on automated instantiation of services which is crucial for advanced usability i.e., how to prepare and present services ready to be executed while limiting useless interactions with users? To this end, we exploit Description Logics reasoning through semantic matching to (i) identify useful parts of a user profile that satisfy services requirements (i.e., input parameters) and (ii) compute the description required by a service to be executed but not provided by the user profile. Our approach, part of the EC-funded project SOA4All, was evaluated on its applicability in real world scenarios with end-users."
1701,"Web services have been emerging as a promising technology for business integration. Transactional support to integrated businesses via composing individual Web services is a critical issue. Current Web services protocols (e.g. BPEL4WS) have been proposed to deal with this issue on a strong assumption that each Web service is compensatable for a recovery purpose. It is arguable that Web services composition requires more transactional support beyond the compensation-based solution. This paper looks into the problem of transactional support for composing and scheduling those Web services that may have different transactional properties. The transactional properties of workflow constructs, which are fundamental to the composition of Web services, are thoroughly investigated. The concept of a connection point is introduced to derive the transactional properties of composite Web services. The scheduling issue of composite Web services is also discussed."
1702,"In this paper, we present the approach and architecture of TARGET: two-way Web service router gateway, for two-way Web service interaction crossing enterprise domain and firewall. It provides a full support for asynchronous outbound operation and event notification in communication services. TARGET addresses an acute issue for Internet applications that today's enterprise NATs and firewalls only allow outbound HTTP request from the inside to the outside and block any request from the outside to the inside, which is a serious problem for two-way Web services. TARGET is a generic solution to allow two-way Web service interaction to traverse legitimately through NATs and strictly configured firewalls; and it is based on two-way SOAP message tunneling, service local registry, and service routing to bridge two-way Web service interaction. A research TARGET system has been implemented and applied to real time communication services, e.g. conferencing. Extensive experiments on TARGET are performed, and its performance with various sizes of SOAP messages is studied. The applicability and feasibility of TARGET for two-way Web service interaction is verified"
1703,"Service oriented architecture (SOA) is currently supported by an infrastructure that facilitates model driven development, but reuse issues have received less attention. A key problem in reuse is variability management: how to support the specification and usage of the variation points of the system. In this paper, we propose an approach for model-level variability management, assuming UML activity diagrams as the notation for service composition models in SOA. The approach is based on specifying variation points using so-called specialization patterns, which have been originally developed to support task-driven specialization of application frameworks. We demonstrate the approach with a prototype tool and an example. The approach leads to a guided model customization process focusing on the variation points, making it easy to produce a variant of a basic model."
1704,"Quality of Service (QoS) has become a standard way of evaluating web services and selecting the one that suites user interests the best. Traditional methods adopt a fixed set of QoS parameters and typical ones include response time, fee, and availability. There currently lacks an effective way of identifying quality features that users are actually interested in when choosing a service. Meanwhile, the traditional way of collecting QoS values relies on either public information released by service providers or test results from repeatedly invoking a service. Therefore, the values can be heavily affected by authenticity of the provider offered information or the quality/configuration of the test code/environment. As a result, existing QoS evaluation methods are not applicable to subject features, such as usability and affordability, where the values depend on user personal judgement. In this paper, we propose a novel approach to extracting domain-related QoS features, ranking those features based on their interestingness, evaluating the value of these features through sentiment analysis on user reviews. More specifically, we leverage natural language processing techniques and machine learning approaches to identify top QoS features that users are interested in and simultaneously learn their sentiment orientation towards those features. We model the problem as sentiment classification, where relevant terms in a review are modeled as features that determine whether a review is positive or negative. Logistic regression is used so that the impact of these terms are learned simultaneously when the classifier is learned through a supervised learning process. The nontrivial terms are selected as the candidate QoS featured. A comprehensive experiment has been conducted on a real-world dataset and the result demonstrates the effectiveness of our approach."
1705,"From the analysis of some hard drawbacks faced with system service and composite service management today, a novel multi-agent-assisted and WS-management-based composite service management scheme: MWS-MCS is introduced in this paper. Firstly, we propose model architecture of this scheme. The prototype system had proved the feasibility of this design scheme. At last, we conclude this paper and analyze the prospective research challenges."
1706,"Policies provide flexible grammar to describe the capabilities, requirements and general characteristics. In loosely-coupled Web services domain, policy has been considered as a possible mechanism to describe the characteristics of services, associate and evolve with these services dynamically. However there lacks a systematic framework to take into account the employment of policies in Web services as a whole. In this paper, based on the lifecycle of policies, a general policy framework for Web services is proposed, including policy generation, enforcement, publication, negotiation and evolution. In different modules of the framework, a set of mechanisms and tools are discussed and presented. At the end of the paper, a policy enabled telecom open service delivery platform is presented. Telecom operators open their network capabilities through deploying their services as Web services on the platform, following Parlay X Web services specification. Third-party value added service providers use WSDL and policies to access services. Policy framework, including security policy, reliable messaging policy and other QoS policy management has proved its efficiency in the real world system."
1707,"It is challenging to conduct quality of experience (QoE) evaluations of web-based streaming video services effectively and efficiently. Aiming to overcome this challenge, we have created QoECenter, a web-based visual platform that innovatively facilitates comprehensive QoE evaluations of the streaming video services. QoECenter offers a holistic approach to conducting the QoE evaluations via an integrated set of technologies for source video classification, QoS realization of video encoding and network transmission, and context-aware user experience data gathering and analysis. From a QoECenter consumer's viewpoint, three kinds of data are required for an end-to-end streaming video QoE evaluation: video source level data, system process level data, and end user level data. QoECenter provides visual interfaces for parameter setting and data acquisition for each data level, and supports both objective and subjective datadriven QoE analyses. A QoECenter consumer can easily conduct comparative QoE evaluations like running easy-to-use visual applications. The effectiveness and efficiency design objectives of QoECenter have been validated by various real experiments."
1709,"Dynamic system adaptation to continuously changing business requirements and environment conditions has been a common demand for IT over the last years. In spite of constant innovation in service computing, current technology is not able to keep up with business ever growing need faster time to market. The Service Refinement Cycle (SRC) has been proposed for providing flexibility and power for creating new services by composing existing services with integrated policies. We developed and deployed a Service Execution Environment (SEE) for the SRC using existing technology, such as Web Services, XACML and WS-BPEL. As a proof of concept prototype, we also developed an Online Radio Service that makes extensive use of the key SRC features and is executed by our SEE."
1710,"We identify parameters impacting Web services dependability, describe the methods of dependability enhancement by redundancy in space and redundancy in time, and perform a series of experiments to evaluate the availability of Web services. To increase the availability of Web services, we employ several replication schemes and compare them with a single service. The Web services are coordinated by a replication manager. It provides a round robin algorithm for scheduling the workload of the Web services and keeps updating the availability of each Web service. The replication algorithm and the detailed system configuration are described. Experiments are performed to evaluate the resulting service availability. Modeling on the Web services with Petri-net is constructed and verified through experiments with different applications. With the parameters obtained from the experiments, the proposed model can be engaged to demonstrate the characteristics of the Web service."
1711,"Web service composition is a process to compose homogenous or heterogeneous services together in order to create value-added services. Many non-functional features including QoS and user preferences have been adopted to guide such a process. However, two issues are observed: (1) the expressiveness of user preference is subject to quantitative preferences without proper use of qualitative preferences, (2) a highly preferred composite service may not be trustworthy, or a highly trustworthy composite service may not be preferable. To address these issues, we combine both qualitative and quantitative preferences as well as service trust together in the process of service composition. We aim to obtain optimal web service compositions that can satisfy these (potentially conflicting) constraints as much as possible. Experimental results demonstrate the efficiency and effectiveness of our approach in comparison with other counterparts."
1712,"This paper describes the research conducted to develop Nedgty, the open source Web services firewall. Nedgty secures Web services by applying business specific rules in a centralized manner. It has the ability to secure Web services against denial of service, buffer overflow, and XML denial of service attacks; as well as having an authorization mechanism."
1713,"Semantic Annotation plays an essential role in automatic service discovery and composition. However, existing approaches and tools cannot achieve high annotation quality to ensure the semantic service application. Meanwhile, the semi-automatic strategies for improving the annotation quality are time-consuming. To further improve the efficiency as well as the quality of the annotation, this paper presents an effective method involving human-computer interaction to further optimize the annotation procedure. Besides employing the feedback and propagation strategy to semi-automatically improve the annotation quality, the strategy to involve the manual annotation is developed when the efficiency of semi-automatically strategy is related low. To optimize the manual annotation procedure, a clustering based approach is presented to select the most impacted candidates to optimize the annotation improvement. In addition, to help the annotators to choose the correct annotation, the local ontology restriction based method is further designed to improve the recommendation performance. The experiments show that our approach effectively involving the human intervention can significantly improve the annotation quality, faster the quality improvement procedure and reduce the manual load by increasing the recommendation accuracy."
1714,"Quality of service (QoS) is an important consideration in the dynamic service selection in the context of service oriented architectures. This paper extends previous work on QoS brokering for SOAs by designing, implementing, and experimentally evaluating a service selection QoS broker that maximizes a utility function for service consumers. Utility functions allow stakeholders to ascribe a value to the usefulness of a system as a function of several attributes such as response time, throughput, and availability. This work assumes that consumers of services provide to a QoS broker their utility functions and their cost constraints on the requested services. Service providers register with the broker by providing service demands for each of the resources used by the services provided and cost functions for each of the services. Consumers request services from the QoS broker, which selects a service provider that maximizes the consumer's utility function subject to its cost constraint. The QoS broker uses analytic queuing models to predict the QoS values of the various services that could be selected under varying workload conditions. The broker and services were implemented using a J2EE/Weblogic platform and experiments were conducted to evaluate the broker's efficacy. Results showed that the broker adequately adapts its selection of service providers according to cost constraints."
1715,"Mashup editors, like Yahoo Pipes and IBM Lotus Mashup Maker, allow non-programmer end-users to ldquomash-uprdquo information sources and services to meet their information needs. However, with the increasing number of services, information sources and complex operations like filtering and joining, even an easy to use editor is not sufficient. MashupAdvisor aims to assist mashup creators to build higher quality mashups in less time. Based on the current state of a mashup, the MashupAdvisor quietly suggests outputs (goals) that the user might want to include in the final mashup. MashupAdvisor exploits a repository of mashups to estimate the popularity of specific outputs, and makes suggestions using the conditional probability that an output will be included, given the current state of the mashup. When a suggestion is accepted, MashupAdvisor uses a semantic matching algorithm and a metric planner to modify the mashup to produce the suggested output. Our prototype was implemented on top of IBM Lotus MashupMaker and our initial results show that it is effective."
1716,"With the advances in smartphones users install abundant apps to facilitate their daily lives. Both users and related developers have increasing requirements to understand the mobile App usage pattern, for individual and commercial use. Respectively, personalized App recommendation methods and systems have emerged as a novel attractive topic that can demonstrate the human App usage behavior. The mobile Apps recommendation can serve as a cornerstone for a variety of intelligent services, such as fast-launching UIs, intelligent user-phone interactions, and battery management of cellphones. In this paper, we develop a novel App recommendation framework combining the historical App usage data with the sequence of recently-used Apps. Specifically, our framework is an extension of the user-based collaborative filtering technique, where the set of nearest neighbors is employed for training the prediction model. However, our prediction scheme is constructed on the temporal sequential data and is modeled by using the chain-augmented Naive Bayes model. Experiments with a real mobile Internet record dataset demonstrate that the accuracy of our framework outperforms several baseline App recommendation approaches."
1717,"Services computing paradigm together with Web services have significantly promoted the automation of business process in enterprise. Prevalent service composition technologies, such as WS-BPEL and WSCI, provide promising means to deal with machine-to-machine communication. Traditionally, in the phase of business process modeling, there usually require some human-involved tasks. Recent new technologies such as BPEL4People and Human Task begin to consider involving human interaction in business process. However, such approaches still have some limitations. On one hand, they exactly require some extensions of current BPEL standards. As a result, the existing business processes have to be rewritten and redeployed. On the other hand, they yet lack of the development and deployment supports of flexible and reusable user interfaces in business process. In this paper, we address these issues by enabling human interaction in business process with rich web applications. Our approach models human tasks as services, and can be seamlessly integrated to current BPEL without any modifications to existing engine and processes. We further support building human task presentations from service-oriented rich user interfaces. During the process execution, the corresponding task stakeholders can select, configure and compose these reusable and rich UI components according to their own application context."
1718,"Discovering and assembling individual Web services into more complex yet new and more useful Web processes has received significant attention from academia recently. In this paper, we explore using pre and post-conditions of Web services to enable their automatic composition. Also, we present a novel technique for discovering semantic relations between pre and post-conditions of different services using their ontological descriptions. This enables determining services with complementary functions and generating a semantic Web of services. Our technique takes semantic similarity of pre and post-conditions into account and builds on our earlier work on discovering semantic relationships between interfaces (input and output) of Web services. A comprehensive classification of existing composition techniques is also included"
1720,"Web service paradigm and related technologies have provided favorable means for the realization of collaborative business processes. From both conceptual and implementation points of view, the business processes are based on a centralized management approach. Nevertheless, it is very well known that the enterprise-wide process management where processes may span multiple organizational units requires particular considerations on scalability, heterogeneity, availability and privacy issues, that in turn, require particular consideration on decentralization. In this paper, our aim is to reconcile the decentralization of processes as a step towards the enterprise-wide solutions. We propose a methodology for transforming a centralized process specification into a form that is amenable to a distributed execution and to incorporate the necessary synchronization between different processing entities. The proposed technique has the advantage of being flexible that it computes the abstract constructs and provides a generalized approach to the decentralization of processes."
1721,"In this demo paper, we present a new data service composition sequence generation approach to solve the ad-hoc data query problem in EDMIS. Our approach allows end users to input some keywords, and then the data services related are found and the Top-K data services composition sequences are generated as output."
1722,"The Web services and service-oriented architectures (SOA) vision by Helland, P. (2005) is about building large-scale distributed applications by composing coarse-grained autonomous services in a flexible architecture that can adapt to changing business requirements. These services interact by exchanging one-way messages through standardized message processing and transport protocols. This vision is being driven by economic imperatives for integration and automation across administrative and organizational boundaries. This paper presents a concise yet expressive model for service contracts to describe messaging behavior. The idea is simple: we use Boolean conditions to specify when a message can be sent and received, where the conditions refer only to other messages in the service contract - that is, conditions only refer to a service's externalized messaging state and not to internal state"
1724,"When Web services play more important roles in software development, the corresponding software dedicated for Web services testing has attracted more attention. This paper proposes a new mutation testing method based on the requirement model presented by OWL-S, which not only improves the test efficiency, but also makes the test work undergoing automatically. How to define the mutant operators, generate mutants and test cases automatically according to the requirement model are discussed in detail by the author."
1725,"With the increasing number of Web services available on the Internet, how to recommend Web services to interested users effectively and efficiently remains to be a big challenge. At present, collaborative filtering (CF) is the most widely used technique in the design of recommender systems to handle information overload. For Web services, however, it is difficult for user to collect personalized QoS (Quality of Service)data and other explicit feedbacks such as ratings. In most cases, only a part of the implicit feedbacks (e.g., watchlist) is available in service registry. In this paper, we leverage implicit feedback from user's watchlist to build a CF-based recommender system for Web service. Our main contribution is to transform implicit feedbacks into explicit ratings to improve the accuracy of service recommendation. More specifically, we first construct binary user-service rating matrix according to the implicit feedback from the watchlist. Then, temporal and tag preference are combined into the original rating matrix to generate a more accurate pseudo rating matrix, which can reflect users' different preference on services in their own watchlists. Finally, we use traditional user-based CF method to produce a personalized service recommendation list with corresponding pseudo ratings. Moreover, the empirical experiments based on ProgrammableWeb show that compared with traditional log-based CF method, the recommender system with temporal and tag preference is more accurate and precise."
1726,"Axis2, the next generation of Apache Web services middleware, is an effort to re-architecture Apache Web service stack to incorporate the changes in Web services. Among many improvements, Axis2 provides first class messaging and SOAP extension supports together with a novel lightweight streaming based XML processing model. The architecture is build on top of a simple and extensible core that provides the basic abstractions for the rest of the system. We present the design and the thought process behind the key abstractions by breaking down the architecture into three topics, XML processing model, extensible SOAP processing model and messaging framework. This paper explains the overall architecture while concentrating on the three topics, and demonstrate how they all fit together to yield Axis2"
1727,"In this paper, we address the optimization problem of SLA-constrained service composition. Focusing on the main drawbacks of traditional approaches surveyed:1) the difficulties in preference definition and weight assignment, 2) the limitation of linear utility function for identifying preferred skyline solutions, and 3) the poor efficiency and scalability of algorithms, we present a systematic approach of combining the weighted Tchebycheff distance with skyline computation to cope with this optimization problem. More specifically, we first propose a fuzzy linguistic preference model that can help service composer elicit, represent and establish consistent preference relations upon QoS dimensions. Then we present a weighting procedure to transform the preference relations into numeric weights that are used in the Tchebycheff distance as quantified measurement of preference for skyline solutions. Finally we propose a hybrid evolutionary algorithm to heuristically find preferred skyline solutions in an efficient way. The algorithm is further evaluated by a set of experimental studies."
1728,"Application Programming Interfaces (APIs), which are emerging web services in general, are increasing with a rapid speed in recent years. With so many APIs, many management platforms have been developed and deployed, leading to the boom of API markets, that are similar to the mobile App markets. Meanwhile, it has become more and more difficult to select and manage APIs. In reality, most existing management platforms typically recommend currently popular APIs to developers. However, the fact that popularity of API varies over time is ignored in those platforms, leading to the difficulty of recommending APIs that are just released but may be popular in the near future. To tackle this challenge, an approach of predicting the popularity of APIs is proposed in this paper. Predicting the popularity of API can not only be used for API ranking, recommendation and selection, but also make it more convenient for API providers and consumers to manage or select API respectively. In this paper, we propose a time-aware linear model to predict the API popularity, using time series feature of APIs and API's self-features such as its' provider ranking and description features, which are called heterogeneous features in our paper. Comprehensive experiments have been conducted on a real-world Programmable Web dataset with 613 real APIs. The experimental results show that our model has a better performance, when compared with some other state-of-the-art prediction models."
1729,"Modern information systems may exploit numerous web services for communication. Each web service may exploit its own XML format for data representation which causes problems with their integration and evolution. Manual integration and management of evolution of the XML formats may be very hard. In this paper, we present a novel method which exploits a conceptual diagram. We introduce an algorithm which helps a domain expert to map the XML formats to the conceptual diagram. It measures similarities between the XML formats and the diagram and adjusts them on the base of the input from the expert. The result is a precise mapping. The diagram then integrates the XML formats and facilitates their evolution - a change can be made only once in the diagram and propagated to the XML formats."
1730,"Whilst the vision of a robust service-oriented architecture (SOA) is very seductive it engenders many technical challenges. The main challenge is the development and support of runtime cross-standard service activation and interoperation. Such interoperation will provide a vital stepping-stone towards the integration of the emerging SOA standards and legacy services - those developed using existing middleware architectures such as DCOM, CORBA, J2EE, Web service, JXTA, and Jini. Much related works already exist including WSIF framework, which provides APIs to support design-time invocation of cross-standard Web services deployed on multiple SOAP packages. However, this paper focuses on a runtime self-regenerative mechanism related to adaptive service invocation code. The paper will present a runtime service adaptation mechanism, which supports end-users to dynamically adapt to variations in the execution environment, without altering their original design, crossing multiple standards (both synchronous and asynchronous invocation models) and middleware architectures. This paper also introduces a proposed ""Polyarchical Middleware"" architecture to support such self-regenerative service adaptation. An illustrative example will be used to describe the approach and the current implementation. The paper will conclude with general remarks and mention of further work."
1731,"Web-based API is a new trend for publishing services. To correctly use the API, developers should follow certain service specifications. Data contract is a service specification to express the constraints over the data model used in the APIs. Data contracts, however are not always readily available in a formalized format if not undocumented at all. In this paper, we present an approach to infer formal data contracts for Web-based API. The approach integrates information of the parameters, error messages and testing result of Web-based API. We demonstrate how this approach infers complicated data preconditions for Web-based API in the real-world Web API platforms."
1732,"Software metrics are vital for the management of software development, especially when a new technology is being adopted and established practices have yet to emerge. As a kind of software components, Web service technology has flourished and attracted a flurry of research activities. Despite the vast amount of research on mechanisms of Web services, there have been little investigations of the overall nature of existing Web services from a software component point of view. This paper is the first attempt to compare Web services with other software components in terms of established metrics in software engineering, including object oriented metrics and interface metrics. In this study we conclude that there are statistical differences between the interface, variable name and other OO metrics when one compares a large sample of Web services with typical OO systems. The distributions obtained give insight into the typical characteristics of Web services and can be used to identify candidates for wrapping into Web services."
1733,"Extracting information from the Web is a complex task with different components which can either be generic or specific to the task, going from downloading a given page, following links, querying a Web-based applications via an HTML form and the HTTP protocol, querying a Web service via the SOAP protocol, etc. Therefore building Web services which proceed to executing an information tasks can not be simply hard coded (i.e. written and compiled once and for all in a given programming language). In order to be able to build flexible information extraction Web Services we need to be able to compose different sub tasks together. We propose a, XML-based language to describe information extraction Web services as the compositions of existing Web services and specific functions. The usefulness the proposed framework is demonstrated by three real world applications. (1) Search engines: we show how to describe a task which queries Google's Web service, retrieves more information on the results by querying their respective HTTP servers, and filters them according to this information. (2) E-commerce sites : an information extraction Web service giving access to an existing HTML-based e-commerce online application such as Amazon is built. (3) Patent extraction: a last example shows how to describe an information extraction Web service which allows to query a Web-based application, extract the set of result links, follow them, and extract the needed information on the result pages. In all three applications the generated description can be easily modified and completed to further respond the user's needs and create value-added Web services."
1734,"Environments in which Web service compositions (WSC) operate are often dynamic. We address the problem of which service to query for up-to-date information in order to adapt a hierarchical WSC, given that queries are not free. Previously,the value of changed information (VOC) has been proposed to select those services for querying whose revised non-functional information is expected to bring about the most change in the composition. In this paper, we present an approach for utilizing VOC in the context of a WSC composed of services and lower level WSCs, which induces a natural hierarchy over the composition."
1735,"With the development of smart devices and the popularization of social media, communication between users is becoming increasingly active in the online space, revitalizing a new relationships formed and developed. Individuals are connecting with various people, which makes many companies actively employ social media to communicate with existing and potential customers for marketing. While social networks comprise a mix of various activities such as liking, commenting, tagging, and following, little is known about the way in which liking and tagging characterize one's social networks. In this paper, we define two types of a like network: tag-based like network, non-tag-based like network and present a comparative analysis with regard to their structural and temporal aspects. Our study results show (1) a significant difference in the network size and the degree of components between the two networks and (2) the potential of the tag-based like network to have more followers than the non-tag-based like network. We highlight how the tag-based like network can be utilized to find users with the same interests, supporting additional, interest based social connection and interaction. Our study insights are expected to be developed and utilized as a web service system."
1736,"Augmenting web services with explicit semantics forms the foundation of Service Oriented Architectures (SOAs) automation. As more and more Semantic Web Services (SWSs) are deployed, similar SWSs could have quite different quality-of-service (QoS) levels. The QoS-aware discovery becomes an important challenge. While some efforts try to solve it via Constraint Programming (CP), they suffer from the purely syntactic matchmaking method. Furthermore, the construction of constraints and the selection of services are completely dependent on the literal translation from QoS descriptions, which increase obstacles to actually apply CP. In this paper, we propose a semantic QoS-aware framework for SWSs discovery by combining the semantic matchmaking and CP. Initially, a QoS ontology is presented to define QoS data into service descriptions. Then the ontology reasoning is adopted to change previous syntactic matchmaking into a semantic way. Through confirming the compatibility of concepts, complex QoS conditions are solved as constraints and a selection algorithm is proposed to obtain the optimal offer. Finally, the prototype implementation of our framework is discussed and a SWSs discovery case is used to illustrate the comprehensive discovery process."
1737,"Majority of service discovery research considers only primitive services as a suitable match for a given query while service combinations are not allowed. However, many realistic queries cannot be matched by individual services and only a combination of several services can satisfy such queries. Allowing service combinations or proper compositions of primitive services as a valid match introduces problems such as unwanted side-effects (i.e., producing an effect that is not requested), effect duplicities (i.e., producing some effect more than once) and contradictory effects (i.e., producing both an effect and its negation). Also the ranking of matched services has to be reconsidered for service combinations. In this paper, we address all the mentioned issues and present a matchmaking algorithm for retrieval of the best top k collision-free service combinations satisfying a given query."
1738,The article presents the Web services framework in the Kepler scientific workflow system and illustrates them with a real-world example.
1739,"Emerging as the popular choice for leading Internet companies to expose internal data and resources, RESTful Web services are attracting increasing attention in the industry. While automating WSDL/SOAP based Web service composition has been extensively studied in the research community, automated RESTful Web service composition in the context of service-oriented architecture (SOA), to the best of our knowledge, is less explored. As an early paper addressing this problem, this paper discusses the challenges of composing RESTful Web services and proposes a formal model for describing individual Web services and automating the composition. It demonstrates our approach by applying it to a real-world RESTful Web service composition problem. This paper represents our initial efforts towards the problem of automated RESTful Web service composition. We are hoping that it will draw interests from the research community on Web services, and engage more researchers in this challenge."
1740,"Controversial or complex topics often exhibit the backfire effect, where users' opinions harden in the face of facts to the contrary. We present initial work towards developing an online social collaborative argumentation system to verify alternative facts and misinformation by also including users' emotional associations with those stances. Our goal is to help users more effectively explore and understand their possibly subconscious biases in an effort to overcome the backfire effect and formulate more varied insights into complex and controversial topics. In order to aid this process, we model their emotional profile on such topics and combine it with a proposition profile, based on the semantic and collaborative content of propositions. We develop an algorithm to generate sentiment-based models of claims and propositions which we can filter based on users' inferred beliefs and the strength of those beliefs."
1741,"Composite service evolution is one of the most important challenges to deal with in the field of service composition. In particular, this paper presents, LiveMig, an approach to live migration of composite service instance, which is a critical step for online composite service evolution. In LiveMig, a set of change operations preserving soundness is first defined. Second, a live instance state migration algorithm is proposed to determine if the state migration is allowed or not, and to compute the exact state after the migration. Finally, the correctness of LiveMig is theoretically proved, and an extensive set of simulations are performed to show its feasibility and effectiveness."
1742,"In recent years, due to an increasing overload of information on the Internet, there are many scenarios where Recommender Systems (RSs) are employed to provide suggestions to user groups. However, most proposed approaches of group recommendations simply aggregate individual ratings or individual prediction results, rather than comprehensively investigating the hidden correlative information between members and the group, which results in inferior recommendation performance. In this paper, we propose a new approach, RWR-UTM, for group recommendations based on the combination of an integrated probabilistic topic model - a User Topic Model (UTM) and the Random Walk with Restart (RWR) method. The UTM provides a latent framework of users, groups, and items by exploiting both the users' preference profiles and the items' content information, which together can describe group interests and item features in a more complete manner. This latent framework is then combined with RWR to predict the preference degrees of groups to unrated items by detecting comprehensive latent relationships. In particular, we devised two group-based recommendation algorithms on the basis of different recommendation strategies. Finally, we conducted experiments to evaluate our approach and compare it with other state-of-the-art approaches using the real-world CAMRa2011 data-set. The results demonstrate the advantage of our approach over comparative ones."
1743,"Identity management is a central point to the security of large applications such cloud services. The identity providers (IdPs) offer services that handle critical information of users. Usually, this kind of information is stored with special care in these providers and intrusions do not necessarily result in security violations. But intrusions may implant malicious behaviors which modify the action of these authentication services. In this way, unauthorized accesses may be achieved for denying accesses to legitimate users of the system. In this paper we introduce an approach for intrusion tolerance to ensure the correct behavior in authentication of large systems, even in presence of possible intrusions."
1744,"Learning latent features of Web services will greatly boost the ability of search engine to discover relevant services. Extracted information from Web Service Description Language (WSDL) documents of services is less efficient due to the limited usage of data source. Recently, a number of ongoing works have indicated incorporating service tag, a textual symbol provides additional contextual and semantic information, helps to enhance the process of service discovery. However, a large number of relevant tags for Web services are difficult to obtain in practice. In this paper, we propose a Web service Tag Learning system to address this issue. WT Learning system adopts sparse learning technique to fully understand the structure of high dimensional textual information extracted from WSDL documents and tags. Meanwhile, our proposed system implements Alternative Direction Method of Multiplier (ADMM) strategy, which accelerates solving process in Big Data environment. Extensive experiments are conducted based on real-world dataset, which consists of 24,569 Web services. The results demonstrate the effectiveness of WT Learning system. Specifically, our system outperforms other state-of-the-art frameworks in tag classification and recommendation tasks, with 29.6% and 27.1% performance gaining respectively."
1745,"Mobile Edge Computing (MEC) is designed to extend the edge of the cloud network to decrease latency and network congestion, which would significantly improve the quality of experience (QoE) of adaptive streaming service for mobile users. This paper proposes QCSS, a QoE-aware control plane for adaptive streaming service over MEC infrastructures. QCSS aims to assure high QoE delivery of online streaming service to mobile users. The design of QCSS features: 1) a timeslot system with a look-ahead window for calculating cost of edge node switch and video quality adaption (to balance network load and reduce latency); 2) conducting service adaption via a set of cooperative action components running on client devices, edge nodes, and center nodes (to ensure a smooth viewing experience); 3) constructing a flexible QoE model and extending the scope and meaning of user-perceived experience. The effectiveness of QCSS has been validated via three real datasets. The validation results show that the proposed QCSS can improve QoE performance and network load performance for adaptive streaming service over MEC Infrastructures."
1746,"As the provision of services and the use of personal data expands, the need for services to explicitly detail what personal data a service handles and in which manner becomes paramount in order to achieve a fully transparent, ethical and personalized user experience. Services usually require access to sensitive information and may distribute this information to third parties. Service consumers need to be informed about the ways their data are used and about the actors involved in this process. Universal service descriptions that can be used to cover any business service are required to provide interoperability. In this paper, we describe our work on the privacy module for the Linked Unified Service Description Language (USDL). We expand the language by introducing a new module that allows the specification of privacy properties for business services. We have considered recent advances in data protection for its creation and provide a method, accompanied by a software tool, to examine the validity of privacy policy descriptions with Linked USDL Privacy module."
1747,"Service-oriented Architecture (SOA) comprises a number of loosely-coupled independent services, which collaborate, interact and share data to accomplish incoming requests. A service invocation can involve multiple services, where each service accesses, processes and shares the client's data. These interactions may share data with unauthorized services and violate client's privacy. The client has no means of identifying if a violation occurred because it has no control over the service invocations beyond its trust domain. Such interactions introduce new security challenges which are not present in traditional systems. This paper proposes a data-centric approach for privacy preserving access control in SOA. Benefits of the proposed approach include the ability to dynamically define access polices by the clients and control data access at the time of each service interaction. A realistic healthcare scenario is used to evaluate the implementation of the proposed solution which validates its viability."
1748,"The purpose of OWL-S is to support effective automation of various Web services related activities including service discovery, composition, execution and monitoring. For our work, we are interested in automated Web services composition for a business process model. OWL-S process ontology provides a standard language for describing the composition of Web services. Thus we can treat composed Web services as a process model. However, since OWL-S is an XML based syntax description which is used for automatic agent processing, it is difficult to read and understand for human reader, especially for those with little XML knowledge but from a process modelling background, for example, businessmen. Therefore it can not be easily and directly used for process modelling tasks. The Fundamental Business Process Modelling Language (FBPML) is a diagram based business process modelling language which merges IDEF3, RAD and PSL. It has a precise operational semantics and describes business processes in a conventional logic. Because it is diagram based, it can be easily accepted by processes modelers who normally are familiar with such diagrams. However, its problem is that it does not provide a direct specification for Web services based agents. This paper bridges the gap via conceptual mapping between two different process modelling languages, FBPML and OWL-S."
1749,"Web services will only have advantages over existing technologies if the service binding can be performed dynamically. However, existing service description languages do not contain enough information for a computer agent to do the selection automatically during runtime on behalf of the user. This results from the fact that in most approaches the offer description language doubles as a re-guest language, which prevents the requestor from a precise formulation of requests and preferences. Therefore, in this paper, we emphasize the need for a distinguished service request language that allows to capture all of the requestor's preferences. We present a concrete technique to represent such preference-containing requests, which is based on fuzzy object sets."
1750,"Large-scale observing systems are poised to become the dominant means of study for a variety of natural phenomena. These systems are comprised of hundreds to thousands of instruments that must be queried, managed, and shared in a scalable fashion. Services-oriented architectures (SOAs) are widely recognized as the preferred framework for building scalable and extensible cyber infrastructure. By applying SOA concepts, we created a framework for organizing observing system resources. Guided by this framework, we developed Web services, custom workflow applications, and an integrated user interface of monitors and controls for managing instruments in large-scale sensor network observing systems. In this paper we present our approach and discuss its application to the NSF EarthScope USArray large-scale seismic observing system"
1752,"Service-oriented applications can be expensive to test because services are hosted remotely, are potentially shared among many users, and may have costs associated with their invocation. In this paper, we present an approach for reducing the costs of testing such applications. The key observation underlying our approach is that certain aspects of an application can be tested using locally deployed semantic service stubs, instead of actual remote services.A semantic service stub incorporates some of the service functionality, such as verifying preconditions and generating output messages based on post conditions. We illustrate how semantic stubs can enable the client test suite to be partitioned into subsets, some of which need not be executed using remote services. We also present a case study that demonstrates the feasibility of the approach, and potential cost savings for testing. The main benefits of our approach are that it can (1) reduce the number of test cases that need to be run to invoke remote services, (2) ensure that certain aspects of application functionality are well-tested before service integration occurs."
1753,"Service Oriented Computing (SOC) has facilitated a paradigm shift in software provisioning models: software gets consumed as a ""service"" providing enormous benefits, however lack of security assurance of third-party services is hampering their wider adoption in business- and security-critical domains. Security certification typically provides the required assurance, however applying it as is to SOC is infeasible, given that the natural language representation of resulting certificates is a major obstacle for typical SOC scenarios like service discovery, service composition and so on. To overcome the limitations of existing security certificates we present the concept of a digital security certificate for services. It is realized by a language which enables the representation of a security certificate in a structured, machine processable manner that would enable automated reasoning to be performed on them and thus make it feasible for certified security features to be part of typical SOC scenarios."
1755,Presents the welcome message from the conference proceedings.
1756,"Internet of Things (IoT) represents a cyber-physical world where physical things are interconnected on the Web. This paper presents an architecture designed for Energy-efficient Inter-organizational wireless sensor data collection Framework (EnIF). Environmental monitoring and urban sensing are two major application scenarios in IoT. Different from the traditional sensor environments, environmental sensing in IoT may require battery-powered nodes to perform the sensing tasks. Such a requirement raises a critical challenge to ensure that sensor data gathering can be collected in a timely and energy-efficient manner. Although numerous energy-efficient approaches for IoT scenarios have been proposed, previous works assumed the entire network was managed by a single organization in which the network establishment and communication have been pre-configured. This assumption is inconsistent with the fact that IoT is established in a federated network with heterogeneous devices controlled by different organizations. The aim of the framework is to enable a dynamic inter-organizational collaborative topology towards saving energy from data transmissions using a service-oriented architecture."
1757,"Innovation in technologies such as XML and Web services, has led to an interest in business processes. Consequently, several languages for the execution of business processes have been created. Nevertheless, as these languages cannot be used in the early stages of the development process of Web information systems (WIS), it is necessary to include methodologies that allow the systems analysts to understand the business process as well as to model the services composition. This work presents a process for the business process development based on Web services, which starts with the identification of the services that are offered to the user and ends with the definition of a service composition model. This process is illustrated by means of a WIS for the management of medical images that we have taken as a case study. Because it is one of the most widely used for Web services composition, we have chosen the BPEL4WS language for the implementation of the business processes. However, we have found several limitations in such language which are also described in this paper"
1758,"Among the approaches that investigate the similarity between web services, hardly any concentrates on the impacts from contexts. In this paper we introduce service2vec which is an approach to represent web services as service embeddings based on a recent popular deep learning technique word2vec. Our approach composes and combines web services to be a document that is trained by the modeling technique of word2vec. As a result, each web service in the document is vectorized. By taking the advantage of word2vec, the resulting service embeddings of service2vec can be used to illustrate the contextual relations between web services. The experimental results suggest that service2vec can deliver contextual similarity between web services."
1759,"Management architectures are well discussed in the literature, but their application in real life settings has not been as well covered. Automatic management of a system involves many more complexities than closing the control-loop by reacting to sensor data and executing corrective actions. In this paper, we discuss those complexities and propose solutions to those problems on top of Hasthi management framework, where Hasthi is a robust, scalable, and distributed management framework that enables users to manage a system by enforcing management logic authored by users themselves. Furthermore, we present in detail a real life case study, which uses Hasthi to manage a large, SOA based, e-science cyberinfrastructure."
1760,"Air quality has become a major global concern for human beings involving all social stratums, for both developing and developed countries. Web service of precise and early air pollution forecasting is of great importance as it allows people to pro-actively take preventative and protective measurements. As an endeavor on the course of machine learning based air quality forecasting, this paper presents an initiative and its technological details in solving this challenging problem. Specifically, this work involves three major highlights regarding with both algorithmic innovation and deployment with its impact: 1) We propose a multi-channel ensemble learning framework, 2) We propose a new supervised feature learning and extraction method, i.e. sufficient statistics feature mapping based on Deep Boltzman Machine, which serves as a building block for our learning system, 3) We target our air pollution prediction method to the city of Beijing, China as it is at the forefront for battling against air pollution, which is embodied as a web service for prediction. Extensive experiments of real time air pollution forecasting on the real-world data demonstrates the effectiveness of the proposed method and value of the deployed web service system."
1761,"This paper presents a hybrid approach for automatic composition of Web services that generates semantic input-output matching compositions minimizing the number of services and optimizing the global QoS. The proposed approach has four main steps: 1) generation of the composition graph for a request, 2) computation of the optimal QoS of the composition graph, 3) multi-step optimizations of the graph to identify equivalent and dominated services, and 4) hybrid local-global search to extract the optimal QoS with the minimum number of services. A validation with the datasets of the Web Service Challenge 2009-2010 is also provided."
1762,We propose a framework to discover proximate IoT service relationships based on spatio-temporal features. We introduce a spatio-temporal proximity model in terms of spatial-proximity and temporal-proximity to discard insignificant IoT service relationships. The proximity model focuses on quantifying the correlation strength among IoT services from time and location aspects. A new algorithm is proposed to discover proximate spatio-temporal IoT service relationships. We also present preliminary experimental results.
1763,"Complex and dynamic web service compositions may introduce unpredictable and unintentional sharing of security-sensitive data (e.g., credit card numbers) as well as unexpected vulnerabilities that cause information leak. This paper describes a fine-grain access policy specification of security-sensitive data items for each component web service. We propose the SF-Guard architecture to enforce these access policies at component web services. A prototype implementation of SF-Guard (on Apache Axis2) and its evaluation show that effective protection of security-sensitive information can be achieved at low overhead (a few percent addition to response time) while preserving the functionality of flexible web service composition."
1764,"With the evolution of mobile devices and app eco system, all major content providers develop services in native apps and web-apps. Web-apps have an inherent advantage of platform independent and uniform experience across devices, but the page load time, battery usage, and bandwidth consumption have to be improved. To solve some of these issues, there has been an introduction of Progressive Web Apps (PWA) by content providers. PWA apps use an effective caching policy on web resources based on its property. But the webpages are overwhelmingly non-PWA compliant. There is a need for non-PWA web pages to be effective in bandwidth utilization so that we can avoid the exchange of needless resources. This paper proposes a new method to inject a service worker into a webpage at Proxy Servers. This service worker is packaged along with non-PWA webpage as a response to web browser requests. The web engine then runs the service worker to identify stale web resources which in turn will be helpful to avoid transfer of redundant web resources. The proposed approach was evaluated using top 25 Non-PWA sites from Alexa 100 websites for one month. It saved an average of 25% data traffic and also provided an offline experience of these websites. This proposal can be further extended to create a generalized framework for seamlessly converting Non-PWA apps to PWA apps."
1765,"With the number of e-Business applications dramatically increasing, service level agreement (SLA) plays an important part in Web services. A SLA is a combination of several quality of services (QoS), such as security, performance, and availability, agreed between a customer and a service provider. Most existing research addresses only one QoS metric, and in the case of the response time, the average time to process and complete a job is typically used. In this paper, we study trustworthiness, percentile response time and availability. We consider all these qualities for a trust-based resource allocation problem which typically arises in Web services applications. We formulate the trust-based resource allocation problem as an optimization problem under SLA constraints, and we solve it using an efficient numerical procedure"
1766,"In dynamic e-business, organizations collaborate in a just-in-time fashion using loosely coupled services. To ensure interoperability of the services, behavioral mismatches between their protocols need to be resolved in a fast and efficient way, which can be done with protocol adaptors. We present an efficient, automated method to construct (if possible) a minimal protocol adaptor with parallelism for two asynchronously communicating business protocols. A minimal adaptor only processes those messages that cause the mismatch, and has less message overhead at run-time than a non-minimal adaptor. Existing methods only build adaptors that are sequential, synchronous, or non-minimal. We show that the proposed method increases the efficiency of service adaption both at run-time and design-time."
1767,"As more and more web services are available over the Internet, people have many choices to use one or some of them to fulfill their tasks. When the task is adequtely complicated, it is necessary to compose the related web services to complete it effectively and efficiently. Many algorithms have already been proposed to solve the web service composition problem by trying to find the candidate solutions with the best quality of service(QoS) while satisfying the functional goal. However, we may always find that the final solutions contain redundant services that is unnecessary and may cause extra executing costs. Hence, it is significant to remove the redundant services in the service composition process. In this paper, we address the above issues and propose a novel web service composition algorithm by integrating both QoS optimization and redundancy removal. Experimental results on the benchmark WSC'09 datasets have proven that our algorithm keeps less redundant services remained in the solution while keeping very good QoS performance."
1768,"QoS-aware service composition is the generation of a business process to fulfill functional goals and optimize the QoS criteria at the same time. People may focus on the optimization of a single QoS criterion or a set of QoS criteria. We find that though many composition algorithms can get the optimal QoS values, the solutions obtained can possibly contain redundant services, the removal of which does not worsen the QoS value of the solution. In the literatures using Web Service Challenge (WSC) open data sets, the removable services can be over 30% of the services in the solutions. This common problem has been ignored so far. The fundamental reason is that execution cost is not one of the criteria to optimize. Even in the cases when execution costs are not explicitly given for each service, we are still motivated to reduce the number of services in the final solution by assuming each service takes unit cost. In this paper, we study the redundancy removal problem to further optimize the QoS optimal solutions obtained by QoS-aware service composition algorithms. We model the redundancy removal problem as an integer programming problem. Though solvable using a standard solver, we present an algorithm to solve the problem in this specific context and it proves to have better performance than a standard integer programming solver. We also present the results of our data experiments."
1769,"Due to the popularity of using web services to deliver services on the Web, a clear view of how they are being consumed is becoming critical. Researchers have been trying multiple methods to reveal actual service orchestration patterns from service logs. However, most of the discovery methods have taken deterministic approaches, and hence, they do not provide enough allowance to cater for incomplete data and noises. On the other hand, most investigations do not take combinatorial explosion into consideration leading to scalability problem. Moreover, asynchronous web service invocations and distributed executions also make it difficult to identify service patterns due to the randomness in log record generation. In this paper, probabilistic topic mining class of solutions are applied to reveal web service orchestration patterns from service logs, in which robust approximation methods are available to provide scalability. Data sparsity problem in service log is also investigated by using biterm topic model (BTM) and comparing its results with traditional latent Dirichlet allocation (LDA) model. In addition, a topic matching method is introduced based on the Hungarian method on Jensen-Shannon divergence matrix, whilst notions of aggJSD and autoJSD are also introduced to measure topic diversity between matched topic sets and within a single topic set respectively. Experiment results confirm that BTM can be used for service logs with short log entries and with sparsity larger than 90% approximately."
1770,"In this paper, we explore service recommendation and selection in the reusable composition context. The goal is to aid developers finding the most appropriate services in their composition tasks. We specifically focus on mashups, a domain that increasingly targets people without sophisticated programming knowledge. We propose a probabilistic matrix factorization approach with implicit correlation regularization to solve this problem. In particular, we advocate that the co-invocation of services in mashups is driven by both explicit textual similarity and implicit correlation of services, and therefore develop a latent variable model to uncover the latent connections between services by analyzing their co-invocation patterns. We crawled a real dataset from Programmable Web, and extensively evaluated the effectiveness of our proposed approach."
1771,"Existing Web services notification and eventing standards are useful in many applications, but they have serious limitations precluding large-scale deployments: it is impossible to use IP multicast or for recipients to forward messages to others and scalable notification trees must be setup manually. We propose a design free of such limitations that could serve as a basis for extending or complementing these standards. The approach emerges from our prior work on QSM (Ostrowski et al., 2006), a new Web services eventing platform that can scale to extremely large environments"
1772,"In order to co-ordinate multiple resource providers in grid environment to meet a common objective, support for negotiation is needed to establish a contract between the user and the resource providers that clearly states the QoS required, restrictions on resource utilization and penalties during violation of the objective. Strength of the negotiation process mainly depends on the selection of resources for negotiation. Currently, no grid meta scheduler supports SLA negotiation. We propose a deviation based resource ordering algorithm (DRS) that orders and selects the resources for negotiation based on their deviation value computed from the parameters in the job request against their current availability in the resources. Also, we propose mutual agreement protocol (MAP) to negotiate with resources in order to get their commitment against the job request. We simulate this negotiation process and compare it against gridway meta scheduler that shows improved performance in terms of average SLA creation time, success rate and throughput."
1774,"Summary form only given. This tutorial will introduce the participants to the area of QoS specification and management for XML Web services. It will explain the importance of this topic and why the widely used basic Web service technologies are not enough. Further, it will give an overview of a number of languages developed for QoS specification for Web services, as well as a number of research infrastructures, industrial products, and standardization proposals that offer some forms of QoS management for Web services. The achieved results and open topics for future research will be critically analyzed."
1775,"With the extensive applications of Web services on the Internet, to locate target services in an accurate and efficient way becomes increasingly difficult. At present, semantic Web service is regarded as the most promising approach to address the challenge. In this paper, an ontology service description language and a query language, OSDL and OSQL are proposed respectively, and then a service matchmaking algorithm based on interface semantics is proposed. The problem of service matchmaking can be attributed to the similarity between the semantic pair, and the similarity is the quantization of their relationship, which is discovered through reasoning on ontology. Unlike the existing semantics-based service matchmaking algorithms, which focus on the relationships among the successors and ancestors of ontology classes, the proposed one also pays attention to the comparability among classes and their property classes. The experiment data indicate that it is worthwhile to take the point into consideration to receive a higher recall and precision."
1776,"Along with the progress of the enterprise globalization, alliance and cooperation have become an important means for enterprises to improve their competitiveness in the market. Yet, most current methods for Service Composition Optimization (SCO) fail to address the Alliance Relation (AR) between services and assume that services are independent of each other. To address this issue, this paper presents an alliance-aware service composition method. Firstly, the fundamental properties of the AR are given based on a multi-granularity service composition model. Secondly, alliance relation granularity is coarsened into a relation granulation quotient space and the domain elements are matched reversely with service compositions, thereby reducing the complexity of query and computation of the AR. Finally, a Relation Granularity-aware Particle Swarm Optimization Algorithm (RG-PSO) is proposed based on relation granulation quotient space to solve the alliance-aware SCO prolem. Substantial experimental results show that the proposed model and algorithm are effective and efficient."
1777,"Service compositions need to continuously self- adapt to cope with unexpected failures. In this context adaptation becomes a fundamental requirement that must be elicited along with the other functional and non functional requirements. Beside modelling, effective adaptation also demands means to trigger it at runtime as soon as the actual behavior of the composition deviates from stated requirements. This paper extends traditional goal models with adaptive goals to support continuous adaptation. Goals become live, runtime entities whose satisfaction level is dynamically updated. Furthermore, boundary infringement triggers adaptation capabilities. The paper also provides a methodology to trace goals onto the underlying composition, assess goals satisfaction at runtime, and activate adaptation consequently. All the key elements are demonstrated on the definition of the process to control an advanced washing machine."
1779,"Process similarity search is an effective way to manage a large number of business process models. However, there exists no benchmark dataset that can be used to evaluate the performance of the existing process similarity search algorithms. To solve this problem, we have constructed a benchmark dataset that modeled by Petri-net. In this paper, the benchmark dataset totally consists of 100 process models, where we have marked out 10 search models and their corresponding 10 relevant models (including itself). And for each search model, the ranking order of its relevant models is derived from user studies. The dataset and the codes of corresponding similarity search algorithms are available to the public on a website
<sup>1</sup>
."
1780,"Collaborative filtering is one of widely used Web service recommendation techniques. In QoS-based Web service recommendation, predicting missing QoS values of services is often required. There have been several methods of Web service recommendation based on collaborative filtering, but seldom have they considered locations of both users and services in predicting QoS values of Web services. Actually, locations of users or services do have remarkable impacts on values of QoS factors, such as response time, throughput, and reliability. In this paper, we propose a method of location-aware collaborative filtering to recommend Web services to users by incorporating locations of both users and services. Different from existing user-based collaborative filtering for finding similar users for a target user, instead of searching entire set of users, we concentrate on users physically near to the target user. Similarly, we also modify existing service similarity measurement of collaborative filtering by employing service location information. After finding similar users and services, we use the similarity measurement to predict missing QoS values based on a hybrid collaborative filtering technique. Web service candidates with the top QoS values are recommended to users. To validate our method, we conduct series of large-scale experiments based on a real-world Web service QoS dataset. Experimental results show that the location-aware method improves performance of recommendation significantly."
1781,"Current QoS-aware automatic service composition queries over a network of Web services are often one-time innature. After a network of Web services is built, such queries are issued once, and answers are found from the scratch. The underlying assumption is that the participating Web services are rather static so that their functional and non-functional parameters seldom change. However, such an assumption is often baseless. New services come and go, service APIs change gradually, and QoS values fluctuate. Therefore, a support for efficiently handling ""continuous"" service composition queries is desired. In this paper, we propose an event driven continuous query algorithm for QoS-aware automatic service composition problem to cope with different types of dynamic services. Moreover, we integrated this algorithm in our service composition system, QSynth. Finally, we evaluate our proposal using both real QoS data and synthetic Web service data and show the superior performance of ours, compared to the state-of-the art solution which won the performance championship of Web Service Challenge in 2009 and 2010."
1782,"Many service providers including large enterprises have released their own applications (apps) that incorporate HTTP clients to facilitate the communications with their servers. The workflows of and APIs used by a web app and its corresponding mobile app are not always the same. We call the APIs found in apps private web APIs in that they are only supposed to be invoked by apps that developed by the service providers themselves. However, checking the origin of an HTTP request is very difficult, and private web APIs can be easily invoked by other entities. Hence, it is imperative to study if private web APIs provide the same level of security checks and validations as their public counterparts. To automatically discover the undocumented private APIs in Android apps, we design a system that uses static analysis to find the activities that invoke web APIs. Our system then runs the discovered activities on a customized Android system to monitor its HTTP requests and responses. We evaluated our system on 76 popular apps on the Google Play market. Our system successfully run 48 apps and discovered many private server-side APIs from more than 30 apps. Further manual investigation discovered that 9 of the apps have vulnerabilities that would enable API misuse and session hijacking."
1783,"Computing has reached the time of distributed applications everywhere. Service-oriented architectures are more and more used to organize such complex and highly dynamic applications into business processes calling services discovered in registries at load-time. In this context, Quality of Service (QoS) and agility in business processes become key issues. Instead of binding business processes to services at load-time, this paper proposes to monitor sets of candidate services for their current QoS and to choose among them at call-time. This new form of late-binding paves the way to more agile and robust applications called adaptive business processes. Besides the conceptual background and implementation of this late-binding in an industrial-strength web service platform, this paper presents the LCP-net formalism introduced to provide programmers with a mean to express qualitatively their preferences among the different QoS properties of services, hence tackling the multi-criteria decision making arising from the run-time choice among candidate services given several unrelated QoS properties."
1784,"MapReduce has recently gained a lot of attention as a parallel programming model for scalable data-intensive business and scientific analysis. In order to benefit from this powerful programming model in a scientific workflow environment, we propose a MapReduce-enabled scientific workflow composition framework consisting of: i) a dataflow based scientific workflow model that separates the declaration of the workflow interface from the definition of its functional body; ii) a set of dataflow constructs, including Map, Reduce, Loop, and Conditional, and their composition semantics to enable MapReduce-style scientific workflows; iii) an XML-based scientific workflow specification language, called WSL, in which both Map and Reduce are fully composable with other dataflow constructs in both flat and hierarchical manners. Besides leveraging the power of MapReduce to the workflow level, our workflow composition framework is unique in that workflows are the only operands for composition; in this way, our approach elegantly solves the two-world problem of existing composition frameworks, in which composition needs to deal with both the world of tasks and the world of workflows. The proposed framework is implemented and a case study is conducted to validate our techniques."
1785,"Web services are getting popular in the domain of business to business electronic commerce and in automating information exchange between business processes. The use of handheld devices such as PDAs and smart phones is spreading rapidly in the business community. Accessing Web services from small devices is very common these days, but there is hardly any solution available to host Web services on such devices. There are a number of challenges to host a Web service on handheld device. These include limited resources and the lack of the execution environment to host Web services. Because of the resource constraints, it is not possible to use the existing Web service provider toolkits to host the Web service. In this paper, a light weight Web service provider toolkit is proposed that can be used for a variety of handheld devices. The insights gained into this Web service provider system are discussed in detail."
1786,"In this paper, we propose an approach for approximate matching of OWL-S process model. We also propose a similarity measure that captures structural and semantic differences between two process models. To do so, we reduce the process matching to a graph matching problem and we adapt existing algorithms for this purpose."
1787,"Automatically finding suitable Web services given a request is a difficult problem because the interface descriptions of Web services are often terse and cryptic. Dictionary and information retrieval based techniques have proven useful in disambiguating the semantics of service descriptions, but they are limited in their capability to consider the relationships between the words describing the Web services. Current ontology-based approaches typically require a user to explicitly create domain ontologies. This paper presents a novel technique that significantly improves the quality of semantic Web service matching by (1) automatically generating ontologies based on Web service descriptions and (2) using these ontologies to guide the mapping between Web services. Our approach differs from earlier work on service matching by considering the relationship between words rather than treating them as a bag of unrelated words. The experimental results indicate that with our unsupervised approach we can eliminate up to 70% of incorrect matches that are made by dictionary-based approaches."
1788,"In this paper, we introduce the authorization issues for Web Services. We introduce the authorization service provided by Microsoft/spl reg/ .NET MyServices and then briefly describe our proposed modifications and extensions to the authorization service. We discuss the application of the extended authorization model to a healthcare system built using Web Services. We used the XML access control language (XACL) to specify policies in XML and control access to the patient records stored in XML format. We then evaluated the suitability of XACL as an authorization policy language for Web Services."
1789,"Business process management, service-oriented architectures and software back-engineering heavily rely on the fundamental processes of mining of processes and web service business protocols from log files. Model extraction and mining aim at the (re)discovery of the behavior of a running model implementation using solely its interaction and activity traces, and no a priori information on the target model. This paper presents an approach for correlating messages and extracting the business protocol of a web service in the realistic scenario in which correlation information is entirely absent from interaction and activity logs. Correlation is achieved through deterministic computations that result in an extremely efficient method whose extensive experiments have shown its solid reliability, robustness when dealing with complex structures, and very high performance and scalability. This approach and the underlying algorithms extend what is actually possible to achieve in the web service business protocol mining domain using incomplete and noisy data logs, and opens new horizons in back-engineering of web services. The theoretical and experimental results clearly show the leap forward achieved herein."
1790,"We propose the Service Control Technology for the Web-telecom coordination service, and we show our proposal effectiveness. In the NGN (Next-Generation Network), many new applications are expected using open network interfaces. In particular, we study the SDP (Service Delivery Platform) for coordinating existing Internet services and telecom services. In this paper, we propose the Service control technology, which handles common process required in the telecom trigger Web-telecom coordination service. Our method achieves improvement in usability of telecom user, reliability of the coordination service, and processing performance. We implement our method two types (Proxy type, Callable type) and measured processing performance. In comparison to related technologies, we show our method effectiveness."
1791,"Resilience is an important factor in designing web service oriented systems due to frequent failures arising in runtime. These failures derive from the stochastic and uncertainty nature of a composite web service. Service providers need to rapidly address issue when a fault occurs in system running. But it is not easy to locate and fix the faults only using the log generated by the system. In this paper, we propose a resilient framework to automatically generate a fault handling strategy for each failed service to improve the efficiency of fault handling. In the framework, we design and implement three components including exception analyzer, decision maker, and strategy selector. First, The exception analyzer builds a record, derived from the system log generated by an application, for each failed service. Next, the decision maker adopts a k-means clustering approach to construct a decision including the fault handling to each failed service in a scope. Then, the strategy selector uses an integer program solver to generate the solution to strategy selection problem that is boiled down to the optimization problem. The experiment shows that the framework can improve resilience of Web service-oriented systems under acceptable overheads, and meanwhile the accuracy of fault handling strategy is over 95%."
1792,"Current Web services are able to interoperate successfully with most basic data types. However, due to the limited functionality of existing data binding tools, they still experiment difficulties manipulating more complex XML data types, forcing programmers to work at the XML level. In this paper we propose a business model for web services where data binding tools not only generate the WSDL, but also provide portable binding extensions for manipulating theXSD types. These binding extensions can be integrated into any other binding tool, overcoming their limitations. Because these extensions are written in XML, this model conforms to the principle of platform independence. In addition, this model does not suppose any extra programming effort to neither service providers or clients. The approach has been validated with the creation of several extensions, which has been ported into Java and PHP clients. Our preliminary results show no performance penalty."
1793,"With the proliferation of internet technologies, publish/subscribe systems have gained wide usage as a middleware. However for this model, catering large number of publishers and subscribers while retaining acceptable performance is still a challenge. Therefore, this paper presents two parallelization strategies to improve message delivery of such systems. Furthermore, we discuss other techniques which can be adopted to increase the performance of the middleware. Finally, we conclude with an empirical study, which establishes the comparative merit of those two parallelization strategies in contrast to serial implementations."
1795,"SOAP is the standard protocol for message exchange in Web service environments. As an XML-based protocol, SOAP is not suitable for the transmission of large amounts of binary data. This fact has been addressed by the SOAP messages with attachments specification, which regulates the transfer of a SOAP message together with an arbitrary number of binary attachments composed within a MIME multipart/related message. Although this leads to a reduction of transmission overhead, Web service communication using SOAP messages with attachments still lacks communication and processing flexibility. In this paper, we present a novel and more flexible way of handling attachments in SOAP-based Web service environments. In contrast to SOAP messages with attachments, our approach offers message forwarding without additional communication cost and demand-driven evaluation and transmission of binary data, thus providing the opportunity to save time by overlapping service execution and data transmission"
1796,"While the usage of web services has increased explosively in recent years, very few studies examined the characteristics of web services using large-scale real data for a long period of time. In this paper, we present one such a large scale longitudinal analysis of publicly available web services of SOAP-based and RESTful types. For the period of roughly one year and from five different world-wide locations, we closely monitor the ups and downs of various basic properties of web services and their QoS values using a total of 825,132 real web services."
1797,"QoS prediction for Web services is a hot research problem in the field of services computing. As one of the most important methods for QoS prediction, Collaborative Filtering (CF) makes prediction based on the historical QoS data contributed by similar users and services. The key issue in this process is to detect the unreliable data offered by untrustworthy users, which has attracted limited attentions so far. The utilization of unreliable data decreases the prediction accuracy greatly. In this paper, we propose a novel credibility-aware QoS prediction method (named CAP) to address this problem. Our method first employs two-phase K-means clustering to identify the untrustworthy users, which clusters QoS values for untrustworthy index calculation in the first phase and clusters users according to their index in the second phase, and then predicts the missing QoS value based on the credible clustering information. The evaluation results demonstrate that CAP provides considerable improvement on the prediction accuracy compared with other approaches and is robust against various percentages of untrustworthy users."
1798,"Web services composition search systems have received a great deal of attention recently. However, current solutions have limitations of inefficiency and including redundant Web services in the results. In this paper, we proposed a redundant-free Web services composition search based on a two phase algorithm. In the forward phase, the candidate composition will be found efficiently by searching the link Index. In the backward phase, redundant-free Web services compositions are generated from the candidate composition by using the concepts of tokens. Experimental results demonstrate the performance benefits of our proposed techniques compared to state-of-the-art composition approaches."
1799,"Fast deployment of mobile Internet makes Web services often consumed under a multi-dimensional spatiotemporal model, wherein a specific service client could keep active while its location is changing. Recommending Web services for such clients must be able to predict unknown QoS values with the target client's service requesting time and location taken into account, e.g., Performing the prediction via a set of measured multi-dimensional QoS data. Most QoS prediction methods focus on the QoS characteristics for one specific dimension, e.g., Time or location, and do not exploit the structural relationships among the multi-dimensional QoS data. This paper proposes an integrated QoS prediction approach which unifies the modeling of multi-dimensional QoS data via multi-linear-algebra based concepts of tensor and enables efficient service recommendation for Web service based mobile clients via tensor decomposition and reconstruction optimization algorithms. Comparative experimental evaluation results show that the proposed QoS prediction approach could result in much better accuracy in recommending Web services than several other representative ones."
1800,"In order to make a composite web service meet user's response-time requirement, the proper instance of each member web service should be selected and bound. In the literature, all the member web services of a composite web service are treated equally during the response-time management, but without considering their different capabilities of effecting the user's satisfaction. However, in a certain using scenario, some member services are more sensitive than others, that is to say, in a given composite web service, if these sensitive services delayed, the decline of users' satisfaction is remarkably greater than that when other services delayed. In this article, a using scenario oriented response-time management method is proposed to reduce the delaying risk of the time sensitive web services, and thus to improve the user's satisfaction of a composite web service. Our experiments validated the efficiency of the proposed method."
1801,"With the increasing storage capacity, processing power and battery capabilities, mobile devices are now able to providing services instead of just being service consumers. This paper introduces a middleware for time independent mobile Web services (Mob-WS) that avoids the overheads of long durational synchronous communication. Details of communication architecture and interaction among the middleware components is presented and discussed. The Bluetooth binding for SOAP has been developed and briefly presented."
1802,"In recent years, RESTful Web services have been rapidly developed and deployed, because of the advantages of lightweight, flexibility and extensibility, etc. However, most RESTful services are described in heterogeneous and ordinary HTML pages, which makes them really difficult to be identified and crawled automatically from the Internet. In this paper we propose a hybrid classifier framework called co-NV for automatic identification of RESTful services on the Web. In our framework, web pages are analyzed and filtered according to the contents and structure characteristics of HTML documents, with Naïve Bayes classifier and Vector Space Model (VSM) respectively. Experiments with real RESTful services prove that our framework works effectively with high precision and recall rate, and is very practical."
1803,"The use of process fragments to leverage reuse of process models is well established in the literature. Process fragments are manually or semi-automatically extracted and mainly focus on the textual or behavioural semantics of the process models that they are extracted from. However, in many use cases we also need to use these fragments to derive synthetic process models that satisfy specific structural properties. In order to tackle this challenge we propose a method for automatically generating synthetic, representative, executable process models expressed in Business Process Model and Notation 2.0 (BPMN 2.0) with respect to specific user-defined structural criteria. Our method identifies, selects, and combines recurring sub-structures discovered in a collection of thousands of real world process models. The recurring sub-structures are seen as an extended type of process fragments. For our method we have developed a proof-of-concept prototype and for this we discuss the experimental results obtained from its evaluation."
1805,"We propose a framework and polynomial algorithms for semantic-based automated Web service orchestration, fully compliant with semantic Web technologies. The approach exploits the recently proposed concept abduction inference service in description logics to solve concept covering problems. We present how the proposed approach deals with not exact solutions, computing an approximate orchestration with respect to an agent request modeled using a significant subset of OWL-DL."
1806,"IT service providers use applications to support their business processes. The need for specialized IT management functionality and information generates a multitude and diversity of management applications that can be recognized in one IT provider's scenario. To run IT and to provide IT services effectively and efficiently, management applications have to be integrated along operational processes. This article introduces an approach to integrate management applications by leveraging a service-oriented architecture (SOA). Therefore, a sufficient understanding of IT service provider's processes and cooperative roles is essential. The presented SOA defines how to integrate management applications loosely coupled in a process-oriented manner, bridging the gap between operational processes and management applications."
1807,"Modern cloud applications are based on microservice architectures. The deployment of these microservice based applications often requires that every constituent service starts after all its dependencies are configured and running properly. It is also common that these dependencies generate dynamic data that needs to be supplied to other services too at starting time. More complex scenarios require additionally interchanging data in other phases of the microservices lifecycle. One alternative to solve these dependencies is to describe the deployment of microservice applications manually-using scripts-which allows IT operators to precisely define when a service is ready to start serving other components. However, synchronization by scripting is tedious, error prone and hard to maintain. Other solutions offer specific languages to describe service dependencies, along with tool support that interpret scripts in these languages to take care of starting services in the proper order. These tools are either very rich but complex to use, or fail in providing sophisticated ways to describe what it means for a service to be ready. Moreover, the communication layer between services, if supplied, is based on intermediate entities and non-trivial network protocols. This paper proposes pipekit as a solution, by offering a container orchestration language which focuses on simplicity (pipekit is similar to Docker Compose) and is equipped with directives to define when a service is ready. The pipekit tool provides a communication layer for moving data between services, implemented using shared storage. This shared storage provides a very simple interface to move artifacts between services, and greatly simplifies the synchronization logic of pipekit by using semaphores at the file system level."
1808,"Large enterprises have built very large ""on-premise"" data-sets that are critical to many business functions. With the availability of cloud-based storage, many of these enterprises are considering whether and how to make some of this data available on the cloud. One motivation is to offload the processing of new mobile application workloads from the on-premise system to the cloud. Another motivation is to improve the performance of these mobile applications. However, because of the importance of this data, and because of regulatory constraints, many enterprises are unwilling to simply move their data from an on-premise environment to the cloud. Instead, they prefer to keep the ""master"" version of the data on-premise, while projecting a subset of the data to the cloud. Several challenges face these enterprises. First, how can large data-sets be efficiently made available on the cloud with minimal disruption to the ongoing on-premise business function? Second, how can this data be represented in a way that will be useful to cloud developers? Typically, cloud developers want data represented in a way that is easily consumable by REST APIs, but the on-premise representation may not be amenable to such usage. Our INTEGRATOR project addresses these challenges by providing an integrated cloud/on-premise data-service. Importantly, the INTEGRATOR architecture is broadly applicable across various back-end systems. In this paper we describe the INTEGRATOR architecture and a prototype implementation for a specific on-premise system. We examine alternative architectures - ""table based"" and ""business object based"" - and explain why we chose the business object approach."
1809,"Traditional web services transaction processing mechanism handle exception by forward recovery and backward recovery. These compensation mechanisms often lead to waste of resources and time. In this paper, we propose a framework for predicting outcomes of service executions as part of service compositions which allows to choose service instances that are likely to lead to a successful result in the first instance and thus reduces the need for invoking costly recovery mechanisms. The framework makes use of watchdogs to maintain an awareness of service availability and a pre-coordinator which has oversight of the whole composite Web service and acts as a control center. An analysis of a scenario shows that we cannot only provide users with a more satisfactory result, but also can reduce the overhead costs of resources and waste."
1810,"Traditionally, the composition of Web services to create mashups has been achieved by using an application server as a mediator between a client browser and services. To avoid this bottleneck, mashups are sometimes implemented so that Web service composition takes place directly from the end userpsilas browser. Creating such implementations is difficult because developers must manage the separation of software into various distributable pieces, in different languages, and coordinate their communication. In this paper we describe a middleware for managing Web service mashups in a disciplined, and flexible way. We build upon the established abstraction of XML pipelines, but describe a new approach for selectively partitioning pipeline components between a browser client and application server. We provide a performance evaluation for a common mashup application scenario."
1811,"WS-Policy is a standard to express requirements and capabilities in Web service systems. Policies are based on domain-specific assertions. In this paper we present a lightweight approach to semantic annotations of policy assertions. The approach allows matching of requirements and capabilities based not only on the syntactical representation of their corresponding assertions but also on their semantic meaning. Besides vocabulary mismatches our approach can also handle granularity mismatches, e.g. if two capabilities in combination satisfy a single requirement. We present a validation of our approach consisting of a performance evaluation and the realization of a use case, both based on our implementation of the semantic policy matching algorithm. We furthermore show the advantages of our approach compared to existing related work."
1812,"As a de facto standard, OWL-S enables the semantic description of Web services, resulting in the automated discovery, selection, and composition of Web services. However, the complexity of the OWL-S grammar makes it difficult to construct an OWL-S ontology manually. This paper presents an efficient method for generating OWL-S ontologies from UML diagrams, which are widely used for software design and development. The proposed method uses a class diagram to represent a domain ontology and sequence or activity diagrams to represent the behavior of a business process. It also uses UML profiles to support various features of OWL-S. Finally, an XMI file extracted from UML diagrams is transformed into an OWL-S ontology via an XSLT script. Experimental results with a large volume of UML diagrams show that the proposed method deals with the control flow of complex processes and is superior to previous methods."
1813,"In this paper we present the concept of activity-centric collaboration using service-oriented architectures (ACCUSO), which addresses the requirements arising from ad-hoc collaboration in mobile teams. In ACCUSO, activities are used to map human actions to Web services exploiting the potential benefits of SOA, such as service discovery and binding at run time. The possibility to compose activities hierarchically from sub-activities and to redesign running activities provides the process-flexibility required in ad-hoc collaboration. We expand the notion of service orientation by introducing human-provided services (HpS) which provide functionality not realizable through software services. HpS are ""implemented"" by human actors (possibly being mobile), which remains transparent to the system, thereby allowing for the provisioning of HpS based on conventional WS-infrastructure. The feasibility and applicability of ACCUSO is demonstrated through a proof-of-concept implementation."
1814,"The ultimate goal of the semantic Web is to enable automated collaboration over the Internet, based on ontologies as semantic terminology definitions and Web services as computational facilities accessible over the Web. An essential functionality for collaboration support on the semantic Web is detection of entities, services, and other resources that are to be used for achieving a successful collaboration. This is commonly referred to as discovery, wherefore the emerging concept of semantic Web services promises more effective support than conventional Web service technologies: based on exhaustive semantic description frameworks, intelligent mechanisms are envisioned for discovery, composition, and contracting of Web services. This paper outlines an approach for automated collaboration support using semantic Web services, and presents the realization of semantically driven discovery of cooperation partners and usable Web services as a main component for collaboration establishment."
1815,"Algorithms for composing Web services (WS) traditionally utilize the functional and quality-of-service parameters of candidate services to decide which services to include in the composition. Users often have differing experiences with a WS. While trust in a WS is multi-faceted and consists of security and behavioral aspects, our focus in this paper is on the latter. We adopt a formal model for trust in a WS, which meets many of our intuitions about trustworthy WSs. We hypothesize predictors of a positive experience with a WS and conduct a small pilot study to explore correlations between subjects' experiences with WSs in a composition and the predictor values for those WSs. Furthermore, we show how we may derive trust for compositions from trust models of individual services. We conclude by presenting and evaluating a novel framework, called Wisp, that utilizes the trust models and, in combination with any WS composition tool, chooses compositions to deploy that are deemed most trustworthy."
1816,"The existing service discovery infrastructure with UDDI as the de facto standard, is limited in that it does not support more complex searching based on matching business processes. Two business processes match if they agree on their simple services, their processing order as well as any mandatory or optional requirements for the service. This matching semantics can be formalized by modelling business processes as annotated finite state automata (aFSAs) and deciding emptiness of the intersection aFSA. Computing the intersection of aFSAs and deciding emptiness are computationally expensive, being more than quadratic on the number of states and transitions, thus does not scale for large service repositories. This paper presents an approach for indexing and matching business processes modeled as aFSAs, for the purpose of service discovery. Evaluation of this approach shows a performance gain of several orders of magnitude over sequential matching and a linear complexity with regard to the data set size"
1817,"We present DIA, a Web services-based infrastructure for the Discovery, Integration, and Analysis of geoscience data, tools, and services. DIA provides a collaborative environment where scientists can share their resources (e.g., geochemical data, filtering services, etc.) by registering them through well-defined ontologies. We have developed a planetary materials ontology in OWL for this purpose. The ontology is used by different geoscientists (using Web services) to explore, extract, and integrate information from different heterogeneous data sets. The DIA system is now in its final pre-release phase. It is currently made accessible to a few geoscientists for conducting usability analyses, and it will eventually be made available to the community at large through the geoscience portal (GEON) at the San Diego Supercomputer Center."
1819,"In the service-oriented architecture, the components deployed on application servers are published as Web services. Though many researches focus on how to authorize at the Web service level currently, there is little work involving the authorization gap between the service and its component implementation. This paper tries to bridge the gap by proposing a service-oriented trust management model, which expands the application server's capability to deal with more complex trust relationship between service users and services, and supplies a flexible trust management mechanism to integrate authentication and authorization together. Moreover, the model provides a finer granularity access control, sustains delegation between users, and has a certain extent reasoning capability. The model has been implemented in a J2EE application server, and the experiment has demonstrated that the model has high flexibility and scalability"
1820,"Business protocols are becoming a necessary part of Web services description [4]. The work presented in [4] investigates mechanisms for analyzing the compatibility and the substitution (i.e., replaceability) of Web services based on their functional properties. In this paper, we focus on the replaceability analysis. Whether a service can replace another depends not only on their functional properties but also on non functional requirements (e.g., privacy policies). We propose a privacy-aware protocol replaceability approach to extend the work presented in [4] by privacy properties. We introduce a rule-based privacy model and we extend business protocols, leading to what we call private business protocols. Finally, a private replaceability analysis of private business protocols is discussed. We mainly investigate compatibility issues, that is whether one private business protocol can support the same set of conversations with respect to the privacy requirements."
1821,"Unification and automation of RESTful web services' documentation and descriptions is currently receiving increasing attention. The open-source OpenAPI Specification (formerly known as Swagger) has become core of this effort and has been adopted by a number of major companies. It allows the description of RESTful web services using objects represented in JSON or YAML file formats. As a result, the created descriptions are human and machine-readable, but not machine-understandable. In this paper, we propose a nonintrusive approach for the addition of semantic annotations (similar to RDFa and JSON-LD for HTML) to specific fields of the OpenAPI Specification. We created a lightweight vocabulary for describing RESTful web services using this specification. Furthermore, we practically demonstrate how OpenAPI objects can be enriched with semantic descriptions in a minimally invasive way by adding URIs in the values of chosen OpenAPI properties."
1822,"A service platform, ideally, should provide automation features for users to achieve their tasks by minimum human's work. Although web services themselves could automate some of the human work, existing systems tend to lack such an automation point of view in their designs. Furthermore, scalability, availability, reusability and customizability should be provided altogether for a platform to be practically useful. We propose an architecture which ultimately automates everything in using services as much as possible. Exploiting the increasing number of freely available software, our Kachako platform transfers services themselves in a data-centric way to arbitrary servers that users wish to use. While Kachako is primary provided as a large integrated system with graphical user interfaces, Kachako is designed in a modular way to adapt various demands of users by allowing partial reuse of modules. End users can use Kachako without any programming work, while programmers can customize the system or add their own services very easily."
1823,"Web services are required to handle non-functional concerns. Such concerns can be divided to common features (security, reliability, manageability, etc.) and business specific requirements. Many of the common features are handled by toolkits such as Microsoft Web services enhancements, IBM emerging technologies toolkit and apache axis. The two main problems this paper aims to solve is the inability of current toolkits to apply a feature to multiple Web services in a single process and the fact that current toolkits require developer's knowledge to apply features to Web services. As a solution, this paper proposes an aspect oriented approach for applying features to Web services. The aspect oriented approach presented by this paper should make the process of applying features to Web services less resource consuming and more flexible than the approach taken by current toolkits."
1824,"Web service has already been an important paradigm for web applications. Growing number of services need efficiently locating the desired web services. The similarity metric of web services plays important role in service search and classification. The very small text fragments in WSDL of web services are unsuitable for applying the traditional IR techniques. We describe our approach which supports the similarity search and classification of service operations. The approach firstly employs the external knowledge to compute the semantic distance of terms from two compared services. The similarity of services is measured upon these distances. Previous researches treat terms within the same WSDL documents as the isolated words and neglect the semantic association among them, hence lower down the accuracy of the similarity metric. We provide our method which tries to reflect the underlying semantics of web services by utilizing the terms within WSDL fully. The experiments show that our method works well on both service classification and query."
1825,"The Service registry, the yellow pages of Service-Oriented Architecture (SOA), plays a central role in SOA-based service systems. The service registry has to be scalable to manage large number of services along with their requirements on storage and discovery. Based on our previous work on feature-based services quantification, we characterize services according to their diverse functional and non-functional requirements, and represent them as string formats which can be stored, probed, and indexed by efficient data structures, such as hash table and Bloom filter. Then, we propose a comprehensive service-storage solution using the counting Bloom filter (CBF). The application of CBF enables us to structure candidate services into separate groups, resulting in an accelerated services discovery process. The contributions of this research work include a new approach to manage large number of services based on quantified service features, and a storage architecture design to support service discovery. Experimental results strongly support these claims."
1826,"For period of time customers have demand for more reusable and manageable service-oriented components for order-to-cash (O2C) solution so they can be easily reconfigured and managed to adapt to business changes quickly. In this paper, we present a service-oriented business process optimization model that minimizes potential revenue leakage through process improvements. We introduce service componentization approach to decompose business processes to identify reusable services in SOA solution context. We adopt Really-Simple-Syndication (RSS) technology to realize a collaborative dispute management solution based on service-oriented architecture (SOA). The proposed approach can help streamline the dispute management process with revenue increase and higher customer satisfaction"
1827,In this work we construct partial order plans from a pool of atomic services described in OWL-S. We make extensions to Partial Order Planning to allow multiple conditional effects in action definitions. The purpose is to handle the uncertain behavior of Web services with incomplete initial information. We post-process the partial order plan to auto-generate a workflow model. We developed a method to identify a subset of workflow patterns from the solution plan to create a workflow diagram.
1828,"As the rapid development of Internet of things, a large number of devices, instruments and sensors will connect and collaborate to achieve Smart Planet. One of the key challenges is to integrate devices into business process. SOA is an ideal infrastructure for business process management as applications are invoked using standard interfaces and protocols. It's convenient to use device-oriented web services (doWS) to encapsulate devices functions. A doWS may conflict with other doWS because devices can't be controlled by more than one client at the same time. This brings additional complexity to web service composition. In this paper, we propose a comprehensive device collaboration model to doWS choreography. This model includes device layer, doWS layer, compatible resource layer and process layer, where device layer represent devices, doWS layer encapsulates devices functions, compatible resource layer abstracts compatible device operations as resource and avoid of conflict, process layer describes the collaboration process. Analysis shows that, this model can integrate devices with Web Services, and can achieve the conflict detection of doWS. This model is effective in the Internet of things."
1829,"Traditional Web services are function oriented, where various developed standards mainly assist business applications to expose their functional descriptions, but each service consumer is required to develop different presentation logics for the same business logic respectively. Although works on the interactive Web service (IWS) make some progress and achieve the result of WSRP, researchers keep on making efforts to encapsulate user interface with the functional interface so as to present available information of content and facilitate the direct interactions between users and back-end services. Regarding IWS this paradigm of Web services, service selection shifts focus from function orientation to presentation orientation. This paper proposes a novel IWS description model with the extension of an element-view and its three sub-elements including presentation, content and interaction, with which IWS can be described more completely and accurately. Based on the description model, an IWS selection model with matching rules is developed, which can meet diverse selection requirements of service consumers at multiple levels and aspects"
1831,"Efficient query processing over a large amount of business process models is important for managing the business process model repository. The structural similarity between two process models is considered as the main measurement for ranking the process models for a given search model. Current business process query methods are inefficient since too many expensive computations of the graph edit distance are involved for constructing the elements mapping as well as deriving the structural similarity. To address this, using Petri-net as the modelling method, this paper presents the Hungarian algorithm based query method, where we firstly define the context similarity for a pair of place nodes that are from different process models by taking into account both the common paths and common transitions, then transform the elements (e.g., The transitions and the places) mapping to classical assignment problem that can be solved by Hungarian algorithm efficiently. In this way, we can save a lot of time for searching the best combination of elements mapping. Finally, we use the common method of the graph edit distance to measure the structural similarity based on the found best combination of elements mapping."
1832,"NoSQL systems are deployed as the core components for delivering big data Web services today. With growing main memory capacity, we witness the growing interest and deployment of in-memory NoSQL services (IM-NoSQL), which are designed to maximize the utilization of DRAM for ultra low latency services. To address the volatility of DRAM for in-memory computing services, persistence and failure recovery are important functionality for IM-NoSQL. In this paper we report an extensive measurement study on the performance of persistence and recovery for IM-NoSQL. We evaluate the performance and effectiveness of several common mechanisms used for persistence and recovery in the presence of server crashes, such as snapshot and logging based approaches. Through this study, we are able to answer some of the most frequently asked questions in provisioning of IM-NoSQL services: (i) Can an IM-NoSQL system work effectively when the available memory is insufficient to load the whole dataset? (ii) What is the overhead of maintaining snapshot compared to logging? (iii) How fast an IM-NoSQL system can recover in the presence of failure? And (iv) how does an IM-NoSQL system respond to the different persistence models? We report our comprehensive measurement results on execution, persistence and recovery performance of Redis, a representative implementation of IM-NoSQL services."
1833,"This paper presents a table-driven streaming XML parsing methodology, called TDX. TDX expedites XML parsing by pre-recording the states of an XML parser in tabular form and by utilizing an efficient runtime streaming parsing engine based on a push-down automaton. The parsing tables are automatically produced from the XML schemas of a WSDL service description. Because the schema constraints are pre-encoded in a parsing table, the approach effectively implements a schema-specific XML parsing technique that combines parsing and validation into a single pass. This significantly increases the performance of XML Web services, which results in better response time and may reduce the impact of the flash-crowd effect. To implement TDX, we developed a parser construction toolkit to automatically construct parsers in C code from WSDLs and XML schemas. We applied the toolkit to an example Web services application and measured the raw performance compared to popular high-performance parsers written in C/C++, such as eXpat, gSOAP, and Xerces. The performance results show that TDX can be an order of magnitude faster"
1835,"Design defects are symptoms of poor design and implementation solutions adopted by developers during the development of their software systems. While the research community devoted a lot of effort to studying and devising approaches for detecting the traditional design defects in object-oriented (OO) applications, little knowledge and support is available for an emerging category of Web service interface design defects. Indeed, it has been shown that service designers and developers tend to pay little attention to their service interfaces design. Such design defects can be subjectively interpreted and hence detected in different ways. In this paper, we propose a novel approach, named WS3D, using machine learning techniques that combines Support Vector Machine (SVM) and Simulated Annealing (SA) to learn from real world examples of service design defects. WS3D has been empirically evaluated on a benchmark of Web services from 14 different application domains. We compared WS3D with the state-of-theart approaches which rely on traditional declarative techniques to detect service design defects by combining metrics and threshold values. Results show that WS3D outperforms the the compared approaches in terms of accuracy with a precision and recall scores of 91% and 94%, respectively."
1837,"This paper proposes a multi-request cooperative-integrating mechanism to optimize concurrent multi-applications in service-oriented wireless sensor networks (WSNs). Specifically, a sensor node is encapsulated as one or multiple WSN services, which can be categorized into service classes. A service network is constructed by considering the invocation relationship between service classes. Candidate service class chains are recommended. These service classes chains will be instantiated by available WSN services, which can be reduced to a multi-objective and multi-constraint optimization problem, where the spatial-and temporal-constraints, and energy efficiency of the network, are taken into consideration. This combinational optimization problem is solved by adopting heuristic algorithms. Experimental results show that this technique improves the shareability and energy efficiency for supporting concurrent applications."
1838,"This paper addresses the problem of querying Knowledge bases (KBs) that store semantic big data. For efficiently querying data the most important factor is cache replacement policy, which determines the overall query response. As cache is limited in size, less frequently accessed data should be removed to provide more space to hot triples (frequently accessed). So, to achieve a similar performance to RDBMS, we proposed an Adaptive Cache Replacement (ACR) policy that predict the hot triples from query log. Moreover, performance bottleneck of triplestore, makes realworld application difficult. To achieve a closer performance similar to RDBMS, we have proposed an Adaptive Cache Replacement (ACR) policy that predict the hot triples from query log. Our proposed algorithm effectively replaces cache with high accuracy. To implement cache replacement policy, we have applied exponential smoothing, a forecast method, to collect most frequently accessed triples. The evaluation result shows that the proposed scheme outperforms the existing cache replacement policies, such as LRU (least recently used) and LFU (least frequently used), in terms of higher hit rates and less time overhead."
1840,"Distributed Denial of Service (DDoS) attacks are still among the most urgent threats to the modern Internet. Recently, application layer DDoS attacks against web servers are becoming popular, resulting in great revenue losses to victims. A systematic evaluation on the impacts of different DDoS attack methods is vital for the protection of web servers. In this paper, we examine the impacts of application layer DDoS attacks, including existing attacks against HTTP/1.1 and the new attacks proposed by us against HTTP/2.0. Moreover, to better understand attackers' capabilities of launching severe application layer DDoS attacks, we design a new measurement method to remotely infer the performance of web servers and a method to differentiate dynamic and static URLs. We have collected and tailored 4 existing tools to launch 5 different DDoS attacks against HTTP/1.1 and developed a new DDoS tool to perform 5 different DDoS attacks against HTTP/2.0. By conducting extensive experiments in a testbed with two e-commercial websites running Apache and Nginx, we carefully evaluate the impacts of different DDoS attacks. The results show that the new remote measurement method is able to detect the effects caused by different DDoS attacks. Moreover, the attack impacts are affected by URLs, server architectures, and attack methods."
1841,"The increasing number of mobile services makes users confused to select appropriate services among plenty of service icons or links. Current developers always choose to recommend recently or mostly used services to users, but these approaches neglect the relations between user states and environment information and invocations, and the recommendation results will not be accurate when the mobile services are invoked evenly. In this paper, we propose a novel approach to recommend services on mobile devices to user. Firstly, we design a user behavior model by taking advantage of user's mobile context information like time and location to describe the user states. Secondly, we design a generate model to explain how the sequential service invocations are generated by analyzing the collected sequential history record of mobile users. Thirdly, we adopt logistic model tree approach to determine user state according to given mobile context information, and recommend services to user according to his user state. The experiment results show that our approach performs better than baseline approaches."
1843,"This paper proposes a novel context-aware cloud service selection model based on the comparison and aggregation of subjective assessment extracted from cloud user feedback and objective assessment from quantitative performance testing. In this model, objective assessment provided by some professional testing parties is used as a benchmark to filter out potentially biased subjective assessment from cloud users, then objective assessment and subjective assessment are aggregated to evaluate the overall performance of cloud services according to potential cloud users' personalized requests. Moreover, our model takes the contexts of objective assessment and subjective assessment into account. By calculating the similarity between different contexts, the benchmark level of objective assessment is dynamically adjusted according to context similarity, which makes the following comparison and aggregation process more accurate and effective. After aggregation, the final results can quantitatively reflect the overall quality of cloud services. Finally, our proposed model is evaluated through the experiments executed in different conditions."
1844,"To meet the demand for rapid Web-service development, developers seek to efficiently consume existing data from multiple sources into a form that can be used by the new service. In this paper, we discuss our SAKURA prototype in which we created two services to facilitate such data consumption. The first, a “resource-aggregation” service, gives developers a “composed object” abstraction across multiple Web-services. The second, a “business object” caching service, improves the performance of accessing existing Web-services from the new application. We describe these two services and illustrate their benefits in the context of an extended scenario."
1845,"With an overwhelming number of web services online, recommending services for automatic mashup creation greatly facilitates the composition process of developers. Various approaches have been proposed for the task. However, these approaches concentrate on improving the recommending accuracy of an individual service, which give rise to two problems: (1) Top-ranked services may be highly redundant with the same functionality, and (2) The cooperation relations among services are ignored. Therefore, we argue that services should be recommended not individually, but collectively. In this paper, we focus on the problem of recommending service sets instead of services. A service set contains a list of functionally distinct services that collectively match different aspects of functional requirements and are more inclined to compose together following mashup composition patterns. To this end, we propose a novel recommendation framework consisting of two stages: Service Set Generation Stage and Service Set Ranking Stage. We also perform an experimental evaluation on ProgrammableWeb dataset to demonstrate the effectiveness of our framework."
1847,"The use of Social Media for event detection, such as detection of natural disasters, has gained a booming interest from research community as Social Media has become an immensely important source of real-time information. However, it poses a number of challenges with respect to high volume, noisy information and lack of geo-tagged data. Extraction of high quality information (e.g., Accurate locations of events) while maintaining good performance (e.g., Low latency) are the major problems. In this paper, we propose two approaches for tackling these issues: an augmented Explicit Semantic Analysis approach for rapid classification and a composition of clustering algorithms for location estimation. Our experiments demonstrate over 98% in precision, recall and F-measure when classifying Social Media data while producing a 20% improvement in location estimation due to clustering composition approach. We implement these approaches as part of the landslide detection service LITMUS, which is live and openly accessible for continued evaluation and use."
1848,"This paper addresses issues of strategy in Web-service composition, and inter-site collaboration in general. The results are useful for both human and artificial agents of a site who are responsible for determining Web-service agreements that are most profitable for that site. We discuss three specific questions in this area. (1) How should the profit resulting from a composition of Web-services be divided among the participants? (2) How can we counter the risk of service-providers misrepresenting their services in an attempt to gain a larger share of the profit? (3) Are stable configurations guaranteed or feasible when each service-provider's decisions on how to collaborate (permit compositions) are guided solely by the goal of maximizing its profit?"
1849,"The business process modelling notation (BPMN) is a graph-oriented language in which control and action nodes can be connected almost arbitrarily. It is supported by various modelling tools but so far no systems can directly execute BPMN models. The business process execution language for Web services (BPEL) on the other hand is a mainly block-structured language supported by several execution platforms. In the current setting, mapping BPMN models to BPEL code is a necessary step towards unified and standards-based business process development environments. It turns out that this mapping is challenging from a scientific viewpoint as BPMN and BPEL represent two fundamentally different classes of languages. Existing methods for mapping BPMN to BPEL impose limitations on the structure of the source model. This paper proposes a technique that overcomes these limitations. Beyond its direct relevance in the context of BPMN and BPEL, this technique addresses difficult problems that arise generally when translating between flow-based languages with parallelism"
1850,"Web services have been common technologies for providing services over the Internet. In the future, it is expected that a usage model that autonomously integrates services will become ordinary in, for example, B2B integration. However, such method of executing pre-defined process lacks flexibility because a service requestor must know in advance the interface of all integrated services. Accordingly, there is a need for a method that composes a process from the requestor's requirement independent of the interface of services. This paper proposes the architecture and a method for composing a process automatically. Our approach is formulating Web services function and a requestor's requirement as the transitions of his state by means of situation calculus and calculating what process is matched with the requirement. In our method, we model message-oriented execution of service. Moreover, our method is capable of composing not just a sequential process but also a process including conditional choices. Therefore, our method has the basic feature to be able to compose nondeterministic Web service."
1851,"This paper addresses role-based decomposition of a business process model (based on a subset of WS-BPEL, using explicit data links. A mechanism is presented for partitioning a business process so that each partition can be enacted by a different participant. An important goal is to disconnect the partitioning itself from the design of the business process, simplifying the reassignment of activities to different entities. The result is several (compliant) BPEL processes, one for each participant, as well as the information needed to wire them together at deployment time and ensuring correct instance-level connections at runtime. We present details of partitioning and successfully running a sample process with three participants"
1852,"Web service is subject to frequent changes during its lifecycle. Web service evolution is a widely discussed topic. Many related problems have also been generated from Web service evolution such as Web service adaptation, Web service versioning and Web service change management. To treat with these issues efficiently, a complete evolution model for Web service should be built. In this paper, we introduce our change-centric model for Web service evolution and how we use it to design, execute, and adapt to the changes during Web service evolution."
1853,"Service computing technologies have been widely applied to many application domains to facilitate rapid system integration for desired goals. However, existing service models need to be enhanced for cyber-physical systems (CPS) and internet-of-things (IoT). In this paper, we develop an ontology model for the specification of services in CPS/IoT. First, we discuss the major differences in modeling software services and services in CPS/IoT. Then, we propose a novel PT-SOA (PT stands for physical things) model, which is mainly extended from OWL-S, to enhance existing service models for CPS/IoT systems. Finally, we show a case study system to illustrate how our model can facilitate proper service selection and composition."
1854,"Service-Oriented Architecture enables the composition of loosely coupled services provided with varying Quality of Service (QoS) levels. Given a composition, finding the set of services that optimizes some QoS attributes under given QoS constraints has been shown to be NP-hard. Therefore, heuristic algorithms are widely used, finding acceptable solutions in polynomial time. Still the time complexity of such algorithms can be prohibitive for real-time use, especially if the algorithms are required to run until they find near-optimal solutions. Thus, we propose a heuristic approach based on Hill-Climbing that makes effective use of an initial bias computed with Linear Programming, and works on a reduced search space. In our evaluation, we show that our approach finds near-optimal solutions and achieves a low time complexity."
1855,"The growth of the Internet has been accompanied by the growth of e-services (e.g. e-commerce, e-health). This proliferation of e-services and the increasing attacks on them by malicious individuals have highlighted the need for e-service security. The security requirements of an e-service may be specified in an e-service security policy. The provider of the e-service is then responsible for implementing the security measures contained in the policy. However, a service consumer may have security preferences that are not reflected in the provider's e-service security policy (e.g. defense contractors may require higher levels of security). In order for service providers to reach a wider market, a way of customizing a security policy to a particular consumer is needed. We derive the content of an e-service security policy and propose a flexible approach that allows an e-service provider and consumer to negotiate to an agreed-upon e-service security policy. In addition, we examine how our approach may be implemented in a Web services environment and briefly describe the design of our security policy negotiation prototype."
1856,"Driven by developments such as mobile computing, cloud computing infrastructure, DevOps and elastic computing, the microservice architectural style has emerged as a new alternative to the monolithic style for designing large software systems. Monolithic legacy applications in industry undergo a migration to microservice-oriented architectures. A key challenge in this context is the extraction of microservices from existing monolithic code bases. While informal migration patterns and techniques exist, there is a lack of formal models and automated support tools in that area. This paper tackles that challenge by presenting a formal microservice extraction model to allow algorithmic recommendation of microservice candidates in a refactoring and migration scenario. The formal model is implemented in a web-based prototype. A performance evaluation demonstrates that the presented approach provides adequate performance. The recommendation quality is evaluated quantitatively by custom microservice-specific metrics. The results show that the produced microservice candidates lower the average development team size down to half of the original size or lower. Furthermore, the size of recommended microservice conforms with microservice sizing reported by empirical surveys and the domain-specific redundancy among different microservices is kept at a low rate."
1857,"The increasing use of the Web for everyday tasks is making Web services an essential part of the Internet customer's daily life. Users query the Internet for a required Web service and get back a set of Web services that may or may not satisfy their request. To get the most relevant Web services that fulfill the user's request, the user has to construct the request using the keywords that best describe the user's objective and match correctly with the Web Service name or location. Clustering Web services based on function similarities would greatly boost the ability of Web services search engines to retrieve the most relevant Web services. This paper proposes a novel technique to mine Web Service Description Language (WSDL) documents and cluster them into functionally similar Web service groups. The application of our approach to real Web services description files has shown good performance for clustering Web services based on function similarity, as a predecessor step to retrieving the relevant Web services for a user request by search engines."
1858,"Web services orchestrations and choreographies require establishing quality of service (QoS) contracts with the user. This is achieved by performing QoS composition, based on contracts established between the orchestration and the called Web services. These contracts are typically stated in the form of hard guarantees (e.g., response time always less than 5 msec). In this paper we propose using soft contracts instead. Soft contracts are characterized by means of probability distributions for QoS parameters. We show how to compose such contracts, to yield a global contract (probabilistic) for the orchestration. Our approach is implemented by the TOrQuE tool. Experiments on TOrQuE show that overly pessimistic contracts can be avoided and significant room for safe overbooking exists."
1859,"Most of trustworthy web service selection simply focus on individual reputation and ignore the collaboration reputation between services. To enhance the collaboration trust during web service selection, a reputation model called collaboration reputation is proposed. The reputation model is built on web service collaboration network(WSCN), which is constructed in terms of the composite service execution log. Thus, the WSCN aims to maintain the trustworthy collaboration alliance among web services, In WSCN, the collaboration reputation can be assessed by two metrics, one called invoking reputation is computed by recommendation, which is selected from the community structure hiding in WSCN, the other is assessed by the invoked web service. In addition, the web service selection based on WSCN is designed."
1860,"In this paper, we present an approach to enable access control for semantic Web services. Our approach builds on the idea of autonomous granting of access rights, decision making based on independent trust structures and respects privacy requirements of the users. Our framework allows the specification and computation of complex access control policies in a manageable and efficient way. Therefore, our approach is useful not only in Web services based applications (typically client-server architecture) but also in peer-to-peer and agent-based applications."
1861,"The capability to easily find useful services (software applications, software components, scientific computations) becomes increasingly critical in several fields. Current approaches for services retrieval are mostly limited to the matching of their inputs/outputs. Recent works have demonstrated that this approach is not sufficient to discover relevant components. In this paper we argue that, in many situations, the service discovery should be based on the specification of service behavior (in particular, the conversation protocol). The idea behind is to develop matching techniques that operate on behavior models and allow delivery of partial matches and evaluation of semantic distance between these matches and the user requirements. Consequently, even if a service satisfying exactly the user requirements does not exist, the most similar ones will be retrieved and proposed for reuse by extension or modification. To do so, we reduce the problem of behavioral matching to a graph matching problem and we adapt existing algorithms for this purpose. A prototype is presented (available as a Web service) which takes as input two conversation protocols and evaluates the semantic distance between them; the prototype provides also the script of edit operations that can be used to alter the first model to render it identical with the second one"
1862,"A workflow can be represented as a set of Web services and a specification for the control and data flows among these services. It can also be represented as a colored Petri net (CPN), which is a graphical and mathematical modeling tool. In multiagent systems (MAS), a workflow is a dynamic set of tasks performed by a set of agents to reach a shared goal. We show herein that commitments among agents can be used to model a workflow and coordinate their execution of it. This paper presents methodologies to map an OWLS model for a workflow to a CPN, and then to infer commitments and causal relationships from the CPN graph. With our methodologies, agents can collaboratively enact a workflow through commitment-based formalisms"
1863,"The real value of Web services under the SOA paradigm lies in their ability to be assembled to obtain a new functionality. Assembling Web services can be achieved through a standard called BPEL, which creates executable processes by orchestrating Web service invocations. The problem with BPEL is the inability to separate the process description from its realization. In other words, it requires a prior retrieval of concrete Web services, which can be very challenging regarding the issues surrounding service discovery and selection. In this paper, we propose to separate a BPEL process description from its realization. We extend the notion of abstract BPEL processes, in order to enable developers to describe their desired orchestrations abstractly without identifying concrete services, according to three levels: the needed functionality, the expected QoS levels, and the composition flow. Then, the abstract BPEL process is realized by a selection framework that automatically discovers, classifies, and selects suitable services to render the process executable. Backup services are also discovered to assure the continuity of the realized process."
1864,"Service-centric systems exist in a very dynamic environment, which requires these systems to adapt at runtime in order to keep fulfilling its QoS requirements to its users. In order to create self-adaptive service-centric systems, developers not only design the service architecture, but need to design the self-adaptability aspects in a structured way. Current service standards and technologies do not provide a general architecture in which all aspects for self-adaptability can be designed. In this paper we propose DySOA (dynamic service-oriented architecture), an architecture that extends service-centric applications to make them self-adaptive. DySOA allows developers to explicitly model the process and components that deal with determining the QoS of the running system, with evaluating the QoS, and with reconfiguring the system when necessary. Having the DySOA elements explicit enables separation of concerns, making the DySOA elements adaptable at runtime and reusable in next versions. We demonstrate how to use DySOA with an example."
1865,"The Web services enable interaction among remote and diverse applications running on disparate platforms. While the service providers offer generic service response to meet the requirements of diverse client applications, it is the responsibility of the application to extract relevant data from the response. This causes performance overheads for power and network bandwidth limited pervasive devices due to networking and parsing excess data. It is thus important for services and pervasive applications to interact in a way that optimally utilizes the resources of the pervasive device. In this paper we describe an efficient interaction methodology between Web services and pervasive applications that optimizes the service quality for the device. We extend the quality of service definition beyond service response time to include power and airtime utilization and allow pervasive applications to be aware about it to dynamically adapt to the services and the available resources of the device in realtime."
1866,"Over the past decade, classic client side applications with model-view-controller (MVC) architecture haven't changed much but become more complex. In this paper, we present an approach of building desktop applications with Web Services in an explicit message-based MVC paradigm. By integrating with our publish/subscribe messaging middleware, it makes SVG browser (a Microsoft PowerPoint like client application) with Web service style interfaces universally accessible from different client platforms - Windows, Linux, MacOS, PalmOS and other customized ones. Performance data suggests that this scheme of building application around messages is a practical architecture for the next generation Web application client."
1867,Provides a listing of current society officers.
1868,"Recently, mashups are gaining tremendous popularity as an important Web 2.0 application. Mashups provide end-users with an opportunity to create personalized Web services which aggregate and manipulate data from multiple diverse sources distributed across the Web. However, this increase in personalization also results in new scalability and performance challenges. Surprisingly, there are very few studies on the performance aspect of mashups. In this paper, we propose two novel techniques to enhance the scalability and performance of mashup platforms. The first is an efficient mashup merging scheme that avoids duplicate computations and unnecessary data retrievals by detecting common operator sequences in different mashups and executing them together. Second, we propose a canonical form-based mashup reordering scheme that not only transforms individual mashups to their most efficient forms but also increases the effectiveness of mashup merging. This paper also reports a number of experiments studying the benefits and costs of the proposed techniques."
1869,"In recent research it turned out that Boolean verification of digital signatures in the context of WS-Security is likely to fail: If parts of a SOAP message are signed and the signature verification applied to the whole document returns true, then nevertheless the document may have been significantly altered.In this paper, we provide a detailed analysis on the possible scenarios that enable these signature wrapping attacks. Derived from this analysis, we propose a new solution that uses a subset of XPath instead of ID attributes to point to the signed subtree, and show that this solution is both efficient and secure."
1870,"In this paper, we suggest a simple but efficient solution for the Web service composition. When we search Web services for composition, we visit the service which gives the biggest number of new responses, because then there is a higher probability to invoke more other Web services. And also we suggest optimization techniques to get optimized composition result. Optimization processes consist of two phases: one is to remove unnecessary Web services and the other is to find the best starting point of a composition. Test results show that the proposed composing and optimization methods can compose Web services in optimal length in fast way."
1871,"We have developed an application service hosting middleware for enterprise information systems, ""Polimatica"", upon which a policy-customizable private virtual organization (pVO) is built. The pVO is a collection of customizable application services in the global grid computing environment. ""Polimatica"" is distinctive in terms of abstraction capabilities in (1) policy instance description, (2) view synthesis of an extensive resource data model, (3) operation aggregation on the OGSI extension interfaces. The performance to enforce the policy instances is measured in a laboratory testbed for an enterprise ASP, providing B2B e-commerce (BizEngine/WS) and interactive media communication service (LiveComm). We have verified that, in 60 seconds, the feedback loop in a pVO adjusts the application services as customized given the changing environment."
1872,"In this paper, we propose an automatic, self-adaptive trust model for component-based service-oriented architecture that is based on Bayesian model. The focus of this model is the ""quantitative trust on action"". Both the computation model and its updating mechanism are given. Through our implementation of this model using Monte-Carlo algorithm, we show the feasibility and practicability of our proposal as well as its ability to rectify any unfair advertisement and to track the capability of highly dynamic service"
1873,"Web services are rapidly emerging as a popular standard technology for sharing data and functionality among heterogeneous systems. Service providers and consumers are loosely coupled and distributed across the network, either within an organization or across organizational boundaries, and therefore, performance becomes a major concern in such a distributed environment. Furthermore, XML is widely used as message format for service providers and consumers in Web services environment. XML message packaging and parsing brings extra overhead to both ends. Web services response latency, as well as throughput, is becoming a bottleneck problem. In this paper, We propose a consistency-preserving mechanism for Web services response caching, which reduces the volume of data transmitted without semantic interpretation of service requests or responses, and accelerates the services response finally. It achieves this reduction through the use of cryptographic hashing to detect similarities with previous results. Experiments with an initial prototype called SigsitAcclerator indicate that our mechanism can lead to significant performance improvement over more straightforward techniques."
1874,"Chip multi-processors (CMPs), commonly referred to as multi-core processors, are being widely adopted for deployment as part of the grid infrastructure. In CMPs, multiple cores can independently execute different threads. This change in computer architecture requires corresponding design modifications in programming paradigms, including grid middleware tools, to harness the opportunities presented by multi- core processors. Simple and naive implementations of grid middleware on multi-core systems can severely impact performance. The goal of developing an optimized multi-threaded grid middleware for emerging multi-core processors will be realized only if researchers and developers have access to an in-depth analysis of the impact of several low level microarchitectural parameters on performance. None of the current grid simulators and emulators provides feedback at the micro-architectural level. We have designed an emulation framework, Multi-core Grid (McGrid), to analyze and provide insightful feedback on the performance limitations, bottlenecks, and optimization opportunities for grid middleware on multi-core systems."
1875,"The hybrid semantic Web service matchmaker WSMOMX applies different matching filters to retrieve WSML service descriptions that are semantically relevant to a given query with respect to seven degrees of hybrid matching. These degrees are recursively computed by aggregated valuations of ontology-based type matching, logical constraint and relation matching, and syntactic similarity as well. In this paper, we provide results of our experimental evaluation of the performance of WSMO-MX. In summary, it turned out that hybrid semantic matching of WSML-MX services can outperform logic-based only semantic service matching."
1876,"A Web service interface is considered as a contract between Web service providers and their subscribers. The subscribers do not have access to the source code of the services but only to the interface containing a set of operations. However, the interface may change over time to meet new requirements. These changes affect the implementation of the subscribers' software. Thus, these clients need to understand the changes introduced to the previous releases of the Web services to co-evolve their own implementation to support the new release. Current studies are limited to the detection of only atomic changes (e.g. add and delete) and not able to detect complex/composite refactorings (merge operations, extract operation, etc.). In this paper, we propose to consider structural and textual similarities, based on a genetic algorithm, when analyzing the evolution of Web services to detect complex changes applied between multiple releases. The validation of our detection technique, on more than 110 releases of 6 real-world Web services, shows an average precision and recall respectively higher than 86% and 89%."
1877,"Reselection of composition service is one of the core research issues in the service computing field. Most of the existing researches for this problem are based on the assumption that the tasks involved are independent. However, in practical scenar-ios, the QoS of some candidate services have correlations with other services, which makes the corresponding tasks be correlated with each other. This leads the QoS used to determine the binding relationship between tasks and concrete services to be inaccurate, so the reselected composite service is not the optimal one in the real executing environment for these existing reselection methods. To address this problem, this paper considers task correlations for runtime rebinding. Firstly, the QoS dependencies among services are extracted from the log repository through the APRIORI data mining method. Then, the acquired QoS dependencies are mapped to the tasks correlations by the defined mapping rules. Finally, the reselection process is implemented by making the tasks which have related relationships as a task unit, and the related services of each task unit as its candidate service set. The effectiveness of this approach, in terms of time and quality, is demonstrated via experiments."
1878,"Web services have been emerging as a promising technology for business process integrations. Due to their long-duration and loosely coupled properties, Web service based applications require transactional support beyond traditional transactions. Some Web service standards have been proposed to deal with the transaction aspect of Web service applications. Compensation is a commonly used mechanism in these standards for backward recovery. However, the compensation mechanism usually adopted is too fixed and cannot satisfy the various requirements of different applications. In this paper, we first analyse the compensation protocol of current standards. Then we enrich the protocol by allowing flexible compensation and extend our proposed multiple-compensation mechanism in Web service environment. The implementation of the extended compensation mechanism is discussed and the incorporation of the mechanism into current standards is also addressed"
1879,"This paper presents the discovery engine implemented for the INFRAWEBS project, which combines a traditional IR-based pre-filtering step and a logic-based matching implemented in Prolog. The logic-based step of discovery uses a novel technique based on Prolog-style unification of terms. This approach performs well in finding matches of intersection type, and it also provides possibilities to compare, rank and explain these matches. Furthermore, it can support matching and ranking based on users' preferences compared to the added value offered by services."
1880,"In this paper we describe a scalable service for customized data validation and content assistance by means of domain-independent, user-provided sets of complex data constraints. We present an integrated architecture and a particular implementation that combines the use of existing grammar- and rule-based schema languages that allows providers to specify rules in a declarative manner. The integrated architecture provides a way to semi-automatically fill in form fields by calling the proposed service, which enumerates domains from the previously stored data validation constraints. Additionally, better error reporting can be achieved by leveraging structure from the rules' definitions. The proposed architecture has been shown to be practical and is in production use by a large organization, successfully fulfilling its role."
1881,"The composition of multiple services that are deployed on smart devices generally incurs significant communication overheads, especially when an optimized composition is pursued. A parallel approach for service composition is proposed for obtaining the optimal solution with minimum executing time efficiently. In this approach, the request for a composite service is represented by a function graph which is decomposed into multiple path-structured sub graphs firstly. Then messages are sent among the service nodes to search for the corresponding sub-solutions. Finally, a Branch-and-Bound strategy is applied to generate the optimal solution over these sub-solutions. The experiments show our optimization strategies can reduce the time and communication cost considerably under different experimental settings."
1882,"When a passenger is going to book an airline flight, after he inputs his query conditions, he has to search from a long flight list to find the one that best matchs his preferences. Therefore, a friendly personalized flight recommender system is a necessary tool for online travel service companies. However, the majority of current approaches do not work well since in most cases, there is no enough historical order information on the route the customers may plan to take. In this paper, we propose an approach for building a Personalize Flight recommendation Service (PFS). In PFS, the attributes' information of flight orders is organized into several domains, and a tensor factorization method is used to recommend flights to users. Specifically, PFS applies a history-based weighting strategy to better model users' biases. PFS is also implemented in a distributed and parallel way to increase its execution speed when dealing with a huge amount of flight data. Extensive experiments on both a real flight order dataset and a synthetic large-scale dataset show the superiority of our model against other comparative ones."
1883,"With the rapid adoption of Service Oriented Architecture (SOA), increasingly more application-level services are developed through composing service components offered by different service providers. While such application development mode offers advantages in terms of cost-effectiveness and flexibility, application developers cannot understand or deal with risks potentially resulting from vulnerabilities within composed services due to non-transparency of the service providers. Furthermore, some of the vulnerabilities in practice are deeply hidden in dependency structures underlying composed services, thus making even the service providers fail to know the vulnerabilities. This paper proposes a risk-evaluation assisted service selection system, called Risk Evaluation-as-a-Service(or REaaS), which aims to assist application developers to understand vulnerability risks hidden within alternative services when the developers at first attempt to adopt their applications. In particular, for a given application developer's service selection requirement, REaaS produces a ranking list based upon vulnerability risks of alternative services to serve as a guideline regarding which service has the lowest potential risks (e.g., Bugs) for this application deployment. REaaS achieves this goal through the following three steps: 1) generating a package dependency graph for each alternative service, 2) assigning threat-degrees to packages in each dependency graph, and 3) analyzing each dependency graph and evaluating service-risk of each service. We have built a REaaS prototype and used real case study to demonstrate the practicality of REaaS."
1884,"This paper presents InfoParser, an Infoset driven XML processing mechanism aimed to amend current SOAP engines by substituting InfosetharrXML binding for ObjectharrXML (O/X) mapping, which is responsible for several acute issues in Web services, including O/X impedance mismatch, portability, XML alienation, complexity and memory overhead. The proposed InfoParser approach not only avoids issues in object based framework, but also significantly reduces the complexity in XML centric approach. InfoParser can be described formally by just a few set-theoretic equations, yet it offers a promising alternative to current options in moving Web services towards more semantics driven, light weight and loosely coupled programming paradigm that can work on any devices. We have implemented a prototype InfoParser in Java and the preliminary experiments indicate that the approach is feasible."
1885,"Service selection is a key issue in the Future Internet, where applications are built by composing services and content offered by different service providers. Most existing service selection schemas only focus on QoS properties of services such as throughput, latency and response time, or on their trust and reputation level. By contrast, the risk of privacy breaches arising from the selection of component services whose privacy policy is not compliant with customers' privacy preferences is largely ignored. In this paper, we propose a novel privacy-preserving Web service composition and selection approach which (i) makes it possible to verify the compliance between users' privacy requirements and providers' privacy policies and (ii) ranks the composite Web services with respect to the privacy level they offer. We demonstrate our approach using a travel agency Web service as an example of service composition."
1886,"As Web services and service oriented architectures are adopted, it is increasingly important to have standard and interoperable means to deploy and configure Web services. Within the Global Grid Forum, HP, NEC, and Softricity have been developing a standard for configuration description, deployment, and lifecycle management (CDDLM). In order to prove its feasibility, reference implementations are being developed. This paper describes an independent reference implementation of CDDLM and the experience in using Web Services for deployment in a standardized manner. Our main contributions are: the lessons learned in implementing this WS-based standard and an architecture for implementing CDDLM"
1887,"Service relations facilitate the automation of service reuse. Most of studies on the service relations focus on the inputs and outputs. However, different Web Services tend to utilize the same parameters without formally specifying their constraints. Due to this, the semantics, introduced by semantic annotation, is still not rich enough for accurate descriptions, thus generating a large number of inappropriate service relations. To address this, we propose an approach for augmenting semantics of Web Services based on constraints, which can be regarded as a complement to semantic annotation. The semantics is augmented via a hybrid analysis of heterogeneous constraints, including the server constraint and object constraint."
1888,"Traditional software development involves reusing and building upon off-the-shelf software libraries. This paradigm provides compile time availability of both, component interfaces as well as corresponding implementations. It enables developers to write programs that build on existing functionality without worrying much about the runtime. Service oriented computing presents a different environment that involves actively running components in the form of services. Here, a search step becomes necessary to first discover the component(s) that satisfy the specified requirements. Moreover, services may come up or go down dynamically leading to expiry of searched results which makes dependent programs highly brittle. In this paper, we propose a paradigm shift from objects to services as first class entities. We present service matching techniques defined across design time and runtime. Using these techniques we define language level operations involving services as operands leading to a novel approach for services based software development."
1889,"SOAP-based Web services is a middleware technology marketed as the solution to easy data exchange between heterogeneous IT architectures. The large number of scenarios, in which this technology is used, has introduced demands for new extensions raising its complexity. However, this has also introduced a large variety of new attacks. In this paper, we investigate an automatic evaluation of Web service specific Denial of Service (DoS) attacks. We present a new fully automated plugin for the WS-Attacker penetration testing tool implementing major DoS attacks. Our tool determines the attack success without having physical access to the target machine, using a novel blackbox approach. We give an overview of our design decisions and present the evaluation results using common Web service frameworks and systems."
1890,"Web services offer an opportunity to redesign a variety of older systems to exploit the advantages of a flexible, extensible, secure set of standards. In this paper we explore the objective of improving Internet messaging (email) by redesigning it as a family of Web services, an approach we call WSEmail. We illustrate an architecture and describe some applications. Since increased flexibility often mitigates against security and performance, we focus on steps for proving security properties and measuring the performance of our system with its security operations. In particular, we demonstrate an automated proof using TulaFale and ProVerif of a correspondence theorem for an application called on-demand attachments. We also provide performance measures for the basic WSEmail functions in a prototype we have implemented using .NET. Our experiments show a latency of about a quarter of a second per transaction under load."
1891,"Nowadays, executions of composite Web services are typically conducted by heavyweight centralized engines. A centralized is a potential processing and communication bottleneck as well as a central point of failure, and its cost of deployment is usually too high for a large number of small businesses or end-users. In addition, it presents some weaknesses dealing with privacy issues. These drawbacks, and requirements such as loose coupling, are pushing service infrastructures toward more decentralized and dynamic interaction schemes. In this paper, we propose a decentralized alternative system for the execution of composite Web services based on the chemical analogy. The chemical paradigm provides a high-level execution model that allows executing composite services in a fully decentralized manner. Our architecture is composed by nodes communicating through a shared space containing both control and data flows, allowing to distribute the composition among nodes without the need for any centralized coordination."
1893,"The association-rule-based recommendation model is one of the most widely used commercial recommendation engines in e-commerce websites. Existing studies mostly focus on how to select eligible rules to enhance the recommendation performance, but the efficiency of recommendation has been paid few attentions. To remedy this, this paper develops a distributed-computing framework for improving the computational efficiency of rule-based recommendation. Specifically, a tree-typed structure called Ordered-Patterns Forest (OPF) is designed to compress and store frequent patterns. Then, we transform eligible rules mining to a path-searching problem on OPF, and present a path-searching algorithm running on single machine. Finally, a load-balanced strategy for data partitioning is clarified. Experimental results demonstrate that the efficiency improved remarkably by the proposed OPF, compared with the traditional Brute-Force method."
1894,"Facing changing environments and evolving business rules, composite services ought to be adaptable, even at run-time. Existing mainstream service composition languages and execution engines exhibit insufficient support for variability and adaptability to cater for dynamic changes. Research efforts have been put on the extension of the languages and argumentation of the engines. However, how to ensure the correctness for the adaptation of a running composite service instance and minimize unnecessary re-execution of component services remains a challenge. To address this problem, we propose a model-based approach that allows run-time adaptation of composite services. It is based on an instance transfer mechanism that transfers an active instance of the old service composition schema to a appropriate state of the new schema. Algorithms are proposed to find the appropriate destination states of the transformation. After the migration, the suspended instances can resume their execution according to the new schema. An example based on a FindRoute composite service is also included."
1895,"The execution of composite web services with WS-BPEL relies on externally autonomous Web services. This implies the need to constantly monitor the running behavior of the involved parties. Moreover, monitoring the execution of such processes is critical to enforce business policies and meet reliability goals. This paper proposes a stateful aspect extension to WS-BPEL, as a solution to support flexible behavior pattern monitoring for composite Web services. Specifically, in the stateful aspect, history-based pointcut specifies the pattern of interest within a range, while advice describes the associated action to manage the process if the specified pattern occurs. We also present its implementation based on finite state automata through runtime weaving mechanism. Our experiments indicate the proposed monitoring approach incurs minimal overhead."
1896,"Web services composition has been gaining interest over the last years as it leverages the capabilities to offer complex operations resulting from the aggregation of Web services offered by different organizations. As composite Web services are often long-running, loosely coupled and cross-organizational applications, advanced transactional support is required to ensure reliable execution. In addition, in the presence of multiple Web services with equivalent functionality, users will discriminate the alternatives based on their quality of service (QoS). This paper address the issue of selecting and composing Web services not only according to their functional requirements but also to their transactional properties and QoS characteristics defined using a quality model. In this model, Web services are selected in a way that satisfies user preferences. These preferences are expressed as weights over QoS criterion and as risk level defining semantically the transactional requirements."
1897,"Most work on adaptive workflows offers insufficient flexibility to enforce complex policies regarding dynamic, evolvable and robust workflows. In addition, many proposed approaches require customized workflow engines. This paper presents a portable framework for realistic enforcement of dynamic adaptation policies in business processes. The framework is based on the Model-View-Controller (MVC) pattern, commonly used for adding dynamism to web pages. To enhance reusability, our approach supports separation of adaptation logic from the functional workflow and modularization of workflow tasks in reusable aspects. The main idea is to design a workflow process as a template, where tasks can be specified on an abstract level. Concrete implementations of the tasks, modeled as aspects, are then selected from a library according to a policy-based adaptation logic. This logic is implemented using a general purpose language that offers an extensible and flexible solution to enforce any type of policy. We evaluate by means of a case study on workflow confidentiality to what extent an approach using standards-based technologies allows application-specific adaptation of running workflow instances."
1898,"An increasing number of Web services are appearing, users as well as software agents and other web services need to be able to find, select, understand and invoke these services. Today, Web services (e.g. travel services, book selling services, stock reporting services etc) are discovered and invoked manually by human users, which hardcode the interaction between their own programs and the available Web services. Web services standards, such as UDDI, WSDL and SOAP, contribute to this vision by facilitating the interoperation between Web services and software agents or users. As a consequence of these standards is becoming by increasingly easier to connect Web services with their clients. The drawback of these standards is that there is no support for automatic interoperation, and therefore they implicitly assume that a programmer will hardcode the interaction between Web services and his own programs."
1899,"The proliferation of fake news in today's digital world has moved beyond a specific election cycle and now commands headlines globally. In this paper, we propose countering the spread of fake news on social networks by leveraging these crowds to instead help verify alternative facts. We present a prototype social argumentation framework to verify the validity of proposed alternative facts to help curb the propagation of fake news. We utilize fundamental argumentation ideas in a graph-theoretic framework that also incorporates semantic web and linked data principles. The argumentation structure is crowdsourced and mediated by expert moderators in a virtual community."
1900,"The variation of contexts in which a Web service could be used and the resulting variation in quality of service (QoS) expectations makes a clear case for further research to extend Web services management platforms with more sophisticated control mechanisms to cater for differentiated service offerings. However, most Web services platforms are based on a best-effort model, which treats all requests uniformly, without any type of service differentiation or prioritization. This paper presents WS-DiffServ, a service differentiation middleware based on prioritization which leverages service requestor profile to classify service requests. We first explore the typical generic requirements of a differential QoS support in Web services management. We then present in detail the design of a priority based differentiated responsiveness platform for Web services"
1901,"WS-coordination and WS-CF provide support for distributed activities that require coordination in an environment of Web services. Both specifications are generic coordination frameworks, not specific for any application. Despite their generic character, up to now only transactional protocols have been defined, which leverage these coordination frameworks. In this work we map a distributed activity, that it is not related to transactions, onto the coordination model underlying WS-coordination and WS-CF. Consequently we identify the necessary characteristics of a distributed activity that leverages the coordination frameworks of WS-coordination and WS-CF. We also examine limits of these coordination frameworks and suggest additional mechanisms where support for coordination in particular use cases is not sufficient. In contrast to coordinating distributed transactions, additional problems could arise when other types of activities are subject to coordination. These problems are studied, classified and reviewed in conjunction with the architectural differences of WS-coordination and WS-CF."
1902,"The current proliferation of software services means users should be supported when selecting one service out of the many which meet consumer's needs. Recommender Systems provide such support for selecting products, yet their direct application to software services is not straightforward. In this paper, we derive three requirements for software service recommender systems and then propose a hybrid recommendation approach to address these requirements and provide effective recommendations in conditions of scarce user feedback. The approach combines semantic Content-based reasoning and context-dependent Collaborative Filtering. The paper ends with the experiments based on a realistic set of semantic services against existing approaches, demonstrating how our approach can produce effective recommendation using semantic reasoning over service specifications."
1903,"The poor performance of WS-security (WSS) processing is often a topic of concern and prevents its wider adoption. We focused on byte-level similarities in WSS messages and implemented a template-based WSS processor. Inside the processor an automaton is employed that matches the incoming messages and extracts signature values and/or encrypted values. WSS operations including XML canonicalization are performed against the extracted values, without costly XML parsing and traversal. This is more than twice as fast as the DOM-based WSS processor and our prior work with a stream-based processor."
1904,"With the rapidly growing number of Web services, how to identify high quality Web services becomes a hot research topic. User-side QoS evaluation on Web services is a key measurement to choose the optimal Web service from a set of Web services with similar functions. The user-side QoS data is acquired through the invocation of services from different locations. However, in real world, the QoS data of Web services is sparse and not timely. Though there are already a lot of research works on the sparsity and timeliness of QoS data respectively, it's still a lack of a prediction model which can combine both of these features. In order to solve this challenging problem, we put forward a novel model, called CAPred, to provide timely QoS prediction. Our model cut the historical QoS data into several time slices. Each time slice is a 2-dimension matrix. CAPred firstly processes every time slice to fill the empty part of the matrix, then utilizes all of the historical data to predict current QoS values of Web services. And at last we demonstrate two applications, those are recommendation and selection, which utilize the QoS prediction results of our model. The experimental results indicate the high feasibility and efficiency of our model."
1905,"The recent approaches for Web services composition tend to integrate heterogeneous business processes executed in Peer-to-Peer networks. In such networks, component Web services are invoked on independent peers and are orchestrated according to the transactional requirements defined by the designers or the users of the composite Web service. Since component Web services can be dynamically invoked and are generally implemented as black boxes, concurrency between them may appear. This paper presents the transactional execution model of composite Web services exploiting the transactional properties of their component Web services. The proposed concurrency control is ensured by a decentralized serialization graph based on an optimistic protocol and on the hierarchical structure of the composition. The globally correct execution of the composite Web service is achieved by communication among dependent subtrans-actions and the peers they have accessed."
1907,"Grid computing provides the basic software infrastructure for integrating geographically distributed resources and services through standardized grid services. One of the key challenges to enable the broader use of grid services beyond the domain of scientific computing is the ability to perform complex tasks that require the modeling and coordination of the enactment of a number of distributed grid services. Workflow technology is a good candidate for supporting grid service flow. However, traditional workflow is static, thus unable to exploit the dynamic information available in the grid and respond to the dynamic nature of the grid. In this paper, we present an adaptive framework that provides adaptive management of grid service flows. The framework is based on an adaptive grid service flow model and is supported by an event-trigger-rule (ETR) technology that will be used to trigger rules in a distributed fashion to adapt a grid service flow to the dynamic grid environment and the changing requirements of a grid application."
1908,"The 'Grid' is an emerging infrastructure providing coordinated and consistent services to access distributed, heterogeneous computational and information storage resources amongst autonomous organizations. Data grids are being built across the world as the next generation inter-organizational data management systems for coordinated sharing of data as well as storage resources. A data grid is a location independent logical namespace consisting persistent global identifiers for digital entities, storage resources and users in an autonomous inter-organizational collaboration. Data Grid Management Systems (DGMS) provide services on the data grid’s logical name space for management and organization of digital entities and resources. These data-intensive environments involve long run processes, which could be aggregated as Grid Workflows (Gridflows). Gridflows are executed as peer-to-peer workflows in the data grid infrastructure by the Gridflow Management Systems. The tutorial will introduce the grid, provide use-cases in large projects, examine requirements, introduce some possible solutions for managing data in the grid and using a services-oriented infrastructure. It will take real-world examples from current systems, in particular the Storage Resource Broker and the SDSC Matrix project."
1909,"In recent years, REST (Representational State Transfer) has received much attention for designing scalable web services in various domains. There is an increasing interest on its application to real-time communication web services. Based on a case study of CSTA services, we found that communication services exhibit a combination of complex message exchange and stateful behavior patterns, including multi-states, two-way interaction, event-driven, multi-resources, multi-responses and session management, that need to be modeled properly within the framework of REST. To address these challenges, we propose three REST design patterns: session, event subscription and relationship using REST composition. This approach leads to a systematic one-to-one transformation from CSTA to REST. We hope such a study on a concrete use case can contribute to a better understanding of REST and lead to a seamless convergence of communication services with the infrastructure of the Web."
1910,"Service oriented Architecture (SOA) has been widely used in service computing applications and this fact has been encouraged the publication over the Web in a Web Service format. Whereas the number of Web services published on the Web is growing up, discovery techniques must be improved so as to retrieve more desirable services. Nowadays, the most commonly used technique is semantic filters based on ontological concepts. However, such mechanisms can leave out some important Web services of the matching process, because of their structural relationship not mentioned in an ontology. In order to overcome such problems, some authors have proposed a hybrid approach to combine traditional syntactic and semantic approaches. These proposals remain restricted especially with respect to complexity, precision and time of execution, thus making such solutions in most of cases unfeasible. In this paper, we combine semantic filters based on functional properties with a structural approach, analyzing each neighbor relationship in an ontology. The results showed a considerable improvement in terms of performance and a complexity reduction with respect to other existing techniques. Furthermore, we implement a tool called OWLS-S Discovery in order to simplify the use of our approach by developers."
1911,"Service computing has become a dominant paradigm enabling the building of complex service-oriented systems, with the aim of business added-value. Because these systems are inevitably based on uncontrollable services on the unpredictable Internet, it is important to find effective ways of maximizing the profit of service-oriented systems in such unreliable environment. In this paper, we propose an analytic approach that employs a build-time analysis of the runtime dynamics of service execution to maximize the net profit from delivering composite services under full probability of uncertainty. We also present methods for improving the optimization efficiency, including reusing intermediate computation results and adopting specialized profit optimization algorithms. The superiority of the proposed approach is both theoretically proved and empirically demonstrated through experiments."
1912,"Web component-based development is a challenging development paradigm, whose attraction to practitioners is increasing more and more. One of the main advantages of this paradigm is the ability to build customizable and composable web application modules as independent units of development, and to share them with other developers by publishing them in libraries as COTS (Commercial Off The Shelf) or free components. In parallel, since many years, Web services confirmed their status of one of the most pertinent solutions for a service provider, like Google or Amazon, to open its solutions for third party development. In this paper, we present an approach to migrate existing web component-based applications to a set of primitive and composite Web services and deploy them on a web service provider. This transformation helps server-side web application developers in transforming their ""user interface""-based web components into a set of web services intended for remote code extensions. We implemented our solution on a collection of Java-related technologies. Java EE components are the input of the proposed implementation, and a set of Java Web services with their WSDL interfaces, choreographies and orchestrations of these services are provided at output."
1913,"In this paper we measured and analyzed the workload on Yahoo! Video, the 2nd largest U.S. video sharing site, to understand its nature and the impact on online video data center design. We discovered interesting statistical properties on both static and temporal dimensions of the workload; they include file duration and popularity distributions, arrival rate dynamics and predictability, and workload stationarity and burstiness. Complemented with queueing-theoretic techniques, we extended our understanding on the measurement data with a virtual data center design assuming the same workload as measured, which reveals results regarding the impact of workload arrival distribution, service level agreements (SLAs) and workload scheduling schemes on the design and operations of such large-scale video distribution systems."
1914,"With the rapid development of cloud computing, large scale of cloud services are provided to users. Recommender systems have been proven to be valuable tools to deal with information overload and be able to provide appropriate recommendations to users. The cloud environment is dynamic and uncertain, which makes the quality of cloud services time-sensitive. However, most existing recommender systems did not take temporal influence into consideration, therefore could not accommodate the dynamic cloud environment. In view of this challenge, we propose a temporal-aware hybrid collaborative recommendation method for cloud service. It aims at providing users with appropriate recommendations from time-sensitive cloud services. In our method, by distinguishing temporal QoS metrics from stable QoS metrics, temporal influence is integrated into classical neighborhood-based collaborative recommender algorithm. Besides, to get an optimal recommendation, a temporal-aware latent factor model based on tensor decomposition is proposed and combined to improve the recommendation performance. Finally, experiments are designed and conducted to demonstrate the efficiency of our method."
1915,"Businesses are, nowadays, keen on organizing their business processes in the most flexible way, as to cope with the tough competition. Two major concerns are emerging, while dealing with the construction of new process functionalities: (1) shortening the development periods, and (2) managing the privacy risks. An effective solution for dealing with both concerns, consists of reusing fragments of existing business processes. These fragments need to be declared as safe, however, from a privacy perspective. In this paper, we propose a comprehensive approach for decomposing existing business processes into reusable fragments, that are privacy-aware; i.e., these fragments do not allow the disclosure of so-called sensitive associations."
1916,"Service level agreements (SLAs) impose many non-functional requirements on services. Business analysts specify and check these requirements in business process models using tools such as IBM WebSphere Business Modeler. System integrators on the other hand use service composition tools such as IBM WebSphere Integration Developer to create service composition models, which specify the integration of services. However, system integrators rarely verify SLA compliance in their proposed composition designs. Instead, SLA compliance is verified after the composed services are deployed in the field. To improve the quality of the composed services, we propose a framework to verify SLA compliance in composed services at design time. The framework re-uses information in business process models to simulate services and verify the non-functional requirements before the service deployment. To demonstrate our framework, we built a prototype using an industrial process simulation engine from IBM WebSphere Business Modeler and integrate it into an industrial service composition tool. Through a case study, we demonstrate that our framework and the prototype assist system integrators in composing services while considering the non-functional requirements."
1917,"The introduction of verifiable computation came as a result of the increasingly common phenomenon of ""outsourcing"" computation to untrusted servers and also to the growing desire of weak clients to outsource computational tasks to more powerful computation services like in cloud computing. Verifiable computation enables a computer to offload the computation of some function, to other perhaps untrusted cloud servers, while maintaining verifiable results. The servers evaluate the function and return the result with a proof that the computation of the function was carried out correctly. In the previous setting of verifiable computation, there is only one data provider. But in practice, there exist scenarios such as a network of sensors where each sensor collects data (e.g. air temperature in a certain area of a city) and stores them on servers. A control unit performs computation (e.g. the average air temperature of the city in certain period) on the outsourced data on the cloud (e.g. Amazon Cloud). When the control unit receives the results, it wants to verify the correctness of the computation results returned by the servers. For this scenario, we define a novel two-server multiclient verifiable computation service framework for outsourced data. An efficient construction is proposed, whose security is based on the existence of one-way functions. There are two advantages in our construction: (1) The size of the proof vouching for the correctness of computation result is independent with the number of data providers. (2) The verification only needs two equality tests executed by one who wants to get the computation result. We also experimentally analyze our construction and show our construction is very efficient in practice."
1918,"In this paper, we introduce a novel concept called collective skyline to deal with the problem of multiple users preferences. We then conduct a set of experiments that demonstrate the effectiveness of the introduced concept."
1919,"Current Web services composition proposals, such as BPML, WSBPEL, WSCI, and OWLS, provide solutions for describing the control and data flows in Web service composition. However, such proposals remain at the descriptive level, without providing any kind of mechanisms or tool support for analysis and verification. Therefore, there is a growing interest for the verification techniques which enable designers to test and repair design errors even before actual running of the service, or allow designers to detect erroneous properties and formally verify whether the service process design does have certain desired properties. In this paper, we propose to verify Web services composition using an event driven approach. We assume Web services that are coordinated by a composition process expressed in WSBPEL and we use event calculus to specify the properties and requirements to be monitored."
1920,"We propose a novel approach to discover workflow models from event logs. The proposed approach addresses two major limitations of current process mining approaches. First, they assume either a single workflow model for the entire event log or the availability of workflow ids that can be used to group logs associated with the same workflow model together. Nonetheless, these assumptions are oversimplified as a complex system typically runs multiple workflow models, all of which share the same log system. Second, existing process mining approaches do not consider the usage patterns of workflow users. Most systems support multi-users and each user is typically associated with (or use) certain number of operation sequences, which may all follow one or several workflow models. Hence, we propose to leverage User Behavior Patterns (or UBPs) to improve the outcome of process mining. In particular, we exploit machine learning techniques to incorporate UBPs into sequence clustering for workflow model discovery. We model a UBP as a probabilistic distribution on sequences, which allows to compute the distance between a UBP and any sequence. We apply three-way matrix factorization onto a UBP-sequence distance matrix to co-cluster users and sequences. In this way, users that share similar UBPs are grouped together while the clustering of similar sequences will lead to the discovery of workflow models. An comprehensive experimental study is conducted to demonstrate the effectiveness and efficiency of the proposed approach."
1921,"Social services are more and more popular with the development of web and mobile internet. Behind social services, graph model is the key data structure representing the relationship among social entities. Partitioning social graphs according to graph connectivity is an important technique for the competency of social services including parallel computing, scalability, and analytics. This paper proposes a novel diffusion based approach for partitioning social graphs. A brand-new Random Walks model with Constant Source and Sink nodes (RWCSS) is devised to analogize a dynamic balanced diffusion procedure on graphs. The stationary state of RWCSS provides the closeness measurement between nodes which can serve as an effective global estimation for partitioning. Through incorporating the RWCSS model into the Bubble Framework, an efficient graph partitioning algorithm (Bubble-RWCSS) is implemented in a way gradually improving the partitioning quality with global consideration in each iteration. The evaluation on public well-known social graphs demonstrates the promising performance of the proposed method."
1922,"Many web services represent their artifacts in the semi-structural format. Such artifacts may or may not be structurally complex. Many existing test case prioritization techniques however treat test cases of different complexity generically. In this paper, we exploit the insights on the structural similarity of XML-based artifacts between test cases, and propose a family of test case prioritization techniques that iteratively selects test case pairs without replacement. The validation experiment shows that these techniques can be more cost-effective than the studied existing techniques in exposing faults."
1923,"Web services (WS) are integrated software components that facilitate interoperable machine-to-machine interaction over a network. In the era of Web 2.0, companies worldwide are actively deploying Web services within their business environments. As a result, designing effective Web service recommendation mechanisms based on Quality of Service (QoS) is attracting more attention. However, traditional Neighborhood-based Collaborative Filtering (CF) models fail to capture the actual relationships between users or services due to data sparsity. On the other hand, Random Walk (RW) algorithm, which has been categorized as a sparsity-tolerant recommendation approach, suffers from poor performance in terms of recommendation accuracy. In this paper, we aim at designing a recommendation model that achieves high recommendation accuracy over the transitional RW based model. First, we propose an Integrated-Model QoS-based Graph (IMQG), in which users and services represent the nodes while weighted QoS magnitudes and User/Service similarity measurements serve as the edges. We use Jaccard coefficient in several variants to separately compute similarities of both Users and Services. Then, Top-k Random Walk algorithm is applied to generate final recommendation list to active users. Finally, to demonstrate the effectiveness of our model, comprehensive experiments are conducted on a real-world QoS dataset. Analysis of the results shows high improvement in recommendation accuracy with more tolerance to data sparsity."
1924,"Web services advocate loosely coupled systems, although current loosely coupled applications are limited to stateless services. The reason for this limitation is the lack of a method supporting matchmaking of state dependent services exemplarily specified in BPEL. In particular, the sender's requirement that the receiver must support all possible messages sent at a certain state are not captured by models currently used for service discovery. Annotated deterministic finite state automata provide this expressiveness. In this paper the transformation of a local process specification given in BPEL to annotated deterministic finite state automata is presented."
1925,"Traditional transaction semantics are not appropriate for business activities that involve long-running transactions in a loosely-coupled distributed environment, in particular, for Web services that operate between different enterprises over the Internet. In this paper we describe a novel reservation-based extended transaction protocol that can be used to coordinate such business activities. The protocol avoids the use of compensating transactions, which can result in undesirable effects. In our protocol, each task within a business activity is executed as two steps. The first step involves an explicit reservation of resources. The second step involves the confirmation or cancellation of the reservation. Each step is executed as a separate traditional short-running transaction. We show how our protocol can be implemented as a reservation protocol on top of the Web services transaction specification or, alternatively, as a coordination protocol on top of the Web services coordination specification."
1926,"Reinforcement learning has emerged as a powerful tool to compose and adapt Web services in open and dynamic environments. However, the most common applications of reinforcement learning algorithms are relatively inefficient in their use of experience data, which may affect the stability of the learning process. In particular, they make just one learning update for each interaction experience. This paper proposes two novel algorithms that aim to achieve greater data efficiency by saving experience data and using it in aggregate to make updates to the learned policy. The first algorithm introduces an offline learning scheme for service composition where the online learning task is transformed into a series of supervised learning steps. The second algorithm presents a coordination mechanism in order to enable multiple agents to learn the service composition task cooperatively. The results of our experiments show the effectiveness of the proposed algorithms compared to their online learning counterparts."
1927,"In the era of Internet of Things and dynamically interconnected systems, real time data becomes a new industrial asset, used to create new opportunities for operations improvement and to increase industrial value through the capitalisation of immaterial assets. In the smart factory, big data acquisition, analysis and visualisation pave the way to the manufacturing servitization, defined as the strategic innovation of organisations capabilities and processes to shift from product offering to an integrated ""product plus service"" offering. According to this vision, interconnected physical systems are associated with a cyber twin, where innovative services for big data management should be provided. In this paper, we propose an interactive data exploration framework, that poses a service-oriented perspective on the smart factory. Large amounts of data are incrementally collected from physical systems, organized and analysed on the cloud and new services are provided to enable data exploration. Such services implement novel data summarisation techniques, based on clustering, to manage data abundance, and data relevance evaluation techniques, aimed to focus the attention on relevant data that is being explored. Services are based on a multi-dimensional model, that is suited for supporting the iterative and multi-step exploration of Big Data."
1928,"The main objective of composing web services is to identify usable web services through discovery and to orchestrate or assemble selected services according to the goal specification. In this paper, we formulate and study a framework of composing web services through discovery from a given goal service. A general algorithm for composition with or without a goal service invocation request is developed. Two notions of completeness, ""schema completeness"" and ""instance completeness"", are defined, which measure the ability of how thoroughly an algorithm can find a composition. The two notions correspond to compositions without or with a goal service request, resp. We show that schema completeness can be achieved by depth-first or breadth-first search combined with a tightening strategy. Further, the breadth-first search avoids redundancy. We also show that while instance complete algorithms exist, they generally need do invoke all candidate services."
1929,"Current service-oriented architecture (SOA) focuses on service composition for application development, i.e. application composition, which is perceived as volatile (i.e. ""compose one time and use one time.""). This paper focuses on service composition information (i.e. service dependency) management and dependency-aware service composition for service development (i.e. service composition). This paper explores service composition and service dependency by proposing an extended SOA model, including: 1) establishing a dependency-aware service-oriented architecture (DSOA) that specifies dependency-aware service interactions, i.e. service publication, discovery, composition and binding; 2) developing an upper service dependency ontology for non-volatile DSOA service composition; and 3) demonstrating the validation of DSOA service composition by implementing a DSOA service manager."
1930,"Traditional particle swarm optimization algorithms (PSO) targeted to solve large scale problems are mostly serial, such as CCPSO2, and the computing time is very long in general. Therefore, this paper presents a novel parallel PSO, which explores the usage of new probability distribution functions for the replacement of traditional Gaussian and Cauchy distributions, and the combination of GPSO and LPSO to make use of space exploration and speed up the convergence. As to the implementation of algorithm parallelization, we adopt the Spark platform, which is one of the currently most popular big data processing tools. We make modification to dynamic grouping and multiple calculations, in order to increase the degree of parallelism, reduce the computation time and improve algorithm efficiency as far as possible. Multiple computing refers to that in each single distribution of tasks, one computing node processes the particle position information of multiple algorithms. In the control of space exploration and convergence rate, we present a more efficient method to explore the solution space, which controls the convergence rate to enhance the exploration to a greater extent and also ensures fast convergence rate at the later stage, thus, it not only guarantees the calculation speed, but also improves the optimization effect as more as possible. We used twenty LSGO benchmark functions in CEC'2010 to make experiments, showing that the proposed algorithm could obtain satisfactory results, and for some functions, it outperforms DECC and MLCC."
1931,"Behavioral correctness of service compositions refers to the absence of service interaction flaws, so that essential service properties like deadlock freedom are preserved and correctness properties related to safety and liveness are assured. Model checking is a widespread technique and it is based on extracting an abstract model representation of the program defining a service orchestration or choreography. During model extraction, the original structure of the service composition cannot be preserved and backwards traceability of the verification findings is not possible. We propose a rigorous analysis within the BIP component framework. Being rigorous means that the analyst is able to reason on which properties hold and why. The BIP language offers a sound execution semantics for a minimal set of primitives and constructs for modeling and composing layered components. We formally define the WS-BPEL 2.0 execution semantics and we provide a structure-preserving translation (embedding) of WS-BPEL to BIP. Structure preservation is feasible, due to the formally grounded expressiveness properties of BIP. As a proof of concept, we apply the developed embedding to a sample BPEL program and present the analysis results for a safety property. By exploiting the BIP model structure we interpret the analysis findings in terms of the service interactions stated in the BPEL source code. A significant benefit of BIP is that it applies compositional reasoning on the model structure to guarantee essential correctness properties and avoid, as much as possible, the scalability limitations of conventional model checking."
1932,"In this paper, we study the communication over IP based on web services. We describe the service-oriented communication framework of WIP, Web Services Initiation Protocol, and focus on its web centric networking infrastructure. In our approach, WIP endpoints are exposed as resources on the Web, publishable by their UDDI service registries. They can be searched and discovered by regular web search engines, e.g. Google, upon which they can be networked and linked for scalable and distributed communication over IP. In our approach, the WIP endpoint is a web services based communication endpoint for communication over IP. It is also a service endpoint for software-as-a-service (SaaS) that can integrate, marshup, and enable advanced SaaS applications in communication. Moreover, we introduce the presence aware communication framework in WIP utilizing its extensible and service-oriented infrastructure. An advanced WIP communication system prototype has been implemented. It supports the advanced features described in this paper, including distributed communication, networking, security, presence, etc. Experimental results are presented, and the proposed approach and methods are validated."
1933,"Interoperability refers to the ability of software and hardware on multiple machines from multiple vendors to communicate with each other without significant changes on either side. Web services streamline enterprise application integration by making it easier to tie applications running on heterogeneous platforms together helping them communicate correctly, effectively and at reduced cost. The difficulty of integration is a function of the level of interoperability between the applications being integrated. Even though there are established standards for Web services messaging (SOAP), description (WSDL) and discovery and registry (UDDI), custom implementations of these protocols by individual vendors has led to various interoperability issues. Web Services Interoperability Organization (WS-I) recently released Basic Profile 1.0, addressing the interoperability issues with core Web services standards. In this paper, we look at some of the key Web services interoperability issues pointed out by practitioners and critically examine how Basic Profile 1.0 addresses them. Further we also examine some limitations of the profile. We conclude by evaluating a popular Web services implementation against the profile and present our observations."
1934,"Quality of Web Service (QoWS) support for Mobility-aware Web services (MWS) is critical for mobile users since it relies on the available resources on mobile devices consuming these services. In this paper, we propose a selection model for MWS based on QoWS and device resources requirements. The main purpose of the model is to support the client in selecting MWS based on desired QoWS as well as on its device resources availability. We propose a verification scheme to verify the conformity of claimed MWS QoWS and required device resources compared to the published one. The verification is used to support selection of MWS. The implementation of our model is discussed and the importance of our verification scheme is highlighted."
1935,"Web services are rapidly becoming the technology of choice for integrating distributed application components in heterogeneous computing environments. In this paper, we present a novel e-commerce prototype application that itself would not be feasible without the leverage provided by underlying Web services technologies for flexible design and rapid prototyping. The application is a model-driven service brokerage that allows service providers to model, advertise, validate, and create a wide range of services (e.g. Internet access services such as DSL, cable, video-on-demand, etc) in an open marketplace in an automated fashion. The application answers real needs expressed by today's large service providers and goes beyond the current state-of-the-art in online communication marketplaces. It consists of several functional components, i.e. a service dependency modeling tool, brokerage engine, location services, and workflow engine - that have been independently developed on diverse platforms, e.g., J2EE, .NET. To efficiently integrate these components, we have designed a distributed Web services architecture, in which one Web service functions as a ""hub"" between previously disconnected components, another works as a ""wrapper"" of legacy data systems, while another ""orchestrates"" invocations of these services. Use of Web services has enabled parallel and independent development and testing, greatly increasing productivity and reducing time to get the system operational. It has also fostered the development of new brokerage features, which would have been difficult to plan without first experimenting with a ""live"" system."
1936,"The mobile shopping assistant (MSA) is a mobile application platform to deliver real-time, in-store, and personalized services, such as personalized product offerings and in-store customer advisory support, to improve the shopping experiences of in-store customers. The service delivery network that powers the MSA involves retail stores and their business partners such as manufacturers. This paper presents the core technologies that we developed in this cross-organizational service network to support the MSA and its personalized services, with focus on service delivery, customer behavior understanding and information sharing. Our event-based techniques allow customers, stores and manufacturers to deliver and consume the services in a loosely coupled manner, thus solving a critical store-specific real-time engagement problem in a seamless way. Service response tracking enables the stores to construct a comprehensive view of a customerpsilas in-store shopping behavior. Finally, the cross-organizational authorization-based access control mechanism effectively enforces information sharing between the stores and their partners."
1937,"Web services can be described as local autonomous routines communicating with each other through message exchange. Hence a good understanding of the messages defined in each Web service is crucial to enabling automatic composition. This paper presents a semantic specification framework for analyzing the functional composability of autonomous Web services. As the locally described Web services contain both semantics and business protocols, we model the former using ontologies and the latter by finite-state machines. A layered approach is adopted to analyze the Web service composability, which shows how to check whether two Web services are composable or not. Based on the specification and analysis, a polynomial-time algorithm is devised for checking the composability of Web services efficiently."
1938,"With the rapid development of SOA (Service Oriented Architecture), an increasing number of Web services have been published on the Internet. How to recommend suitable Web services to users becomes a challenging problem. Existing Web services recommendation approaches based on collaborative filtering mainly focus on QoS (Quality of Service) prediction. Recommending services based on users' ratings on services are seldom reported since such explicit feedback data is difficult to collect. In this paper, we report a dataset of implicit feedback on real-world Web services, which consist of more than 280,000 user-service interaction records, 65,000 service users and 15,000 Web services or mashups. In addition, time is becoming an increasingly important factor in recommenders since time effects influence users' preferences to a large extent. Based on the collected dataset, we propose a time-aware service recommendation approach. Temporal information is sufficiently considered in our approach, where three time effects are analyzed and modeled including user bias shifting, Web service bias shifting, and user preference shifting. Experimental results show that the proposed approach outperforms seven existing collaborative filtering approaches on the prediction accuracy."
1939,"Predictive industrial maintenance promotes proactive scheduling of maintenance to minimize unexpected device faults. A fault is not always isolated and may be formed by a propagation of trivial anomalies, which are regarded as service events herein. In this paper, we firstly propose an algorithm for generating service event correlation. Such correlations can show us lots of clues to the anomaly/fault propagation. The correlations are encapsulated into service hyperlinks as our previous works did, and thus we depict the anomaly/fault propagation as service event routing among services via the refined service hyperlinks. Our scenario illustrates that a trivial anomaly may propagate into different faults under different service event correlations. It indicates that the destination of a service event is often uncertain. Therefore, this paper further proposes a heuristic approach to handle the uncertainty problem. Extensive experiments have been made to verify the effectiveness of the approach."
1940,"In this paper, we propose an approach for the verification of declarative Web services composition processes using satisfiability solving. The need for the satisfiability solving approach stems from the nature of declarative processes which are defined by only specifying the constraints that mark the boundary of the solution to the composition process. As a result the state space of a declarative process can be significantly large, as the process is only partially defined and all the transitions have not been explicitly defined. Further, as the conflict clauses returned by the SAT solver can be significantly large for complex processes and verification requirements, we propose a filtering criteria and defined patterns for identifying the clauses of interest for process verification."
2018,"Dealing with the provisioning of cloud services granted by Security SLAs is a very challenging research topic. At the state of the art, the main related issues involve: (i) representing security features so that they are understandable by both customers and providers and measurable (by means of verifiable security-related Service Level Objectives (SLOs)), (ii) automating the provisioning of security mechanisms able to grant desired security features (by means of a security-driven resource allocation process), and (iii) continuously monitoring the services in order to verify the fulfillment of specified Security SLOs (by means of cloud security monitoring solutions). We propose to face the Security SLA life cycle management with a framework able to enrich cloud applications with security features. In this paper we (i) present a novel Security SLA model and (ii) illustrate a security-driven planning process that can be adopted to determine the (optimum) deployment of security-related software components. Such process takes into account both specific implementation constraints of the security components to be deployed and customers security requirements, and enables the automatic provisioning and configuration of all needed resources. In order to demonstrate the applicability of the approach, we present and discuss a practical application of the model on a real case study."
2029,"The overwhelming amount of various monitoring and log data generated in multitier IT systems makes problem determination one of the most expensive and labor-intensive tasks in IT Services arena. Particularly the initial step of problem classification is complicated by error propagation making secondary problems surfacing on multiple dependent resources. In this paper, we propose to automate the process of problem classification by leveraging machine learning. The main focus is to categorize the problem a user experiences by recognizing the real root cause specificity leveraging available training data such as monitoring and logs across the systems. We transform the structure of the problem into a hierarchy using an existing taxonomy. We then propose an efficient hierarchical incremental learning algorithm which is capable of adjusting its internal local classifier parameters in realtime. Comparing to the traditional batch learning algorithms, this online solution decreases the computational complexity of the training process by learning from new instances on an incremental fashion. Our approach significantly reduces the memory required to store the training instances. We demonstrate the efficiency of our approach by learning hierarchical problem patterns for several issues occurring in distributed web applications. Experimental results show that our approach substantially outperforms previous methods."
2034,"Models of Web service compositions that are both readable and verifiable will benefit organizations that integrate purportedly reusable Web services. Colored Petri nets (CPNs) are at once verifiable and visually expressive, capable of presenting subtle flaws in service composition. Constructing CPN models from business process execution language (BPEL) artifacts had been a manual process requiring human judgment. Building on results from the workflow community, we automate the mapping of artifacts written in BPEL to models used by CPN Tools - a formal verification environment for development, simulation, and model checking of colored Petri nets. We extend related work that already converts BPEL to Petri nets, to reflect hierarchy and data type (color in CPN terminology), while improving model layout. We present a prototype implementation that mines both a BPEL artifact and the Petri net generated from it by an existing tool. The prototype partitions the Petri net into subnets, lays them out, colors them, and generates their XML file for import into CPN tools. Our results include depictions of subnets produced and initial simulation results for a well-known case study."
2036,"Service computing promotes a large number of web-delivered services, including web services, APIs and data feeds. Composing data, functionalities and even UI from these web-delivered services into a single web application, usually called service mashup, becomes a popular web development paradigm. The web-delivered services can be modeled as mashup components, while the development of mashup actually yields a set of inter-connected mashup components. The growing popularity of mashup components enriches functionality and user experiences, while the possible connections among components are complex and difficult to mashup developers, who might be non-professional programmers or even end-users, as actions over one component may have potential impacts on another. This paper proposes a novel approach for recommending developers in terms of navigation and completion of mashup components with a large-scale components repository. From data-driven perspective, we model the relationships between mashup components by a generic layered-graph model. Developers are allowed to select some initial components as starting point, while a graph-based algorithm recommends how to navigate to potentially relevant mashup components and complete the relevant mashup application. We experimentally demonstrate the efficiency and effectiveness of our approach for rapid mashup construction."
2073,"Presents an editorial which explores the market fr cloud-based mobile computing systems. The concept of cloud-based mobile applications has emerged recently to address the limitations of pure mobile applications by leveraging the vast resources available at computing clouds. Traditionally, mobile applications benefit by outsourcing a part of the program execution to a cloud-based system to compute the (intermediate) results and send back these results for integration by the relevant mobile application. By doing so, the program code, data as well as device-specific information, such as user locations, can be made available to a cloud. Infrastructures to support computation outsourcing of the above type and its variants have been widely studied; and the associated security and privacy issues are hot topics in research."
2087,"Enforcing access control in composite services is essential in distributed multidomain environment. Many advanced access control models have been developed to secure web services at execution time. However, they do not consider access control validation at composition time, resulting in high execution-time failure rate of composite services due to access control violations. Performing composition-time access control validation is not straightforward. First, many candidate compositions need to be considered and validating them can be costly. Second, some service composers may not be trusted to access protected policies and validation has to be done remotely. Another major issue with existing models is that they do not consider information flow control in composite services, which may result in undesirable information leakage. To resolve all these problems, we develop a novel three-phase composition protocol integrating information flow control. To reduce the policy evaluation cost, we use historical information to efficiently evaluate and prune candidate compositions and perform local/remote policy evaluation only on top candidates. To achieve effective and efficient information flow control, we introduce the novel concept of transformation factor to model the computation effect of intermediate services. Experimental studies show significant performance benefit of the proposed mechanism."
2088,"Automated physical resource management of large-scale Internet Technology (IT) systems requires dynamic configuration of both application-level and system-level parameters. The existence of large number of tunable parameters makes it difficult to design a feedback controller that adjusts these parameters effectively in order to achieve application-level performance targets. In this paper, we introduce a new approach for simplified control architecture of large-scale IT systems based on dimension reduction techniques. It combines online selection of critical control knobs through LASSO-a powerful L
<sub>1</sub>
-constrained fitting method/Compressive Sensing (CS)-a L
<sub>1</sub>
-optimization method, and adaptive control of the identified knobs. The latter relies on the online estimation of the input-output model with the selected control knobs using the recursive least square (RLS) method and a self-tuning linear quadratic (LQ) optimal controller for output regulation. The results of both a numerical simulation in Matlab and a realistic case are presented to demonstrate the effectiveness of our approach."
2089,"Provenance has become increasingly important in scientific workflows and services computing to capture the derivation history of a data product, including the original data sources, intermediate data products, and the steps that were applied to produce the data product. In many cases, both scientific results and the used protocol are sensitive and effective access control mechanisms are essential to protect their confidentiality. In this paper, we propose: 1) a formal scientific workflow provenance model as the basis for querying and access control for workflow provenance; 2) a security model for fine-grained access control for multilevel provenance and an algorithm for the derivation of a full security specification based on inheritance, overriding, and conflict resolution; 3) a formalization of the notion of security views and an algorithm for security view derivation; and 4) a formalization of the notion of secure abstraction views and an algorithm for its computation. A prototype called SecProv has been developed, and experiments show the effectiveness and efficiency of our approach."
2090,"Services are perishable and are simultaneously produced and consumed. A reservation is a traditional and effective means for coordinating service demand and supply. In recent years, computerized reservation systems have been used widely by many service vendors such as airline companies and hotels to satisfy their customer's demands and improve their profits. In this paper, we propose and describe a more flexible reservation method called ""YuuZuu"" reservation that motivates cooperation among customers with different preferences for services. It increases utilization of vendor-provided services. The DREAM reservation system, which is an implementation of ""YuuZuu"" reservation, comprises three functions: (1) reservation allocation, (2) demand analysis, and (3) price optimization. Preliminary experiments show that the DREAM reservation system outperforms a standard reservation system when some customers are insistent upon detailed preferences and others are not, which, we believe, reflects real-world conditions."
2091,"Cloud computing improves utilization and flexibility in allocating computing resources while reducing the infrastructural costs. However, in many cases cloud technology is still proprietary and tainted by security issues rooted in the multi-user and hybrid cloud environment. A lack of secure connectivity in a hybrid cloud environment hinders the adaptation of clouds by scientific communities that require scaling-out of the local infrastructure using publicly available resources for large-scale experiments. In this article, we present a case study of the DII-HEP secure cloud infrastructure and propose an approach to securely scale-out a private cloud deployment to public clouds in order to support hybrid cloud scenarios. A challenge in such scenarios is that cloud vendors may offer varying and possibly incompatible ways to isolate and interconnect virtual machines located in different cloud networks. Our approach is tenant driven in the sense that the tenant provides its connectivity mechanism. We provide a qualitative and quantitative analysis of a number of alternatives to solve this problem. We have chosen one of the standardized alternatives, Host Identity Protocol, for further experimentation in a production system because it supports legacy applications in a topologically-independent and secure way."
2100,"We advocate to create a spot Internet transit market, where transit is sold using the under-utilized backbone capacity at a lower price. The providers can improve profit by capitalizing the perishable capacity, and customers can buy transit on demand without a minimum commitment level for elastic traffic, and as a result improve their surplus (i.e., utility gains). We conduct a systematic study of the economical benefits of spot transit both theoretically and empirically. We propose a simple analytical framework with a general demand function, and solve the pricing problem to maximize the expected profit, taking into account the potential revenue loss of regular transit when spot transit traffic hikes. We prove the price advantage of spot transit, as well as the profit and surplus improvements for tier-1 ISPs and customers, respectively. Using real-world price data and traffic statistics of six IXPs with more than 1,000 ISPs, we evaluate spot transit and show that significant financial benefits can be achieved in both absolute and relative terms, robust to parameter values."
2102,"High energy consumption of cloud data centers is a matter of great concern. Dynamic consolidation of Virtual Machines (VMs) presents a significant opportunity to save energy in data centers. A VM consolidation approach uses live migration of VMs so that some of the under-loaded Physical Machines (PMs) can be switched-off or put into a low-power mode. On the other hand, achieving the desired level of Quality of Service (QoS) between cloud providers and their users is critical. Therefore, the main challenge is to reduce energy consumption of data centers while satisfying QoS requirements. In this paper, we present a distributed system architecture to perform dynamic VM consolidation to reduce energy consumption of cloud data centers while maintaining the desired QoS. Since the VM consolidation problem is strictly NP-hard, we use an online optimization metaheuristic algorithm called Ant Colony System (ACS). The proposed ACS-based VM Consolidation (ACS-VMC) approach finds a near-optimal solution based on a specified objective function. Experimental results on real workload traces show that ACS-VMC reduces energy consumption while maintaining the required performance levels in a cloud data center. It outperforms existing VM consolidation approaches in terms of energy consumption, number of VM migrations, and QoS requirements concerning performance."
2144,"Cloud system management is complex due to their diversity and frequent runtime changes. Cloud systems were previously managed through cloud specific management tools that focus on optimising technical metrics, such as performance. However, business users care business metrics (such as cost and revenue) more than technical metrics. To address these issues, this paper proposes a unified business-driven cloud management framework, which enables optimisation of business metrics without limiting business to a specific cloud provider. The main contributions include: (1) a taxonomy which defines a set of actions, events and metrics for unified cloud management; (2) a cloud management policy language that specifies cloud management policies from a business perspective; and (3) middleware architecture that allows business-driven management of diverse clouds. The proposed solutions are evaluated in terms of feasibility, functional correctness, generality, usefulness, and performance."
2147,"The eXtensible Access Control Markup Language (XACML) is the de facto language to specify access control policies for web services. XACML has an RBAC profile (XACML-RBAC) to support role-based access control policies. We extend this profile with an administrative RBAC profile, which we refer to as the XACML-ARBAC profile. One of the advantages of doing so is to use policies based on RBAC model to administrate XACML-RBAC policies. Because using permissions granted by XACML-ARBAC policies alter XACML-RBAC policies, enforcing XACML-ARBAC polices requires some concurrency control within XACML access controller's runtime. In order to solve this concurrency problem, we propose a session-aware administrative model for RBAC, and enhance the XACML policy evaluation runtime using a locking mechanism. Experimental study shows reconcilable performance characteristics of our enhancements to Sun's XACML reference implementation."
2168,"Cloud Computing has been increasingly accepted as a promising computing paradigm in industry, with one of the most common delivery models being Infrastructure as a Service (IaaS). An increasing number of providers have started to supply public IaaS services with different terminologies, definitions, and goals. As such, understanding the full scope of performance evaluation of candidate services would be crucial and beneficial for both service customers (e.g., cost-benefit analysis) and providers (e.g., direction of improvement). Given the numerous and diverse IaaS service features to be evaluated, a natural strategy is to implement different types of evaluation experiments separately. Unfortunately, it could be hard to fairly distinguish between different experimental types due to different environments and techniques that may be adopted by different evaluators. To overcome such obstacles, we have first established a novel taxonomy to help profile and clarify the nature of IaaS services performance evaluation and then built a three-layer conceptual model to generalize the existing performance evaluation practices. Using relevant elements/classifiers in the taxonomy and conceptual model, evaluators can construct natural language-style descriptions and experimental design blueprints to outline the evaluation scope and also to guide new evaluation implementations. In essence, the generated descriptions and blueprints abstractly define and characterize the actual evaluation work. This enables relatively fair and rational comparisons between different performance evaluations according to their abstract characteristics."
2179,"We address the problem of usage license administration in federated settings. This problem arises whenever organizations, such as educational or research groups or institutions, share resources for business and scientific reasons. In such settings, each user's usage of a licensed resource is typically supported by the user's organization. License administration involves satisfying legal requirements while applying organizational strategies for effective resource usage, and carrying out suitable accounting and audit controls. We propose an approach, Licit, wherein an agent represents each resource sharing site and administers licenses in collaboration with other agents. We show how to represent a variety of usage licenses formally as executable policies and provide a simple information model using which each party can specify both the attributes involved in its licenses and how to resolve them. Our architecture naturally accommodates a variety of site-specific (i.e., custom) strategies for license administration. Licit has been implemented in a popular open source framework for virtual computing, and yields performance results indicating its practical feasibility."
2203,"The articles in this special issue focus on the use of cloud computing for social computing applications. Currently, two complimentary Internet based research areas are emerging: social computing and cloud computing. On the one hand, social computing empowers individual users with relatively low technological sophistication to use the web to engage in social interactions, contribute their expertise and share their content, experiences and opinions. On the other hand, cloud computing offers everyone sophisticated computing infrastructures and resources as utilities, so that individual users with relatively low computing knowledge can have at their disposal a high performing computing infrastructure with little investment. Together, these two complementary technological advances form the backbone of our digitized world, when coupled with the rise of sensors,mobile devices and the internet of things. Of course, they also face significant challenges. These articles look at each area and describe how they complement each other. The Social Web has become an important means of communication for everyone: individuals, organizations, and governments all use it to disseminate and share information, offer opinions and engage in discussions. "
2230,"Web Services technology provided several advantages over other technologies, however it still has serious limitations, including high latency and high protocol overhead. To improve performance, SOAP network traffic needs to be substantially reduced. This paper presents a novel approach, called similarity-based SOAP multicast protocol (SMP), to address the issue of latency. SMP reduces network traffic by aggregating syntactically similar SOAP messages to form a ""compact SMP message"";. The addresses of clients are encoded as strings in the SMP message header. The similarity of SOAP messages is measured in pairs and is based both on the message template and on the values of each XML tag in the messages. Each XML node in a SOAP message is indexed with an identifier and its position in the SOAP message. Only the indexed form of a SOAP message is sent to clients. Intermediary routers along the paths from server to clients parse the content of each SMP message passing through them and perform necessary operations to forward it to neighbouring routers. Experiments show that SMP can achieve up to 70% reduction in network traffic compared to traditional SOAP unicast."
2231,"The four papers in this special section aim to present high-quality contributions and innovations in this interdisciplinary area of mobile big data technologies, systems, and services, especially mobile big data management and innovative applications."
2241,"The increasing popularity of clouds drives researchers to find answers to a large variety of new and challenging questions. Through extensive experimental measurements, we show variance in performance and scalability of clouds for two non-trivial scenarios. In the first scenario, we target the public Infrastructure as a Service (IaaS) clouds, and study the case when a multi-tier application is migrated from a traditional datacenter to one of the three IaaS clouds. To validate our findings in the first scenario, we conduct similar study with three private clouds built using three mainstream hypervisors. We used the RUBBoS benchmark application and compared its performance and scalability when hosted in Amazon EC2, Open Cirrus, and Emulab. Our results show that a best-performing configuration in one cloud can become the worst-performing configuration in another cloud. Subsequently, we identified several system level bottlenecks such as high context switching and network driver processing overheads that degraded the performance. We experimentally evaluate concrete alternative approaches as practical solutions to address these problems. We then built the three private clouds using a commercial hypervisor (CVM), Xen, and KVM respectively and evaluated performance characteristics using both RUBBoS and Cloudstone benchmark applications. The three clouds show significant performance variations; for instance, Xen outperforms CVM by 75 percent on the read-write RUBBoS workload and CVM outperforms Xen by over 10 percent on the Cloudstone workload. These observed problems were confirmed at a finer granularity through micro-benchmark experiments that measure component performance directly."
2274,"Mashup is a web technology that allows different service providers to flexibly integrate their expertise and to deliver highly customizable services to their customers. Data mashup is a special type of mashup application that aims at integrating data from multiple data providers depending on the user's request. However, integrating data from multiple sources brings about three challenges: 1) Simply joining multiple private data sets together would reveal the sensitive information to the other data providers. 2) The integrated (mashup) data could potentially sharpen the identification of individuals and, therefore, reveal their person-specific sensitive information that was not available before the mashup. 3) The mashup data from multiple sources often contain many data attributes. When enforcing a traditional privacy model, such as K-anonymity, the high-dimensional data would suffer from the problem known as the curse of high dimensionality, resulting in useless data for further data analysis. In this paper, we study and resolve a privacy problem in a real-life mashup application for the online advertising industry in social networks, and propose a service-oriented architecture along with a privacy-preserving data mashup algorithm to address the aforementioned challenges. Experiments on real-life data suggest that our proposed architecture and algorithm is effective for simultaneously preserving both privacy and information utility on the mashup data. To the best of our knowledge, this is the first work that integrates high-dimensional data for mashup service."
2293,"We propose a system, called EVolution of Long-term Composed Services (Ev-LCS), to address the change management issues in long-term composed services (LCSs). An LCS is a dynamic collaboration between autonomous web services that collectively provide a value-added service. It has a long-term commitment to its users. We first present a formal model, which provides the grounding semantics to support the automation of change management. We present a set of change operators that allow to specify a change in a precise and formal manner. We then propose a change enactment strategy that actually implements the changes. We develop a prototype system for the proposed Ev-LCS to demonstrate its effectiveness. We also conduct an experimental study to assess the performance of the change management approach."
2300,"With the popularity of Web Services and Service-Oriented Architecture (SOA), quality assurance of SOA applications, such as testing, has become a research focus. Programs implemented by the Business Process Execution Language for Web Services (WS-BPEL), which can be used to compose partner Web Services into composite Web Services, are one popular kind of SOA applications. The unique features of WS-BPEL programs bring new challenges into testing. A test case for testing a WS-BPEL program is a sequence of messages that can be received by the WS-BPEL program under test. Previous research has not studied the challenges of message-sequence generation induced by unique features of WS-BPEL as a new language. In this paper, we present a novel methodology to generate effective message sequences for testing WS-BPEL programs. To capture the order relationship in a message sequence and the constraints on correlated messages imposed by WS-BPEL's routing mechanism, we model the WS-BPEL program under test as a message-sequence graph (MSG), and generate message sequences based on MSG. We performed experiments for our method and two other techniques with six WS-BPEL programs. The results show that the message sequences generated by using our method can effectively expose faults in the WS-BPEL programs."
2320,"The paper presents a taxonomy of notions of consistency in the context of dynamic migration of business processes covering two decades of research on dynamic process change in various workflow approaches including graph based and net based approaches. A consolidated view that brings out the generic nature of the consistency problem independent of the respective workflow model contexts is developed through the taxonomy framework. The consistency models are unified based on a common Petri net formalism. The classification into nine major consistency classes is derived primarily from differences arising out of the past, present and future of the post migration state. These are further grouped into four categories: structure based, trail based, lookahead trace based and live consistency models. Known usages of these models from the literature are also discussed. Moreover, several examples of dynamic workflow migration scenarios are presented to demonstrate the usefulness and applicability of the consistency models."
2329,"Quality of service (QoS) guarantee is an important component of service recommendation. Generally, some QoS values of a service are unknown to its users who has never invoked it before, and therefore the accurate prediction of unknown QoS values is significant for the successful deployment of web service-based applications. Collaborative filtering is an important method for predicting missing values, and has thus been widely adopted in the prediction of unknown QoS values. However, collaborative filtering originated from the processing of subjective data, such as movie scores. The QoS data of web services are usually objective, meaning that existing collaborative filtering-based approaches are not always applicable for unknown QoS values. Based on real world web service QoS data and a number of experiments, in this paper, we determine some important characteristics of objective QoS datasets that have never been found before. We propose a prediction algorithm to realize these characteristics, allowing the unknown QoS values to be predicted accurately. Experimental results show that the proposed algorithm predicts unknown web service QoS values more accurately than other existing approaches."
2376,"Service-based software systems (SBSSs) are widely deployed due to the growing trend of distributed computing and cloud computing. It is important to ensure high quality of an SBSS, especially in a strongly competitive market. Existing works on SBSS reliability usually assumed independence of service failures. However, the fact that resource sharing exists in different levels of SBSS operations invalidates this assumption. Ignorance of failure dependencies have been discussed as potentially affecting system reliability predictions and lowering the benefits of design diversity, as typically seen in high-reliability systems. In this paper, we propose a reliability framework that incorporates failure dependence modeling, system reliability modeling, as well as reliability analysis for individual services and for failure sources. The framework is also capable of analyzing the internal structures of popular software fault tolerant (FT) schemes. The proposed method is applied to a travel agency system based upon a real-world practice for verifying its accuracy of reliability modeling and effectiveness of varied reliability measures. The results show that failure dependence of the services is an essential factor for analyzing any valuable SBSS system. Further, a set of reliability measures with different capabilities and complexities are available for assisting SBSS engineers with system improvements."
2384,"User request trace-oriented monitoring is an effective method to improve the reliability of cloud services. However, there are some difficulties in getting useful traces in practice, which hinder the development of trace-oriented monitoring research. In this paper, we release a fine-grained user request-centric open trace data set, called TraceBench, which is collected in a real-world cloud storage service deployed in a real environment. When collecting, we consider different scenarios, involving multiple scales of clusters, different kinds of user requests, various speeds of workloads, many types of injected faults, etc. To validate the usability and authenticity, we have employed TraceBench in several trace-oriented monitoring topics, such as anomaly detection, performance problem diagnosis, and temporal invariant mining. The results show that TraceBench well supports these research topics. In addition, we have also carried out an extensive data analysis based on TraceBench, which validates the high quality of the data set."
2386,Presents the introductory editorial for this issue of the publication
2407,"A cloud mashup is composed of multiple services with shared datasets and integrated functionalities. For example, the elastic compute cloud (EC2) provided by Amazon Web Service (AWS), the authentication and authorization services provided by Facebook, and the Map service provided by Google can all be mashed up to deliver real-time, personalized driving route recommendation service. To discover qualified services and compose them with guaranteed quality of service (QoS), we propose an integrated skyline query processing method for building up cloud mashup applications. We use a similarity test to achieve optimal localized skyline. This mashup method scales well with the growing number of cloud sites involved in the mashup applications. Faster skyline selection, reduced composition time, dataset sharing, and resources integration assure the QoS over multiple clouds. We experiment with the quality of Web service (QWS) benchmark over 10,000 Web services along six QoS dimensions. By utilizing block-elimination, data-space partitioning, and service similarity pruning, the skyline process is shortened by three times, when compared with two state-of-the-art methods."
2427,"Current QoS-aware automatic service composition queries over a network of web services are often one-time in nature. After a network of web services is built, such queries are issued once, and answers are found from the scratch. The underlying assumption is that the participating web services are rather static, which means that their functional and non-functional parameters seldom change. However, such an assumption is often baseless. New services come and go, service APIs change gradually, and QoS values fluctuate in practice. Therefore, a support for efficiently adapting service composition is desired. In this paper, we propose an event driven continuous query algorithm to intelligently cope with different types of dynamic services, and thus enable evolution of service composition. Evaluation using both real QoS data and synthetic web service data shows its superior performance, compared with the state-of-the-art solution which won the performance championship of Web Service Challenge in 2009 and 2010. Moreover, we generalize a new graph problem: dynamic single-source optimal-Directed Acyclic Graphs (DAG) problem based on the above work. It is argued that API recommendation for framework evolution, as another typical application, could also be modeled and addressed efficiently using the proposed new graph approach."
2439,"With the development of cloud computing, outsourcing data to cloud server attracts lots of attentions. To guarantee the security and achieve flexibly fine-grained file access control, attribute based encryption (ABE) was proposed and used in cloud storage system. However, user revocation is the primary issue in ABE schemes. In this article, we provide a ciphertext-policy attribute based encryption (CP-ABE) scheme with efficient user revocation for cloud storage system. The issue of user revocation can be solved efficiently by introducing the concept of user group. When any user leaves, the group manager will update users' private keys except for those who have been revoked. Additionally, CP-ABE scheme has heavy computation cost, as it grows linearly with the complexity for the access structure. To reduce the computation cost, we outsource high computation load to cloud service providers without leaking file content and secret keys. Notably, our scheme can withstand collusion attack performed by revoked users cooperating with existing users. We prove the security of our scheme under the divisible computation Diffie-Hellman assumption. The result of our experiment shows computation cost for local devices is relatively low and can be constant. Our scheme is suitable for resource constrained devices."
2460,"The electronic representation of a contract for a business-to-business (B2B) partnership should be such that it can be used by a monitoring service for compliance checking of B2B interactions at runtime, ensuring that the interactions match the rights and obligations that each partner has promised to honor. With this view in mind, the paper develops a model for checking contractual compliance of business interactions. Specifically, the paper develops a novel way of representing contract clauses using business rules, that is specially suited to compliance checking and describes what events need to be captured from the underlying messaging middleware and how they can be processed in a careful manner to evaluate contractual compliance."
2475,"Software as a service is a well accepted software deployment and distribution model that has grown exponentially in the last few years. One of the biggest benefits of SaaS is the automated composition of different services in a composite system. It allows users to automatically find and bind these services (to maximize the productivity), meeting both functional and non-functional requirements. In this paper, we present a framework for modeling the dependency relationships among different Quality of Service parameters of the component services. Our proposed approach considers the different invocation patterns of component services, and presents a service composition framework to model the dependencies and uses the global QoS for service selection. We evaluate the efficiency of our proposed technique on the WSDream-QoS Dataset [1] ."
2492,"Web service composition is a process to compose homogenous or heterogeneous services together in order to create value-added services. Many non-functional features including QoS and user preferences have been adopted to guide such a process. However, two issues are observed: (1) the expressiveness of user preference is subject to quantitative preferences without proper use of qualitative preferences; (2) a highly preferred composite service may not be trustworthy, or a highly trustworthy composite service may not be preferable. The existing studies concentrate on either user preference or service trust, and fail to provide a systematic solution to integrate both user preference and service trust together for service compositions. To address these issues, we combine both qualitative and quantitative preferences as well as service trust together in the process of service composition. We investigate the application of heuristic algorithms on multi-objective optimization for the service composition problem. A new hybrid nature inspired intelligent algorithm is also proposed and compared with other popular heuristic algorithms. We aim to obtain optimal web service compositions that can satisfy these (potentially conflicting) constraints as much as possible. Results demonstrate the efficiency and effectiveness of our approach in comparison with other counterparts."
2495,"Network Coordinate (NC) systems provide a scalable means for Internet distance prediction and are useful for various Internet-based services, such as cloud or web-based services. Decentralized, matrix factorization-based NC (MFNC) systems have received particular attention recently. They can serve large-scale distributed services (as opposed to centralized NC systems) and do not need to satisfy the triangle inequality (as opposed to Euclidean-based NC systems). However, because of their decentralized nature, MFNC systems are vulnerable to various malicious attacks. In this paper, we provide the first study on attacks toward MFNC systems, and propose a trust and reputation-based approach called NCShield to counter such attacks. It is fully decentralized and can easily be customized. Different from previous approaches, NCShield is able to distinguish between legitimate distance variations and malicious distance alterations. Using four representative data sets from the Internet, we show that NCShield can defend against not only the typical disorder, repulsion and isolation attacks, but also more advanced attacks such as frog-boiling attacks. For example, when selecting node pairs with a shorter distance than a predefined threshold in an online game scenario, even if 30 percent of nodes are malicious, NCShield can reduce the false positive rate from 45.5 to 3.7 percent."
2519,"Community Cloud Computing is an emerging and promising computing model for a specific community with common concerns, such as security, compliance and jurisdiction. It utilizes the spare resources of networked computers to provide the facilities so that the community gains services from the cloud. The effective collaboration among the community clouds offers a powerful computing capacity for complex tasks containing the subtasks that need data exchange. Selecting the best group of community clouds that are the most economy-efficient, communication-efficient, secured, and trusted to accomplish a complex task is very challenging. To address this problem, we first formulate a computational model for multi-community-cloud collaboration, namely MG
<sup>3</sup>
. The proposed model is then optimized from four aspects: minimizing the sum of access cost and monetary cost, maximizing the security-level agreement and trust among the community clouds. Furthermore, an efficient and comprehensive selection algorithm is devised to extract the best group of community clouds in MG
<sup>3</sup>
. Finally, the extensive simulation experiments and performance analysis of the proposed algorithm are conducted. The results demonstrate that the proposed algorithm outperforms the minimal set coverings based algorithm and the random algorithm. Moreover, the proposed comprehensive community clouds selection algorithm can guarantee good global performance in terms of access cost, monetary cost, security level and trust between user and community clouds."
2523,"E-contract evolves over a period of time due to changes in e-contract environment. E-contract evolution adversely affects the execution of e-contracts. An e-contract is specified by a model at conceptual level, supported by a database management system (DBMS) at logical level and by both DBMS and workflow management system (WFMS) at implementation level. Any changes in the design as well as runtime environment during e-contract enactment must be reflected at all levels. Conventional modeling approaches simply model the e-contracts as specified workflows and execute them. Since, e-contracts are complex in nature, such models have to undergo large number of transformations during e-contract enactment. Metamodeling approach guides the correctness of transformed models by generating appropriate model instances according to e-contract constraints to support evolution. A metamodel has structural artifacts to capture the relationships among contract elements and model the required specifications and semantics present in an e-contract as a template. In this paper, we develop 1) an active metamodeling approach by a) introducing the taxonomy of evolution operations and b) handling metaevents to facilitate the structural and behavioral conformance during e-contracts evolution, and 2) an ER*
<sup>EC</sup>
 architecture for enacting evolving e-contracts. Our methodology actively capture behavior features from e-contract executions to drive e-contract evolution."
2559,"Offering real-time data security for petabytes of data is important for cloud computing. A recent survey on cloud security states that the security of users' data has the highest priority as well as concern. We believe this can only be able to achieve with an approach that is systematic, adoptable and well-structured. Therefore, this paper has developed a framework known as Cloud Computing Adoption Framework (CCAF) which has been customized for securing cloud data. This paper explains the overview, rationale and components in the CCAF to protect data security. CCAF is illustrated by the system design based on the requirements and the implementation demonstrated by the CCAF multi-layered security. Since our Data Center has 10 petabytes of data, there is a huge task to provide real-time protection and quarantine. We use Business Process Modeling Notation (BPMN) to simulate how data is in use. The use of BPMN simulation allows us to evaluate the chosen security performances before actual implementation. Results show that the time to take control of security breach can take between 50 and 125 hours. This means that additional security is required to ensure all data is well-protected in the crucial 125 hours. This paper has also demonstrated that CCAF multi-layered security can protect data in real-time and it has three layers of security: 1) firewall and access control; 2) identity management and intrusion prevention and 3) convergent encryption. To validate CCAF, this paper has undertaken two sets of ethical-hacking experiments involved with penetration testing with 10,000 trojans and viruses. The CCAF multi-layered security can block 9,919 viruses and trojans which can be destroyed in seconds and the remaining ones can be quarantined or isolated. The experiments show although the percentage of blocking can decrease for continuous injection of viruses and trojans, 97.43 percent of them can be quarantined. Our CCAF multi-layered security has an average of 20 percent bet..."
2577,"Recently, cloud computing rapidly expands as an alternative to conventional computing due to it can provide a flexible, dynamic and resilient infrastructure for both academic and business environments. In public cloud environment, the client moves its data to public cloud server (PCS) and cannot control its remote data. Thus, information security is an important problem in public cloud storage, such as data confidentiality, integrity, and availability. In some cases, the client has no ability to check its remote data possession, such as the client is in prison because of committing crime, on the ocean-going vessel, in the battlefield because of the war, and so on. It has to delegate the remote data possession checking task to some proxy. In this paper, we study proxy provable data possession (PPDP). In public clouds, PPDP is a matter of crucial importance when the client cannot perform the remote data possession checking. We study the PPDP system model, the security model, and the design method. Based on the bilinear pairing technique, we design an efficient PPDP protocol. Through security analysis and performance analysis, our protocol is provable secure and efficient."
2595,"The increasing interest of researchers in service oriented architecture (SOA) for wireless sensor networks (WSNs) is opening new unexplored venues in the field of WSNs. In service oriented systems, services are configured and composed of various other services and thus perform complex tasks. In such composite services, the geospatial locations of services and their coverage is of vital importance as they signify the geospatial relevance of the service to the area of interest to the user. In this paper, we present a service-oriented system for WSNs that is capable of performing service configuration under geospatial and relevancy constraints. We present and evaluate “Cost Based Model (CBM)” and “Gain Based Model (GBM)” approaches to capture the relevancy of services hosted on WSN nodes in composite service configuration. The system is resilient to failures and can operate in manual or autonomous recovery modes. The system supports three service configuration methods namely, distributed, centralized and hybrid. Furthermore, we present a novel emulation mechanism for testing the performance of our proposed relevancy models and show that our system efficiently configures services."
2596,"The cost of electricity contributes significantly to the operating expense incurred in hosting cloud services. It is necessary to consider this cost while charging the consumers for their service utilization. In this work, we arrive at a metering mechanism for cloud services, in which the price of a cloud service tracks the variable input cost of electricity from a smart grid. The power-aware cloud metering developed here is a dynamic pricing and billing model where tariff for a cloud service is varied in accordance with the input electricity cost. We arrive at a model for power consumption of virtual machines hosted on the cloud infrastructure. This power consumption model is used in calculating the cost of operation of the service. A cloud instance leased by a consumer is billed based on the cost of operation obtained, and its resource utilization. Experimental results validate the approach presented."
2609,"Mobile, pervasive computing environments respond to users' requirements by providing access to and composition of various services over networked devices. In such an environment, service composition needs to satisfy a request's goal, and be mobile-aware even throughout service discovery and service execution. A composite service also needs to be adaptable to cope with the environment's dynamic network topology. Existing composition solutions employ goal-oriented planning to provide flexible composition, and assign service providers at runtime, to avoid composition failure. However, these solutions have limited support for complex service flows and composite service adaptation. This paper proposes a self-organizing, goal-driven service model for task resolution and execution in mobile pervasive environments. In particular, it proposes a decentralized heuristic planning algorithm based on backward-chaining to support flexible service discovery. Further, we introduce an adaptation architecture that allows execution paths to dynamically adapt, which reduces failures, and lessens re-execution effort for failure recovery. Simulation results show the suitability of the proposed mechanism in pervasive computing environments where providers are mobile, and it is uncertain what services are available. Our evaluation additionally reveals the model's limits with regard to network dynamism and resource constraints."
2611,"The notion of collaborative scientific workflow was coined to address the increasing need for collaborative data analytics using the scientific workflow technique. In such collaborative environments, adequate access control policies are necessary for controlling the sharing of workflows, data products, and provenance information among collaborating parties. Meanwhile, it is important to ensure that the evolution of workflow provenance access control policies meets certain qualities to guarantee the correctness and performance of the policy enforcement engine. To address this concern, this paper proposes a rolebased access control model for scientific workflow provenance. Three quality requirements are defined for scientific workflow provenance access control policies - consistency, completeness, and conciseness. A mapping mechanism from the specifications of workflows to their counterparts in the provenance is developed to preserve quality properties."
2612,"In the new era of computing, SaaS software with different architectural characteristics might be priced in different ways. Even though both pricing and architectural characteristics are responsible for the success of the offering, the relationship between architectural and pricing characteristics has not been studied before. The present study fills this gap by employing a multi-case research. The findings accentuate that flexible and well-designed architecture enables different pricing models, however, poorly designed architecture limits also the pricing. Scalability and high level of modularity are the major enablers of a great variety of pricing models. Using public cloud services may lead to introducing usage-based pricing or in the contrary, making the pricing simpler. Applying multi-tenancy lowers the customizability, consequently the customers' negotiation power decreases. Pricing may give special requirements to the architectural design, such as scalability, customizability and additional components."
2613,"Simulation is known to be an effective technique to understand and manage traffic in cities of developed countries. However, in developing countries, traffic management is lacking due to a wide diversity of vehicles on the road, their chaotic movement, little instrumentation to sense traffic state and limited funds to create IT and physical infrastructure to ameliorate the situation. Under these conditions, in this paper, we present our approach of using the Megaffic traffic simulator as a service to gain actionable insights for two use-cases and cities in India, a first. Our approach is general to be readily used in other use cases and cities, and our results give new insights: (a) using demographics data, traffic demand can be reduced if timings of government offices are altered in Delhi, (b) using a mobile company's Call Data Record (CDR) data to mine trajectories anonymously, one can take effective traffic actions while organizing events in Mumbai at local scale."
2614,"Timed properties are an important quality criterion in Business-to-Business (B2B) Web service compositions. To guarantee the correctness of these compositions, the deadlock freeness as well as some non-functional properties such as timed constraints should be satisfied. Since there are some recent research efforts concerning the correctness of service composition from the structural and behavioral compatibility perspective, this paper will mainly focus on the timed properties verification. Existing verification techniques however need to know all the activities and activities' time delays of the whole process, thus these techniques are infeasible when some business parties are unwilling to reveal their internal process for privacy or business reasons. To address this problem, a timing property preserving public view approach is proposed. By composing those public views published by all the participants, timed verification can be conducted so that business parties can identify suitable services that meet their timed requirements. A case study of B2B collaboration is included to illustrate our approach."
2615,"This paper addresses workload allocation techniques for clusters of computers. The workload in question is homogenous or heterogeneous. Homogeneous workload contains only QoS-demanding jobs (QDJ) or nonQoS jobs (NQJ) while heterogeneous workload is a mix of QDJs and NQJs. The processing platform used is a single cluster or multiple clusters of computers. Two workload allocation strategies (called ORT and OMR) are developed for homogeneous workloads by establishing and numerically solving optimisation equation sets. The ORT strategy achieves the optimised mean response time for homogeneous NQJ workload; while the OMR strategy achieves the optimised mean miss rate for homogeneous QDJ workload. Based on ORT and OMR, a heterogeneous workload allocation strategy is developed to dynamically partition the clusters into two parts. Each part is managed by ORT or OMR to exclusively process NQJs or QDJs. The judicial partitioning achieves an optimised comprehensive performance, which combines the mean response time and the mean miss rate. The effectiveness of these workload allocation techniques is demonstrated through queueing-theoretical analysis as well as through experimental studies. These techniques can be applied to e-business workload management to improve the distribution of different types of requests in clusters of servers."
2616,"There is a global consensus on the need to reduce our collective carbon footprint. While much research attention has focused on developing alternative energy sources, automotive technologies or waste disposal techniques, we often ignore the fact that the ability to optimize (existing) operations to reduce their emissions impact is fundamental to this exercise. We believe that by transforming the problem into the domain of Business Process Management (BPM) we can leverage the rich expertise in this field to address issues associated with identifying areas for improvement, understanding the implication and performing carbon footprint minimization. We will use the term ``Green BPM'' to describe a novel class of technologies that leverage and extend existing BPM technology to enable process design, analysis, execution and monitoring in a manner informed by the carbon footprint of process designs and instances. This article describes the first steps in the development of this class of technologies."
2617,"In the past decade, scientific workflow systems have significantly improved scientists' ability to structure scientific processes, use computational resources, and analyze their data more efficiently. Such productivity can be further enhanced by sharing, reusing, and repurposing existing tasks and workflows across different users and institutes. However, existing scientific workflow systems are mainly single-user oriented with limited sharing and reusing functionalities. To overcome such limitations, we propose a folksonomy-based social workflow recommendation system to improve workflow design productivity. Our contributions are: i) We developed a web-based workflow design environment (called Web bench) to allow users to create workflows and collaboratively annotate and categorize them using social tags. The resulted folksonomy improves workflow search ability and share ability. ii) We proposed several workflow recommendation strategies to automatically or semi-automatically augment an in-progress workflow, leveraging both structural and semantic similarities between workflows and guiding information extracted from previously created workflows in the database. iii) We implemented the proposed environment and strategies in a prototype based on the DATAVIEW scientific workflow management system and validated our approach with numerous use cases."
2618,"Chinese enterprises have been conducting low-end processing for foreign brands. In recent years, they want to get rid of this high-pay low-income pattern and develop towards the high-end of the value chain. Most of them are transforming to service-focused enterprises that aim to provide customers with customized service. In service-focused enterprises, the human asset - like industry experts, technology experts, product experts and so on - is very important. Sometimes, the services provided by experts can directly affect the satisfaction of customers. Service-focused enterprises always have to face this challenge: multiple projects occur in emergency situations at the same time and need to dispatch multiple suitable experts from multiple places in order to respond. Under the constraint of human asset requirements on time and cost, this paper establishes an uncompromised resource allocation model for human asset emergency response. According to the characteristics of human asset emergency issues, the mathematical model can be presented to help solve the largescale emergency difficulties among multiple projects, multiple experts' types and multiple experts' locations. Finally, a case study is carried on to prove the algorithm's effectiveness."
2619,"With the fast growth of Information Technology (IT), organisations rely mostly on web services, cloud services and recently on Big Data Analytics services (BDA services), in order to support their business services. To securely use these services, service clients sign a Service Level Agreement (SLA) with service providers, regarding a particular service provision. Typically, SLAs define the properties that need to be preserved during the provision of a service (e.g., quality of service properties) and actions that will be applied if the service provision violates the defined properties (e.g., penalties or re-negotiation). Whilst significant research has focused on monitoring SLAs during the provision of services, the exploration and validation of the potential consequences of SLAs for the involved parties prior to putting them in operation is not addressed by existing research. In this paper, we present an approach to SLA validation that is based model checking. Our approach is based on the translation of SLAs expressed in WS-Agreement into models of the probabilistic model checker PRISM and the validation of SLA properties using the model checking capabilities of this tool."
2620,"Sensor applications are typically composed of a number of functional components that run distributedly on the nodes of a sensor network, communicating and interacting with one another. Service composition is emerging as a viable approach towards the automatic synthesis of such sensor applications. However, for service composition to be practical, it has to comply with policies that define security and management constraints on the use of these service components and the interconnections amongst them. Prior research efforts have primarily focused on efficient evaluation of security policies during the composition process, which is not sufficient when generic network management constraints need to be expressed and evaluated. In this work, we propose a policy model and evaluation approach that enables us to define and check attribute-based policies, for controlling the sensor service composition process. Attribute-based policies are generic and allows us to express a wider spectrum of constraints than currently possible. Using this model and based on a previously-proposed sensor service composition algorithm, we introduce a policy evaluation method that allows for efficient checking of policy constraints. We further present a novel implementation of the proposed approach in the IBM Sensor Fabric, a middleware framework that simplifies the development of distributed, sensor network services. We also present preliminary performance evaluation results using our prototype."
2621,"We present a model about DOM-based Web document segmentation using the semistructure information of Web pages. This model builds DOM tree of the Web page by parsing HTML tags which organize structure of the Web page. By improving traditional plain text segmentation algorithms, we expand these algorithms to suit Web text segmentation. Then, with the boundaries between the nodes in the DOM tree, precision of segmentation results can be increased further."
2622,"With the proliferation of Web services, more and more functionally equivalent services are being published by service providers on the Web. Although more services mean more flexibility for consumers, it also increases the burden of choosing as consumers may have little or no past experience with the service they will interact with. Therefore, reputation systems have been proposed and are playing a crucial role in the service-oriented environment. Current reputation systems are mainly built upon the explicit feedback or rating given by consumers after experiencing the service. Unfortunately, services at the cold-start stage, prior to being rated, face the rating scarcity problem. In this paper, we focus on this problem and address it through a novel reputation model that uses the Elo algorithm to consider consumer-implicit information in a graph analysis approach. A theoretical analysis is conducted to identify the sufficient and necessary condition for the model to converge to a stable state. Furthermore, experiments confirm our model outperforms the widely adopted reputation algorithm in both accuracy and convergence in the situation of rating scarcity."
2623,"The selection, composition and integration of Transport Telematic Services (TTSs) is crucial for achieving cooperative Intelligent Transport Systems (ITS). To enable future adaptation, models for selecting and composing TTSs needs to take into account possible future modifications, upgrades or downgrades of different TTSs without tipping off the benefit edge. To achieve this, a Strategic Service Selection Problem (SSSP) for TTSs is presented in this article. The problem involves selecting a set of TTSs that maximizes net societal benefits over a strategic time period, e.g., 10 years. The formulation of the problem offers possibilities to study design alternatives taking into account future modifications, extensions, upgrades or downgrades of different TTSs. Two decisive factors affecting the choices and modifications of TTSs are studied: 1) the effects of allowing market forces to drive the choices of TTSs, and 2) the effect of using Governmental policies to mandate the introduction of TTSs, e.g., Road User Charging and eCall. Case study results indicate that in determining combinations of TTSs that can be deployed in a period of 10 years, enforcing too many TTSs can retard the ability of the market to generate net benefits even though the results could be that more TTSs will be deployed."
2624,"Storage management is becoming the largest component in the overall cost of storage ownership. Most organizations are trying to either consolidate their storage management operations or outsource them to a storage service provider (SSP) in order to contain the management costs. Currently, there do not exist any planning tools that help the clients and the SSPs in figuring out the best outsourcing option. In this paper we present a planning tool, Brahma, that specifically addresses the above mentioned problem, as Brahma is capable of providing solutions where the management tasks are split between the client and SSP at a finer granularity. Our tool is unique because: (a) in addition to hardware/software resources, it also takes human skill set as an input; (b) it takes planning time window as input because plans that are optimal for a given time period (e.g. a month) might not necessarily be the most optimum for a different time period (e.g. a year); (c) it can be used separately by both the client and the SSP to do their respective planning; (d) it allows the client and the SSP to propose alternative solutions if certain input service level agreements can be relaxed. We have implemented BRAHMA, and our experiment results show that there definitely are cost benefits that one can attain by having a tool with the above mentioned functional properties."
2625,"Large monolithic business systems, as for example the business suite Mysap from SAP, are increasingly restructured in system architectures with fine-granular components, which are loosely coupled by Web services. Their intra-system collaboration has usually no need for dynamic discovery with UDDI. Those architectures show many advantages, but cause also a performance problem, which might become an inhibitor to their use in operational systems. We experienced that problem with I/sup 3/M, an instant messaging system we developed and present as an example. Measurements with I/sup 3/M show that the performance overhead of Web service invocation among collocated system components may decrease the system performance dramatically. We propose a solution to the performance problem that preserves the benefits of Web service invocations, but provides the performance of ""normal"" component invocations."
2626,"Modern business process modeling environments support distributed development by means of model version control, i.e., comparison and merging of two different model versions. This is a challenging task since most modeling languages support an almost arbitrary creation of process models. Thus, in multi-developer environments, process models or parts of them are often syntactically very different but semantically equivalent. Hence, the comparison of business process models must be performed on a semantic level rather then on a syntactic level. For the domain of business process modeling, this problem is yet unsolved. This paper describes an approach that allows the semantic comparison of different business process models using a normal form. For that purpose, the process models are fully automatically translated into process model terms and normalized using a term rewriting system. The resulting normal forms can be efficiently compared and easily be used for reconciliation. Our approach enables the semantic comparison of business process models ignoring syntactic redundancies."
2627,"Businesses recently began shifting from proprietary models towards industry standards. Today, when businesses are faced with medium and large SOA projects, they strive to standardize their business process models, information models, and message models by complying with known industry standards. As service orientation and composition become more important, these standards also become critical for efficient SOA integration. However, in doing so, the businesses face a new challenge - the challenge to effectively utilize the industry standards models. Using industry standards models effectively is difficult since they are typically large and complex, thus resulting in the creation of vast amounts of inter-relationships. To meet this challenge, we describe a new approach that provides a holistic view of these heterogeneous SOA models and corresponding industry standards. Such a unified view must provide access to heterogeneous data sources and models, allow search and query of them as a single model source all across the SOA solution stack, and provide advanced services such as traceability, scoping, and impact analysis. To validate the proposed holistic approach, we describe a model catalog and repository system and present its use on a field-based example."
2628,"The Internet of Things (IoT) offers the promise of integrating the digital world of the Internet with the physical world in which we live. But realizing this promise necessitates a systematic approach to integrating the sensors, actuators, and information on which they operate into the Internet we know today. This paper reports the design and development of an open community-oriented platform aiming to support federated sensor data as a service, featuring interoperability and reusability of heterogeneous sensor data and data services. The concepts of virtual sensors and virtual devices are identified as central autonomic units to model scalable and context-aware configurable/reconfigurable sensor data and services. The decoupling of the storage and management of sensor data and platform-oriented metadata enables the handling of both discrete and streaming sensor data. A cloud computing-empowered prototyping system has been established as a proof of concept to host smart community-oriented sensor data and services."
2629,"The paper discusses the application of Web services and utility computing for inventory optimisation in manufacturing systems. The business case is highlighted from an inventory optimisation case study conducted with a UK based small-medium enterprise (SME) positioned within the discrete manufacturing industry. To measure the performance of the SME, a tried and tested methodology was applied. The challenges faced by organisations in cost effectively optimising inventory are discussed, detailing the traditional approaches and solutions, and exploring the alternative utility computing model and its associated benefits. The proposed functionality of a Web service solution is simulated in the case study to highlight the business benefits it could deliver. This provides the foundations for the design and architecture of the Web-service solution delivered as a utility computing model. The paper also details how the solution would interact with an enterprise resource planning (ERP) system to automate the optimisation process."
2630,"Metaheuristics grid (MHGrid) is a service oriented grid application that enables the user to solve almost any global optimization problem using metaheuristics techniques. Two problems potentially limit the generality of MHGrid over the problem type space; having a fixed set of solvers and lacking the solver-problem relation semantics. The set of strategies enforced to resolve these two problems are: offering the solvers as services, enabling the user to define his parallelization model, allowing the user to add his own service and maintaining service-based functionalities on both the middleware layer and application layer. This paper explains the design, architecture and implementation of the SOA that MHGrid endorses that would allow the enforcing of the resolving strategies."
2631,This paper addresses the problem of mapping business contract conditions onto the messages and rules that represent service interactions in a collaborative business process. We describe why this mapping is not straightforward by means of an example. We then consider a message-driven process language as a target for the mapping and use this mapping solution to discuss broad range of problems related to the mapping problem
2632,"In this paper, we are interested in the service selection problem. In the variant that we consider, the problem's inputs consists of: (1) a services' composition described as an extended business process, (2) a set of concrete services that can be associated with the nodes of the services' composition. The objective is to decide the best mapping between concrete services and composition nodes such as to minimize a penalty function. We consider for its resolution a view that was poorly investigated until now. It consists of mapping the service selection onto the constraint satisfaction problem. Based on the know-how in the resolution of this latter problem, we propose a new parallel algorithm for the service selection problem in shared memory contexts. The parallelization of service selection was investigated in the past. However, we differ from existing works on three points. Firstly, we adopt a more classical representation of the services' composition and parallelize a distinct sequential algorithm. Secondly, our solution is more scalable because the creation of parallelism is not limited by the structure of the composition graph. Finally, we combine two well-investigated techniques for parallelization: the domain decomposition and the work stealing. The proposed parallel algorithm is based on a general framework for which multiple instantiations are possible. We also propose an evaluation of the different instantiations and analyze their speed-ups. The experimental results show that super-linear speed-ups can be reached with our parallel framework."
2633,"In this paper we examine the problem of rich information environments and the need to narrow the agents attention to what is important for them to interact and later to evaluate and transfer reputation values, using attention allocation technique (AA). We also argue that this cannot be done without the aid of service oriented architecture (SOA). Reputation is used in our work as a service, presenting a new concept- that is reputation-as-a-service (RaaS). We then present a service-oriented model for optimizing the presentation and the use of reputation in order to maximize its value to both users and providers."
2634,"Resilience against unexpected server failures is a key desirable function of quality-assured service systems. A good capacity planning decision should cost-effectively allocate spare capacity for exploiting failure resilience mechanisms. In this paper, we propose an optimal capacity planning algorithm for server-cluster based service systems,particularly the ones that provision composite services via several servers. The algorithm takes into account two commonly used failure resilience mechanisms: intra-cluster load-controlling and inter-cluster failover. The goal is to minimize the resource cost while assuring service levels on the end-to-end throughput and response time of provisioned composite services under normal conditions and server failure conditions. We illustrate that the stated goal can be formalized as a capacity planning optimization problem and can be solved mathematically via convex analysis and linear optimization techniques. We also quantitatively demonstrate that the proposed algorithm can find the min-cost capacity planning solution that assures the end-to-end performance of managed composite services for both the non-failure case and the common server failure cases in a three-tier web-based service system with multiple server clusters. To the best of our knowledge, this paper presents the first research effort in optimizing the cost of supporting failure resilience for quality-assured composite services."
2635,"Semantic annotation of Web Services can facilitate the automated service discovery and composition. At present, however, many solutions suffer from redundant annotations or imprecise derived annotations. The fundamental task to address the issue is to find parameters that have same semantics in a large number of Web Services. This paper proposes a classification based approach for identifying parameters that are semantically equivalent, laying the foundation for non-redundant annotation. Subsequently the paper presents the non-redundant annotation, in which the parameter space is firstly reduced, and then semantic annotation is performed on the reduced parameter space, finally annotation results of the reduced parameter space are expanded to the original parameter space. To evaluate the final annotation results, the paper gives a methodology based on service discovery. The experimental results indicate that the parameter association approach can achieve outstanding accuracy in identifying the equivalence between parameters. Moreover, it is suggested that the non-redundant annotation can greatly reduce redundant annotations, and can augment the semantics of Web Services with adequate accuracy, which can achieve similar performance in service discovery as OWL-S services do."
2636,"With increasing globalization and outsourcing, IT service providers need a way to assess the resilience of their operations (and of their vendors) to understand the impact that various failures can have on their output, and to guide their decisions on where to best invest for remediation. In this paper, we present the resiliency maturity index (RMI), a framework for characterizing and evaluating the resiliency of an IT services organization. The framework consists of (a) a model for capturing the hierarchical component structure of the organization and the relationships among (sub)components, and (b) a method to determine a quantitative score that indicates the overall resiliency maturity of the organization. We describe a case study of the application of RMI to a hypothetical IT service provider organization, based on our experience of applying it to actual real-world delivery centers. We also demonstrate the utility of the RMI framework through various realistic scenarios such as making investment decisions, .assessing the impact of organizational expansion, and outsourcing decisions."
2637,"In this paper, we present a system called ReAct which, given a problem/incident description, helps the service agents to easily identify set of actions and the possible action sequence to resolve the issue mentioned in the ticket. Th eframework uses unstructured text analysis on historical ticket data to find the next best action steps and uses visualization to help user choose the most suitable option."
2638,"Services computing technology enables scientists to expose data and computational resources wrapped as publicly accessible Web services. However, our study indicates that scientific services are currently poorly reused in an ad hoc style. This project aims to help domain scientists find interested services and reuse successful processes to attain their research purposes in the form of workflows. In contrast to existing interface-based services discovery approaches, this paper proposes a novel approach of proactively recommending services in a workflow composition process, based on service usage history. The underpinning is a People-Service-Workflow (PSW) network that models existing scientific artifacts, services and workflows, and their past usage relationships into a social network. Various social network analysis techniques are applied to discover hidden knowledge accrued. A prototyping search engine has been developed as a proof of concept, and is seamlessly integrated as a plug-in into the Tavern a workbench, a widely used scientific workflow management tool."
2639,"A composite service can be constructed with the arbitrary combination of sequential, parallel, loop, and conditional structures. In this paper, we propose a general solution to calculate the QoS for composite services with complex structures. We also show QoS-based service selection can be conducted based on the proposed QoS calculation method. An application example is given to show the effectiveness of the method."
2640,"IBM defines autonomic computing as ""an approach to build computing systems that are self-managed and operate with a minimum of human interference"". To build an autonomic system, some characteristics such as self-knowledge, self-optimization, self-healing, self-defense, among others, have to be built into it. Service oriented architecture (SOA) is an architectural paradigm that promotes development of software systems as sets of collaborative and loosely coupled services. It defines principals for achieving service interoperability, agility, reusability, integration and such others across heterogeneous distributed software systems. It is widely accepted that Web services are entities that provide many out-of-the-box capabilities for building service oriented software systems. Since the SOA approach has been proposed recently, various standards for addressing areas such as service orchestration, security, contract definition languages, policy exchange mechanisms and such others, are presently under development. One such key area that is gaining a lot of attention recently is Web service manageability. This paper discusses an approach for modeling the manageability of a Web service at design time and implementing that model to impart autonomic capabilities to it at run time. Of the various characteristics of autonomic systems self-knowledge, self-recovery and self-optimization are discussed herein."
2641,"With the rapid development of SOC (Service oriented computing), the automated service composition has become an important research direction. Through automated service composition, business processes need not to be constructed in advance, which helps to improve the flexibility of service composition. The current research on automated service composition is mainly based on AI techniques, and a common domain-oriented knowledge base is usually required to perform the heuristic planning. In practice, it is impossible for the knowledge base to characterize the personalized requirements of different users, so the AI-based methods can not apply to the user-centric application scenarios. In this paper, we propose PASS, a novel approach to personalized automated service composition. With PASS, both the hard-constraints represented by user's initial state, and the soft-constraints represented by user preferences can be satisfied in the process of automated service composition. Furthermore, three algorithms are designed to implement preference-aware automated service composition. In these algorithms, the Pareto dominance principle and relaxation degree are used to select the most satisfied composite service for users. Finally, comprehensive simulations are conducted to evaluate the performance and effectiveness of the proposed algorithms."
2642,"Migrating to microservices (microservitization) enables optimising the autonomy, replaceability, decentralised governance and traceability of software architectures. Despite the hype for microservitization , the state of the art still lacks consensus on the definition of microservices, their properties and their modelling techniques. This paper summarises views of microservices from informal literature to reflect on the foundational context of this paradigm shift. A strong foundational context can advance our understanding of microservitization and help guide software architects in addressing its design problems. One such design problem is finalising the optimal level of granularity of a microservice architecture. Related design trade-offs include: balancing the size and number of microservices in an architecture and balancing the nonfunctional requirement satisfaction levels of the individual microservices as well as their satisfaction for the overall system. We propose how self-adaptivity can assist in addressing these design trade-offs and discuss some of the challenges such a selfadaptive solution. We use a hypothetical online movie streaming system to motivate these design trade-offs. A solution roadmap is presented in terms of the phases of a feedback control loop."
2643,"The paper reports on research to design and implement a service-based, secure collaborative workflow system for distributed aircraft maintenance environment (DAME). This has been developed by the Universities of York, Sheffield, Oxford and Leeds, and industrial partners, Rolls-Royce and Data Systems and Solutions (DS&amp;S). DAME is a prototype system to support aircraft engine maintenance and predictive services. It is an example of a virtual organisation with grid-based services running at the four universities sites. To meet the industrial requirements strong security is implemented to protect commercially sensitive services and data. Application services have been coordinated using the DAME workflow management system to automate business processes and control collaborative access. A dynamic workflow-team policy is used to authorise user access to workflow instances and corresponding service instances. The paper includes aspects of an initial evaluation with the industrial participants and illustrates the feasibility of using DAME in an industrial environment."
2644,"Recent research has demonstrated the benefits of replication of requests with canceling, which initiates multiple concurrent replicas of a request and uses the first successful result, immediately removing the remaining replicas of the completed request from the system. This paper suggests that the benefits of replication may come at the risk of an abrupt system transition to an undesirable highly congested equilibrium. To expose, evaluate, and ultimately manage these risk/benefit trade-offs, we generalize the replication strategy by: (a) accounting for the possible inefficiency of ""remote"" service, (b) allowing replication only when static routing fails to identify an idle ""local"" server, and (c) requiring one or more replicas of the same request to be completed to improve fault tolerance using a majority rule decision. Due to the intractability of the Markov performance model, our analysis is based on mean-field and fluid approximations. Future research should evaluate the accuracy of assertions based on these approximations, and ultimately develop practical solutions for optimization of various performance trade-offs in distributed systems with replication."
2645,"For service-oriented architectures that span multiple businesses, organizations must transfer information back-and-forth about their available services. Because of the potential large volume, it is unreasonable and impractical to expect human practitioners to handle the number of interactions desired and/or required on a continual basis. Intelligent agents offer the adaptability and flexibility to handle the knowledge transfer that must occur in order to share web service offerings. Agents that operate in this domain will require specialized communication protocols that effectively transfer service-oriented information. This paper introduces an architecture and specialized communication procedures designed for this sort of knowledge sharing environment. We show that these procedures perform reasonably when evaluated using current agent communication technologies."
2646,"A holistic view of the network is key to the successful operation of many distributed, cloud-based, and service-oriented computing architectures. Supporting network-aware applications and application-driven networks requires a detailed representation of network resources, including multi-layer topologies, associated measurement data, and in-the-network service location and availability information. The rapid development of increasingly configurable and dynamic networks has increased the demand for information services that can accurately and efficiently store and expose the state of the network. This work introduces our Unified Network Information Service (UNIS), designed to represent physical and virtual networks and services. We describe the UNIS network data model and its RESTful interface, which provide a common interface to topology, service, and measurement resources. In addition, we describe the security mechanisms built into the UNIS framework. Our analysis of the UNIS implementation shows significant performance and scalability gains over an existing and widely-deployed topology, service registration, and lookup information service architecture."
2647,"Self help portals are increasingly popular means for enabling users to find information, resolve problems, and process transactions directly without calling contact centers. Such portals can result in faster problem resolution for users and cost savings for the contact center. However, the effectiveness of self help portals is often limited by the topicality, recency, and relevance of the knowledge (documents, etc.) that users are provided access to. In this paper, we present a system and an architecture for automatically mining the top issues - questions people are calling contact centers for - and presenting corresponding solution documents to users through self help portals. Running the top issues mining regularly (e.g. hourly, daily, etc.) ensures dynamically updated and relevant content on the portals and can greatly reduce costs at contact centers by avoiding calls. Furthermore, top issues mining can highlight the knowledge gap or the issues for which no solution documents currently exist. We describe our end to end system, present algorithms for mining, and discuss the knowledge gap that can prevent self enabling portals from realizing its potential benefits."
2648,"Besides performance, dependability and energy efficiency are two critical concerns during the design, development and management of large-scale services computing systems. In this paper, we jointly consider the performance, dependability and energy efficiency, and optimize the dependability-aware energy efficiency of services computing systems by maximizing the quality of service and dependability revenue and minimizing energy costs. Markov reward models are put forward, and quantitative analysis of them is carried out. In addition, the methodologies for hierarchical model composition and state aggregation are proposed. Furthermore, the optimization problem is formulated as an average reward criterion Markov decision problem, and the algorithm to solve it is introduced. Finally, the LANL service systems are analyzed and optimized as a case study to illuminate how this approach can apply to large-scale systems in reality."
2649,"This paper proposes a hybrid approach in using Google Distance and WordNet together in a new method for the semantic similarity matching stage of web services discovery. We provide comparisons, using services recall and precision metrics, between our hybrid approach and our earlier lightweight Google Distance-based approach for web services discovery. Our performance evaluation demonstrates the trade-offs between the Google Distance only approach and a WordNet-assisted Google Distance hybrid approach for similarity matching in the web services discovery phase. Further, the impact of both approaches on QoS-enabled web service composition is described for 7 representative web transactions in the Travel domain."
2650,"Web services are often deployed with critical software bugs that may be maliciously exploited. Developers often trust on penetration testing tools to detect those vulnerabilities but the effectiveness of such technique is limited by the lack of information on the internal state of the tested services. This paper proposes a new approach for the detection of injection vulnerabilities in web services. The approach uses attack signatures and interface monitoring to increase the visibility of the penetration testing process, yet without needing to access web service's internals (as these are frequently not available). To demonstrate the feasibility of the approach we implemented a prototype tool to detect SQL Injection vulnerabilities in SOAP. An experimental evaluation comparing this prototype with three commercial penetration testers was conducted. Results show that our prototype is able to achieve much higher detection coverage than those testers while avoiding false positives, indicating that the proposed approach can be used in real development scenarios."
2651,"This tutorial will share with the audience on how to leverage the foundational knowledge of Service- Oriented Architecture (SOA) and Web services to build service-oriented business consulting method, enable Software As Services and Services As Software. Advanced SOA techniques are covered in the following topics: services publishing and discovery, business services relationship modeling, business requirements-driven services composition, services value chain collaboration, business process integration and modeling, enterprise modeling, and project-based business performance management. A Services Engineering method and delivery framework will be discussed as a case study. The target audiences are all-level researchers, practitioners, and students. From this tutorial, the audiences could learn the actual service delivery processes, technologies, and methodologies in the entire service delivery life cycle. This tutorial material is created for the IEEE Body of Knowledge initiative on Services Computing, which is sponsored by the IEEE Computer Society Technical Committee on Services Computing."
2652,"Small to medium businesses (SMBs) are an integral part of the U.S. economy, accounting for about half of private-sector output and employing more than half of private-sector workers. Managing limited resources as well as providing additional services that meet the range of customer expectations and improve the overall customer experience is a non-trivial activity for SMBs, who like any business, must focus on the seemingly conflicting goals of minimizing cost while maximizing customer value and profit. Queue management has been a problem for many years as an organization's ability (or inability) to adequately address queue management often has a direct correlation to the quality a customer associates with an overall service experience. This study describes Queue Admin, a database driven, online application designed to support SMBs in the management of various queues uncovering the impact that Queue Admin's design and delivery has on the quality of customers' overall service experience."
2653,"SOA provides a flexible framework tor better-integrated systems that meet business needs. However, the existing methods are not successful in helping business analysts to devise appropriate business services. Herein, a novel approach is presented to address service identification and thereby developing sound SOA. In contrast with conventional methodologies, we press on the criticality of exploiting the information of dynamic business behavior to develop business services"
2654,"Service-oriented architectures (SOA) facilitate the provision and orchestration of business services to enable a faster adoption to changing business demands. Several approaches have been described to generate executable description of service orchestrations based on visual business process models. These models describe workflows and related information on an abstract level supporting business analysts to state and verify business requirements. In previous work, we have adopted this approach to simplify the security engineering in service-oriented architectures. We foster a model-driven approach based on the integration of security annotations in visual modelling notation. These annotations are gathered and translated to a domain-independent security model that facilitates the generation of enforceable security configurations (e.g. WSSecurityPolicy). In this paper, we introduce our security meta-model for SOA that constitutes the foundation for our model-driven approach. Based on a model for service interactions that describes the exchange of information in a service-based system, we define a model to express security requirements and policies, and introduce a mapping to WS-Policy and WS-SecurityPolicy."
2655,"In current service composition efforts, the intermediate products such as data and related process fragments are neglected in many cases when the deviation occurs from predefined composition. The lack of adequate provenance support makes it not convenient when building the composition in an exploratory fashion. In this paper, we present ViPen model, and illustrate its operations to enhance the provenance for flexible deviation in service composition. Meanwhile, a partially ordered relation “derivation” is formed which guarantees the new derived processes can reuse previous knowledge effectively. A case study of one bioinformatics experiment is explained to verify our work."
2656,"Service grid is an infrastructure for service-oriented collective intelligence. It provides a set of enabling functionalities to support coordination of services, such as service registries, service composition, access control, and monitoring. To form the service-oriented collective intelligence, various types of services need to be connected on the service grid, and managed by the service grid operator. However, it is difficult for single service grid operator to gather and organize services in various domains. Therefore, building service grids in different domains and connecting these service grids are essential for expanding service-oriented collective intelligence across domains. To this end, we have designed a service domain model to specialize general-purpose service grid to a specific domain and realize interoperability among service grids. Moreover, we have also developed service grid federation architecture to share service registries, compose services across service grids, and control and monitor accesses to the composite services. Finally, we have applied the proposed architecture to the language service domain to construct the Language Grid."
2657,"In recent years, service-oriented computing has been successfully adopted by the industry and used in numerous projects. This raises however new challenges, especially with respect to service selection and ranking in dynamic environments. Selection and ranking are mostly based on first-order predicates and comparison functions, expressed in the consumer side. Such approaches suffer from several shortcomings. The expression language is too complex for most non-technical persons and yet not powerful enough to capture external information, which limits the use of contextual information. Also, current solutions lack flexibility to handle dynamic environments. This paper proposes to integrate algorithms based on the Formal Concept Analysis theory to extend service-oriented component models. The resulting framework externalizes service selection and ranking. Results are integrated in the Apache Felix iPOJO component model."
2658,"Mobile Applications are rapidly emerging as a convenient medium for using a variety of services. In ubiquitous environment, the challenge relies on developing applications that sense and react to environmental changes to provide a value-added user experience. This context awareness property improves application usability. Context information can be related to the environment, user but also to the device itself. Since smart phones are battery-powered, in an ideal scenario, the application will self-adapt and adjust its behavior according to the current battery status of the device. In this paper, we address the challenge of building self-adaptive battery-aware applications for mobile devices. Our solution is based on a dynamic feature configuration to adapt efficiently to the device status. We propose a distributed battery-awareness architecture where both the mobile client and the server side optimize the battery utilization when needed. We also propose three layers of dynamic adaptation: user features' availability adaptation, internal features' behavior adaptation, and data consumption adaptation. As illustrated by the experiments, doing the right adaptation in both server and client sides increases the lifetime span of the mobile device and hence the availability of the application."
2659,"The properties of data and activities in business processes can be used to greatly facilitate several relevant tasks performed at design-and run-time, such as fragmentation, compliance checking, or top-down design. Business processes are often described using workflows, and we present an approach to mechanically infer business domain-specific attributes of workflow components, including data items, activities, and elements of sub-workflows, from known attributes of workflow inputs and the structure of the workflow by modeling these components as concepts and applying sharing analysis applied to a Horn clause representation of the workflow. The analysis is applicable to workflows featuring complex control and data dependencies, embedded control constructs, such as loops and branches, and embedded component services."
2660,"We define a service delivery system as a set of interacting entities that are involved in the delivery of one or more business services. A service operating system manages the processes and resources within a service delivery system. This paper presents our on-going work on developing a formal model for these concepts, with the goal of clearly and precisely describing the delivery behavior of service systems. The model lays the groundwork for reasoning about the scenarios that occur in service delivery."
2661,"With the growing demand for service-oriented applications, the complexity of service change management is increasing. Existing work essentially addresses change decisions from a technical perspective (e.g. versioning, compatibility), but providers need to make decisions considering the business impact in terms of clients affected, revenues, costs and penalties. This paper suggests the use of Business Intelligence and Data Warehousing techniques to support business-oriented decisions throughout service life-cycle in a deep change context, i.e. a portfolio of services consumed in large scale by direct/indirect clients. The approach is centered on financial and usage indicators related to the service provision business, a data warehouse that provides a unified and integrated view of these indicators according to different analysis perspectives, and a data warehousing architecture that integrates heterogeneous data sources. We illustrate the impact analysis support provided by the approach through a case study inspired by a real world scenario."
2662,"Nowadays, more and more service consumers pay great attention to QoS (Quality of Service) when they find and select appropriate Web services. For most of the approaches to QoS-aware Web service recommendation, the list of Web services recommended to target users is generally obtained based on rating-oriented predictions, aiming at predicting the potential ratings that a target user may assign to the unrated services as accurately as possible. However, in some scenarios, high accuracy of rating predictions may not necessarily lead to satisfactory recommendation results. In this paper, we propose a ranking-oriented hybrid approach by combining item-based collaborative filtering techniques and latent factor models to address the problem of Web services ranking. In particular, the similarity between two Web services is measured in terms of the correlation coefficient between their rankings instead of between their ratings. Comprehensive experiments on the QoS data set composed of real-world Web services are conducted to test our approach, and the experimental results demonstrate that our approach outperforms other competing approaches."
2663,"Research in the field of semantic Web services aims at automating the discovery, selection, composition and management of Web services based on semantic descriptions. However, the applicability of many solutions developed in this field is hampered by the costs associated with semantically annotating large repositories of Web services. To overcome this gap we propose a practical method for semantically annotating collections of XML Schemas and Web service interfaces. We have evaluated this method on a large repository of governmental Web services. The evaluation shows that relatively simple techniques are surprisingly cost-effective, saving hundreds of man-hours of semantic annotation effort. Moreover, the proposed method does not assume the availability of a preexisting ontology or controlled vocabulary. Instead, the space of annotations is dynamically built during the annotation process."
2664,"WS-BPEL is a standard language to model business processes. Control flow is modeled explicitly using links. Data is passed via shared variables and there is no notion of explicit data links. However, explicit data links are an important means to reason about business process models. We present an algorithm to derive explicit data links in WS-BPEL processes. By considering dead path elimination as defined in WS-BPEL, we reduce the number of derived data links when compared to existing approaches that ignore dead path elimination."
2665,"Personal cloud storage is rapidly gaining popularity. A number of Internet service providers, such as Google and Baidu, entered this emerging market and developed a variety of cloud storage services. These ubiquitous services allow people to access personal files all over the world at anytime. With the prevalence of mobile Internet and rich media on web, more and more people use cloud storage for storing working documents, music, private photos and movies. Nevertheless, the size of the media files is often beyond the upper limit that a normal form-based file upload allows hence dedicated large-file upload services are required to be developed. Although various cloud vendors offer versatile cloud storage services, very little is known about the detailed development and deployment of the large-file upload services. This paper proposes a complete solution of large-file upload service, with the contributions in manyfold: Firstly, we do not limit the maximum size of a large file that can be uploaded. This is extremely practical to store huge database files from ERP tools. Secondly, we developed large-file upload service APIs that have very strict verification of correctness, to reduce the risk of data inconsistency. Thirdly, we extends the service developed recently for team collaboration with the capability of handling large files. Fourthly, this paper is arguably the first one that formalizes the testing and deployment procedures of large-file upload services with the help of Docker. In general, most large-file upload services are exposed to the public, facing security and performance issues, which brings much concern. With the proposed Docker-based deployment strategy, we can replicate the large-file upload service agilely and locally, to satisfy massive private or local deployment of KDrive. Finally, we evaluate and analyze the proposed strategies and technologies in accordance to the experimental results."
2666,"Over the past years services computing has become an emerging science that is highly regarded as a necessary technology not only by research but by industry as well. In the same context, the advent of cloud computing gave to services and web applications a whole new perspective and potential. Regardless of the rapid evolution in the fields of services and web technologies, ensuring the QoS of computing resources still remains an important topic. To this end, monitoring computing resources and application execution is an integral part of the services computing value chain. In this paper we present the architectural design and implementation of a service framework that monitors the resources of a physical as well as virtual infrastructure. Our solution extends Nagios, a widely used monitoring toolkit, through the implementation of NEB2REST, a Restful Event Brokering module."
2667,"Automatic composition of semantic Web services should make use of the ontology in which the services are specified. While the approaches can strongly benefit from doing so, they have to deal with the frame and ramification problems, necessitating worst-case exponential reasoning even to determine the outcome of applying a single Web service. The existing approaches to composition either ignore the background ontology, matching Web services based on concept names and hence removing the need for reasoning; or they employ full-scale reasoning and suffer from the unavoidable performance deficiencies. In our work, we instead look for interesting classes of ontologies where the required reasoning is polynomial. We define a formalism for semantic Web service composition. We present polynomial-time methods for dealing with several of the most commonly used ontology modelling constructs; further extensions are possible. We prove that our methods are correct. We are currently developing an implementation of our techniques."
2668,"Although quality requirements (QRs) are considered as being of crucial importance in today's service-oriented systems, existing approaches almost exclusively deal with QRs from the service providers' perspective (namely late QRs). The motivation for this paper is to address the analysis of QRs from the service customers' perspective (namely early QRs). The work presented in this paper focuses on the way that early QRs may be modeled and evaluated and demonstrates this approach on a stock trading service system - a real-world practice taken from an international firm in the financial sector. We focus on three QRs that are critical to service systems and especially that of stock trading, namely performance, availability and security. We introduce a modeling paradigm that extends the well-known UML Activity Diagram by explicitly representing these three QRs as an integral part of business process modeling and augments this modeling with quantitative and qualitative reasoning that together provide the means for enhanced decision making by service customers."
2669,"We propose a framework for intelligent document routing that extends XML technologies and enables realtime update of business routing logic. A novel feature of our system is the ability to semi-automate the process of creating and updating the routing rules. This is achieved by a closed loop system that performs clustering and inductive rule learning of document features. This framework was tested and deployed in a large real-world enterprise environment resulting in reduced development costs and simplified administration. The user interacts with the system using an application to graphically create, edit and publish the business processes"
2670,"Today organizations and business enterprises of all sizes need to deal with unprecedented amounts of digital information, creating challenging demands for mass storage and on-demand storage services. The current trend of clustered scale-out storage systems use symmetric active replication based clustering middleware to provide continuous availability and high throughput. Such architectures provide significant gains in terms of cost, scalability and performance of mass storage and storage services. However, a fundamental limitation of such an architecture is its vulnerability to application-induced massive dependent failures of the clustering middleware. In this paper, we propose hierarchical middleware architectures that improve availability and reliability in scale-out storage systems while continuing to deliver the cost and performance advantages and a single system image (SSI). Hierarchical middleware architectures organize critical cluster management services into an overlay network that provides application fault isolation and eliminates symmetric clustering middleware as a single-point-of-failure. We present an in-depth evaluation of hierarchical middlewares based on an industry-strength storage system. Our results show that hierarchical architectures can significantly improve availability and reliability of scale-out storage clusters."
2671,"Today's Internet is shifting towards a larger and smarter scenario known as the Internet of Things (IoT). The IoT envisions a multitude of heterogeneous objects and interactions with physical environments. In this environment, locating desirable services is challenging due to the considerable diversity, large number, dynamic behavior, and geographical distribution of the services provided by physical objects. In this paper, we propose a context-aware semantics-based service discovery mechanism - LOCA that can effectively locate services based on the context requirements. The proposed discovery framework is built on a fully distributed peer-to-peer (P2P) architecture which is scalable and robust. Moreover, a key feature of the discovery mechanism is its support of content and path locality. This feature can enhance the integrity of an organization and thus greatly improve the security of the organization. The effectiveness of the proposed framework is demonstrated through comprehensive simulation studies."
2672,"In ambient intelligence (AmI) environments, some services provided by AmI devices are often not visible to users and to other devices. The existing approaches deal with services' composition and discovery as two independent parts. In this paper, we propose an alternative approach based on logical reasoning agent system. This system is supported by a communication protocol where agents discover automatically services provided in their environment and construct dynamically composite services. The service composition is constructed from an exchange of idiomatic expressions among agents and users, while the discovery process takes the form of an information request via the communication protocol. The advantage of this approach is that agents are able to acquire knowledge from each other and when interacting with users. This capability will facilitate the satisfaction of user's requirements in an intelligent way. This study shows that agents are able to satisfy new services previously unknown to the system."
2674,"Many real-life organizations are hierarchies of largely autonomous, heterogeneous members (individuals or other organizations), often exhibiting rich policies. We restrict our attention to organizations that monitor their environment, collate events, determine compliance of their behaviors with their policies, and potentially act in anticipation of events to ensure the satisfaction of their policies. This paper models cross-organizational service agreements as resulting in the formation of organizations. This paper emphasizes the importance of proactive policy-based governance in organizations (modeled as multiagent systems) and provides a novel architecture supporting policy monitoring, governance, and enactment. This paper provides an initial formalization and discusses the compliance and completeness of behaviors produced from specified policies. To demonstrate the practical utility of this approach, it is implemented using an existing policy engine and messaging middleware."
2675,"The rapid growth in both the number and diversity of Web services raises new requirement of clustering techniques to facilitate the service discovery, service repository management etc. Existing clustering methods of Web services primarily focus on using the semantic distances between service features, e.g., topic vectors, mined from WSDL documents. However, these quality topic vectors are hard to be obtained due to the lack of abundant textual information in Web service description documents. In practice, prior knowledge from human's trajectory of utilizing Web services could be helpful in improving the accuracy of Web services clustering. With an analysis in the dataset of Web services and Mashups from ProgrammableWeb, we observe that Web services Mashuped together are highly likely to belong to different clusters and Web services being annotated with identical tags tend to be within the same cluster. Based on these observations, this paper proposes an efficient clustering approach for Web services. The approach firstly uses a probabilistic topic model to elicit the latent topic vectors from Web service description documents. It then performs clustering based on the K-means++ algorithm by incorporating parameters representing above mentioned prior knowledge. A comprehensive evaluation is conducted to validate the performance of our proposed approach based on a ground truth dataset crawled from ProgrammableWeb. Experimental comparisons of the approaches with and without these prior knowledge considerations show that our approach has a significant improvement on the clustering accuracy."
2676,"How to deploy more services while keeping the Quality of Services is one of the key challenges faced by the resource management of cloud platforms, especially for PaaS. Existing approaches focus mainly on cloud platforms which mainly host small number of applications, and consider few features of different applications. In this paper, we present SORM, a Service-Oriented Resource Management mechanism on cloud platforms. The core of SORM is a service feature model which involves resource consumption and request variance of services. For each server, SORM deploys service instances with complementary resource consumption, so as to improve resource utilization. SORM also divides servers into three pools and deploys service instances onto different pools, mainly based on their request variance features, so as to reduce computational over-head of resource management and keep cloud platforms stable. We evaluate the effectiveness and efficiency of SORM by simulation experiments and find that: compared with one exiting approach. SORM can deploy 3.6 times more services with nearly 74.1% time cost."
2677,"The relationship-focused and credential-focused identity management are both user-centric notions in Service-oriented architecture (SOA). For composite services, pure user-centric identity management is inefficient because each sub-service may authenticate and authorize users and users need participate in every identity provisioning transaction. If the above two paradigms are unified into the universal identity management model where identity information and privileges are delegatable, user-centricity will be more feasible in SOA. This paper aims to extend WS-Federation to build a universal identity management model based on anonymous credentials, which provides the delegation of anonymous credentials and combines identity metasystem to support easy-to-use, consistent experience and transparent security. In addition, the concept of self-generated pseudonym is introduced to construct efficient anonymous delegation model."
2678,"While the potential benefits of multi-business interaction automation are truly phenomenal, the complexity of business has increased dramatically, and building reliable and secure e-business application systems becomes an important issue. Industry standards for Web service composition, such as BPEL, provide the notion of LRT (Long running transaction) for the execution of business processes in Web service collaborations. Formal verification of BPEL programs and specifications has become a hot topic. However, the notion of LRT described in BPEL is purely local and occurs within a single business process instance. In this work a two-step model checking method, from simply to the complex, is proposed for multi-business process. A preliminary step is to judge business correctness on external logic transition relations amongst businesses with various simple symbolic model checking tools, and then make further checking interaction validation based on internal system behavior in multi-business coordination with model checking verification environment for mobile processes. The typical scenario is illustrated to show how model checking is applied to verify the reliability of multi-business coordination. All of these have important implications for ensuring multi-business process coordination."
2679,"A goal driven intention extraction approach is proposed to automate the process of extracting user intention from the original Web service request terms. We introduce a method for analyzing the request terms to fit user intension, so that the service provided is more suitable for the user. The input terms are parsed into different word sense sets. With the lexical dictionary and domain ontology, possible senses of the terms are identified. A goal structure is constructed to help the identification of goal models which represent the user intention. By combining the information of request terms and goal structures, one or more goal models are identified. A goal selector selects a candidate goal model from generated goal models to represent the user intention. A service request agent is designed to generate and execute plans to satisfy the goals."
2681,"Though there are some existing data service mashup tools, it is still challenging for those developers with no or little programming skills to develop data service mashups to solve the situational and ad-hoc business problems. This paper focuses on the problem of interactively recommending useful assistance at every step during the development of data service mashups under the condition that the mashup plan can't be determined in advance. This paper analyzes the problem with a motivating scenario, introduces the core definitions and an approach to dataflow pattern based recommendation. Inspired by the idea that there exist dataflow patterns for certain integration functionalities, several types of data service mashup patterns are defined. Then the interactively data service mashup recommendation method is proposed based on them. We also associate a set of tags to represent the situation and the inputs and outputs of data service model, and incorporate it in the recommendation method. Experiment results show that the dataflow pattern based recommendation approach for data service mashup is effective."
2682,"Nowadays, application needs loose-coupling approach allowing asynchronous evolution. Service-oriented computing is a paradigm that utilizes services as fundamental elements for application design, and fulfills loose-coupling requirement. However, services themselves are typically only used to abstract remote functionality; the service-oriented approach proffered by web services does not promote an implementation model managing service dynamism. This paper proposes iPOJO, a service-oriented component to implement services. However, this component model does not only manage dynamic service interactions; it also provides an extensibility mechanism. IPOJO Components can manage other non-functional properties as persistency, security, autonomic management, eventing... To illustrate the approach, the paper presents a usage of iPOJO on a residential gateway."
2683,"With the development of Web service technology, the quantity of the Web services published on the Internet is increasing rapidly and they are connected through input and output mapping. Although the properties of service network have been explored, the social relationships among services are not formally defined and there lacks practical approach to identify social relationships among real world services. Thus, we define three fundamental relations between Web services in a precise way: substitution, competition and collaboration. Moreover, based on 4 different granularity level Web Service Networks (WSN), an automatic service social relation annotation approach is proposed. We conduct experiments on a corpus crawled from the Internet. The experimental results show that the accuracy of single operation recommendation based on our method is up to 90% on average."
2684,"Current Web services standards enable publishing service descriptions and finding services on a match based on criteria such as method signatures or service category, but provide no basis for selecting a good service or for comparing ratings of services. This work presents a conceptual model of agent mediated Web service to address the flexible services selection and combination. At the end, an example is described to demonstrate how the conceptual model works."
2685,"Configuration plays a central role in the deployment and management of Web infrastructures and applications. A configuration often consists of assigning ""values"" to a pre-defined set of parameters defined in one or more files. Although the task of assigning values to (configuration) parameters looks simple, configuring infrastructures and applications is a very complex process. In this paper we present a framework for defining and analyzing configuration of an Apache server. We define the notion of ""configuration space"" of an Apache server as a set of possible values that can be assigned to configuration parameters. We then define the notion of an ""obstacle"" and ""forbidden region"" in the configuration space that should be avoided. We model configuration space using a logical framework based on OWL (Web ontology language). The obstacles and forbidden regions in the configuration space are modeled as constraints in the logical framework. These obstacles and forbidden regions are essentially ""anti-patterns"" that a typical installation should avoid. Given an instance of a configuration (that is, a ""point"" in the configuration space) we then check if the instance is ""obstacle free"" using logical reasoning."
2686,"This paper presents an agent based model for discovery, brokering and allocation of cost effective resources to computational jobs. A wireless grid environment of virtual organizations is considered. Agents are employed to perform resource brokering and allocation tasks. The model is simulated for different wireless grid scenarios, to test its operational effectiveness"
2687,"With the development of process recommendation, dynamic adaptation and automatic modeling, the requirement of explicit and formalized expression of activity dependence relation in the business domain is becoming more and more urgent. However, these relations more exist in the minds of domain experts or in the unstructured documents, which leads process modeling and adaptation are a time-consuming and error-prone process. To solve this problem, a relation mining method is proposed for obtaining activity dependence relations. The formal description of these relations is defined in control flow perspective, which is expressed as serial-dependence relations and parallel-dependence relations in the form of three tuples after analyzing all the control flow patterns. And a mining algorithm is proposed for mining these two types of relations based on the process model. The correctness and performance of this algorithm are verified by a large number of experiments, and the experimental results show this method can quickly and accurately extract all the activity dependence relations from the existing process models in a business domain."
2688,"Modern service-oriented systems have increasingly complex loosely-coupled architectures that often exhibit poor performance and resource efficiency and have high operating costs. This is due to the inability to predict at run-time the effect of dynamic changes in the system environment (e.g., varying service workloads) and adapt the system configuration accordingly. In this paper, we describe a long-term vision and approach for designing systems with built-in self-aware performance and resource management capabilities. We advocate the use of architecture-level performance models extracted dynamically from the evolving system configuration and maintained automatically during operation. The models will be exploited at run-time to adapt the system to changes in the environment ensuring that resources are utilized efficiently and performance requirements are continuously satisfied."
2689,"A significant challenge of successful application of the service-oriented architecture (SOA) in large-scale distributed systems is the quality of service (QoS) management, which provides various QoS guarantee levels for concurrent clients through effective resource allocations and adaptations. In this paper, we propose a policy-based approach for specifying QoS management strategies and enforcing QoS guarantees. This approach enables easy adaptation of new business rules and adaptation to system resource changes. This approach is also effective for supporting QoS management, as demonstrated in our experiments in a publish/subscribe system."
2690,"As a mega-trend in IT, cloud computing provides different types of cloud services. With these cloud services, a computing environment becomes very complex and heterogeneous since there are a large number of traditional and mobile applications and it is quite common for those applications to invoke the services. We call these applications as service-based applications. There are two potential problems with service-based applications; low QoS and limited manageability. To remedy the two problems, we propose a comprehensive framework for applying dynamic architecture and the concept of autonomic service management. This framework is called Service-based Ecosystem (SEco) where the configuration is dynamically changed to maintain the consistent level of the quality. To practically realize architectural dynamism, we first propose an autonomous management process and its key enabling methods on how to enable the architecture to be dynamic. We believe that dynamic architecture is an essential engineering technique in enabling truly autonomic management of quality in service-based applications."
2691,"Mashup, a way to compose a new service from existing services, is expected to play a great role in internet of things (IoT) environment. Many recent researches suggest that mashup in IoT environment is possible with existing web mashup technology if each thing exposes its functionalities as a web service. However, this approach may have limitations in dealing with many heterogeneous devices and computation scalability in the presence of large number of things involved. In this paper, we propose a cloud-based IoT mashup service model, called IoT Mashup as a Service (IoTMaaS), to overcome heterogeneity of devices by following the model driven architecture principles and computational scalability based on cloud computing paradigm. We also design a cloud platform on which IoTMaaS be executed in harmony with stakeholders such as end users, device manufacturers, and cloud computing providers. We proved the concept of the architecture by implementing a prototype platform with an vacant room detection application."
2692,"Service Oriented Architecture (SOA) has been widely applied in a range of systems such as Embedded Systems, Enterprise Information Systems and Cyber Physical Systems. These systems nowadays show System of Systems (SoS) characteristics including large-scale, consisting of software and hardware components, and cooperative processes among independent systems. It is very important to validate functional requirements and evaluate non-functional requirements of systems precisely in earlier design phase by executable architecting methodology. This paper aims to provide an executable modeling approach to SOA by bringing together Model Driven Service Engineering (MDSE) with Service oriented architecture Modeling Language (SoaML) and Modeling &amp; Simulation methodology based on Discrete Event System Specification (DEVS). First, the business architecture and system architecture are built with SoaML, then the Extended DEVS Modeling Language (E-DEVSML) is used as a model transformation intermediary to make SOA models executable in an automated code generation process, finally, the early validation and evaluation of this SOA are done through a generated DEVS simulation. To demonstrate the applicability of this approach we introduce an aircraft docking process in an airport scenario as the case study."
2693,"Traditional service discovery and selection approaches which rely mostly on centralized architectures, have been proven inadequate in the pervasive environment of the Internet of Things (IoT). In such settings, where decentralization of decision-making is mandatory, bio-inspired computing paradigms have emerged due to their inherent capability to operate without any central control. In this paper, taking inspiration from the widely studied bio-inspired Response Threshold Model, a decentralized service discovery and selection model is proposed. Preliminary results indicate that the proposed approach exhibits efficient scalability and routing performance."
2694,Context aware services are the next generation of the service computing paradigm. In this Industry report we explore a case study of creating context-aware services for the telecommunications industry.
2695,"Negotiation processes taking place between two or more parties need to agree on a viable execution mechanism. Auctioning protocols have proven useful as electronic negotiation mechanisms in the past. Auctions are a way of determining the price of a resource in a dynamic way. Additionally, auctions have well defined rules such as winner and loser determination, time restrictions or minimum price increment. These restrictions are necessary to ensure fair and transparent resource allocation. However, these rules are limiting flexibility of consumers and providers. In this paper we introduce a novel negotiation based resource allocation mechanisms using the offer-counteroffer negotiation protocol paradigm. This allocation mechanism shows similarities to the supermarket approach as consumer and provider are able to communicate directly. Further, the price is determined in a dynamic way similar to auctioning. We developed a Bazaar-Extension for CloudSim which simulates negotiation processes with different strategies. In this paper we introduce and analyze a specific Genetic Algorithm based negotiation strategy. For the comparison of the efficiency of resource allocations a novel Bazaar-Score was used."
2696,"An enterprise can consist of disparate technology environments such as legacy, Java, .Net etc and heterogeneous applications such as Customer Relationship Management (CRM), Enterprise Resource Planning (ERP) etc. In addition, there could be multiple versions of the same application deployed in different locations. In this environment, it is very difficult to create and maintain enterprise-wide business processes that can seamlessly work across heterogeneous applications and their versions. Service Oriented Architecture (SOA) could be used to address these problems. This paper presents how a SOA based solution can be implemented by defining business processes based on common service interfaces and a common data model. By following this approach, we have built a robust architecture and business processes that remain unchanged when underlying applications are retired, replaced, or consolidated."
2697,"Service composition has received considerable attention nowadays as a key technology to deliver desired business logics by directly aggregating existing Web services. Considering the dynamic and autonomous nature of Web services, building high-quality software systems by composing third-party services faces novel challenges. As a solution, new techniques have been recently developed to automatically predict the QoS of services in a future time and the prediction result will facilitate in selecting individual services. Nonetheless, limited effort has been devoted to QoS prediction for service composition. To fill out this technical gap, we propose a novel model in this paper that integrates QoS prediction with service composition. The integrated model will lead to a composition result that is not only able to fulfill user requirement during the composition time but also expected to maintain the desired QoS in future. As user requirement is expected to be satisfied by the composition result for a long period of time, significant effort can be reduced for re-composing newly selected services, which usually incurs high cost. We conduct experiments on both real and synthetic QoS datasets to demonstrate the effectiveness of the proposed approach."
2698,"Cloud service monitoring is a critical need for both providers and customers to assess the state of resources and the level of delivery of services. However, existing Cloud service monitoring methods are typically inapplicable in case the targeted service parameters are inaccessible, e.g., Cloud Service Providers do not allow external access to some service parameters for varied reasons (proprietary IPR, privacy issues, security concerns or simply access difficulties). Hence, alternate Cloud service monitoring approaches are needed to address this issue. Nonetheless, Cloud services are not provided in isolation and very often share common resources with other services that naturally introduces service dependencies between different services. In this paper, we propose a novel Cloud service monitoring approach which targets such dependencies to conduct indirect monitoring of inaccessible Cloud service parameters by using monitoring information collected from other Cloud services. The proposed monitoring approach can also assess the reliability of the monitored result. The presented case study validates the applicability of the proposed indirect monitoring approach."
2699,"Reverse geocoding has raised potential privacy concerns, especially regarding the ability to reverse engineer geographical location to obtain the street address of an individual. In most cases, direct reference to the geography of the area mapped that elucidates sensitive information isn't of much interest than having the knowledge of what we can do or find there. Rather than raising a flag, this paper presents a design framework of the space profile-based reverse geocoding service that creates an abstract space which explains area characteristic and can be used to infer probable activities that individuals can be engaged in the area."
2700,"There is a common consensus on the role that social technologies could play in improving business process management. However several challenges continue to undermine this role. In this paper we discuss a specific challenge, which is the lack of formalization to describe cloud resources used by business processes. Building upon our previous work on social business processes we develop a framework that provides a semantic description of cloud resources, strategies to ensure their correct use based on this description, as well as a set of social relations connecting them. Thereby, this framework helps to guarantee a free-of-conflict resource allocation during business process execution. To illustrate our framework doability, an extension of Signavio process editor is developed using a real use case from an industrial partner."
2701,"In past three decades, the world of computation has changed from centralized (client-server not web-based) to distributed systems and now we are getting back to the virtual centralization (Cloud Computing). Location of data and processes makes the difference in the realm of computation. On one hand, an individual has full control on data and processes in his/her computer. On the other hand, we have the cloud computing wherein, the service and data maintenance is provided by some vendor which leaves the client/customer unaware of where the processes are running or where the data is stored. So, logically speaking, the client has no control over it. The cloud computing uses the internet as the communication media. When we look at the security of data in the cloud computing, the vendor has to provide some assurance in service level agreements (SLA) to convince the customer on security issues. Organizations use cloud computing as a service infrastructure, critically like to examine the security and confidentiality issues for their business critical insensitive applications. Yet, guaranteeing the security of corporate data in the ""cloud"" is difficult, if not impossible, as they provide different services like Software as a service (SaaS), Platform as a service (PaaS), and Infrastructure as a service (IaaS). Each service has their own security issues. So the SLA has to describe different levels of security and their complexity based on the services to make the customer understand the security policies that are being implemented. There has to be a standardized way to prepare the SLA irrespective to the providers. This can help some of the enterprises to look forward in using the cloud services. In this paper, we put forward some security issues that have to be included in SLA."
2702,"A key feature of Cloud computing is its agility and flexibility to support the scalability needs of business solutions. Currently, the agility is only limited to the scalability of the compute, memory and storage. To improve an application's agility, we need to monitor &amp; measure solution level metrics and associate the performance of the metrics to the business agility needs of the solution by making real-time scalability or change decisions. In this paper, we illustrate a scaling decision mechanism utilizing the monitoring data from infrastructure, middleware, and business level metrics. We use these performance metrics as input to a causality analysis model to make architecture changes or scalability decisions. Mathematically, we define the causality as a graph to link the changes in the measured metric values to the action of the solution change. The causality analysis follows scalability principles as best practices. They are a) the principle of performance scalability b) principle of contribution margin for scalability, and c) principle of the least cost of SLA compliance. We define these scalability principles as the rules to ensure that the business stakeholder of the solution can maintain or improve their business quality or profit margins as the computing capability scales up or down. To implement those principles, we need to establish the linkages of the business metrics to the decision of changes. To make such linkage, we first utilize causality analysis to identify feasible scaling actions, and then associate those actions with the system, application, and business performance metrics. With the help of causality analysis, we implement a performance monitoring and scaling automation framework for managed solutions using an Open Source Monitoring system."
2703,"This paper presents a framework used for analyzing decisions regarding implementations of service oriented architectures. The framework assesses the business value of SOA by measuring the modification cost, i.e. the effort needed to become service oriented, and the benefits that can be gained for an enterprise using SOA."
2704,"Nowadays, Web Services are considered as de facto and attracting distributed approach of application/services integration over the Internet. Web Services can also operate within communities to improve their visibility and market share. In a community, Web Services usually offer competing and/or complementing services. In this paper, we augment the community approach by defining a specific-purpose community to monitor Web Services operating in any Web Services community. This monitoring community consists of a set of Web Services capable of observing other Web Services. Clients, providers, as well as managers of communities can make use of the monitoring community to check if a Web Service is operating as expected. This paper defines the overall architecture of the monitoring community, the business model behind, different rules and terms to be respected by its members, services it offers to its various classes of customers. The paper also presents promising experimental results using the monitoring community."
2705,"The purpose for achieving strategic alignment between Information Technology (IT) and Business areas refers to the need of large organizations to exploit the IT's ability to deliver good products and services, thereby standing out in a competitive scenario. Within this context, Business Process Management and Service Orientation tend to gain space in organizations, since they are able to automate and optimize processes and services for the business. In the business process analysis and modeling phase, it is important to treat not only functional properties, but also process quality and operating constraints, usually grouped as Non-Functional Properties (NFP); thereby preventing that investments in IT are wasted on inefficiency and rework. In order to address these constraints, Business Level Agreements (BLA) and Service Level Agreements (SLA) should be used. The most prestigious languages for business process modeling, including Business Process Model and Notation (BPMN), lack the representation of these NFPs. The approach proposed in this paper, BLA@BPMN, extends BPMN to embody BLAs, as part of a bigger approach to foster strategic alignment in this context. A BLA is specified in the process model level so that it can be mapped to a set of SLAs at the executable process level."
2706,"With the growing adoption of service oriented computing, Web services are becoming important building blocks of distributed applications. As these services inevitably evolve and change, service version management will increasingly become a key part of the service lifecycle. To address this versioning requirement from the service provider's point of view, we propose two key concepts. The first is to draw a distinction between a service's interface version and its implementation version. The second is to employ a tiered hosting model which echoes this structure, by introducing a version routing point in front of the actual service implementation versions. This version routing point routes versioned requests to the appropriate implementation versions. We have implemented a prototype of our approach on IBM's WebSphere Process Server and WebSphere Enterprise Service Bus. We show our results with a versioned test service to demonstrate the capabilities of our proposal."
2707,"External business processes (choreography models) are currently disconnected from internal processes (workflow models), which leads to synchronisation and verification problems. Connecting these by directly mapping internal to external processes requires a quadratic amount of mappings; an intermediate ontology reduces the amount of necessary mappings but is not trivial to construct due to the variety in workflow meta-models. We introduce our multi meta-model process ontology (m3po) which is based on various existing reference models and languages from the workflow and choreography domain. The m3po ontology relates workflow models to choreography models and allows choreography extraction from internal workflow models. An initial validation is given by translating an IBM Websphere MQ workflow model into the m3po ontology and subsequently extracting an abstract BPEL model from the ontology"
2708,"Summary form only given. Loose coupling is a cornerstone for service-oriented architecture (SOA). Service contracts enable loose coupling, because they hide service-internal details from the outside world behind a facade. In the Web services world, the service contract is commonly understood as a WSDL document possibly decorated with WS-policy annotations. Such formal XML description is not immediately useful to business people. This paper proposes the notion of a service metadata document that contains human-readable business aspects of the service in addition to technical artifacts. Stakeholders at the business and technical level work collaboratively on the service metadata document and use it as a common framework for discussion. We see the service contract as a composition of functional metadata and a set of policies, such as security constraints, access restrictions for user groups, transport and service level agreements, charging, etc. For example, enterprises often have different security requirements when a service is consumed from within the company than when it is consumed by an external business partner. The service lifecycle is initiated by business people on the basis of a service contract template; the first step is a high-level description of the desired service characteristics. In an iterative process the document is then successively refined as it percolates from the management level to service architect and other stakeholders and as business requirements are tested against technical realities. Technical artifacts (WSDL, XSD, etc.) are added to the service contract and could be placed in a registry so that the service is visible to other departments or external partners. The clear distinction between service metadata and policies also helps maintaining a consistent model over runtime WSDL proliferation. Typically, policies are enforced by intermediaries, such as XML firewalls, enterprise service bus (ESB) or other WS-infrastructure. Each such inter..."
2709,"IT outsourcing service providers are increasingly required to balance cost-reduction challenges with quality-improvement goals. Such efforts can be aided by an improved methodology for services solution engineering - which involves requirements analysis, mapping requirements to service provider capabilities, documenting the service design and defining transformations of the customer's environment to facilitate more efficient service delivery. One such improved methodology leverages reuse in the form of a taxonomy of standardized service offerings and a repository of standardized service designs they can be mapped to. In this paper, we describe a prototype system that provides automation for such a methodology, by encoding design policies that are used to assist solution architects in gathering relevant information from the customer and cross checking design decisions. This paper presents the system architecture, discusses sources and models of knowledge, and illustrates how this knowledge can be used with specific examples from the services field."
2710,"Cloud computing has been attracting huge attention in recent years. More and more individuals and organizations have been moving their work into cloud environments because of its flexibility and low-cost. Due to the emergence of a variety of cloud service providers, selecting the most suitable cloud service becomes increasingly important for potential cloud users. In prior studies, the selection and comparison of cloud services usually focus on objective performance analysis based on cloud monitoring and benchmark testing without considering the viewpoints of cloud users who are indeed consuming cloud services. This causes a problem that some vital aspects which concern cloud consumers (e.g., privacy and cloud providers' reputation) are not taken into account in cloud service selection. In this paper, we propose a novel model of cloud service selection by aggregating the information from both the feedback from cloud users and objective performance analysis from a trusted third party. Based on this model, we first propose a framework which supports our cloud service selection approach. Then after classifying subjective assessment and objective assessment, we present a novel cloud service selection approach to aggregate all subjective assessments and objective assessments through a fuzzy simple additive weighting system. In addition, to reduce the bias caused by unreasonable feedback from unprofessional or malicious cloud users, a method is proposed for filtering the feedback from such users. After processing, the aggregated result can quantitatively reflect the overall quality of a cloud service. Finally, a case study is presented to illustrate the advantages of our approach."
2711,"People, processes, technology and information are the essential building blocks for creating a successful IT infrastructure in today's fast-paced, service-focused marketplace. ITIL which is recognized as the de facto standard for service management is a process based approach. ITIL focuses on a set of integrated processes which run the gamut from highly interactive and dynamic processes such as problem determination to highly repeatable processes such as patch deployment which are best handled in a fully automated, non-interactive fashion. The ability to support and integrate the full spectrum of interactivity for these processes with the appropriate level of automation is crucial for the service provider. Also key is the ability to identify opportunities to increase the level of automation as maturity and technology permit. In this paper, we propose a conceptual methodology for IT service management process automation that leverages the ontological relationships between process artifacts and resource artifacts to develop data aware processes for an effective automated approach to integrate both highly automated and human centric process models. The objective is to develop a systematic approach that addresses the needs of an IT organization in order that highly automated operational processes work in conjunction with collaborative human decision centric processes in order to effectively deliver IT services. In addition, we propose a complexity model to assist in identifying automation opportunities to satisfy the need for continuous efficiency and cost improvement."
2712,"This research is concerned with rough concept, a synthesis of rough sets and formal concept analysis. This theory is more expressive than ordinary approaches in knowledge representation and reasoning. Like rough sets theory, this research proposes an accuracy computing approach to characterizing the rough formal concept numerically and the role granules play there. And it also studies the basic relationship between the accuracy computing of rough form concept and granules."
2713,"The advent of grid technology has provided a promising methodology for usage of distributed resources for complex scientific workflow applications. Grid workflow management systems (GWMS) enable even a non-expert grid user to compose workflows and further provide functionality to coordinate execution of the complete workflow thereby masking the grid-specific details from the user. We have designed a non- DAG workflow specification model for workflow composition. Our model allows a user to compose a workflow using directed graphs, thereby allowing modeling of sequence, parallel, choice and iteration patterns in the workflow. We have also provided for structural verification of workflows using Petri net based analysis techniques. A workflow is said to be structurally correct if it does not have errors like deadlock and lack of synchronization. A workflow found correct by structural verification techniques can be executed correctly for all possible workflow instances. We have incorporated our workflow model into a previously existing workflow management system for Sun grid engine [1]. Experiments show that the model allows for composition of wider range of workflow applications. Also the verification procedure provides appropriate error messages to user, indicating the nature and cause of error, when a structural error is detected. We have composed and executed the workflow of EMAN [2], a real-world bio-informatics application. The experimentation results show the efficient utilization of compute power of the grid by the inherent parallel tasks in the EMAN workflow."
2714,"Workflows are useful ways to support scientific researchers in carrying out repetitive analytical tasks on digital information. Web services can provide a useful implementation mechanism for workflows, particularly when they are distributed, i.e., where some of the data or processing resources are remote from the scientist initiating the workflow. While many scientific workflows primarily involve operations on structured or numerical data, all interpretation of results is done in the context of related work in the field, as reflected in the scientific literature. Text mining technology can assist in automatically building helpful pathways into the relevant literature as part of a workflow in order to support the scientific discovery process. We demonstrate how these three technologies - workflows, text mining, and Web services - can be fruitfully combined in order to support bioinformatics researchers investigating the genetic basis of two physiological disorders - Graves' disease and Williams syndrome."
2715,Economic factors and developments of information and communication technologies have forced SMEs to engage in new organizational structures. The emergent process- oriented collaborative networks require integration between distributed and heterogeneous infrastructures. The aim of this article is to present the approach for business process integration and management developed for the footwear sector within the scope of an EU project. Lessons learned and an agenda for future research work are then described. A brief overview of relevant enabling technologies is also made.
2716,"Nowadays many software services are hosted in the Cloud. When there are more requests on these services, there are also more queries sent to the underlying database. In order to keep up with the increasing workload, it is necessary to have multiple servers hosting the data. Some cloud providers offer the full data replication solution. However, this solution only works when the load mainly consists of the read requests, and when the number of write requests increases, it does not scale well. Although data decomposition has been widely used in data-intensive web sites, not much study has been done on how to decompose the underlying data of software services for the purpose of data replication. In this paper, we propose a data-decomposition-based partial replication model for software services. We devise an automatic algorithm for data decomposition under the constraint of the capacity limit of the host machines. We evaluate our approach from two aspects: scalability and performance, using two benchmarks: RUBiS and TPC-W. In the experiment, we test the algorithm using different workload inputs, and also compare our approach with the full data replication approach."
2717,"Today, the techniques for realizing service compositions (e.g. WS-BPEL) have become mature. Nevertheless, when it comes to execution faults within service compositions, many problems are still unsolved. Especially the propagation and global handling of errors in service compositions yet remains an open issue. In this paper, we describe some preliminary results of our ongoing work in the field of fault propagation and exception handling in service compositions. We provide some service classification criteria and show how they relate to service composition fault handling. Further, we present a fault propagation approach for service compositions."
2718,"There is a need for the creation of virtual organizations where services offered by one organization should become accessible to other organizations. The proposed model virtual service-oriented architecture, makes the information discovery process across organizations more flexible and efficient by introducing a cache registry. A Web services architecture is realized using the Websphere message broker and WAS servers. The proposed approach is compared with extended services oriented architecture. Results show that caching significantly improves the response time."
2719,"In this paper, we view the cloud as market place for trading instances of web services, which can be bought or leased by web applications. Applications can buy diversity by selecting web services from multiple cloud sellers in a cloud-based market. We argue that by diversifying the selection, we can improve the dependability of the application and reduce risks associated with service level agreements violations. We propose a novel dynamic adaptive search based software engineering approach, which uses portfolio theory to construct a diversify portfolio of web service instances, traded from multiple cloud providers. The approach systematically evaluates the Quality of Service and risks of the portfolio, compare it to the optimal traded portfolio at a given time. It can then dynamically decide on a new portfolio and adapt the application accordingly. We use a hypothetical scenario to demonstrate the effective use of the portfolio-based optimization."
2720,"Outsourcing service opportunities go through a complex and laborious process beginning with RFP processing and continuing through solutioning, proposal development, costing, pricing, contract definition and closing. A key issue for service providers is to understand the health status of a given service opportunity engagement based on available information at any point in time during the engagement. This capability enables them to bring excelling engagements to a faster close, and identify and help troubled engagements. As a related problem, service providers are interested in understanding the win chances of engagements in order to prioritize their pipelines for planning and resource management purposes. In this paper, we propose a new sentiment-based approach for identifying and monitoring the health of opportunities by analyzing weekly comments made by sales teams for each engagement. We then present a novel method for feature extraction and selection from the engagements' comments. This approach combines text-based and sentiment-based features and identifies highly predictive features that are then used to predict the outcome of engagements. We report on experiments performed on an industrial dataset, which shows our prediction approach achieves a high win prediction accuracy. We describe a method for providing a supplementary confidence level and linguistic features that facilitates the interpretation of the predicted outcome."
2721,"Data is becoming the world's new natural resource and big data use grows quickly. The trend of computing technology is that everything is merged into the Internet and 'big data' are integrated to comprise complete information for collective intelligence. With the increasing size of big data, refining big data themselves to reduce data size while keeping critical data (or useful information) is a new approach direction. In this paper, we provide a novel data consumption model, which separates the consumption of data from the raw data, and thus enable cloud computing for big data applications. We define a new Data-as-a-Product (DaaP) concept, a data product is a small sized summary of the original data and can directly answer users' queries. Thus, we separate the mining of big data into two classes of processing modules: the refine modules to change raw big data into small sized data products, and application-oriented mining modules to discover desired knowledge further for applications from well-defined data products. Our practices of mining big stream data, including medical sensor stream data, streams of text data and trajectory data, demonstrated the efficiency and precision of our DaaP model for answering users' queries."
2722,"Statistics show that most vehicles spend many hours per day in a parking garage, parking lot, or driveway. At the moment, the computing resources of these vehicles are untapped. Inspired by the success of conventional cloud services, a group of researchers have recently introduced the concept of a Vehicular Cloud. The defining difference between vehicular and conventional clouds lie in the distributed ownership and, consequently, the unpredictable availability of computational resources. As cars enter and leave the parking lot, new computational resources become available while others depart creating a dynamic environment where the task of efficiently assigning jobs to cars becomes very challenging. Our main contribution is a fault-tolerant job assignment strategy, based on redundancy, that mitigates the effect of resource volatility of resource availability in vehicular clouds. We offer a theoretical analysis of the expected job completion time in the case where cars do not leave during a checkpoint operation and also in the case where cars may leave while check pointing is in progress, leading to system failure. A comprehensive set of simulations have shown that our theoretical predictions are accurate."
2723,"Integration of heterogeneous information becomes again a requirement with the emergence of large-scale distributed applications such as Web-services based applications. Enterprise service buses (ESB) deals with distribution and communication, but they still do not fix all the mediation issues such as design, deployment and administration of mediators. It turns out however that current solutions are technology-oriented and beyond the scope of most programmers. In this paper, we present an approach that clearly separates the specification of the mediation operations basing on a service component model, and their execution on a distributed ESB. Model and ESB are independent of the targeted middleware used by applications. This work is made within the European-funded S4ALL project (services for all)."
2724,"Finding and recommending suitable services on mobile devices is an important topic in today's society where most people rely on their smartphones to provide solutions to their day-to-day problems or to receive prescribed services. Recent research has attempted to use role-based approaches to recommend mobile services to other members among the same context group. However, these proposed algorithms are inefficient and may not scale to cope with the huge amount of mobile traffics in the real-world. This paper proposes novel algorithms with better runtime complexity, and further extends them to Map Reduce style to take advantage of popular distributed computing platforms. Experiments running on a medium-sized high performance computing cluster demonstrate that our proposed algorithms outperform previous work in running time complexity and scalability."
2725,In SOA applications are built from individual services offered by different providers. Typically an application comprises of several such services usually stemming from different providers leading to the question of which services to select and compose. We present the new concept of composition context together with a novel service selection algorithm. The approach has been evaluated in our test bed and shows good scalability.
2726,"With the rapid growth of service volumes and types, discovering services in an efficient and accurate manner has become a significant challenge in service computing. Service clustering is an important technology to improve the efficiency of service discovery. In this paper, we propose a new service clustering approach, which starts from service documents and is based on the functional semantics of services. This approach, first, extracts service goals from service description documents by using natural language processing technologies. Then it obtains the semantic similarity between two service goals and clusters the services by the K-means algorithm. Experiments conducted on a real-world service dataset crawled from ProgrammableWeb demonstrate the feasibility and the effectiveness of the proposed approach."
2727,"The success achieved by Cloud Computing Services (CCSs) in modern IT scenarios is nowadays a matter of fact and service offerings between providers and customers continuously grow up and widen their scope. Similarly, service offerings are more and more based upon dynamic reservation and allocation of network, storage and computational resources, the hiding of visibility of internal IT components, as well as the pay-per-use paradigm. The main drawback of such a situation is represented by the complexity in composing services to satisfy users' requests, as well as in performance monitoring and service level comparisons. Effective strategies are then needed to model IT service contracts and corresponding Service Level Agreements (SLAs), as well as their composition. However, the lack of expressivity in current SLA specifications and the inadequacy of tools for managing SLA and contract compositions is relevant. Therefore, we present a possible extension of WSLA, a widely known SLA description language, for modeling contracts and SLAs suitable to support contract owners during service composition and monitoring phases. An ad-hoc developed tool based on the usage of directed tree-graphs and of a rule-based engine is examined to assess the feasibility of the proposed model and to simplify SLA and contract composition."
2728,"Information on web and in use of web services is increasing enormously even on hourly bases. On semantic web the information is represented in ontology. For system and services to share the information, a sort of mediation (i.e., mappings) is required. Mappings are established between the ontologies (information sources) of web services for resolving the terminological and conceptual incompatibilities. However, with the discovery of new knowledge in the field and accommodating the knowledge in domain ontologies makes the ontology to evolve from one consistent state to another. This consequently makes existing mappings between ontologies unreliable and staled due to the changes in resources. So there is a need for mapping evolution to eliminate discrepancies from the existing mappings. To re-establish the mappings between dynamic ontologies, existing systems restart the complete mapping process which is time consuming. The approach proposed in this paper provides the benefits of mapping reconciliation between the updated ontologies. It takes less time as compared to the existing systems. It only considers the changed resources and eliminates the staleness from the mappings. This approach uses the change history of ontology to drastically reduce the time required for reconciling mappings among ontologies. Experimental results with four different mapping systems using standard data sets are provided that validate our claims."
2730,"Web services which allow the interoperability and communication of heterogeneous systems in the Web through Internet protocols, are also subject to attacks a well as destructive as sophisticated. Contrariwise, very few solutions exist to ensure the availability of Web services in the presence of these attacks. In order to tackle these issues, we propose a comprehensive and complete attack-tolerance methodology whose characteristics are: i) upstream detection of attacks before their propagation, ii) a failover system to mitigate the effects of the attack and, iii) an active reconfiguration process to mitigate attacks that are not easy or impossible to detect by monitoring. Our approach will leverage and explore, in particular, monitoring, diversity and software engineering techniques for devising a fine-grained attack-tolerance system. We conducted preliminary experiments with an e-health Web service, which is a simplified version of a case study of the European project CLARUS."
2731,"In a typical large-scale data center, a set of applications are hosted over virtual machines (VMs) running on a large number of physical machines (PMs). Such a virtualization technique can be used for conserving power consumption by minimizing the number of PMs that should be turned on according to the application requirements to resource. However, the resource demands for VMs is dynamic in nature since the number of user requests the applications should handle is rapidly changing in practice. It is a great challenge to online reconfigure the VMs (i.e., optimize the number and the locations for the VMs) according to the dynamic resource demands. Especially for the emerging applications of large-scale data centers for cloud computing systems, existing approaches either fails to find a best configuration of VMs or cannot produce a result in an acceptable time. In this paper, we propose an online self-reconfiguration approach for reallocating VMs in large-scale data centers. It first accurately predicts the future workloads of the applications with Brown's quadratic exponential smoothing. Based on such a prediction, it adopts a genetic algorithm to efficiently find the optimal reconfiguration policy. The resource utilization of large-scale cloud computing data centers can thus be improved and their energy consumption can be greatly conserved. We conduct extensive experiments and the results verify that our approach can effectively switch off more unnecessary running PMs comparing with current approaches without a performance degradation of the whole system."
2732,"With the substantial incensement of broadband networks, Internet applications have shifted from simple web browsing to content-centric applications. From the perspective of the content distributor, how to reduce the cost while satisfying quality of service and how to respond timely to users are of great concern. Currently, using multi-cloud technology is a feasible solution to provide more agile and scalable services. Meanwhile, big data techniques, such as Spark and Hadoop, can help content distributors make load-direct decisions more timely and accurately. In this paper, we present a multi-cloud architecture-supported resource allocation and scheduling optimized strategy through CDN (content delivery network) operation big data analysis. We firstly analyze quantities of CDN operation log data on Spark to evaluate quality of service (QoS) between end users and multi-cloud-based CDN operator. Then we perform a long-term resource deployment algorithm to book the minimum resources to meet users' requests with higher QoS and lower cost. We apply the prediction model ARIMA on Spark to predict the short term demand through analyzing a longer time series data. When the predicted resources cannot satisfy burst demand, we design a new multi-cloud extension algorithm to schedule additional cloud resource to handle overload requests and use precopying algorithm to select media contents to be stored in the new prepared cloud. We implement and evaluate our scheme with real operation logs data provided by China's biggest CDN distributor to show the efficiency of our algorithms."
2733,"Workflow has grown to be a primary method in managing processes. Improving the ability to support heterogeneous applications for workflow management system (WFMS) will promote the more popular application of workflow technology in wide areas. Moreover, as e-service is becoming as a focus in business process management, workflow technology is facing the challenge to support e-services in a proper way. We import the enterprise application integration (EAI) technology into workflow enactment service to support heterogeneous applications in a uniform way; and also we introduce some service-related components into workflow reference model to support e-services in processes. A WFMS prototype based on our extended workflow model is also presented. Semantic knowledge is used in this prototype to make service registration and discovery."
2734,"Today grid infrastructures are increasingly required to handle complex service requests requiring configuration and deployment of customized solutions. Existing tools and technologies are labor intensive and result in inefficient and wasteful use of both human and IT resources. In this paper, we describe Harmony II - an architecture for delivering customized solutions using an approach that is policy driven and is amenable to a high degree of automation. Our approach is modular, which facilitates customization using building blocks of IT resources. Lower level raw IT resources are virtualized allowing flexible and policy-based resource sharing. Flexible resource sharing reduces wasteful use of expensive IT resources and allows business goals to dictate how the resources are to be used. Cost savings in IT management are achieved not only by the automation in service delivery, but also by the novel use of building blocks and solution templates"
2735,"A service-oriented architecture (SOA) promises a more flexible intra- and interorganizational integration of heterogeneous application systems. The central design element of a SOA consists of services which encapsulate existing application logic, and can be dynamically combined to form business processes. This paper aims at contributing to the emerging discipline of ""service engineering"" by investigating the design principles for B2B services. The authors draw on an analysis of the literature to derive the main design principles for services and evaluate two alternative design proposals for a concrete application scenario in the automotive industry. The results comprise statements on the quality of the service design in the concrete case as well as an outlook for future topics of research in ""service engineering ""."
2736,"Selecting services from those available according to user preferences plays a important role in Web services composition. Current solutions for service selection focus on selecting services quantitatively. In many domains it is desirable to assess such preference in a qualitative rather than quantitative way. Furthermore, in many practical situations, if the user is reluctant to provide complete preference information or to totally order the values of each attributes in every possible context, it is natural to ask how to model in these incompletely specified cases. This paper uses a qualitative graphical representation tool, called CP-nets, to describe qualitative preference relations in a relatively compact, intuitive, and structured manner under conditional ceteris paribus (all else being equal) preference statements. Our goal is to select the optimal set of services available according to user preference with this technique. Experiment results demonstrate the effectiveness and performance of this approach."
2737,"In this paper, we would like to address the consistency problem for Web content presentation through explicit ownership schemes. Our previous paper (Chua and Chi, 2005) showed that for most content service networks, consistency problem arises because of the mismatching of objects' attributes and pervasive contents. Our solution covers both protocols and supports for most of the necessary functions such as validation, delegation, presentation, ..., etc. that need to be done in the network"
2738,"Scientific Workflow Management Systems are being widely used in recent years for data-intensive analysis tasks or domain-specific discoveries. It often becomes challenging for an individual to effectively analyze the large scale scientific data of relatively higher complexity and dimensions, and requires a collaboration of multiple members of different disciplines. Hence, researchers have focused on designing collaborative workflow management systems. However, consistency management in the face of conflicting concurrent operations of the collaborators is a major challenge in such systems. In this paper, we propose a locking scheme (e.g., collaborator gets write access to non-conflicting components of the workflow at a given time) to facilitate consistency management in collaborative scientific workflow management systems. The proposed method allows locking workflow components at a granular level in addition to supporting locks on a targeted part of the collaborative workflow. We conducted several experiments to analyze the performance of the proposed method in comparison to related existing methods. Our studies show that the proposed method can reduce the average waiting time of a collaborator by up to 36.19% in comparison to existing descendent modular level locking techniques for collaborative scientific workflow management systems."
2739,"The widespread adoption of service oriented architecture owes its popularity to service composition, where several web services are combined dynamically at runtime. As is evident today, the Internet is evolving towards the 'Future Internet'. In this context, web service composition has to deal with the problems of mobility, fault tolerance, reliability and the ultra large scale of the Future Internet. The practice of composition, the most popular variant of which is service orchestration, is expected to face numerous challenges in the Impending Future Internet. This is mainly because it comprises a central control point. Service choreography is widely viewed as an appropriate remedy to these problems as it has a predominantly decentralized orientation. In this paper, we propose a decentralized framework for the purpose of executing a case oriented workflow that would be ideal for the Future Internet. The services in the framework communicate and co-ordinate amongst each other without involving a centralized orchestrator. Further, we propose a technique that models the behavior of rain drops to achieve decentralized service composition. Based on the principles of message based service choreography, the proposed composition technique aids selection and execution of web services. We show how the model achieves service composition leveraging both static and runtime properties of a service. A runtime SOA test-bed to validate the decentralized framework and the composition technique is developed in JAVA. Validation is done via real web services. Multiple workflows are executed to demonstrate the viability of the model in actual deployment. Through experiments and exploration, the technique is found to outperform similar techniques in literature."
2740,"In this paper we look at combining and compressing a set of workflows, such that computation can be minimized. In this context, we look at two novel theoretical problems with applications in workflow systems and services research, which are duals of each other. The first problem looks at merging the maximum number of vertices in two DAGs (directed acyclic graphs) without creating a cycle. We prove that the dual of this problem is the problem of maximizing the length of the LCS (longest common subsequence) between all pairs of topological orderings of the two DAGs. This formulation generalizes to a new definition of LCS between complex structures like workflows or XML documents, which we call M-LCS. Subsequently, we present a taxonomy of the different kinds of problems in this set, and find the M-LCS solution for a tree and a chain with a dynamic programming algorithm. Along with this theoretical formulation, we implement the algorithms in C++ and run it on representative workflows. We evaluate the performance of the M-LCS algorithm on a set of random workflows and observe that it is substantially better than traditional AI based approaches."
2741,"A new fine-grained parallel programming model - Thin Kernel model is brought forth. In this model, the partitions of the parallel tasks are separated from the computational kernel of the problem. The Thin Kernel model produces parallel tasks dynamically at runtime when the tasks are being scheduled which makes the task assignment method in the Thin Kernel model real and distributed. The Thin Kernel model uses dynamic class loading and on-demand code transport. The collaboration of these technologies paves the way for large-scale parallel computing over wide-area networks."
2742,"Web service discovery is a fundamental part in Web service architectures. UDDI is a Web service directory that was primarily intended to discover new Web services in the Internet for business-to-business interaction. Another application domain of Web service discovery is local area networking, where a dynamic Web service discovery at runtime is required. Web services dynamic discovery (WS-discovery) is a specification that deals with local ad-hoc networks. A predefined IP multicast group is used for dynamic discovery within a subnet. In this paper we present an enhancement of WS-discovery that allows for service lookups across subnet boundaries. We propose a dedicated proxy server, that implements cross-subnet discovery while preserving the multicast discovery within each subnet for compatibility and robustness. Thus, this technology becomes usable in enterprise-level networks."
2743,"This paper presents a service spectrum, which defines the prominent characteristics of services in the context of SOA paradigm. The service spectrum comprises service identification, aspects, attributes, metadata, messaging, and repository/management, serving as a foundation to a systematic service engineering discipline, in which the major stages of the service lifecycle are addressed - portfolio assessment, service analysis, service modeling, service realization, service assembly, and service management. This holistic disciplined approach covers the contents, categories, models, activities, artifacts, standards/policies, patterns, and tools to comprehensively incorporate the industry best practices and our past experience in the real-world SOA solution designs."
2744,"To respond quickly to the market, the organization must be able to create, manage and optimize dynamic business process. Building up dynamic process using multi predefined process units can lead to higher efficiency in fulfilling complex business goals. Among all valuable information business process, current business process management systems only reuse the workflow model, which is of a relatively lower level, and cannot fully support process unit composition. This paper suggests that business process should be regarded as a kind of knowledge, and reuse of this knowledge should cover a much wider range. A business knowledge reuse framework is discussed. We define process component as a unit for process knowledge management and reuse. The ontology of process component is expatriated in detail. Process component model is a key factor in realizing process knowledge reuse and organizational learning."
2745,"This paper presents a semantic-based dynamic service composition and adaptation framework. It explores WSMO, a starting point for the semantic-based composer construction using backward-chaining reasoning. WSMO and the orchestration procedure proposed by this model are described, together with the architecture of the proposed framework for semantic-based dynamic service composition and adaptation. The backward-chaining composer and Mule-based executor, are essential parts of the proposed framework. Its usage is illustrated by examples."
2746,"In this paper, we propose a novel learning classifier system with the cooperative co-evolutionary mechanism to obtain accurate user preference information in context-aware mobile service adaptation. Our system can generate new user's initial classifier population to accelerate its converging speed and also help the current user to predict the action corresponding to an uncovered context. Experimental results show the efficiency and effectiveness of our system for mobile service adaptation."
2747,"Different evaluator entities, either human agents (e.g., experts) or software agents (e.g., monitoring services), are involved in the assessment of QoS parameters of candidate services, which leads to diversity in service assessments. This diversity makes the service selection a challenging task, especially when numerous qualities of service criteria and range of providers are considered. To address this problem, this study first presents a consensus-based service assessment methodology that utilizes consensus theory to evaluate the service behavior for single QoS criteria using the power of crowdsourcing. To this end, trust level metrics are introduced to measure the strength of a consensus based on the trustworthiness levels of crowd members. The peers converged to the most trustworthy evaluation. Next, the fuzzy inference engine was used to aggregate each obtained assessed QoS value based on user preferences because we address multiple QoS criteria in real life scenarios. The proposed approach was tested and illustrated via two case studies that prove its applicability."
2748,"This paper presents a meta-model based approach to services applications deployment on constrained execution environments (EEs) which contain heterogeneous services implementations. Indeed, the targeted EEs are conceived to be hidden in electrical equipments and so belong to the embedded/reactive domain and to its constraints. The language provided by our meta-model, is called DLSA (deployment language for services applications), which is a services applications language for deployment under the conditions mentioned above. The deployment manager for services applications (DMSA) using our language was implemented to realize and validate our work. Our approach is motivated by a real case from power distribution."
2750,"In this demo, we present the current status of our visual scientific workflow management system called VIEW, highlighting the following two features: (i) the use of Semantic Web technology to represent, store, and query provenance metadata, leading to an interoperable and extensible provenance system, and (ii) the support of visualization of various provenance graphs and intermediate or final data products of a workflow run in the form of medical images or 3-D graphical models. VIEW seamlessly integrates the interoperability, extensibility, and reasoning advantages of Semantic Web technology, the querying and storage power of a RDBMS, and the appealing visual features of visualization techniques."
2751,"Cloud Computing infrastructures are being increasingly used for running business process activities due to its high performance level and low operating cost. The enterprise QoS requirements are diverse and different resources are offered by Cloud providers in various QoS-based pricing strategies. Furthermore, business process activities are constrained by hard timing constraints and if they are not executed correctly the enterprise will pay penalties costs. Therefore, finding the optimal Cloud resources allocation for a business process becomes a highly challenging problem. While optimizing the Cloud resource allocation cost, it is important to respect activities QoS requirements and temporal constraints and Cloud pricing strategies constraints. The aim of the present paper is to offer a method that assists users finding the optimal pricing strategy for Cloud resource used by business process activities. Basically, we use a binary/(0-1) linear program with an objective function under a set of constraints. In order to show its feasibility, our approach has been implemented and the results of our experiments highlight the effectiveness of our proposed solution."
2752,"New Web services are emerging on the Internet, while some other Web services are obsolete for some reasons. Unfortunately, not all services are developed in accordance with rules of loose coupling in the software engineering. The consequence is that some services work well only with other services of older versions. The situation is much worse when we deal with composite services. Moreover, using current technology to discover proper semantic services for a composite service is time-consuming and inaccurate. To deal with these problems, we proposed a Web service similarities measurement method and a recommendation method. Based on ontology and information retrieval techniques, we compute among Web services. Then the similarities are used to classify services according to their topics, functionality and semantics. Our recommendation method is able to recommend proper component services to the composite service according to the history information of invocations and similar composite services. The experiments show that our clustering method, which is based on matrix decomposition and Ontology technologies, is more accurate than others, and our recommendation method has less average error than others in the series of missing rate."
2753,"A configurable process model is a generic model from which an enterprise can derive and execute process variants that meet its specific needs and contexts. With the advent of cloud computing and its economic pay-per-use model, enterprises are increasingly outsourcing partially or totally their process variants to cloud providers, and recently to cloud federations. A main challenge in this regard is to allocate optimally cloud resources to the process variants' activities. More specifically, an enterprise may be interested in outsourcing only those that result in an optimal deployment. Due to the diversity of the enterprise QoS requirements, the heterogeneity of resources offered by the cloud federation and the large number of possible configurations in a configurable process model, finding the optimal process variant deployment becomes a highly challenging problem. In this paper, we propose a novel approach to solve this problem through a binary/(0-1) linear program with a quadratic objective function under a set of constraints pertinent to both the enterprise and cloud federation requirements. Our prototypical implementation demonstrates the feasibility and the results of our experiments highlight the effectiveness of our proposed solution."
2754,"Contextual collaboration refers to people-to-people collaboration from within IT-enabled business operations. We have designed and implemented a middleware platform for enabling contextual collaboration, and built collaborative applications on top of the platform. The platform embodies three main themes: on-demand, adaptive, and integrated. It is on-demand in that it allows collaboration data and services to be accessed from within business applications as the business need arises. It is adaptive in that it brings the right set of collaboration elements to the user, in a manner sensitive to business context. It integrates collaboration elements across multiple sources, multiple modalities, and multiple vendor technologies. In addition, it integrates structured collaborative activities with ad hoc peer collaboration tools, and with the business processes at large. To validate and leverage the contextual collaboration platform, we have developed an eclipse-based collaborative development environment and a browser that enables collaborative Web applications. Our contextual collaboration platform and applications promise to yield a more productive and more satisfying business practice."
2756,"Geospatial data is playing an increasingly important role in many areas. But there are still many problems associated with the effective use of geospatial data. One is that geospatial data is usually large in volume and is archived at geographically distributed data centers. Another is that the low-level geospatial data contains raw data instead of information that can be directly applied by users. In this paper, we discuss solutions to some of the problems based on the grid and SOA technologies. Specifically, we propose an architecture that can effectively manage large volumes of distributed geospatial data, the concept of virtual geospatial data products, and a mechanism that supports on-demand productions of customized geospatial data products. A prototypical system based on the architecture and concept has been implemented and demonstrated"
2757,"In our earlier work, competitive Web services market was proposed to address the hierarchical structure of large scale service systems. To better reflect the business reality, we model this system using finite capacity queues. To give the service provider more flexibility in controlling the priority level of incoming requests, we propose a performance oriented management architecture based on admission controller in this paper. Moreover, we study the design of an adaptive admission controller based on Markov decision processes and investigate the stationary policy based on different performance requirements."
2758,"Scientific workflows have been widely used by scientists to accelerate research experiments and achieve scientific discoveries. Due to the nature of science, scientific workflows often involve complex workflow design and distributed computation resources, so abnormal events are likely to happen and interrupt the normal execution of workflows. Restarting workflows is often too expensive and painful to do, especially for those long-running workflows. Thus, exception handling plays a significant role in the context of scientific workflows. In this paper, we present our ongoing work on exception handling in the VIEW scientific workflow management system."
2759,"The increasing amount of services published on the Web makes it difficult to discover relevant services for users. Unlike the SOAP-based services that are described by structural WSDL documents, RESTful services, the most popular type of services, are mainly described using short texts. The keyword-based discovery technology for RESTful services adopted by existing service registries is insufficient to obtain accurate services according to user requirements. Moreover, it remains a difficult task for users to specify queries that perfectly reflect their requirements due to the lack of knowledge of their expected service functionalities. In this paper, we propose a goal-oriented service discovery approach, which aims to obtain accurate RESTful services for user functional goals. The approach first groups existing services into clusters using topic models. It then clusters the service goals extracted from the textual descriptions of services by leveraging the topic model trained for services. Based on the service goal clusters, our approach can help users refine their initial queries by recommending similar service goals. Finally, relevant services are obtained by matching the service goals selected by users with those of existing services. Experiments conducted on a real-world service dataset crawled from ProgrammableWeb show the effectiveness of the proposed approach."
2760,"In the context of genome research, the method of gene expression analysis has been used for several years. Related microarray experiments are conducted all over the world, and consequently, a vast amount of microarray data sets are produced. Having access to this variety of repositories, researchers would like to incorporate this data in their analyses processes to increase the statistical significance of their results. Such analyses processes are typical examples of data-intensive processes. In general, data-intensive processes are characterized by (i) a sequence of functional operations processing large amount of data and (ii) the transportation and transformation of huge data sets between the functional operations. To support data-intensive processes, an efficient and scalable environment is required, since the performance is a key factor today. The service-oriented architecture (SOA) is beneficial in this area according to process orchestration and execution. However, the current realization of SOA with Web services and BPEL includes some drawbacks with regard to the performance of the data propagation between Web services. Therefore, we present in this paper our data-aware service-oriented approach to efficiently support such data-intensive processes."
2761,"In Web service ecosystems, business participants, such as service consumers, service providers and service marketplaces are required to form temporary collaborations to support virtual and dynamic business value networks. This requires participants in the collaboration to establish on-demand trust relationships (i.e. a trust federation) between each other and hence enable service sharing or consumption between partners in a secure and trustworthy way. This paper proposes a business-driven approach that can effectively manage the lifecycle of a trust federation according to business decisions taking place in service marketplaces. By leveraging a federation management architecture, we can establish, enforce, update, or dissolve trust relationships required by business collaborations on demand, which greatly reduces the costs for complicated and error-prone trust configurations in individual enterprise domains."
2762,"The flying increase of the requirement for business and scientific computing promotes the development of grid computing, where grid service is the basic unit for modelling computing resources. Integrating grid service with the J2EE resources, such as EJB, JMS and JCA, is a practically feasible approach to make good use of the J2EE computing resources. In addition, J2EE offers an ideal running environment for GT3. We propose an effective integration strategy to integrate the grid service and JCA, to which is not paid enough attention currently. We first present a perspective view of each concept (such as grid service, GT3, and JCA), then put forth the integration strategy in depth, further, we offer a framework of the encapsulating grid services definition. A simple example is also worked out to exemplify the strategy."
2763,"Software-defined networks (SDNs) have been recognized as the next-generation networking paradigm that decouples the data forwarding from the centralized control. To realize the merits of dedicated QoS provisioning and fast route (re-)configuration services over the decoupled SDNs, various QoS requirements in packet delay, loss, and throughput should be supported by an efficient transportation with respect to each specific application. In this paper, a QoS-aware adaptive routing (QAR) is proposed in the designed multi-layer hierarchical SDNs. Specifically, the distributed hierarchical control plane architecture is employed to minimize signaling delay in large SDNs via three-levels design of controllers, i.e., the super, domain (or master), and slave controllers. Furthermore, QAR algorithm is proposed with the aid of reinforcement learning and QoS-aware reward function, achieving a time-efficient, adaptive, QoS-provisioning packet forwarding. Simulation results confirm that QAR outperforms the existing learning solution and provides fast convergence with QoS provisioning, facilitating the practical implementations in large-scale software service-defined networks."
2764,"The list of authors suggested an alternative view on business modeling using public business services in order to meet variable business demands by means of creating the prototype of app store - is-store in network where services of various vendors are collected and simultaneous feedback from the users that attached their personal experience (rating) is received.In this work, the advanced view on the topic is provided with attempt to install a virtual SOA torrent that catches services from the Internet and makes them available to customers. The attempt to unite technical and management approaches in business architecture solving flexibility problem was impossible several years ago when technical possibilities of service-oriented architecture were not able to allow its serious projections on managerial tasks."
2765,"The interplay between labor arbitrage and consistent service delivery in IT outsourcing continues to drive business process standardization. New emerging standards, like ITIL, is defining an industry-wide taxonomy for IT service management. These standards are often at a high level and require substantial investment from service providers to define and implement these standards across the various low-level processes they offer. This paper presents a process management system, called Cyano, that uses social networks and recommendation to greatly increase the effectiveness of process capture and knowledge maintenance. We particularly focus on Cyano's social recommendation engine, called SCOOP, which utilizes the intrinsic graph property of process content for recommendation. More specifically, SCOOP maintains a user-process interaction graph Gand computes the user-to-user similarity scores using the random walk with restart on G. Finally, we evaluate SCOOP in the context of a large-scale deployment of Cyano, with thousands of processes and users."
2766,"Deploying applications on the grid requires both a stable production quality infrastructure, and correct configuration of the user environment and applications. In this paper we discuss two support tools to assist in increasing the chance of successful execution of an application. The first is a set of scripts which we have developed and deployed on the UK e-Science Grid to check at a system level that resources are live and ready for use. These checks might be thought of as a 'heartbeat' for the grid. The second is a support tool targeted at end users which allows them to verify that the specific resources they require are available and that their applications and user security settings are configured correctly. This verification gives a 'dial tone' for the application of the user. We demonstrate their deployment and use."
2767,"The growing significance of international collaborations in research, education, and business fields has raised the demand for the assurance of the quality of the network connections which the projects and applications are realized upon. A large spectrum of examples with diverse requirements is found in areas such as GRID- and Cloud computing, eLearning, and video-conferencing. The consequences of these diverse project and application requirements culminate in the urgent necessity to provide an End-to-End (E2E) guarantee for any customer-specific or user-tailored combination of service-specific Quality of Service (QoS) parameters. The quality of the overall network connections provided to users obviously directly depends on the quality of the involved connection parts. This means that already during the setup negotiation process the quality of the available connection parts has to be considered. Especially for international connections it is common that multiple independent service providers (SPs) realize different connection segments. This means in turn, that during the information exchange about available connection parts not only the technical challenges have to be solved, but also preferences and restrictions of the involved provider domains must be considered. In this paper we present a novel information model for the description of such connections. In the proposed model, a multi-domain view is derived from the single-domain perspectives of each considered SP. This model serves as a pro-found basis for an end-to-end routing algorithm which considers multiple user specific QoS parameters in parallel. The proposed model also accounts for the typically very restrictive SP information policies."
2768,"Bluetooth Low Energy beacon service is the newest Online-to-Offline technology. A beacon application in users smartphone connects an offline physical beacon with an online service produced by a service provider. However, since the BLE beacon separately operates by each service provider, users need to run multiple providers applications to access different providers service. This leads to much energy and resource consumption which potentially hinders other common uses of the smartphone. In this paper, we proposed GS1beacon, a GS1 standard based integrated BLE beacon service platform which could find the desired service regardless of providers applications, and provide multiple services in a single BLE beacon. To realize this, we construct a global discovery system by using an Object Name Service (ONS) and unify format of a BLE beacon ID by using a GS1 ID keys. Additionally, we prototyped GS1beacon service platform and showed its feasibility through case study, and performance evaluation."
2769,"Social networking has emerged as a powerful paradigm that connects people of similar interests. It has shown to be effective in routing queries in referral systems and cellular networks, and also in the production of frequently asked questions (FAQ) repository. In this paper, we extend its applicability to dispatch applications as part of fulfilling problem and change requests in outsourced IT environments. The proposed routing system is tested and validated using a large data set containing over 2 million problem and change records for a large number of outsourced accounts. We replay the requests to mimic the evolution of the underlying system and consider three cases."
2770,"Service-oriented computing is emerging as a paradigm for developing distributed applications. A critical issue of utilising service-oriented computing is to have a scalable, reliable and robust service discovery mechanism. However, traditional service discovery methods using centralised registries can easily suffer from problems such as performance bottleneck and vulnerability to failures in the large scalable service network, thus functioning abnormally. To address these problems, this paper proposes a peer-to-peer based decentralised service discovery approach named Chord4S. Chord4S utilises the data distribution and lookup capabilities of the popular Chord to distribute and discover services in a decentralised manner. Data availability is further improved by distributing service descriptions of functionally-equivalent services to different successor nodes that are organised into a virtual segment in the Chord circle. In addition, the Chord routing protocol is extended to support efficient discovery of multiple services with single request. This enables late negotiation of service level agreements between a service consumer and multiple service providers. The experimental evaluation shows that Chord4S achieves higher data availability and provides efficient query with reasonable overhead."
2771,"The cloud computing paradigm requires solutions supporting customers in the selection of services that satisfy their functional and non-functional requirements. These solutions must i) support the dynamic, multi-cloud nature of service provisioning, ii) manage scenarios where no total preference relation over service properties is available, and iii) prevent providers from misrepresenting or overstating their properties. In this paper we put forward the idea of modeling multi-cloud provisioning scenarios as procurement e-auctions (where the auctioneer is the customer and the bidders are service providers). We introduce a service selection process based on matching and ranking algorithms, and an e-auction mechanism that addresses the above requirements, encouraging trustworthy bids and therefore improving the truthfulness on the e-auction outcome. Finally we describe the implementation of a prototype used to evaluate the performance of our approach with respect to traditional query-based engines."
2772,"Business process interoperability (BPI) is an important issue for distributed applications in collaborative environments. This problem is further complicated in dynamic collaborative environments where stakeholders can quit the network and be replaced by new ones. To date, existing approaches do not provide the necessary flexibility to handle the dynamic nature of the network. In this work a formal framework is developed which combines cross-organizational process specifications with mediation-based solutions to ensure BPI. The concepts defined in this framework are used to build flexible collaboration contracts that better handle the dynamicity of the network. The applicability of the framework has been illustrated through a real-life industrial use-case in the aerospace industry for collaborative aircraft design process. The proposed framework provides higher level concepts than existing process modeling languages which better captures the business intent behind the collaboration."
2773,"Multiservice delivery is an important research issue in wireless sensor networks (WSNs) integrated with cloud computing, especially for heterogeneous applications which require reliable, timely and fair data delivery in adversarial environments. Therefore, both reliability requirements and delay constraints should be taken into account. Moreover, the delivery protocol should be designed to cater for multiple services regarding different traffic types. In this paper, we consider the problem of reliable multiservice delivery in which the source node performs rate control and routing selection based on availability statistics at individual sensors. We model the problem using network utility maximization and incorporate availability estimates into the NUM formulation (ANUM) with delay constraints. In order to handle multiple types of service, the performance of these services is modeled as a utility function of received rate in ANUM. Then we propose a Rate, Routing, and Delay Control (RRDC) algorithm to carry out multiservice delivery in an efficient manner, which is suitable to WSNs in respect of effective utility, latency, and utility fairness. To quantitatively evaluate the impact of unreliable sensors, we define two performance indices, utility degradation index (UDI) and utility fairness index (UFI). Simulation results show that the proposed RRDC algorithm achieves the desired performance among multiple types of service over WSNs in adversarial environments."
2774,"This paper presents a model for creating applications by joining components and sending data in XML format. The model focuses on legacy applications and ones that are hard to be maintained. In order to analyse it, an implementation of some of its elements has been carried out and used for two experiments"
2775,"Pervasive applications are often executed in fluctuating conditions and need frequent adaptations to meet requirements. Autonomic computing techniques are frequently used to automate adaptations to changing execution conditions. However, some administration tasks still have to be performed by human administrators. Such tasks are very complex because of a lack of understanding of the system evolutions. In this paper, we propose to build and link models at runtime of supervised applications in order to simplify the administrators' job. Our approach is illustrated on a health application called actimetrics, developed with the Orange Labs."
2776,"When a service engages in multiple conversations concurrently, incoming messages must be correlated with messages previously sent or received. Languages such as BPEL incorporate correlation as first-class citizen. However, current verification and testing techniques for service implementations largely ignore possible correlation anomalies as they typically focus on isolated conversations. This paper defines the notion of instance isolation and shows how to check this property. For doing so it introduces v*-nets, a Petri net extension with name creation and name passing."
2777,We analyze a scientific protocol developed to support a service devoted to alternative splicing analysis. We explain the process of selection of resources to implement each scientific task and present a methodology for scientific tools analysis and benchmarking to produce sound and efficient scientific protocols.
2778,"Web services availability has been regarded as one of the key properties for (critical) service-oriented applications. Based on analyzing the limitations of current metrics for ""User-perceived"" availability, we propose a service status based availability metric and a corresponding estimating approach. Experiments demonstrate that this metric and the corresponding estimation approach could get reasonable measurement on web services' availability."
2779,"Rapidly changing market conditions and IT systems are forcing companies to adapt their business processes more dynamically than in the past. Most current commercial BPMS products lack the full capability to support changing processes. To address this challenge, we discuss three aspects of dynamic adaptation in a BPM system: model-level, instance-level, and runtime environment changes. We focus on analyzing the patterns of the instance-level navigation changes, including the preconditions, actions and consequences of each change. Based on this analysis, we propose a system design for the runtime environment to support dynamic instance-level changes. We have implemented a prototype of such a system with the motivating scenarios to demonstrate the usefulness of our proposal."
2780,Conference proceedings title page.
2781,Designing E-Business applications in an efficient way has become a competitive necessity rather than a competitive advantage. One of the most important goals for many organizations is to satisfy their clients' service level agreements with respect to the response time and throughput. Adopting Service Oriented Architecture (SOA) during design and implementation promotes communication with the external and internal business entities. Web services are one of the popular technologies to achieve SOA solutions. Lookup web services are broadly used by many service consumers to fetch data which are used by their applications. In this paper we focus on how to efficiently build lookup web services using design patterns. Our goal is to improve the response time (latency) and throughput of lookup web services.
2782,"EURO-NGI's main target is creating and maintaining the most prominent European centre of excellence in next generation Internet design and engineering. A specific objective of the EURO-NGI Network is the development of a Macro-Tool which provides a homogeneous environment for hosting and interrelation of the software tools developed by the research labs of the network. This paper proposes tool integration under a common user access interface as an optimal solution and proposes the Web interface as the most natural option. The paper explains several innovative aspects in the field of remote software tool execution, secure access, customized interfaces and sharing environments. WeBaSeRex is the resulting sharing environment for network planning tools based on these ideas and last sections of the paper show some concrete characteristics and application example."
2783,"This paper proposes an ontology-based grid architecture model in terms of the universal knowledge grid (UKG) for building large-scale distributed knowledge system on the grid. UKG emphasizes geographically distributed high-performance knowledge discovery applications and knowledge integration services. Five core components have been identified: an intelligent composer for user interface, an ontology server providing data integration ontology services, data mining ontology services, knowledge integration ontology services, a metadata directory server maintaining the metadata describing all the data, tools and knowledge, metadata database (db) stores metadata and a knowledge base as container for knowledge. An application example on foreign exchange management (FEM) is also presented."
2784,"Under SOA, multiple services can be aggregated to create a new composite service based on some predefined workflow. The QoS of this composite service is determined by the cooperation of all these Web services. With workflow pipelining, it is unreasonable to improve the overall service performance by only considering individual services without considering the relationship among them. In this paper, we propose to allocate resources by tracing and predicting workloads dynamically with the pipelining of service requests in workflow graph. At any moment, there are a number of service requests being handled by different services. Firstly, we predict future workloads for any requests as soon as they arrive at any service in the workflow. Secondly, we allocate resources for the predicted workloads to enhance the performance by replicating more services to resources. Our target is to maximize the number of successful requests with the constraints of limited resources. Experiment shows that our dynamic resource allocation mechanism is more efficient for enhancing the global performance of composite service than static resource allocation mechanism in general."
2785,"The paper focuses on reasoning about service matchmaking automatically by the relevant rules. It proposes a formal way to describe elements of Web services in WSDL document using Martin-Lof's type theory and gives some rules of service matchmaking. Based on the formal representation, we first regard the progress of services matchmaking as the proof of a proposition, and then prove whether two services are matchmaking with each other."
2786,"Services business generates significant amount of human and machine created data. Search and discovery of relevant information from this data is a critical factor in enhancing workforce efficiency and operational excellence in such a business. The challenge is how to leverage the interplay among information retrieval techniques, role of human experts, and business processes. This paper presents a case for using ""business activity"" (e.g. a sale) as a contextual basis to meet the challenge. We developed a tool based on this approach to address the needs of services professionals. The tool is designed to support role-specific concept based queries that return the most relevant business activities first and subsequently enable retrieval of the most relevant documents and information for the business activity. Any relevant knowledge privy to an expert can be obtained by first finding the most relevant sale, followed by corresponding participant information, and finally by communicating with the participant expert. Business activity as a critical context enhances the search quality while bringing in the social network and business process. This approach suggests a new methodology for search and information management that delivers better business results for the services business."
2787,"B2B and long running B2C process is a complex business process that contains a set of services, state and transaction management, and involves notification of various events occurring during the execution of a process. Business processes are driven by events and each process should be capable to handle the dynamic nature of enterprise and business partners like change in the strategies, policies and exception handling. Execution of a business process at regional or national scale involves diversity in products, terminology and processes involved to carry out a complete business activity. Complete execution of a process requires dynamic composition of distributed services based on events. Heterogeneity, dynamic nature and lack of knowledge among business partners demand a scalable architecture, which can provide seamless interoperable integration of distributed heterogeneous services and automation of business processes with the management of state and transaction, notification of various events, and execution and monitoring of a process. In this paper, we have proposed semantic based service-oriented grid architecture to demonstrate how semantics can be employed in SOA to deliver common vocabulary, knowledge, and automation of a business process and how grid middleware can be used to provide state, transaction, notification, scalability, and execution and monitoring of business processes"
2788,"Cloud computing is a new computing paradigm that takes all resources as services, and it is not only agile, but also scalable. With the development of cloud computing, video on demand has become one of the most popular applications over the Internet. Currently, there is a trend of using cloud data centers and virtualization technologies to expand large-scale video streaming services with higher quality and lower expense. In this paper, we present CSM (Cloud Stream Media), a scheme that books the minimum resources from global data centers to match its demand and dynamically adjusts all resources to effectively meet the users' requests and guarantee a certain kind of quality of service, thus enhances the utilization and decreases the cost. CSM first predicts the stream media's future demand and data center's workload by using ARIMA model, and then performs a locality-aware resource booking (LARB) algorithm to lease the necessary resource from globalized cloud service providers in a long time. In order to handle prediction inaccuracy and the short-term demand peeks, CMS also introduces an inaccurate prediction handle strategy and performs auto scaling. We evaluate our scheme by combining both real world data and simulation. The results show good accuracy of our prediction and about 20% cut of total cost."
2789,"There is a constantly increasing need for dynamically trading intangible goods in today's economies. Electronic markets for intangible goods are on the rise. IT-based services support transaction activities between suppliers and customers. The vital task is negotiating a contract. Managing flexible transactions with varying market partners, considering multidimensional terms and conditions, and acting within short periods of time are crucial challenges for automated negotiation services of electronic markets. The approach of intelligent software agents as a negotiation service is presented. Aspects on negotiation protocol, software agent design and implementation are outlined. Insights into sample experiments and results are given."
2790,"Service modeling and composition is a fundamental method for offering advanced functionality by combining a set of primitive services provided by the system. Unlike in the case of web services for which there is an abundance of reliable resources, in sensor networks, the resources are constrained and communication among nodes is error-prone and unreliable. Such a dynamic environment requires a continuous adaptation of the composition of services. In this paper, we first propose a graph-based model of sensor services that maps to the operational model of sensor networks and is amenable to analysis. Based on this model, we formulate the process of sensor service composition as a cost-optimization problem, which is NP-complete. We then propose two heuristic methods for its solution, the top-down and the bottom-up, and discuss their centralized and distributed implementations. Using simulations, we evaluate their performance."
2791,"Although Web services are intended for short term, ad hoc collaborations, in practice many Web service compositions are offered longterm to customers. While the Web services making up the composition may vary, the structure of the composition is rather fixed. For companies managing such Web service compositions, however, challenges arise which go far beyond simple bilateral contract monitoring. It is not only important to determine whether or not a component (i.e., Web service) in a composition is performing properly, but also to understand what the impact of its performance is on the overall service composition. In this paper we show which challenges emerge and we provide an approach on determining the impact each Web service has on the composition at runtime."
2792,"Our paper develops a Walrasian general equilibrium model based on impersonal networking decisions to investigate the role of switching cost, trading efficiency and fixed leaving cost in a competitive market of e-commerce. Since the general equilibrium in our model is always Pareto optimal as long as nobody can block free entry into any sector and nobody can manipulate relative prices and numbers of specialists, the implications of our model is straightforward that if the e-commerce market is efficient and with lower switching cost, it ensures that network effects of division of labor can be fully exploited and the real income improve, yet the relative price in term of e-commerce service is cheaper. To business practitioners, our model suggests that successful transformation from conventional commerce to e-commerce service is a key for business viability in the future business environment."
2793,"The focus of many organisations shifts towards a new initiative, specifically service-oriented architecture (SOA). The use of SOA enables high-level application integration and the orchestration of business processes in order to realise its full value, including agility and reusability. This shapes the thinking of architects and developers when it comes leveraging computing and information technology (IT) to model, create, operate and manage business processes as services. However, applying SOA initiatives without considering the quality of information makes it impossible for organisations to succeed. Many researchers have looked at the area of information quality (IQ) in terms of the technical aspects of SOA, but there is a gap in the literature when it comes to the process of implementing and deploying an enterprise-wide SOA as a phased, evolutionary process. Thus, this article discusses the importance of IQ throughout the implementation of SOA initiatives. SOA team members from seven case organisations which have been carrying out SOA initiatives were interviewed to gain in-depth insights into IQ in these firms. The results from these various real-world experiences indicate that tying together these two distinctive areas - SOA initiatives and IQ strategy - has a positive effect for a broader audience."
2794,"As a mega-trend in IT, services computing provides different types of services for widely distributed data sharing and aggregation that span heterogeneous networks and domains. With these services, the need to visualize massive data is growing since there are a large number of traditional applications and it is quite common for those applications to invoke the services. Unfortunately, the visualizations in existing systems do not satisfactorily resolve the basic dilemma of being readable both for the global structure of a large service repository and also for detailed data analysis of local service interaction. To address this problem, we analyze the characteristics of data from Web service and present a hybrid data representation that combines the advantages of two representations: Parallel Coordinates are used to show the global structure of a whole repository, while arbitrary portions of the repository can be shown by NodeTrix to better support the analysis of data. A software prototype is described, which we can use to visualize for service browsing and interacting, such as side-by-side overview and detail views, visualization of fast data searching, and smooth transitions between Parallel Coordinates and NodeTrix."
2795,"An essential part of IT outsourcing is to move the customer's IT environment into the service provider's mode of operation, which is known as customer on-boarding. It covers every aspect of transition and transformation, from the time the customer signs the contract to the time the provider can deliver steady-state IT services. In order to improve the repeatability and enforce adoption of best practices, a standard set of processes should be established to direct, control, and measure on-boarding activities for each customer. However, this process is very complex and often gets adapted according to customer environments and requirements. It is very difficult to incorporate process variants needed for diverse scenarios into a single on-boarding process model, so that they can be reused. In this paper, we propose an approach based on ontology and rules to model the standard on-boarding process and configure process variants based on the business context that characterizes various scenarios. Further, semantic rules model adaptation policies and help generate a customized process variant schema on the fly. Based on this framework, we have developed a prototype to support process variant configuration. We also discuss the flexibility of our approach, and present its cost-benefit analysis."
2796,"We study a general attrition problem using unsupervised clustering and statistical approaches. The studied problem comes from retention problem in service industries. Our research provides an end-to-end solution from identifying hot job category to analyze the effectiveness of an incentive program applied to the selected categories. One of the barriers of studying the attrition problem is the lack of detailed features of an individual employee due to the confidentiality restriction. Different from the typical attrition approach that requires detailed individual information, we only use the aggregated attrition data and the internal business need data as the base, and cluster the job categories to give a recommendation. We converted the clustering results in a score for the recommendation. To avoid the monthly fluctuation, we apply exponential decay moving average multiple neighboring months on the snapshot scores to ensure consistent recommendation. The end-to-end solution also includes the impact analysis. By comparing the two general groups, we apply an approach similar to A/B test. We score the selected job categories with an effective score. We can apply this research to large consulting/service companies, and government agencies. For those enterprises or institutes, attrition avoidance is a major consideration as their main assets are their top performance employees. There also exist well-defined job roles and skill categories allowing to us to apply this approach."
2797,"This paper develops a wavelet based distributed ID model approach to analyses network traffic in detail. We adopt wavelet technique to analyze and model ID system to find the attack behavior in network. Our model can post attack characteristic more clearly and, by way of improve the veracity, we compare corresponding network node signals of wavelet decomposition. We simulate attacks and the model can detect attack more precisely than the previous model does."
2798,"The Internet of Things (IoT) refers to the network of objects, devices, machines, vehicles, buildings, and other physical systems with embedded sensing, computing, and communication capabilities, that sense and share and act on real-time information about the physical world. These objects, through standard communication protocols and unique addressing schemes provide services to the final users or systems. With the rise of low cost, low power single board computers, it is possible to perform certain business logic at the edge of the network utilizing such computers. This way the IoT application is distributed across many devices, some running at or near the edge of the network, at different locations and some running in the public or private cloud. This brings in new research and development challenges in the lifecycle of IoT applications, including modeling, deployment and support of non-functional requirements such as security, privacy, performance, provenance etc. In this paper we outline the challenges related to modeling and deployment of IoT applications and potential research directions in resolving these challenges."
2799,"The need to analyze huge amount of data for various business intelligence applications is well known. However, the rate at which enterprise data is generated now demands periodic migration of older data from the operational data warehouse to magnetic tapes. In this paper, we propose an ""Active data archival service"" in which the data is seamlessly archived on the cloud while ensuring that the archived data can be queried without any perceptible change to the end-user. This takes the burden of maintaining the archive off the user and shifts it to the archival service. We discuss the architecture of the service, challenges arising therein due to the federation of data brought on by the archival and how we handle these issues. Specifically, we investigate how the relational data needs to be transformed so that storing and retrieving the data from the cloud is efficient and seamless to the end user. We present our insights through an experimental study using TPC-DS benchmark."
2800,"As telecom carriers migrate their networks to a common IP multimedia subsystem (IMS) based infrastructure, they are confronted with another disruptive change in their business model from the Internet world - Web 2.0. Web 2.0 allows users to generate and collaborate content amongst themselves. Traditionally, telecom carriers, along with their partner content providers, have been the only producers of content in their network for the users. However, today (as we are seeing in the Internet world) users would like to share their content (photos, files, music, and video) with their friends and colleagues. A key attribute in this sharing model is the role of context. Context can be defined with attributes such as presence and location. In order to introduce this model in their networks, it is important for telecom carriers to ensure that they have a proper architecture framework to allow the sharing of content amongst users, keeping context in mind. This paper focuses on the SOA based services model that will allow for the transformation of telecom service providers into a digital media centric content business in a Web 2.0 environment. Based on our industry implementations, we will describe the hierarchy of content services in a layered model of foundational and composite services. This will form our basis for a context aware content services infrastructure."
2801,"Scientific workflows have recently emerged as a new paradigm for scientists to formalize and structure complex and distributed scientific processes to enable and accelerate many scientific discoveries. In contrast to business workflows, which are typically control flow oriented, scientific workflows tend to be dataflow oriented, introducing a new set of requirements for system development. These requirements demand a new architectural design for scientific workflow management systems (SWFMSs). Although several SWFMSs have been developed that provide much experience for future research and development, a study from an architectural perspective is still missing. The main contributions of this paper are: (i) based on a comprehensive survey of the literature and identification of key requirements for SWFMSs, we propose the first reference architecture for SWFMSs, (ii) in compliance with the reference architecture, we further propose a service-oriented architecture for VIEW (a VIsual sciEntificWorkflow management system), (iii) we implement VIEW to validate the feasibility of the proposed architectures, and (iv) we present two case studies to showcase the applications of our VIEW system."
2802,"Business-To-Business Integration (B2Bi) is a key mechanism for enterprises to gain competitive advantage. However,developing B2Bi applications is far from trivial. Inter alia,agreement among integration partners about the business documents and the control flow of business document exchanges, applying suitable communication technologies for overcoming heterogeneous IT landscapes as well as ensuring a Quality of Service (QoS) level that is sufficient for B2Bi are major challenges. In this context, applying choreography languages like ebXML BPSS (ebBP) for agreement among integration partners, orchestration languages like WS-BPEL for specifying partner-specific behavior, and Web Services for communication promises seamless interactions among business partners. In this scenario, the conformance of orchestration models to choreography models and cost-effective development are of paramount importance.Consequently, top-down approaches that automatically translate choreography models into orchestration models have been proposed. By now, the realization of QoS attributes has not yet received the necessary attention that makes such approaches suitable for B2Bi. In this paper, we describe a proof-of-concept implementation of a translation of ebBP choreographies into WS-BPEL orchestrations that respects B2Bi relevant QoS attributes."
2803,"Service-oriented product lines provide a unified representation of variants of the possible applications of a specific domain. The configuration process is often completed by multi-stakeholders collaboratively. However, current configuration methods do not provide means to coordinate concurrent decision making. In this paper, we present a framework that describes the collaborative configuration process of Service-oriented Product lines based on fuzzy quality requirements. First, the configuration objective is obtained based on an improved fuzzy multi-attribute group decision making method. Then, the corresponding multi-objective evolutionary approaches are presented for product configuration and business process configuration that satisfy both functional and non-functional preferences and constraints, respectively."
2804,"One of the key interests in web services is the ability to compose them in order to build more powerful and complex ones running in an interoperable and distributed setting. Several languages, like BPEL, that describe such services have been proposed. Similarly to traditional complex systems, web service compositions may exhibit inappropriate behaviors in the presence of failures. Compensation mechanisms are available to express running services recovery in case of failures. This paper addresses the problem of the correct design of web service compositions in case of failures. It presents a novel correct-by-construction formal approach based on refinement using the Event-B method. The proposed approach defines a compensation mechanism to repair failed services at runtime. It addresses not only behavioral aspects but also, functional ones through the introduction of repairing invariants whose persistence is enforced during compensation at runtime. The proposal is illustrated relying on a case study."
2805,"Research in business process optimization is valuable in improving business process models. However, it is difficult to find real datasets of business processes to evaluate process improvement techniques and tools. In this paper, a symbolic process generator, namely G-DCBP, is introduced to stochastically generate symbolic data-centric business processes that can be used to analyze their properties and evaluate optimization approaches according to end-users' specification."
2806,"With cloud eclipsing the $100B mark, it is clear that the main driver is no longer strictly cost savings. The focus now is to exploit the cloud for innovation, utilizing the agility to expand resources to quickly build out new designs, products, simulations and analysis. As the cloud lowers the unit cost of IT and improves agility, the time to market for applications will improve significantly. Companies will use this agility and speed as competitive advantage. An example of the agility is the adoption by enterprises of the software-defined datacenter (SDDC)[3] model, which allows for the rapid build of environments with composable infrastructures. With adoption of the SDDC model, intelligent and automated management of the SDDC is an immediate priority, required to support the changing workloads and dynamic patterns of the enterprise. Often, security and compliance become an 'after thought', bolted on later when problems arise. In this paper, we will discuss our experience in developing and deploying a centralized management system for public, as well as an Openstack [4] based cloud platform in SoftLayer, with an innovative, analytics-driven 'security compliance as a service' that constantly adjusts to varying compliance requirements based on workload, security and compliance requirements. In this paper we will also focus on techniques we have developed for capturing and replaying the previous state of a failing client virtual machine (VM) image, roll back, and then re-execute to analyze failures related to security or compliance. This technique contributes to agility, since failing VM's with security issues can quickly be analyzed and brought back online, this is often not the case with security problems, where analysis and forensics can take several days/weeks."
2807,"Today's information technologies, such as autonomic computing, offer companies new ways to do businesses. Those that fail to embrace and use such advantages in the next years are in danger to disappear or become zombie ones. In contrast, those pioneering these next-generation technology-business strategies will gain strategic advantage. This paper is about AWS-Net Traveler, which is an autonomic Web services framework for performing business processes in an autonomous mode. The main feature of this proposal is a decentralized architecture that heavily relies on peer-to-peer Web service brokers to coordinate, plan and perform Web service choreographies. We introduce the architecture of AWS-Net Traveler and the description of their main components."
2808,"Collaborative scientific workflow composition has recently been proposed to support collaborative scientific research projects, which require intensive collaboration among scientists with diverse expertise. A collaborative scientific workflow management system allows participating scientists to design and compose common scientific workflows concurrently. Concurrency control has become one of the key challenges in collaborative scientific workflow composition, which aims to facilitate collaboration throughput while ensuring the correctness and consistency of the results generated from concurrent operations. We have previously proposed a locking scheme to support simple scientific workflow compositions, in which workflows are flat (not hierarchical). In this paper, we take a step forward to propose a new granular scientific workflow locking scheme, which captures dependency relationships between workflows and constructs, to support general hierarchical workflow compositions. We also conduct several experiments to evaluate the performance of the concurrency control."
2809,"The majority of service discovery approaches operate on abstract service descriptions. However, a single service often provides a significant number of possible service offers which are not reflected in abstract service descriptions. In this paper, we define a preference-based discovery model which operates on rich search request descriptions and dynamically generated individual service offers. We define search request model that include hard constraints, rich preferences, and flexible input parameters. We use a combination of utility functions and weighted rules for modeling rich preferences. We apply our results to an international shipping scenario in the experiment to prove the feasibility and usefulness of our approach in a realistic scenario."
2810,"We present our work on implementing elemental services for carrying out design optimisation on the grid. The service-oriented approach makes it possible to harness the best available technologies that may often be incompatible due to heterogeneous software environments. It also facilitates the creation of an integrated problem solving environment (PSE) for design optimisation. We explain in detail how state-of-the-art Web services technologies are exploited to build these facilities, yield the desired quality of service, and demonstrate how service based design optimisation workflows constructed using script languages can be used to solve design optimisation problems from different paradigms."
2811,"The publish-discover-compose paradigm of service-oriented computing (SOC) presents a new challenge on the service applicability and adaptability. Services in SOC are not just for dedicated service clients, rather for a group of diverse and unknown potential service clients. Hence, service providers try to develop and publish services which can be applicable to various potential clients. Service clients try to locate right services without knowing much about service providers in advance. A challenging problem with this little coupling between providers and clients is that available services should be highly adaptable to various service clients and various service contexts. To develop such adaptable services, the service variability among potential clients and contexts must carefully be analyzed and designed. However, current works on service oriented analysis and design (SOAD) largely focus on defining processes for developing business processes and services without considering variability in details. In this paper, we analyze the key artifacts of common SOAD methods and identify types of variation points over the artifacts. Then, we present a systematic SOAD process for developing highly adaptable business services. We also present a case study to show the feasibility of the process. By developing adaptable services by using our proposed framework, the applicability and reusability of such services can be increased."
2812,"This paper presents an approach for ranking semantic Web service advertisements with respect to a service request. The use of recall and precision is proposed as suitable measures for determining the degree of match between the request and the advertisement. Ranking is based on the use of the domain ontology to infer the semantic similarity between the parameters of the request and the advertisement. The proposed approach is applicable to several types of ontologies, ranging from simple taxonomies to highly expressive ontologies, such as OWL ontologies."
2813,"With the advent of dynamic e-business and its open standards-based supporting technologies, valuable legacy applications that support essential business processes in enterprises can join this new area of distributed computing. We introduce a framework for enterprise integration with universal Web services model. We propose an SQL markup language based on the Software Integration Markup Language (SIML), which is an XML based markup language designed to EAI."
2814,"Growing numbers of users and many access policies in service-oriented environments cause various problems in protecting resource. In this paper we analyze the relationships of resource attributes to user attributes in all policies, and propose a general attribute based role-based access control (GARBAC) model. The proposed model introduces the notions of composition permission and single attribute expression and composite attribute expression, defines a set of rules based on different attribute expression to assign users to roles. The model is a general access control model, and can support many access control policies, and also can be used to wider application for service."
2815,"Recent accounting scandals involving firms such as such as Enron, Worldcom, Tyco etc. have resulted in the emergence of stringent regulations aimed at safeguarding the interest of enterprise stakeholders. Important regulations such as Sarbanes-Oxley Act (SOX), the US PATRIOT act in US and the International Accounting Standards (IAS) regulation in Europe have diverse and far reaching effects. Organizations are mandated to comply with these regulations by producing certain documents and reports during the audit. This calls for appropriate extensible document lifecycle management solutions involving integration and reconciliation of stored data from heterogeneous sources with quick detailing. This is because these regulations demand easy accessibility of data. While point solutions to document life cycle management exist, they are not cost effective. The paradigm of service oriented architecture (SOA) which emphasizes software development based on well abstracted services has presented application development and adaptation a new dimension. SOA enables flexible deployment of shared reusable compliance services thereby yielding malleable and cost effective compliance solutions. In this paper, we provide a logical SOA based approach for achieving compliance in enterprises and illustrate this approach for a typical compliance need"
2816,"Domain decomposition (D-D) is a key element for most computational fluid dynamics (CFD) parallel computing. For load balancing of computer nodes, the difference between computer nodes is considered in D-D. A domain decomposition service is now provided on a CFD-Grid. This service can also be embedded in other applications based on the CFD-Grid."
2817,"The operation of a cloud-based IT system (system for short) is time-consuming and error-prone due to the system scale, heterogeneity and configuration dependency. Although administrators can manage their systems with various configuration management tools, a plenty of knowledge spanning various domains is necessary. To alleviate this situation, we present a model-driven service MORE (Model-driven Operation seRvicE) to automate the initial deployment and the dynamic configuration of a system. Firstly, a model is proposed to specify the high-level view of a system in the form of a desired deployment topology. Then the topology model is transformed into executable code automatically, bridging the gap between high-level abstractions and low-level details. With those executable code as input, a runtime framework is designed based on a transaction-based self-configuration protocol to achieve automation and configuration consistency. Finally, we evaluate the service abilities (including modeling system topology, automating system provisioning, performing runtime reconfiguration) with a case study."
2818,"Providing a personalized service to a user in a smart environment has been one of the key goals in the area of pervasive computing. The proliferation of individually developed smart devices in the name of Internet of Things opens up a possibility of providing personalized services to a user in an autonomous and distributed manner. As a user's task often involves services supported by multiple devices, capturing a device-specific service preference is not enough to maximize a user's comfort. In this paper, we propose a distributed learning scheme for capturing multiple device service preferences in a smart environment. We exploit multi-agent reinforcement learning (MARL) method where each smart device acts as a reinforcement learning agent to incrementally and cooperatively capture a user specific preference of a task. Experiments confirm that smart devices with the proposed scheme are able to capture multiple device service preferences from a small number of interactions with a user and an environment. Also, the proposed transfer learning method improves learning performance for a new task."
2819,"Service Level Agreements (SLAs) have obvious value for Service-Oriented Computing and have received attention from both academics and industry. However, SLAs still lack a theoretical basis and effective techniques to facilitate automatic SLA establishment. In this paper, we classify negotiations into four types, and focus on the 1-to-1 Web services negotiation between a single service provider and a single service consumer. We make three contributions. Firstly, we represent the 1-to-1 Web services negotiation as a bargaining game. Here, we are interested in a bargain that takes into account the interests of both a service provider and a service consumer, in other words, a fair solution. Secondly, we determine a Nash equilibrium that can be regarded as the fair solution to a two-player bargaining game. We also determine the fair solution to the 1-to-1 Web services negotiation. Finally, we discuss issues that may arise with the 1-to-1 Web services negotiation under credible threats, incomplete information, time constraints, and multiple attributes."
2820,"Intermediaries for e-services continuously gain momentum, powered by a materializing Internet of Services. However, quality of service still exhibits considerable shortcomings, as no structured process to enhance consumer satisfaction is available yet. To improve the match of delivered e-service quality and expected service quality on the consumer side, we develop a portfolio optimization process that integrates both, the consumer's as well as the intermediary's perspective. First, we introduce a toolkit for an e-service oriented gap analysis. Thereupon, we identify monitoring points to measure service quality gaps automatically. A subsequent aggregation of measured data into customized feedback information allows for applying the toolkit to continuously optimize e-service portfolios. Instantiated in the AGORA e-service market, we conclude with a report on our recent implementation results."
2821,"In this paper, we propose an enhanced QoS prediction approach to predict the missing QoS values for QoS-based selection. Compared with existing QoS prediction methods, our proposed A Smooth approach has two differences: 1) using A-cosine equation for similarity calculation to remove the impact of different QoS scale, 2) adding a data Smoothing process to improve the prediction accuracy. An extensive performance study based on a real public dataset is reported to verify the prediction accuracy of our proposed approach."
2822,"Discovering and correcting interaction mismatches in inter-organizational business processes are important for service choreography. Automatic discovery mismatches and providing correcting plans will alleviate burdens on business modelers, however, three major challenges are: 1. by what cost model to evaluate multiple correcting plans; 2. how to avoid generating new mismatches when correcting processses;3. how to effectively reduce the search space size. In this paper, we first extend our previous approach to discover interaction mismatches, and present a cost model for evaluating correcting plans; secondly, we propose an independent modifying region-based method to obtain multiple correcting plans which can avoid generating new mismatches, and reduce the search space for finding out correcting plans. A running example is given to illustrate the validity."
2823,"Privacy oriented communication in social network has been researched extensively due to the dangers of private and personally identifiable information falling into wrong hands. To assist users better manage information disclosed, social network providers have introduced more controls like groups, lists and circles. With multitude of privacy controls on each social network, users are burdened with setting and managing privacy, on each social network separately as the friends grouping cannot be shared between social networks. This necessitates a service model which can be used across social networks, making use of information from all social networks. With the current privacy settings on social networks, users attitude towards privacy and communication cannot be accounted for although it has major influence on who sees what information. We propose to develop a framework for calculating privacy scores of friends from individual ego users' perspective. We contend privacy score would assist user in assessing his or her information sharing behavior and take an informed decision on who sees what information. Privacy scoring considers users' personal attitude i.e., Disposition towards privacy and communication information. Privacy scores are estimated based on privacy scoring function using cubic bezier curve. Our experiments highlight the effective working of the proposed framework in estimating friends privacy scores with various ego users' dispositions to privacy and communication."
2824,"OpenCCS is an SLA-aware resource management system which uses transparent checkpointing of applications and migration of checkpoint datasets for ensuring SLA-compliance also in case of resource outages. Migration of checkpoints presumes a high grade of compatibility between source and target resource. Hence, even in large Grid systems only a small number of resources are eligible migration targets. This short paper describes the concept of virtual execution environments and how they increase the number of potential migration targets. It will also outline an implementation within OpenCCS."
2825,"Integrating heterogeneous scientific dataset and high performance computing resources, and thus providing integrated Web services is an important goal for informazation project of Chinese Academy of Sciences (CAS). Cloud computing delivers computing and storage capacity as a transparent service to different communities of inter-disciplinary scientists, which inspires us to develop a SaaS (Software as a Service) cloud for collaborative virtual laboratory named VLAB-C. VLAB-C provides three key features including the ability of high performance on large scale of data processing and large amount of communities' collaboration, elastic computing for collaborative services, and extensible open framework for customized Web service integration through developing the components such as the micro-kernel container, the cloud infrastructure, and the open service. We gave several experiments on VLAB-C of its performance, which showed our system achieves considerable efficiency with cloud computing."
2826,"Summary form only given. Network operators are now on a journey towards a next generation network (NGN) built on horizontal ""planes"", moving away from traditional vertical structure networks for fixed voice and data communication networks. IP multimedia subsystem (IMS) is a part of wider network transformation to next generation (NG) networks. The IMS core network has a common IP based transport plane and a common signalling or control plane, and it can be accessed by different access networks. IMS provides a control layer based on an end-to-end based session initiation protocol (SIP) that can control a session and remain agnostic of the access network. This innovation in the telecom industry where more intelligence is moving to the edge, can be closely linked to another innovation in the IT industry, where applications are being rendered as re-usable components, or services as we may refer to them. Such components, when made available on the Web are referred to as Web services. IBM's service enablement solution in the NGN space is based on three strategic tenets. First of all, it makes use of the common IP transport plane and the Web services realization mechanism as a backbone for composite services within a single session. This would help network operators to generate revenue in a converged telecom network. Secondly, it uses Web services to automate the entire lifecycle of a service. This leads to automation of the service delivery platform (SDP), which in turn allows third party application providers to deploy services by using the services exposed by network provider. Finally, as premium composite services are deployed to the end user, he/she would have a higher quality of experience (QoE). IMS provides a per-subscriber control of QoS as part of QoE. As the subscriber uses various composite services, the IMS ability to control QoE/QoS, becomes a huge differentiator for IMS over just uncontrolled access over the Internet. In other words, IMS QoE encompasses QoS,..."
2827,"This paper discusses and analyzes the business process layer in the SOA solution stack (S3) model, which is also known as the SOA reference architecture (SOA-RA). Business process layer leverages the service layer to quickly compose and choreograph services and to coordinate business processes to fulfill customer requirements. Based on industry practice, we introduce a set of architectural building blocks of the layer, together with the interdependencies and interactions between them, to componentize the business process layer in the context of SOA reference architecture. We also report industry experiences of applying this layer in SOA solution engagements."
2828,"Considering the facts that existing benchmarks to measure the performance of Web service frameworks simulate only theoretical scenarios such as streaming homogeneous data structures and the computer industry has an established culture of developing performance benchmarks imitating real world scenarios, an effort was made to come up with a benchmark that closely represent the real world business services. The paper concludes that the benchmark represents an unbiased subset of actual scenarios because the ranking and performance patterns of the leading Web services frameworks used in the experiment are consistent with industry wide experiences. Additionally the paper introduces a performance model to analyze Web service frameworks and identifies complexity of the SOAP messages and size of the payloads they carry as two major factors that affect the RTT of the SOAP messages and reveals that a framework that is good at handling complex SOAP messages may not deal with messages that carry larger payloads equally well."
2829,"Business activity monitoring (BAM) is a concept for computer systems which support monitoring situations and the performance of business processes as they execute. The monitoring system needs to capture and to process numerous events while correlating them with other events stored in a database. If the database accesses become very frequent, read-only and write-only accesses can achieve high throughputs by applying well known solutions; a result cache and a batch update. However, the event processing of the monitoring includes both simultaneously. It is the case that is hard for the existing solutions to achieve high throughputs. Here, we introduce a new architecture, HM-hybrid, to process events for monitoring efficiently. It is a hybrid of a result cache and a batch update. And our model-driven approach makes it possible for them to work together in situations that are normally regarded as being unsuited for them. It processed 1207 events per second, about 43000 queries per second. It is 6 times better performance over a naive caching architecture in our experiments. The architecture seems useful for monitoring applications where there are many update and select queries."
2830,"The service-oriented software systems are highly vulnerable and incline to frequent changes. There is a need for mechanisms that allow services to be changed while it keep providing. This work proposes an independent and structured framework to allow dynamic service updates, and it includes all the necessary functionalities that support performing the identified kinds of dynamic changes of service component."
2831,"The way an application is designed and certain patterns thereof, play a significant role and might have a positive or a negative effect on the performance of the application. Some design patterns that have a negative effect on performance, also called performance antipatterns, may become important when evaluating migrating the application to the Cloud. Although there has been work done in the past related to defining performance antipatterns, there has been none that highlights the importance and effects of these performance antipatterns when an application is migrated to Cloud. In this work we present an approach to automatically detect important performance antipatterns in an application, by leveraging static code analysis and information about prospective deployment of the application components on the Cloud. We also experimentally show that these antipatterns may become prominent and pull down the application's performance if the application is migrated to the Cloud. Our results show that the performance of the parts of the application with such antipatterns suffer significantly and hence, the detection of these antipatterns has an overarching significance in the domain of software development for the Cloud. The approach we present here has also been implemented in a prototype cloud migration assessment tool."
2832,"To address the issue of isolated information island in national population information system, we present a kind of multi-layers hierarchical service-oriented architecture. In the architecture, the top nodes are only the requestors of Web services, and all the leaf nodes of the bottom layer are only the providers of Web services. For those nodes of intermediate layers, every node is not only the provider of web services, but also the dispatcher of Web services. All isolated information islands are connected by Web services dispatcher and web services registry database to form a uniform collaborative system. The command (Web service invocation) is recursively multicasted from top layer to bottom layer and statistical result data are gathered from bottom layer to top layer. We implement service integration, information integration and information sharing of national population information system."
2833,"Summary form only given. The technology and information platform (TIP) is a three-dimensional architecture framework to effectively cope with the architecture complexity and manage the architectural assets of information system applications in a service-oriented paradigm. This holistic framework comprises the generic architecture stack (GAS), which is made up of a series of architecture layers, and the contextual spectrums, which consist of the process, abstraction, latitude, and maturity (PALM) dimensions. This paper describes the detailed attributes in the four dimensions of the contextual spectrums. The process dimension covers operations, risk, financial, resources, estimation, planning, execution, policies, governance, compliance, organizational politics, etc. The abstraction dimension deals with what, why, who, where, when, which and how (6W+1H). The latitude dimension includes principles, functional, logical, physical, interface, integration &amp; interoperability, access &amp; delivery, security, quality of services, patterns, standards, tools, skills, and so forth. Finally the maturity dimension is about performance, metrics, competitive assessment, scorecards, capacity maturity, benchmarks, service management, productivity, gap analysis, transition, etc"
2834,"OWL-S is an ontology for specifying semantic web services written using the Web Ontology Language (OWL). In order to leverage the vast array of existing WSDL-based, non-semantic web services, OWL-S provides a grounding (or binding) mechanism that maps OWL ontological input and output concepts to the standard XML data types used in WSDL input and output messages. In this paper, we discuss an approach using the SPecification and EXecution tool (SPEX), that facilitates the automated generation of OWL-S groundings and associated XSLT transformations and also supports the execution of fully-grounded OWL-S-based semantic web services. In addition, we will present a comprehensive example that will demonstrate the use of the SPEX tool for the grounding and execution activities. This work is a follow-on to our previous work in the area of model-driven semantic web services and is the final piece in the end-to-end development approach."
2835,"This paper proposes key requirements and interfaces for the management of life cycle of context-aware and personalized mobile services. The main rationale behind the work presented here is that the service management component should be designed in such as a way that life cycle aspects should emerge from within service management components themselves as an integral part of overall services management. The features of context-aware applications and services should be reflected in life cycle management. Furthermore, standard practices of the mobile network industry must be taken into account for any serious effort to deal with this issue. All these aspects have guided and provided impetus for the work presented. Therefore this work is easily applicable in industry applications and proposes a clear-cut association between requirements and interfaces"
2836,"Nowadays, an increasing number of geospatial Web services (GWSs) are built and being available on the Web for the accessibility and processing of geospatial information. Given the requirement specified by certain users, normally a composition of GWSs, rather than a single GWS, can fulfill this requirement. Consequently, retrieving and recommending sub-chains of possible service invocations is an important research challenge. Leveraging the semantic similarity between the name and text description of parameters, the degree that represents the invocation possibility between operations in GWSs is calculated. The service network model is constructed to capture the possible invocations between operations. Given a user's requirement which is represented in terms of the initial and ending operations, possible sub-chains of operations are retrieved, ranked and recommended. Thereafter, the user can select the most appropriate sub-chain with respect to her specific requirement. The evaluation result based on real GWSs set shows that our technique is applicable in real applications."
2837,"This work presents an information retrieval model based on a kind of probabilistic network. The dependence relationships between terms and documents are represented by the topology of the network. The network contains two parts: the term layer and the document layer. The term relationships are represented as an undirected probabilistic network, with the directed arcs toward the nodes indexed in the document layer. Using a learning algorithm, the topology and the probabilities encoding the strength of the relationships can be learned from the document collection automatically. We also provide a two-part inference process to obtain the relevance of the documents to a given query."
2838,"We present our experiences in building and deploying a text mining solution in services industry settings, specifically in contact centers. We describe the voice of customer (VoC) and customer satisfaction (C-Sat) analysis settings and outline several unique research challenges brought about by this confluence of text mining and industrial services research. We describe our system for integrated text classification, business intelligence and interactive text labeling for C-Sat analysis. We recount invaluable lessons learned as computer science researchers in services research engagements. The system has been deployed in multiple accounts in contact centers and can be extended to any industrial CRM service practice to analyze unstructured text data."
2839,"Many techniques have been proposed to transform BPMN into BPEL. The transformation for single BPMN process is studied well. But multiple BPMN processes often suffer from mismatch issues. This paper aims to better transform multiple BPMN processes containing interaction mismatches. The challenges for such transformation are: ensuring transformed BPEL processes compatible; ensuring transformation interaction-preserved, which means keeping interactions those do not cause transformed processes mismatch preserved and transformed. In the paper, interaction mismatches are classified into two categories: static and runtime interaction mismatch. Both of them can be discovered by our approach based on interaction control-flow relations. For better transformation, discovered mismatch interactions are minimized and only the minimized result will not be transformed. A case study is given to illustrate the approach. The transformation also provides general support for transforming cross-organizational business processes into executable processes."
2840,Service composition is an effective practice to perform complex tasks through varied configurations of simple services. An issue that often arises in such a set-up is the selection of the best service from a group of functionally equivalent ones to cater to each of the various functionalities of the composite application. Previous effort in this direction incorporates utilizing the Quality of Service (QoS) attributes of the services to pick out the best one of the lot. This paper presents a technique to select the optimal service for composition using the average waiting time attribute of the services. The service selected is the one that has the smallest value of the average waiting time. Concepts from queueing theory have been borrowed and customized to estimate the waiting time values of the candidate services. Experiments have been performed wherein selection results using the proposed technique are compared with the selections that are made by simulating an actual scenario and computing the waiting time by observation.
2841,"Personal Mobility Profiles of cellphone users play a crucial role in a wide range of context-aware applications. Transportation mode, a representative feature of users' mobility, can provide much richer context information to the pervasive computing applications. This paper presents a data driven classification model to detect transportation mode from sparse GPS trajectory data. The contributions of this paper lie in the following two aspects. On one hand, we propose a transportation mode classification model based on a deep architecture formed by an SAE model to further improve the inference performance. On the other hand, the relevant features related to the underlying transportation network information are redefined and considered to improve the final classification efficiency and effectiveness. Five representative travel modes-walking, cycling, taking a bus, taking a subway, and driving-are distinguished. Four different models including Bayesian Net, Decision Tree, Random Forest, and Support Vector Machine are compared with our model and tested with the GPS trajectory data in Beijing. As a result, we achieved the average precision and recall accuracy being greater than other models, which are 93.58% and 93.32%, respectively."
2842,"Service integration, the ultimate goal of Shanghai Grid, has created a necessity for more efficient workflow infrastructure. Workflow design tool is one core component that assists in defining workflow processes as well as providing graphical representation of process model through which users can have an easier understanding of the semantics of process. This paper discusses a workflow design tool for Shanghai Grid that has the following features. First, it combines graphical process representation and ECA rules in controlling grid workflow process. Second, integration adapter of the grid workflow system is presented to facilitate the composition of all possible services. Finally, this tool supports hierarchical graph definition that allows workflow coursing and refinement. In this way, it extends the scope of resource sharing and offers a well-layered view for complicated workflow. Design principle and implementation details of workflow design tool for Shanghai Grid are also given in this paper."
2843,"The Service Component Architecture (SCA) is a technology agnostic standard for developing and deploying distributed service-oriented applications. However, SCA does not define standard means for runtime manageability (including introspection and reconfiguration) of SOA applications and of their supporting environment. This paper presents the FraSCAti platform, which brings runtime management features to SCA, and discusses key principles in its design: the adoption of an extended SCA component model for the implementation of SOA applications and of the FraSCAti platform itself; the use of component-based interception techniques for dynamically weaving non-functional services such as transaction management with components. The paper presents micro-benchmarks that show that runtime manageability in the FraSCAti platform is achieved without hindering its performance relative to the de facto reference SCA implementation, Apache's Tuscany."
2845,"Service provision networks are popular platforms for decentralized service management. eBay and Amazon are two representative examples of enabling and hosting service provision networks for their customers. Trust management is a critical component for scaling service provision networks to larger set of participants. This paper presents Service Trust, a quality sensitive and attack resilient trust management facility for service provision networks. Service Trust has three unique features. First, it encapsulates quality-sensitive feedbacks by multi-scale rating scheme and incorporates the variances of user's behaviors into the local trust algorithm. Second, Service Trust measures the similarity of two users' feedback behavior and aggregate the local trust values into the global trust algorithm by exploiting pairwise feedback similarity scores to weight the contributions of local trust values towards the global trust of a participant. Finally, pairwise feedback similarity weighted trust propagation is utilized to further strengthen the robustness of global trust computation against malicious or sparse feedbacks. Experimental evaluation with independent and colluding attack models show that Service Trust is highly resilient to various attacks and highly effective compared to Eigen Trust, one of the most popular and representative trust models to date."
2846,"Recently, grid and peer-to-peer overlays have attracted attention from research communities as well as industry. However, the overwhelming majority of industrial, commercial and academic activities are geared around applications, servers, and middleware. There have been some efforts in the management of the underlying networks (the connectivity for the overlays), but far from enough. This paper proposes a resilient, fault-adaptive, overlay resource management framework, with lightweight state management infrastructure, taken from some ongoing work by the authors in this area. This research aims to solve the problems of: (a) managing distributed network resources in a decentralized way; (b) providing resilient QoS for highly dynamic networks."
2847,"This paper proposes a service-oriented reconfigurable co-processing architecture. The novelty of the architecture is to apply service-oriented concepts to system on chip (SoC) design paradigms and utilizes each processor and IP core as a function unit. Regarded as abstract instructions, tasks can be scheduled to IP core for parallel execution automatically. A uniform IP reconfiguration interface is provided to allow function units replacement at run-time. Neither the applications nor the tool chains need to be redesigned after hardware reconfiguration. To evaluate the SOA concepts, we implemented a prototype on a state-of-art Virtex5 FPGA board with IP cores implemented from EEMBC DENBench. The prototype and experimental results demonstrate it can support a range of hardware accelerators in an efficient manner. Furthermore, results also depict that the architecture takes moderate silicon area affordable power consumption. We believe the SOA approach opens a new direction to combine SOA concepts with reconfigurable computing hardware architectures."
2848,"Service-oriented architecture is a paradigm for modeling and enacting business processes that promotes improved flexibility and monitor ability through the composition of loosely-coupled Web services. However, such process-centric composition, with focus on invoking Web services to reach a stated goal, still does not provide the desired flexibility. Web services implementations are typically locked into the business logic of the business processes that they implement. Additionally, monitoring such Web services compositions becomes cumbersome especially for business analysts and managers who are not IT experts. To address these issues, this paper presents a unified process- and data-centric approach with focus on Web services as a driving element to the changes affecting both processes and data. In this approach a business process is modeled as a collection of interacting (business) artifacts, each of which behaves as per a predefined state transition system called artifact life-cycle (ALC). An ALC is enriched with contextual details, which permits business process execution monitoring. Throughout this paper, a realistic running example in the purchase order domain is used for illustration purposes."
2849,"Global IT Services and Consulting organizations like Tata Consultancy Services (TCS) have been investing considerable resource into the emerging discipline of Services Computing. The address focuses on the business drivers for Services Computing across multiple industries across the world and why this could be the 'silver bullet' for many of our challenges. The address uses the TCS framework for Services Computing as a backdrop - across services strategy, services implementation and most importantly services governance. This is a progressive approach towards bridging the gap between 'what IT delivers' and 'what the business wants' through a well defined 'value measurement framework.' This framework links organizational readiness, solution maturity, and delivery assurance. Finally, the importance of aligning the competencies and roles across different parts of the organization and its stakeholders is examined."
2850,"When deploying a service, software systems required by the service should be configured correctly. However, since software has tens or hundreds of configuration parameters and the parameters of different software may have simple or complex, explicit or implicit dependencies and constraints, to configure multiple software systems in service deployment becomes a difficult, error-prone and time-consuming task. In this paper, we propose a model based approach to automated configuration. Motivated by two real cases found in IBM software products and solutions, our approach has three contributions. Firstly, a meta-model of configuration, called software resource configuration model, is defined for integrating configuration parameters and experiences of different software into a global model. Secondly, a set of configuration rules are designed for verifying incorrect configuration that violates constraints specified by service deployment engineers. Thirdly, a supporting tool, called Comfort, is implemented and evaluated by the motivating cases"
2851,"Web services are vulnerable to various types of security attacks. We address one type of attacks, where applications trying to access services to which they are not authorized. Existing access control for Web services lack of support for global services. As such services are WAN-based, therefore access control needed to deal with various levels of Web services, including global (for composite services) and local level (for Web servers). We propose two access control: SWS-RBAC (for single services) and CWS-RBAC (for global services). Instead of protecting the content of the service's parameters, these models protect the parameters themselves. The proposed approach introduces global roles, which are used in the mapping to local roles of other service providers. To maintain the autonomy of roles between providers, an efficient role-mapping mechanism has been proposed accordingly."
2852,"Workflow is an important part of service grid. It provides a mechanism to describe the various interactions between services, and it can compose new services out of existing ones dynamically. Grid workflow description language is the basis of the establishment and execution of workflows. In this paper, being extended according to the characteristics of grid environment and applications, a new grid workflow description language called GPEL (grid process execution language) is proposed based on the Web service composition language BPEL4WS."
2853,"Enabled by Service-Oriented Architecture (SOA), recently Software as a Service (SaaS) and Cloud computing are gaining momentum in the industry. An open issue is how to ensure accountability in business services offered through Internet. Traditionally a contract is an effective legal means to uphold accountability in business transactions. In this paper, we propose a novel service contract model called OWL-SC for e-Services. Based on OWL-DL and SWRL, OWL-SC model can be used to disclose obligations of both e-Services consumer and e-Services provider. More importantly, the model allows service participants to monitor the service contract execution and keep track of obligation fulfillment for each party during service delivery. We also propose a graphical model SC-CPN based on Colored Petri-Nets (CPN) to formally model contract obligations and their interdependencies. SC-CPN can also be used to validate the correctness of obligations in OWL-SC through simulation and state space analysis. Finally, we use the Congo Book service as an example to illustrate how to use OWL-SC and SC-CPN to build a service contract model."
2854,"Monitoring the status of ongoing sales opportunities in IT service engagements is important for sales teams to improve the win rate of deals. Existing approaches aim at predicting the final outcome, i.e., The eventual chance of winning or losing a deal, given a snapshot of the deal data. While this type of prediction indirectly advises on the deal status, it offers limited guidance and insights. During the engagement progress, there occur numerous milestones and key events whose occurrence and status is important in achieving the desired outcome of the deal. These interim milestones and events may happen in different time intervals during the lifecycle of a deal, depending on the deal size and other parameters. In this paper, we describe a novel Bernoulli-Dirichlet predictive model for predicting the occurrence of key events and milestones within a service engagement process to assist in monitoring the progress of ongoing deals. This model enables predicting the timeline and status of the next event(s), given the current history of milestones activity in the engagement lifecycle. Through such a step-by-step guidance, sales teams may have a higher chance of success by knowing of upcoming events, and preparing to counter undesired events. We show the empirical evidences of significance and impact of such an approach in a real-world service provider environment."
2855,"In today's IT service market customers urge providers to grant guarantees for quality of service (QoS) which are laid down in service level agreements (SLAs). To satisfy customers and to avoid penalties, service providers have to ensure that the agreed SLAs are met. Therefore, it is necessary to be able to effectively deal with resource failures which could endanger the SLAs by affecting the provided services. The effort for recovering from failures should be selected corresponding to the expected SLA violation costs. In this paper we present a framework to automatically determine the impact of resource failures with respect to services and service level agreements. We achieve this by monitoring the service quality from inside and outside the service provider and also by incorporating information about the current and expected future service usage. The expected costs of the resource failures are assessed to select an appropriate recovery alternative. Besides this short term perspective the impact analysis can also be employed to identify critical resources and to improve the service provisioning."
2856,"We propose a novel technique to enhance the IT problem isolation and resolution Maintenance Services by automatically structuring problem tickets consisting of free form, heterogeneous textual data. The originality of our technique consists in applying the Conditional Random Fields (CRFs) supervised learning process to automatically identify individual units of information in the raw data. We apply our technique to identify structural patterns specific to the problem ticket data used in Call Centers since this data is not explicitly structured, is highly noisy, and very heterogeneous in content, making it hard to analyze and search by the remote technical support personnel. We present a study of the accuracy of our experiments."
2857,"To support application-specific quality of service for hosted services, a client of a compute utility requires the ability to schedule the processor resources supplied to its service. We present a user-level scheduling framework that operates in tandem with a standard kernel scheduler to support user-level policies for sharing processor resources. The scheduler operates by sampling the resource consumption of processes and limiting which processes are eligible for scheduling by the kernel. We present a Unix implementation of this framework and show that it can accurately control the rate of execution of compute-bound processes, with low computational overhead, despite its user-level operation. Finally, we demonstrate the scheduler's ability to enforce differentiated qualities of service for a Web-based message board service."
2858,"Using service level agreements (SLAs) is a central concept for the grid commercialization since customers require guarantees for a successful job execution. However, agreeing on SLAs is a business risk for resource providers: system failures can lead to paying tremendous penalties. Accordingly, resource providers require an estimation about the risk of agreeing on an SLA. Furthermore, risks can be published and integrated into the negotiations as an additional SLA parameter. Each offer is linked with a possible execution on a specific resource since the risk of failure depends especially on the resource's stability. Therewith making reservations is the initial step for integrating risk management into the grid. This paper describes the necessary workflow and measurements for selecting a resource according to assessed risks. A global perspective is presented in order to afford user-specific configurations. Concrete examples for several measurements present one possibility for rating and the important input factors"
2859,"With the integration of cloud computing and big data, it is difficult for the masses to discover valuable service from big data. The understanding of historical data and streaming data is fundamental to the value discovery, and the construction of topic knowledge is essential to the understanding of big data. This paper proposes an approach for the construction of topic knowledge based on ontology meta-modeling, and the approach follows three stages: classification, clustering and integration. Furthermore, the realization of the three stages is based on support vector machine, probability computing, and ontology meta-modeling. Finally, experiments on scientific papers of service computing were conducted in order to get the recommended reviewers. The results of the experiments demonstrate the effectiveness of the approach. In conclusion, the approach provides a solution for the value discovery from big data."
2860,"A critical step in the process of reusing existing WSDL-specified services for building web-based applications is the discovery of potentially relevant services. However, the category-based service discovery, such as UDDI, is clearly insufficient. Semantic Web Services, augmenting Web service descriptions using Semantic Web technology, were introduced to facilitate the publication, discovery, and execution of Web services at the semantic level. Semantic matchmaker enhances the capability of UDDI service registries in the Semantic Web Services architecture by applying some matching algorithms between advertisements and requests described in OWL-S to recognize various degrees of matching for Web services. Based on Semantic Web Service framework, semantic matchmaker, specification matching and probabilistic matching approach, this paper proposes a fuzzy-set based semantic similarity matching algorithm for Web Service to support a more automated and veracity service discovery process in the Semantic Web Service Framework."
2861,"Interoperability in sharing work through services between organizations requires understanding the perspective of each partner. Available resources and requirements are widely distributed with heterogeneous descriptions and enactments. No centralized registration and discovery process exists, nor automated means to map the variety of formal organizations within various overlapping domains, some defined and many not. While drawing from a common language, the mission and culture of each organization shapes their particular collection and definition of symbols, vocabulary and signals. Mining execution logs of historical inter-organizational workflows and tapping into explicitly specified domain and organizational knowledge ontologies provides a corpus of information useful to identify emergent patterns. This paper proposes a novel mediator approach using feature alignment demonstrated in previous partnerships, identified through the employment of topic modeling and word sense disambiguation methods, to infer semantic matches between parties without a priori relationships. This model demonstrates an inter-organizational workflow middleware proof of concept coupling workflow management systems interoperating between a set of organizations to automatically resolve meaning, providing the right services for requirements."
2862,"The programming model of the current distributed systems such as middleware CORBA or JAVA RMI is based on remote procedure call. Without the support of high-level API for coordination, developing real-world distributed grid applications in this model is difficult. We present an algorithm for multiparty interaction, a key abstraction of distributed coordination for the future distributed grid programming."
2863,"The third party logistics (3PT) is a fast-growing industry in recent years and has a demand for information integration technology. More and more logistics companies find other Web service providers to integrate their application in order to reduce the cost and improve the efficiency. Web service meets their requirement. Web services are self-contained, Web-enabled applications capable not only of performing business activities on their own, but also possessing the ability to engage other Web services in order to complete higher-order business transactions. As the main functional part of logistics system, freight transportation system assumes the job of scheduling vehicles for the logistics system to complete the transportation task. A freight transportation system which is separated from the logistics system is a typical business case. The prototype system developed on J2EE has a quality of cross-platform and interoperability with other systems."
2864,"Our paper provides a method to manage sustainable service levels in call centers. We defined a service level management process and a related capacity and workforce management that considers Erlang-A operational model for staffing levels. Based on such model, we compute the minimum sustainable price for a service contract based on the fixed service levels, and enable transparent pricing against a certified service level. The model has been implemented in a tool and are being tested in real call centers."
2865,"The capability to easily find useful services (software applications, software components, scientific computations) becomes increasingly critical in several fields. Current approaches for components retrieval are mostly limited to the matching of their inputs/outputs. To go beyond the limits of these approaches, a substantial effort has been done by different works on semantic Web and ontologies by exploiting more knowledge on the semantics of the components. However, this effort still remains insufficient and does not fulfil user needs as many functional or quality aspects are hidden within the specification of components behavior. In this paper we argue that, in many situations, the component discovery process should be based on the specification of this behavior that is the process model, which describes each component. The idea behind is to develop matching techniques that operate on process models and allow delivery of partial matches and evaluation of semantic distance between these matches and the user requirements. Consequently, even if a service satisfying exactly the user requirements does not exist, the most similar ones will be retrieved and proposed for reuse by extension or modification. To do so, we reduce the problem of service behavioral matching to a graph matching problem and we adapt existing algorithms for this purpose."
2866,"The concept of Web services has become a widely applied paradigm in research and industry, with the number of services published on the Internet increasing rapidly over the last few years. Thus, service recommendation is becoming a challenging and time-consuming task due to large search space. Organizing the Web services into clusters is a one of very efficient approach for search space pruning process. In this paper, we proposed cluster-based service recommendation approach. User may want to interact with services that have similar functionalities, which they used to interact. Thus, we consider semantic similarity between services as one factor in the clustering process. Further, user may be interesting to identify the appropriate services to generate value-added service. Thus, we consider the association between services as the second factor. Our approach recommend services for currently invoked service using the generated clusters and services with better QoS values selected by a filtering process. Experimental results show that our approach works effectively."
2867,"Business transformation is a key management initiative that attempts to align people, process and technology of an enterprise more closely with its business strategy and vision. It is an essential part of the competitive business cycle. Existing consulting methods and tools do not address issues such as scalability of methodology, knowledge management, asset reuse, and governance well, to name a few. This paper presents business transformation workbench, a practitioner's tool for business transformation addressing these problems. It implements a methodical approach that was devised to analyze business transformation opportunities and make business cases for transformation initiatives and thereby provides decision-support to the consultants. It provides an intuitive way to evaluate and understand various opportunities in staff and IT consolidation and process standardization. It embodies structured analytical models, both qualitative and quantitative, to enhance the consultants' practices. BT workbench has been instantiated with data from finance management domain and applied to address a client situation as a case study. An alpha testing of the tool was conducted with about dozen practitioners. 90% of the consultants who tested the BT workbench tool felt that the tool would help them do a better job during a client engagement. The tool is currently being piloted with customer engagements in a large IT consulting organization."
2868,"With the number of services presented on the open Internet growing at an explosive speed, it subsequently brings great challenges to the accurate, efficient and automatic retrieval of target services for consumers. At present, many service matchmaking algorithms are proposed. However, most of them deem that a service is equal to a single functional unit or several separate units. In fact, a service may contain several functional units which are not separated from each other. Some implied relations may exist among the functional units so that they can be composed to provide new functions. This paper proposes a new service matchmaking algorithm that takes the composition of functional units within a service into consideration. This algorithm has been applied in a service composition framework called DartFlow. Our experimental data show that this novel service matchmaking outperforms others in terms of the recall rate and precision"
2869,"Creating Web processes using Web service technology gives us the opportunity for selecting new services, which best suit our need at the moment. Doing this automatically requires us to quantify our criteria for selection. In addition, there are challenging issues of correctness and optimality. We present a constraint driven Web service composition tool in METEOR-S, which allows the process designers to bind Web services to an abstract process, based on business and process constraints and generate an executable process. Our approach is to reduce much of the service composition problem to a constraint satisfaction problem. It uses a multiphase approach for constraint analysis. This work was done as part of the METEORS framework, which aims to support the complete lifecycle of semantic Web processes."
2870,"In the original service-oriented view of software provision, loosely-coupled services are brought together at the time of need and unbound immediately following execution, allowing service procurers to focus on selecting services that best correspond to their evolving requirements. This just-in-time approach requires the assessment of quality properties of both the software and the service provision activity in order to judge candidate services. In this paper, we propose and evaluate a multi-perspective quality evaluation model tuned to the needs of this ""just-in-time"" service provision vision. The proposed model uses a hierarchal structure of the quality features that characterize both the software and its provision arrangements from the perspectives of different stakeholders in the service provisioning and consumption process. The development and evaluation reported here took place in two phases: a ""role playing"" user study involving 15 participants to elicit the suitability, applicability and measurability of quality characteristics; and a contextual interview involving 24 users (12 software professionals and 12 general users) to uncover their mental models towards quality and evaluate the resultant characteristics identified from the first study. Our findings were twofold. Firstly, we show that a broader range of considerations encompassing both service quality and quality of service must be accounted for when dealing with software services (e.g. service functionality and service responsiveness). Secondly, we identify and explore the users' mental model of quality within the service-oriented paradigm."
2871,"With the success of the network convergence, we have no more difficulty to transport most of contents and deliver any service. However, nowadays' users desire to have his own services with a richer media as the content. As a result, it challenges to reconsider a new service paradigm and its delivery. Driven by the trend of different mobility and service ubiquity, the service convergence comes into sight as a promising solution facing the NGS context. Moreover, reengineering is needed for the delivery of the converged services. Existing solutions as IN, SOA and WS have not been able to completely cover all the converged service delivery needs. This paper proposes to redesign the service by modelling its structure and converged interfaces. We proposed a loose coupling between service elements (service composition) as well as a more flexible deployment of services (service aggregation) in order to meet converged service delivery."
2872,"Web service discovery is a vital problem in service computing with the increasing number of services. Existing service discovery approaches merely focus on WSDL-based keyword search, semantic matching based on domain knowledge or ontologies, or QoS-based recommendations. The keyword search omits the underlying correlations and semantic knowledge or QoS information is not always available. In this paper, we propose a probabilistic service discovery approach to help web service users to retrieve related services and to improve the search performance. Specifically, we apply a probabilistic model to characterize the latten topics between services and queries, and then propose a matching method based on the topic relevance. Experiments on services from a real service repository confirm the feasibility and efficiency of this proposed method."
2873,"Web application composition can greatly benefit from the utilization of existing frameworks and reusable components, in order to reduce development effort. Frameworks implementing the model-view-controller architectural pattern standardize the development process to a great extent, while business layer components may consist of consumers of existing Web services. On this line of thought a Web application can be seen as a composition of Web services around a user interface flow. In this paper, an approach for the application of model-driven techniques for the automation of the development of such a Web application is presented. Specifically, we present a methodology for the modeling of the application using UML state transition and class diagrams and the generation of the appropriate source code and configuration files. The appropriate UML profiles to assist the service design are defined and the final transformation is performed exploiting model parsing tools."
2874,"The Participatory Design Studio (PDS) provides users across multiple sites the ability to effectively participate in a common design session. Users are provided a variety of shared resources, including the underlying high-speed network, cameras, displays, sound equipment, large data files, and software applications for communicating and for visualizing the artifact being designed. This paper explores the design and implementation of PDS, and describes how it leverages the benefits of a service-oriented architecture (SoA) to provide a highly adaptable, modular, and loosely coupled solution. PDS integrates systems that are hosted on a wide variety of platforms with different management and network domains. This integration goes beyond piping application data from one system to another. Web services provide a cost-effective solution for integrating these systems into a comprehensive SoA"
2875,"This paper discusses a layered workflow software architecture for distributed coordination of workflows over Web services and proposes fundamental enhancements to the web services infrastructure that facilitate the layered workflow software architecture. We verify our layered approach using a detailed simulation and a proof-of-concept prototype implementation. Our proposed architecture for Web service coordination management middleware (WSCMM) is a simple but powerful enhancement to the web service infrastructure enabling the services to locally manage their dependencies and to handle messages resulting from multiple workflows. We have carried out a detailed simulation to identify key components of our middleware architecture. We also compare and contrast our architecture with the current web service technologies. Our experiments demonstrate that we can develop both centralized and distributed workflows over the architecturally enhanced web services with relative simplicity. In addition, our lightweight coordination components with footprint no larger than 150 KB allow these workflows to be executed even on a handheld."
2876,"As a technology that can improve the efficiency of the business process, workflow is drawing more and more attentions of researchers and software product vendors. But there are still many problems waiting to be solved for workflow. One of these problems is the poor adaptability of workflow. This paper intends to enhance the adaptability from the view of supporting dynamic process change. In the paper, a workflow model that is fit for dynamic change is defined and the verification rules are firstly set up for this model. Specifically, our method is based on the concept of executable path of workflow. Based on the executable path, the concepts and algorithms of valid process and complete subprocess are introduced to solve the problems brought by process model dynamic change."
2877,"Managing operational and semantic inter dependencies among software services is a relatively unexplored topic, despite its relevance to automating service deployments and to increased availability. In this paper we describe a framework for structured and programmatic dependency management among services written in the Java/spl trade/ programming language. The framework's interface allows for defining an acyclic graph of dependencies. The graph's structure reflects the startup sequence of managed services. Upon a service failure or intentional termination, the dependencies are consulted to determine which services may be affected. If the dependencies so dictate, a service that has gone off-line will automatically be restarted. The requisite changes, e.g., restarting, will be propagated along the graph's edges to ensure that the required dependencies are satisfied for each service. We demonstrate the usefulness of this framework through real-life case studies."
2878,"Realization of SOA is a challenge and requires well designed common mechanisms to define, create, manage, services through standards compliant pre-built, pre-configured set of functionalities. In this paper we present a solution, experiences in developing an automated resource management system (ARMS), for workforce management, as part of the field services optimization programme of a large telecom services provider. We present a model for SOA realization and application of this model through Radien - a J2EE based SOA framework"
2879,"Data and process management are among the biggest problems that the life sciences face today, particularly in the areas of genomics, proteomics, and metabolomics. This paper provides a design and implementation of a flexible, expandable and easy to use service oriented laboratory information management system (LIMS). A prerequisite to systems biology is the integration of heterogeneous raw and derived experimental data sets, which are stored in numerous life-science databases. However, a wide range of obstacles that relate to access, handling and integration impede the efficient exchange and use of the contents of these databases. The exponential growth of experimental data owing to advances and high- and ultra high-throughput technologies demand more sophisticated data and process management solutions. This service oriented data management tool enables researchers to have straightforward access to the data, and exchange information with other scientists in order to effectively answer today's complex research questions."
2880,"Despite the success of orchestration and choreography for Web services composition, there are still challenges to be overcome such as the difficulty for management of complex control-flow executions, the communication cost associated with service interactions, knowledge of the business process by services and even their compatibility in compositions. As an alternative to traditional approaches, in this paper we focus on an important face of Web services compositions that is the management of control-flow executions and propose the use of WED-flow approach, in which the execution of business processes is driven by changes in data states. In our approach, the control-flow is not a requirement but a consequence of Web services execution, providing greater flexibility for the development and maintenance of applications. The first contribution of this work is the evaluation of possible scenarios for orchestration and choreography. The second contribution is the implementation and validation of WED-flow approach for Web services execution for these possible scenarios."
2881,"In this tutorial, we present the results of abstracting a reference architecture for SOA based on multiple projects during the past 6 years. As an example SOA Solution Reference Architecture, the SOA Solution Stack (S3) defines the layers, architectural building blocks, design decisions, patterns, options and architectural decisions and the separation of concerns needed to model, architect, assemble, deploy and manage an end-to-end solution in the context of a service-oriented approach. The SOA Solution Stack (a.k.a. Service-oriented Solution Stack) provides a blue print for an enterprise or application architecture scope. The SOA Solution Stack is based on establishing the building blocks of SOA: services, components and flows that collectively support business processes and goals. The meta-data underlying each layer and relationship between layers can further facilitate SOA in bridging the gap between business and IT from solution modeling to solution realization."
2882,"Integrated systems are composed of components that exchange information (i.e., interoperate). These components include graphical user interface (GUI) applications (GAPs) and web services. It is difficult to make GAPs interoperate, especially if they are closed and monolithic. Unlike GAPs, web services are applications that are designed to interoperate over the Internet. We propose a novel generic approach for creating integrated systems by composing GAPs with each other and web services efficiently and non-invasively. This approach combines a nonstandard use of accessibility technologies for accessing and controlling GAPs in a uniform way with a visualization mechanism that enables nonprogrammers to compose integrated systems by performing point-and-click, drag-and-drop operations on GAPs and web services. We built a tool based on our approach, and using this tool we created an integrated application that controls two closed and monolithic commercial GAPs and third-party web services. Our evaluation suggests that our approach is effective, and it can be used to create nontrivial integrated systems by composing GAPs with each other and web services."
2883,"Since selecting a web service based on Quality of Services (QoS) is essentially a Multi-Criteria Decision Making (MCDM) problem, various MCDM models would be suitable for implementing the selection systems. A few of the MCDM approaches have been explored in previous research works. In this paper, we propose to use an enhanced PROMETHEE model for QoS-based web service selection. Many selection algorithms assume the independency between the QoS criteria, which is not very accurate. Thus, our first enhancement is to take into account the QoS interdependency by using the Analytical Network Process (ANP) to calculate the weight/priority associated with each criterion. User's QoS requirement is not considered in the original PROMETHEE model. As a consequence, during the process of finding an optimal service, when tradeoff decisions are involved, we may end up with a service which optimizes the overall QoS criteria, however, does not satisfy the user request. To overcome this insufficiency, our second enhancement is to check the outranking flows of each service with respect to the request in the ranking step, so that we know how well a service satisfies the user requirement. A case study is presented to explain the detailed selection process."
2884,"Composing Web services is an active field of research and development. Recently, many efforts are devoted to process integration on world changing services, e.g. business process composition. We present a solution to compose information providing services to achieve more powerful capabilities and free the user from trivial work. Our work serves as an important complement to the existing research on process integration."
2885,"Evolution of process-aware information systems necessitates several modifications to business process models with time. To be able to trace the evolving process models, there is a crucial need of systematic and modular enhancements of processes driven by functional and non-functional requirements. Hence, it is beneficial to record the history of modifications through versions of process models for unforeseen situations. In this paper, we present patterns for process edification that ensures systematic modifications to process models establishing traceability. We introduce process edification through the notion of edification cuts based on patterns applied on process models that results in an enhancement of the model into one of its improved version. The changes over process models occurring in a trace are quantified using various metrics. The approach is applied on different evolving industrial process models and the results are discussed."
2886,"We present an overall framework of services for indoor navigation, which includes Indoor Mapping, Indoor Positioning, Path Planning, and En-route Assistance. Within such framework we focus on an augmented reality (AR) solution for en-route assistance. AR assists the user walking in a multi-floor building by displaying a directional arrow under a camera view, thus freeing the user from knowing his/her position. Our AR solution relies on geomagnetic positioning and north-oriented space coordinates transformation. Therefore, it can work without infrastructure and without relying on GPS. The AR visual interface and the integration with magnetic positioning is the main novelty of our solution, which has been validated by experiments and shows a good performance."
2887,"Many questions that biologists want to answer using the information available from completely sequenced genomes are complex. A graphical environment allows users to visually explore and operate on a sequence. Sequence and annotation data exists in bewildering varieties of types and levels of detail. The graphical environment therefore needs to adapt to this variation to provide the user the best possible visual representation of genomic data in a given context. As more and more online tools and services become available for biologists, mediating software should also be able to integrate and link to them. The Bluejay browser is a Java-based visual environment for exploring biological sequences. Uniquely, Bluejay fully integrates existing gene expression software into a genomic context. Bluejay also differentiates itself from most form-based HTML sequence browsers because it: (i) is highly scalable so that it can visualize a wide range of genomic objects ranging from a large whole genome down to individual nucleotides by using data-transformational Web services; and (ii) dynamically discovers and provides links to disparate resources such as gene annotation data (via XLinks) and semantically- described biological Web Services (via the BioMOBY protocol)."
2888,"One of the interesting aspects of the Web 2.0 'evolution' is the wide-availability of various Web applications as APIs or Web services. These APIs expose informational services on the Web and take many forms of remote invocation of functions using standard Web protocols and XML for data representations, e.g., REST, SOAP/WSDL, XML-RPC, and other approaches. The services (or APIs) are also usually accompanied by user facing Web applications for human-consumption. Canonical examples are Google Maps, Yahoo! Flykr and del.icio.us, EVDB's Eventful's application and API, Amazon.com's S3, ECS, Alexa, and many others. The Ruby programming language and its Rails framework are ideal for programming Web applications and services in the Web 2.0. Ruby's modern and dynamic features make it an excellent language for rapid prototyping and integration of various Web services. Rails' superb support for rapid Web application development, database access, and AJAX, make it well suited for creating front-ends and back-ends to the next generation of Web applications and services. In this tutorial we will take a hands-on deep-dive into the Ruby and Rails platform and learn how they can be used to: (1) create Web applications backed by a relational database, (2) consume Web services, (3) create and deploy APIs or Web services, and (4) mashup of existing Web services and applications. No a priori knowledge of Ruby or Rails is required - although some programming in a modern OO language and Web application development are definite plus."
2889,"Copyright and Reprint Permissions: Abstracting is permitted with credit to the source. Libraries may photocopy beyond the limits of US copyright law, for private use of patrons, those articles in this volume that carry a code at the bottom of the first page, provided that the per-copy fee indicated in the code is paid through the Copyright Clearance Center. The papers in this book comprise the proceedings of the meeting mentioned on the cover and title page. They reflect the authors' opinions and, in the interests of timely dissemination, are published as presented and without change. Their inclusion in this publication does not necessarily constitute endorsement by the editors or the Institute of Electrical and Electronics Engineers, Inc."
2890,"Web service discovery is a key problem as the number of services is expected to increase dramatically. Service discovery at the present time is based primarily on keywords, or interfaces of Web services through the use of ontology. We argue that ""behavior signatures"" as operational level description should play an important role in the service discovery process. In this paper, we propose a new behavior model for Web services using automata and logic formalisms. Roughly, the model associates messages with activities and adopts the IOPR model in OWL-S to describe activities. A new query language is developed to express temporal and semantic properties on service behaviors. Query evaluation algorithms are developed; in particular, an optimization approach using RE-tree and heuristics is shown to improve the performance. Specifically, experimental results show that the use of RE-tree reduces query evaluation time by an order of magnitude and with heuristics it enhances the performance by two orders of magnitude. This is clearly an encouraging starting point."
2891,"Nowadays, there is a big increase in the usage of data analytic applications and services because of the growth in the data produced from many different sources. The QoS properties such as response time, reliability and latency of these services are important factors to decide which services to select. As we know, the energy consumption is becoming a big issue as a result of IT expansion. Therefore, establishing a QoS-based web service selection approach that considers energy consumption as one of the essential QoS properties represents a significant step towards selecting the greener web service. This paper presents an experimental study of energy consumption and latency behavior of data mining algorithms running as web services. Our study shows that, there is a strong relation between the dataset properties such as dataset size, number of attributes, data type, and QoS attributes energy consumption and latency. Based on the findings from our study, a prediction system is built which can be used to predict the energy consumption and latency values for data mining web services on a given dataset, and then these services can be ranked according to their predicted energy and latency values. Experimental results show the effectiveness of our prediction and service selection system."
2892,"Although workflow languages are widely used for composing discrete services, these are not suitable for stream based interactions. In this paper we address the problem of how to extend a conventional Web service composition language with the ability to deal with data streaming services. The paper discusses several modeling alternatives and presents a marker based semantics for safely dealing with pipelined processing in service compositions. The paper also presents application examples that illustrate the advantages of the proposed approach"
2893,"Nowadays, the rapidly changing technology development and the demand to provide high-end solutions to customers needs increase the opportunity of networking various small enterprises, offering service oriented businesses. The existing value network should be, viewed as a fluid infrastructure, clustering together various enterprises (sub-networks). Based on the business scenario of individual enterprises' in a specified time interval, few subnets maybe underutilized and few subnets may be over-utilized, thereby necessitates periodic changes in the distribution of customers. Here we propose a number of redesign operations on the customers' clusters (enterprises), which are sub-networks in the value network, with the conscious to minimize the extra-traffic in the backbone network. Thus we have proposed an optimization re-design problem to reduce the extra-traffic at the value network and maximize the intra-traffic within the clusters (subnets) by considering customers' movement and clusters consolidation. We have utilized Simulated Annealing (SA) algorithm to search for the best solution to the re-clustering problem. Our simulation results show a better trade-off in minimizing the extra-traffic through clusters consolidation and improve efficiency in operations for the service providers."
2894,"Automated service composition as the process of creating new software in an automated fashion has been studied in many different ways over the last decade. However, the impact of automated service composition has been rather small as its utility in real-world applications has not been demonstrated so far. This paper describes the use case of automated machine learning, a real-world scenario in which automated service composition plays an important role. It turns out that most existing service composition approaches are not able to reasonably solve this problem, because it requires to evaluate candidates by executing them during search. We briefly sketch a new service composition algorithm, MLS-PLAN, and illustrate how it can be applied to the problem of automated machine learning."
2895,"Service composition follows a three-party paradigm, i.e., a broker mediates between service providers and service consumers to properly select and compose a set of distributed services together so that requirements raised by consumers are satisfied by the composite service on demand. As the de facto provider of composite services, the broker charges the consumers, on the other hand, it awards cost to the providers whose services are involved in the composite services. Besides traditional quality-oriented optimization from the consumers' point of view, the profit that a broker could earn from the composition is another objective to be optimized. But just as the quality optimization, service selection for profit optimization suffers from dramatic efficiency decline along with the growth in the number of candidate services. On the premise that the expected quality are guaranteed, this paper presents a ""divide and select"" approach for high-efficiency profit optimization, with price as heuristics. This approach can be applied to both static and dynamic pricing scenarios of service composition. Experiments demonstrate the feasibility."
2896,"Correctly described Web service facilitates automatically discovery of Web service in semantic Web. In this paper, we present a content model for a better ontological metadata representation to facilitate Web service description. The proposed content model has two parts: content profile and process model. The content profile consists of three metadata, a core metadata, a structural metadata, and a semantic metadata, by which we can use to describe a Web service's basic properties, content structure and semantic meaning, respectively. The process model provide control construct to design complex service. We also provide a mechanism to transform a content model to a DAML-S Web service."
2897,"The progress of the Internet of Things (IoT) has enhanced real-time accessibility on smart things, and the advent of online social network (e.g. Facebook, Twitter, and Google+) have attracted people in sharing their knowledge and experiences with their social organization. Recently, several researches have shown that online social network could act as a user interface of smart things and smart things could be shared by using online social network, and especially we expect that the integration of both of them provide us with novel services which we collectively call IoT social network. However, it is possible that developers or researchers will address complexity of jointly utilizing and understanding both. In this paper, we propose a novel social networking platform for the Internet of Things, called Lilliput. Lilliput is an ontology-based platform providing information of online social networks (e.g. people's profiles, people's social relationships) and smart things (e.g. context, location) as a social graph. To architect consistent unified ontology-based platform, we conduct a research on what the social relationship among people, objects, and places will be, and propose it as IoT social relationship. Also, we show how to exploit this ontologically-connected people, objects, and places by mainly focusing on Human-oriented Access Control for IoT social network (HAC). With this platform, application developers can build IoT social network application without having to understand details in both online social networks and the Internet of Things. We show the feasibility of our architecture by implementing a prototype platform, building a testbed, and creating one of IoT social network applications, called Sorcerer's Book."
2898,"Provenance, which is one kind of metadata that captures the derivation history of a data product, including its original data sources, intermediate products, and the steps that were applied to produce it, has become increasingly important in services computing and scientific workflows to validate, interpret, and analyze the result of scientific computing. Most existing systems store provenance data captured into their own provenance storages of proprietary provenance models and conduct query processing over the physical provenance storages using query languages, such as SQL, SPARQL, and Query, which are closely coupled to the underlying provenance storage strategies. In this paper, we present OPQL, an OPM-level provenance query language, that is directly defined over the Open Provenance Model (OPM). An OPQL query takes an OPM graph as input and produces an OPM graph as output. Therefore, OPQL queries are not tightly coupled to the underlying provenance storage strategies. Our main contributions are: (i) we design OPQL, including graph patterns and an OPM-based graph algebra for OPQL, that efficiently supports provenance lineage queries, (ii) we implement OPQ Lin our OPMPROV system, where the result of OPQL queries is displayed as an OPM graph via the OPMPROV browser. An experimental study is conducted to evaluate the performance and feasibility of OPQL for provenance querying. To our best knowledge, OPQL is the first OPM-level query language for scientific workflow provenance."
2899,"The convergence of smart field devices and business services stands to profoundly change the way we interact with our environment. This is especially true in the home context. In this paper, we present an open architecture and a dynamic service-oriented gateway running home services. The gateway is based on the OSGi standard and provides mechanisms to integrate different service technologies"
2900,Current business process development is a process that needs to apply software development principles and at the same time incorporate the special requirements of service oriented architecture (SOA). In this paper we discuss the steps and principles involved in developing service oriented finance business process in the domain capital market
2901,"The expanding gap between microprocessor and disk performance has initiated new techniques of providing memory as a service in high-end computing (HEC). Although the processor and disk densities have improved over the last decades, the improvement of disk performance is inferior to that of processors, which is causing a bottleneck for HEC. With the rapid growth of network technology for cluster computers, the idea of accessing memory of an idle peer node has proven to be faster than accessing a disk. With this motivation, many researchers developed systems that offer idle memory as a service. In this paper, we present a brief survey of various systems that offer memory service in improving the disk access performance and discuss the scope of applying service oriented architecture (SOA) for HEC"
2902,"As spatiotemporal sensor data collections are becoming more available, the ability to generate insights from them is turning increasingly complex for scientists in this domain. Organizing such voluminous data and keeping track of analysis results get tedious from day to day. In this paper, we propose a system comprising a distributed workflow execution engine, a storage framework with flexible spatiotemporal data dispersion and indexing scheme for fast data search and retrieval, and a scheduling scheme that aims at minimizing data movement across machines while simultaneously ensuring fair and efficient allocation of resources among multiple users. Our empirical evaluations demonstrate the feasibility of our approach and its ability to scale."
2903,"Remote desktop protocol (RDP) allows a client to communicate with a Windows server. With RDP, you can run applications on a server from a remote client. RDP uses authentication and encryption to prevent traffic from leak. But the methods used have vulnerabilities and may encounter attacks. A security-enhanced approach is proposed and implemented."
2904,"In Application Management Services (AMS), high resource utilization, effective resource planning and optimal assignment of service requests to resources are critical to success. Meeting these objectives requires a systematic and repeatable approach for determining the best way of measuring resource utilization, assessing workload and assigning service requests. In this paper, we present a two-step approach to help achieve the above objectives. We first measure the actual amount of effort that each resource spends on handling each service request (SR) based on a metadata model and a set of SR handling priority rules. Then, we proceed to measure resource utilization and assess SR assignment process based on the effort data calculated in step one."
2905,"Resource Virtualization is one of major capabilities in cloud. Computing and Storage virtualization is widely used in cloud. Networking virtualization becomes a trend of cloud computing. In past, user can only request a networking resource offline, virtual network can be allocation as needed. Allocating the network on demand will become a challenge for network. In this work in progress study, we propose an Enhanced KMP (Knuth-Morris-Pratt) algorithm for Virtual Network Allocation in cloud (EKMPC), which use the concept of KMP."
2906,"As a promising, low-cost, and agile way to develop software, in recent years automatic service composition has been a popular research topic receiving a lot of attentions. For this topic, upon our long-term study and paper reviewed, we present technical survey and observation in this paper, including indispensable background and preliminary knowledge. The survey assumes under traditional composition context. Moreover, following the survey and observation, we suggest two approach patterns and point out possible future challenge as well as direction, especially to the influence of the mature of mobile devices and environment."
2907,"The popularity of cloud service spurs the increasing demands of cloud resources to the cloud service providers. Along with the new business opportunities, the pay-as-you-go model drastically changes the usage pattern and brings technology challenges to effective capacity planning. In this paper, we propose a new method for cloud capacity planning with the goal of fully utilizing the physical resources, as we believe this is one of the emerging problems for cloud providers. To solve this problem, we present an integrated system with intelligent cloud capacity prediction. Considering the unique characteristics of the cloud service that virtual machines are provisioned and de-provisioned frequently to meet the business needs, we propose an asymmetric and heterogeneous measure for modeling the over-estimation, and under-estimation of the capacity. To accurately forecast the capacity, we first divide the change of cloud capacity demand into provisioning and de-provisioning components, and then estimate the individual components respectively. The future provisioning demand is predicted by an ensemble time-series prediction method, while the future de-provisioning is inferred based on the life span distribution and the number of active virtual machines. Our proposed solution is simple and computational efficient, which make it practical for development and deployment. Our solution also has the advantages for generating interpretable predictions. The experimental results on the IBM Smart Cloud Enterprise trace data demonstrate the effectiveness, accuracy and efficiency of our solution."
2908,"Abstract-An intelligent scheduling algorithm based on multi-agents and genetic algorithm is proposed to overcome flexible job-shop automation and optimization problems in manufacturing industry. The algorithm is made of a management-agent, a scheduling-agent and machine-agents.Static scheduling is realized by a customized genetic algorithm,while dynamic scheduling is realized by these coordinative agents. Several scenarios are used to illustrate that the proposed system is practical, efficient and advanced."
2909,"Business process families provide an over-arching representation of the possible business processes of a target domain. They are defined by capturing the similarities and differences among the possible business processes of the target domain. To realize a business process family into a concrete business process model, the variability points of the business process family need to be bounded. The decision on how to bind these variation points boils down to the stakeholders' requirements and needs. Given specific requirements from the stakeholders, the business process family can be configured. This paper formally introduces and empirically evaluates a framework called ConfBPFM that utilizes standard techniques for identifying stakeholders' quality requirements and employs a metaheuristic search algorithm (i.e., Genetic Algorithms) to optimally configure a business process family."
2910,"The rapid development of mobile service ecosystem has made the APPs which access to the Internet an essential part of people's daily life. The APP is becoming increasingly large, and MicroService architecture has become the choice of many APP developers. However, the users' process of obtaining service is becoming tedious while using the APP that integrated various MicroServices, and there are no interactions among MicroServices. Users often install many APPs on their mobile devices. Facing some complex user requirements which go beyond the boundary of a single service, how to find the appropriate services and compose them becomes a critical issue. To solve the problem, in this paper, we propose a client-based MicroServices automatic collaboration(MSAC) framework. First, we build a fine-grained APP automatic collaboration meta-model, including a MicroService model and a collaboration model, to formally describe the issue of MSAC. Based on the meta-model, we develop a Micro Servitization tool to decompose the APP into client-based MicroServices that support the automatic collaboration and encapsulate them as normalized interfaces. We also develop an automatic collaboration engine to implement the automatic MicroService launching and the automatic injection of service parameters. Two real scenario cases demonstrate that our approach can efficiently simplify the user operations, reduce the network traffic and improve the user experience."
2911,"B2B and long running B2C process are complex business processes that contain set of services, state, transaction management, and involve notification of events occur during the execution of process. Business processes are driven by events and evaluating the business processes at regional or national scale reveals diversity in products, terminology and processes involved in carrying out a complete business activity. Because of dynamic nature, heterogeneity and lack of knowledge among business partner there is a need of mechanism to compose services dynamically according to events, and to provide interoperable integration for automation of business process. We proposed convergence of semantic Web, Web services and grid computing to achieve context and location based interoperable integration, event-driven dynamic composition, scalable architecture and middleware support for state, transaction, notification and life-cycle management of business process"
2912,"The efficient execution of data-intensive workflows relies on strategies to enable parallel data processing, such as partitioning and replicating data across distributed resources. The maximum degree of parallelism a workflow can reach during its execution is usually defined at design time. However, designing workflow models capable to provide an efficient use of distributed computing platforms is not a simple task and requires specialized expertise. Furthermore, since Workflow Management Systems see workflow activities as black-boxes, they are not able to automatically explore data parallelism in the workflow execution. To address this problem, in this work we propose a novel method to automatically improve data parallelism in workflows based on annotations that characterize how activities access and consume data. For an annotated workflow model, the method defines a model transformation and a database setup (including data sharding, replication, and indexing) to support data parallelism in a distributed environment. To evaluate this approach, we implemented and tested two workflows that process up to 20.5 million data objects from real-world datasets. We executed each model in 21 different scenarios in a cluster on a public cloud, using a centralized relational database and a distributed NoSQL database. The automatic parallelization created by the proposed method reduced the execution times of these workflows up to 66.6%, without increasing the monetary costs of their execution."
2913,"Rule engines form an essential component of most service execution frameworks in a Service Oriented Architecture (SOA) ecosystem. The efficiency of a service execution framework critically depends on the performance of the rule engine it uses to manage it's operations. Most common rule engines suffer from the fundamental performance issues of the Rete algorithm that they internally use for faster matching of rules against incoming facts. In this paper, we present the design of a scalable architecture of a service rule engine, where a rule clustering and hashing based mechanism is employed for lazy loading of relevant service rules and a prediction based technique for rule evaluation is used for faster actuation of the rules. We present experimental results to demonstrate the efficacy of the proposed rule engine framework over contemporary ones."
2914,"We propose a novel semantic service trust organization that uses an ontological approach to model service trust. In particular, our ontology-based organization supports various trust phases including trust bootstrapping, atomic service trust, trust composition and trust propagation. We describe the implementation of the proposed trust organization."
2915,"Web service discovery is becoming a challenging and time consuming task due to large number of Web services available on the Internet. Organizing the Web services into functionally similar clusters is one of a very efficient approach for reducing the search space. However, similarity calculation methods that are used in current approaches such as string-based, corpus-based, knowledge-based and hybrid methods have problems that include discovering semantic characteristics, loss of semantic information, encoding fine-grained information and shortage of high-quality ontologies. Because of these issues, the approaches couldn't identify the correct clusters for some services and placed them in wrong clusters. As a result of this, cluster performance is reduced. This paper proposes post-filtering approach to increase precision by rearranging services incorrectly clustered. Our approach uses context aware method that learns term similarity by machine learning under domain context. Experimental results show that our post-filtering approach works efficiently."
2916,"Given that the number of ready-to-use web services is steadily growing, services will provide solutions to a vast range of business problems in the future. And thanks to findings in end-user development research, popular service integration platforms such as IFTTT and Zapier emerged, making process automation as easy as never before. This progress will affect software engineering increasingly: as fewer code is programmed, the more activities like identifying and connecting suitable services using integration platforms become prominent. To explore what it is like to engineer software without writing code, we conducted a workshop with experienced software engineers. With the objective of implementing a simple business process by integrating existing services in a mashup, we collected all lessons learned during that workshop and report them in this paper. Based on that experience, we introduce three different levels of integration and outline a software process model containing specific activities for engineering of service mashups."
2917,"With the rapid development of Internet technology, a new architecture named Content-Centric Networking (CCN) has emerged recently. With powerful abilities of data caching and multicast, it has been increasingly popular especially for multimedia applications. However, service markup, in-network caching and routing are not inherently addressed in CCN. In this paper, we make an attempt at filling this gap by designing an ontology-based semantic service markup for CCN. Two components which are Service Identifier (SID) and Service Behavior Description (SBD) are presented in order to decouple the service entity with its original physical location, making it possible for service discovery and caching in CCN environments. Detailed designs are introduced, and a case study of an online multimedia conference system is presented to validate the effectiveness of our approach."
2918,"Matching and scheduling problem proved to be a critical problematic in different domains including Cloud computing. Therefore, to ensure the commercial success of the Cloud computing paradigm, it is necessary to develop methods that allow users to optimize the use of resources. Even though there are several algorithms for scheduling applications in heterogeneous environment such as grid computing, they cannot benefit from the recent advent of Cloud computing. Indeed, these algorithms assume that the number of resources available to users is bounded, this is against the illusion of infinite resources of Cloud computing. Also, only the execution time (makespan) is taken into account. However, Cloud computing business model is based on pay as you go. Accordingly, execution cost begot using a set of resources should be considered. To overcome the limitations of existing works, this paper propose new strategies for matching and scheduling business process instances in the Cloud context. The proposed strategies aim at scheduling business process instances while minimizing two conflicting criteria on the one hand, and ensuring fairness between the considered instances on the other hand. A serie of experiments demonstrate that they present good performances."
2919,"In a service-oriented online social network consisting of service providers and consumers, a service consumer can search trustworthy service providers via the social network. This requires the evaluation of the trustworthiness of a service provider along a certain social trust path from the service consumer to the service provider. However, there are usually many social trust paths between participants in social networks. Thus, a challenging problem is which social trust path is the optimal one that can yield the most trustworthy evaluation result In this paper, we first present a novel complex social network structure and a new concept, Quality of Trust (QoT). We then model the optimal social trust path selection with multiple end-to-end QoT constraints as a Multi-Constrained Optimal Path (MCOP) selection problem which is NP-Complete. For solving this challenging problem, we propose an efficient heuristic algorithm, H_OSTP. The results of our experiments conducted on a large real dataset of online social networks illustrate that our proposed algorithm significantly outperforms existing approaches."
2920,"Service-Oriented Computing (SOC) was introduced some years ago as a paradigm that allows companies to expose their core competencies as services. Yet, we can only benefit from the full potential of SOC if we explore the possibility of composing services. In practice, service composition enables the development of complex systems by combining existing services, which makes it possible to integrate business processes across different companies. Several languages have been defined to specify service compositions, such as WSBPEL and WS-CDL. However, these languages lack formal semantics, so that they do not allow the verification of the service composition behavior by means of properties such as the absence of deadlocks and livelocks. In this paper, we present a structured survey on initiatives in which formal description techniques were applied to specify and possibly verify service compositions. As a result of this study, we identified some open challenges and some topics for future work."
2921,"Quality of Service (QoS) is often essential when the service consumer looks for a single service from a large pool. However, QoS of a composite service is aggregated in a rough way because the concrete service providers in the composition are seen as independent ones. In reality, business relationships such as dependencies and conflicts often exist among service providers, which unavoidably affect some QoS dimensions when the corresponding providers are selected to realise a composite service. Therefore, effects of business relationships must be analysed in the service selection process. This research proposes a composite service selection approach with the full consideration of business relationships. A formal business rule description language is defined to describe various types of business relationships and their effects on QoS. This research adopts the genetic algorithm to discover the near optimal service composition plan. Business rules are incorporated in computing fitness values and performing crossover and mutation functions. The experimental results demonstrate that the proposed approach is able to handle various business rules properly in selection."
2922,"Service oriented architecture (SOA) is an approach for building distributed systems that deliver application functionality as a set of self-contained business-aligned services. It promises lower integration costs, increased reusability and improved enterprise agility and adaptability. One of the key obstacles for SOA is the presence of existing legacy applications that support critical business processes but are inflexible and hard to adapt for integration with other business applications. This paper presents a survey of key approaches to integrate/transform legacy applications into services to participate in an enterprise-wide SOA. It also presents a decision framework to guide architects in selecting the optimal combination of legacy modernization options"
2924,"Formal specification and verification support of time-related constraints constitute fundamental challenges for any Business Process Management (BPM) system. Reluctantly, the literature on the subject of formal specification and verification of advanced temporal constraints such as absolute temporal constraints associated with relative temporal constraints is scarce. In this paper, we propose a novel approach enabling the formal specification and verification of advanced temporal constraints of business processes. The particularity of our approach is that it caters for relative and absolute related temporal constraints while relying on the dependencies that can exist between theses constraints. In fact, it is important to deal with such dependencies to handle the violations that can arise as soon as possible at design step. To do so, we propose a formal approach which relies on the timed automata formalism. In this context, we propose a set of mapping rules and algorithms where the semantic of timed automata is preserved even if we deal with absolute and relative temporal constraints. Using the defined formal model, we investigate a model checking based verification process that aims at validating business processes against their absolute and relative temporal constraints."
2925,"The composition of services, as envisioned in the Internet of Things and Web of Things paradigms, is a most relevant component. To be effective, services composition should choose among the discovered services the most suitable ones and this a non-trivial task given the (typically) very large number of available services. In this work we propose a framework for assessing the reputation of a set of services as a measure for the suitability of such services in coping with a given task. We take as a starting point an approach proposed for team recommendation that aims at maximize the overall team reputation, based on the feedback that each team member receives from other members. It is important to point out that several important differences among the approaches exist - which we highlight in the paper - that do not allow for a mere adaptation from the team recommendation setting."
2926,"Frequently used queries in bioinformatics applications should be given special treatment. This is because: (1) there are certain queries submitted by scientists and researchers over and over again, and (2) biological databases are usually too big and complicated to be rapidly queried every time a user submits a query. We propose in this study a query processor called IRTG that expedites the processing of frequently used queries. IRTG can be considered as a type of middleware running at a central site. It classifies queries per their types and ranks the query types based on their frequencies. The results of queries whose types are classified as frequently used types will be cached at the middleware. Thereafter, each frequently used query of these types will be answered from the middleware rather than from remote sites, which improves the query's response time. We evaluate IRTG experimentally."
2927,"With the increase in the number of cars in big cities, it has become a nightmare for car owners to find available parking spaces. Some studies report that around 25-30% traffic on the roads are cars finding available spaces for parking. While many researches have been conducted to create a smart parking system, most of them only focus on some particular areas and environments without any common standards. As a result, this hurts users' experience as they have to use many different applications to access various APIs when traveling to different places. Thus, in this study, we wish to open the discussion to realize a global and common base for smart parking services by proposing a smart parking systems based on GS1 global standards. By utilizing common and global standards, our proposed architecture could be used globally and also easy to extend with different services. We have also implemented a prototype system which can support parking lots in Busan city and 9 Korean Airports to prove the feasibility of our architecture."
2928,"With the proliferation of service-based systems (SBSs), more and more functionally equivalent services with different Quality-of-Service (QoS) are emerging. System vendors can select from these services to create service compositions to fulfill users' multi-dimensional QoS requirements. In order to address the QoS-aware service selection problem, various approaches have been proposed in recent years. However, most of them assume that the users' QoS requirements are deterministic and different QoS dimensions of the requirements are independent of each other. In fact, correlations may exist between different QoS dimensions of users' QoS requirements, which make existing service selection approaches impractical. In this paper, we propose SCORE-QoS (Service Selection based on Correlated QoS Requirements), in which the correlated QoS requirements are represented with QoS correlation functions and integrated into the Constraint Optimization Problem that models the service selection process. Two types of correlated QoS requirements, with and without optimization goals, are considered in SCORE-QoS. The experimental results show that our approach can effectively and efficiently select services for composition based on correlated QoS requirements."
2929,"Finding parking at a desired spot and time is tough, especially in an urban area. Users may also want to pay more to get a parking spot during urgent need. Although the existing infrastructure-based approaches can solve this problem partially, they require a high initial investment and maintenance cost. As a consequence, deploying such approaches on a large scale in the real world is infeasible. A more economically feasible solution is using crowdsourcing-based approaches where a user near a free parking spot informs the interested users about the available parking spot in exchange for some forms of incentives. However, most of the crowdsourced approaches suffer due to the lack of proper models for price negotiation, information verification, and assurance. In this paper, we propose ParkBid, a crowdsourcing-based parking service for automobiles where the information of free parking is circulated among the interested users following a bidding process. ParkBid determines the incentives for providing any parking information based on several primitives, such as time, location, reputation, urgency, etc. Along with a detailed discussion about the challenges in ParkBid for both the users and information providers, we provide a set of policies as countermeasures. Also, we present an extensive simulation to evaluate the impact of ParkBid for users under different circumstances. Our experimental results show that users can save a significant amount of time and can have more success rate during searching for a free parking spot using ParkBid."
2930,"Service based applications leveraging different features are today the underpinnings of enterprise computing. However, current Web services composition systems make only use of functional requirements in the selection process of component Web services while transactional consistency is a crucial parameter of most business applications. In this work, we propose a formal approach for enforcing transactional requirements in Web service compositions using Event-B formalism. Our solution enables specifying composite Web services where partner Web services are selected at runtime according to the transactional requirements that are defined based on the Acceptable Termination States concept. The resulting composite Web service is compliant with the consistency requirements expressed by business application designers. Moreover, we incrementally combine proof based models with model checking to trace possible errors and then come back to make proofs of the proof obligations."
2931,"Summary form only given. We address the question, ""in the brave new world of Web services and service-oriented architectures (SOA), how does data fit in?"" We bring data modeling concepts to bear on the world of services, yielding an approach in which enterprise data access is handled by a collection of interrelated data services. We show how the approach can be realized on a foundation of XML standards, namely XML Schema, Web services, and XQuery. We show that this approach provides a uniform and declarative framework for integrating enterprise data assets that are drawn from disparate underlying sources, including both queryable and non-queryable data sources as well as data that is encapsulated by Web services. We also show that the approach yields data services that are easily and efficiently reusable."
2932,"This paper describes the realization of Tele-portal, an agent-based Web services platform. Tele-portal utilizes two important concepts in the field of distributed computing: the concept of services and agent. We present the whole architecture of the Tele-portal system. The main contribution of this work is that it provides a methodology and prototype implementation of a service-oriented electronic commerce environment. We suggest that this approach has the flexibility necessary to provide agent-oriented designs for open and complex application, and has value for future maintenance and extension of these systems."
2933,"Smart buildings are aimed at monitoring and controlling building facilities through a Building Management System (BMS). While current BMSs are based on processing logs of devices deployed in the building, this paper enables supervision and control of building by the use of semantic technologies. A common information base, as a core data model, is defined, which describes and defines formally the main physical and conceptual building elements (namely: assets, spaces, data points, incidents and key performance indicators), their characteristics and interrelationships, as well as the constraints that apply to them. For instantiation purposes, we relied on a logical framework based on the existential rules, which allows to describe any domain as a set of facts, a set of rules and a set of constraints. We have implemented a fragment of our logical model as a proof of concept, where two real-world scenarios are implemented as demonstrators of the FUSE-IT project. The former is about access control to a data center, and the latter is about temperature anomaly correlated with the heater functioning in a given zone of a building. The aim of these experiments is to illustrate the functional capabilities of our approach for smart control in building management."
2934,"Evolved from our engineering experience, this paper presents a mathematical framework to define and analyze an SOA (service-oriented architecture) model. SOA model, composed by design elements, is represented as directed graph based on graph theory. For each design element, two directed graphs are created to reflect the panoramic view and relationships of this design element with other design elements of same model, and used for impact and completion analysis of this design element. A numerical value called the relative importance indicator is computed to quantify the relationship between any two design elements. This indicator forms a matrix that is used as a base for more advanced analysis, such as model partition, model coupling, and variation-oriented design. Some future research directions such as model reduction are discussed at the end of this paper."
2935,"Workflows are often used to represent enterprise-type activities, and authorisation control is an important security consideration in enterprise-level applications. Role-Based Access Control (RBAC) is a popular authorisation control scheme under which users are assigned to certain roles, and the roles are associated with permissions. This paper presents a novel mechanism for modelling workflow execution in cluster-based resource pools under Role-Based Access Control (RBAC) schemes. Our modelling approach uses Coloured Timed Petri-Nets, and various authorisation constraints are modelled, including role constraints, temporal constraints, cardinality constraints, Binding of Duty and Separation of Duty constraints, etc. The interactions between workflow authorisation and workflow execution are also captured in the model. In this paper, the modelling mechanism is developed in such a fashion that the construction of the authorisation model for a workflow can be automated. This feature is very helpful in modelling a large collection of authorisation policies or complex workflows. A Petri-net simulation tool, the CPN-Tool, is utilised to implement the developed modelling mechanism and simulate the constructed model. Both system-level performance (e.g., utilisation of resource pools) and application-level performance (e.g., workflow response time) can be obtained from model simulations. This work can be used to plan system capacity and investigate the impact of authorization policies on system and application performance."
2936,"Security issues of cloud computing are always being concerned by customers. Research on a virtual machine's quantitative or qualitative value of risk will be a good start to know the security status of a cloud data center. Risk assessment is a solution for really understanding security procedures of the network and information system, analyzing where security threats come from and how much loss the risk can cause. By means of the combination of risk assessment with cloud computing, we can assess the risk value of virtual machines, and the security of data center can be ensured by administrator who has ability to quickly locate the risk points and easily control and reduce the risks. In this paper, we present VMRaS (a novel virtual machine risk assessment scheme in the cloud environment), a scheme that can assess the risk of a virtual machine. First, we introduce the process, criteria and algorithms of risk assessment. And then we present the design and implementation of VMRaS. We evaluate a prototype of VMRaS which is deployed on an Open Stack-based cloud computing resource management platform. The result shows that VMRaS works well in the Open Stack-based cloud environment."
2937,"A service-based SaaS application can achieve economies of scale by sharing partner services between tenants at runtime, following the Single-Instance Multi-Tenancy (SIMT) model. However, supporting runtime sharing with tenant-specific functional and performance variations in an SIMT application is challenging. We propose an approach, Software-Defined Service Networking (SDSN), to addressing this challenge. SDSN realizes an SIMT application with a managed service network (SN) and a set of managed virtual service networks (VSNs) that share the SN. The SN connects a set of services according to their capability and interoperability. To achieve the requirements of a particular tenant, a VSN uses a subset of the services under a particular architecture and a control policy. We describe how the SN and its VSNs are designed and enacted to achieve the SIMT objectives. We show the feasibility of the SDSN, demonstrate the utilization benefits it achieves, and quantify the runtime overhead it incurs."
2938,"Neither provider nor user are willing to use Web services (WSs) technologies in sensitive domains without some measure of security. The users, on one hand, want to guard their personal information against unauthorized parties. While, the providers, on the other hand, wish to hide their trade secrets and internal know-how. In our previous works, we suggested the use of the Symbolic Observation Graph (SOG) as means to abstract WSs and verify their opacity. Opacity, being a formal security property, seeks to ensure the absence of secret leakage to unwanted parties. Thus, it is able to tell both parties (user and provider) if their chosen WS is secure (opaque) or not. This dichotomous (Yes/No) assertion, however, does not offer a measure of how opaque the service is. In this paper, we propose a novel opacity quantification approach that gives an opacity degree for the SOG abstraction of a Web service. This opacity degree will act as metric allowing users to choose more opaque (secure) services, while making it possible for providers to improve their WS designs for higher opacity degrees."
2939,"The US health care system is undergoing a substantial policy reform. One key dynamic of the health system is the rapid adoption of electronic health records (EHR). The analytic studies on the EHR will deliver insights and/or knowledge to improve the overall quality and efficiency of healthcare. On the other hand, Service-oriented thinking is one of the fastest growing paradigms in information technology. Service-oriented analytic workflows can bring together various analytic computing tools and compute resources offered as services to answer complex research questions. However, analytic studies on the population health data often require both the domain knowledge and the expertise in data manipulation. Selecting the appropriate analytic module and defining data and analytic workflows can be challenging even for an expert data scientist. Therefore, assistance from experts need to be offered at various stages of the study. These issues motivated us to develop a novel service-oriented analytic platform with guided workflows for population health studies. In this paper, we illustrate the different human roles that are needed to conduct health analytics, provide a modeling to elaborate the interactions between the human roles, and elaborate the service-oriented architecture of the system. A reference implementation is detailed to demonstrate the practical use of the approach and platform."
2940,"Runtime anomalies occurring to service-based systems (SBSs) must be located and fixed in a timely manner in order to guarantee successful delivery of outcomes in response to user requests. Monitoring all component services constantly is impractical due to excessive resource consumption. Inspecting all component services upon anomalies is time-consuming and thus also impractical. In this work, we propose a novel approach that employs spectrum-based fault localisation techniques to locate runtime anomalies in SBSs. Large-scale experiments are conducted and experimental results are presented to demonstrate the effectiveness and efficiency of the proposed approach."
2941,"The problem of server sprawl is common in data centers of most business organizations. It is most often the case that an application is run on dedicated servers. This leads to situations where organizations end up having numerous servers that remain under-utilized most of the times. The servers, in such scenarios, are allocated more resources (disk, cpu and memory) than are justified by their present workloads. Consolidating multiple underutilized servers into a fewer number of non-dedicated servers that can host multiple applications is an effective tool for businesses to enhance their returns on investment. The problem can be modeled as a variant of the bin packing problem where items to be packed are the servers being consolidated and bins are the target servers. The sizes of the servers/items being packed are resource utilizations which are obtained from the performance trace data. Here we describe a novel two stage heuristic algorithm for taking care of the ""bin-item"" incompatibility constraints that are inherent in any server consolidation problem. The model is able to solve extremely large instances of problem in a reasonable amount of time."
2942,"With the increased use of small screen devices in today's society, there is an increasing need to access useful information through these devices. Despite the popularity of smartphones, most websites are designed for a desktop computer monitor. Current approaches to make these websites small screen friendly, result in website images being handled in a naive fashion. For example, images are simply resized or removed with no regards to their content or purpose. This paper proposes a middleware service, involving both image analysis and image classification techniques, to determine the purpose of these images within a website. This knowledge can then be used to relocate, resample, resize and/or remove these images accordingly."
2943,"Geosciences Web portals are becoming increasingly important for supporting geoscientists in their research. The GEO-SEED portal is a repository of geosciences web services metadata, represented in Resource Description Framework (RDF), which supports management and discovery by machines and automated agents. This project uses SPARQL, the W3C standard for querying RDF, to support discovery of web services. SPARQL query performance becomes more critical as the amount of RDF metadata in the repository increases. Most existing RDF storage systems are based on relational database technology. The problem with most of these systems is that their query performance is limited by the use of fixed schema mapping strategies. In this paper, we present our system, called S2ST, which addresses this issue in the context of geosciences web services metadata management. Our experiments show that S2ST provides better SPARQL performance than existing relational RDF storage systems."
2944,"With meeting the demand of the higher recall ratio and precision ratio of service discovery, a service migration mechanism is proposed to achieve efficient service search. In this paper, service migration architecture is proposed for realizing semantic annotation and service migration. And an ontology alignment mechanism is proposed for maintaining the consistency of ontology concepts. In order to realize efficient service discovery, an algorithm based on semantic annotation is proposed. The experiments show that our proposed method solves the search problem of low efficiency based on keyword, and it effectively improves the recall ratio and precision ratio of web service discovery."
2945,"This paper describes a framework that we have developed to integrate proactive SLA negotiation with dynamic service discovery to provide cohesive runtime support for both these activities. The proactive negotiation of SLAs as part of service discovery is necessary for reducing the extent of interruptions during the operation of a service based system when the need for replacing services in it arises. The developed framework discovers alternative candidate constituent services for a service client application, and negotiates/agrees but does not activate SLAs with these services until the need for using a service becomes necessary. A prototype tool has been implemented to realize the framework. This prototype is discussed in the paper along with the results of the initial evaluation of the framework."
2946,"From online multi-player games to Web search, the Web is now populated by a multitude of online applications. The main question we try to answer in this paper is which of those applications can be characterized as traditional services and therefore benefit from the concepts, techniques, and methods developed in traditional services theory. Our answer is to define online service applications as applications where (1) the user does not control most means of production; (2) the user is a significant part of the input to the production process. We then discuss how some traditional service techniques can be applied to online service applications by considering aspects of the design and evaluation of the human-computer interface (HCI) of online services."
2947,"Web service composition is a mechanism for creating new services (with new functionalities) from the existing services. A business model defines the different parties involved in service provisioning and their relationships. Current Web service business models do not handle Web service composition explicitly, so we propose a novel business model to alleviate this deficiency. In this proposal, we introduce one new business role and two specialized business roles. We also propose four new relations for interaction between the business roles. We provide a concrete means for implementing two of the proposed interactions by extending the subscription API of UDDI version 3 with two new methods"
2948,"Log-based business operation analysis is getting more and more attention from enterprise decision makers. However, at the very first step of the analysis service, two primary obstacles are always encountered: how to process a wide variety of local event log formats and how to handle personally identifiable information weaved in an event log. Due to these obstacles, typical business analysts who do not have programming skills have lost business opportunities at the early stages. We propose a privacy-preserving data curation specification language, BELAS, for such analysts and present experimental results that show how most of a real-life event log could be processed in process analysis services."
2949,"Recently many organizations have accumulated data from such various sources as web and network sensors and constructed large-scale archives. Some would like to publish their archives to public to facilitate the activities of other organizations, but the scale of the archives causes problems. Therefore, we propose the concept of data-intensive services, which publish large-scale archives. We show the architecture for data-intensive services and focus on the following fundamental functional properties: 1) enhancing search, 2) preprocessing, 3) and asynchronous transfer. We also developed a reference implementation of a framework for data-intensive services and applied it to a web archive that contains about 2 billion documents and greatly improved the access performance to the web archive at small development cost."
2950,"We present an optimization approach for service compositions in large-scale service-oriented systems that are subject to Quality of Service (QoS) constraints. In particular, we leverage a composition model that allows a flexible specification of QoS constraints by using constraint hierarchies. We propose an extensible met heuristic framework for optimizing such compositions. It provides coherent implementation of common met heuristic functionalities, such as the objective function, improved mutation or neighbor generation. We implement three met heuristic algorithms that leverage these improved operations. The experiments show the efficiency of these implementations and the improved convergence behavior compared to purely randomized met heuristic operators."
2951,"The inherent uncertainty of Web service is the most important characteristic due to its deployment and invocation within a real and highly dynamic Internet environment. Web service composition with uncertainty (U-WSC) has become an important research issue in service computing. Although some research has been done on U-WSC via non-deterministic planning in Artificial Intelligence, they cannot handle the situation that uncertain Web services with the same functionality exist in a service repository and could not get all of possible solution plans that constitute an uncertain composition solution for a given request. To solve above research challenges, this paper models a U-WSC problem into a U-WSC planning problem. Accordingly, two novel uncertain planning algorithms using heuristic search called UCLAO* and BHUC, are presented to solve the U-WSC planning problem with state space reduction, which leads to high efficiency of finding a service composition solution. We have conducted empirical experiments based on a running example in e-commerce application as well as our large-scale simulated datasets. The experimental results demonstrate that our proposed algorithms outperform the state-of-the-art non-deterministic planning algorithms in terms of effectiveness, efficiency and scalability."
2952,"Domain-specific languages promise an unprecedented integration of business and IT aspects in software development. This translates into a stronger focus on user requirements, higher adaptability, and shorter time-to-market. DSLs provide the opportunity to bring business actors and IT experts closer together by raising the mutual understanding of the models underpinning software development. A closer cooperation in modeling improves the understanding of systems and allows for experimentation. Business actors can identify their business processes and resources in the models and can experiment with them. This paper presents work-in-progress addressing a model layer for the dynamic composition and adaptation of coarse-grained web services through configuration information. Our domain-specific configuration language (DSCL) enables IT experts and business actors to concentrate on model representations that reflect individually tailored compositions of generic portal services. Our approach fosters modeling on two different levels of abstraction. Business actors define high-level models focusing on the definition of processes across coarse-grained services. Low-level concepts complete technical aspects that are abstracted away in high-level concepts."
2953,"In a services computing environment, a large amount of sensitive data is hosted by service providers. As the owners of data might have different security requirements for their data, many systems allow each data item to have its own access control policy. For privacy and security reasons, some data owners and clients might want their access control policies and credentials to remain secret to the service providers which handle the access control of the data. That is, the service providers cannot comprehend the contents of the access control policies and the credentials. This paper proposed a role-based access control scheme. It uses cryptographic techniques to obscure data's access control policies and clients' credentials. The scheme is efficient as the policy enforcement process only uses the information available on the service provider that carries out the access control. A prototype of the scheme has been implemented and the execution time of the scheme were measured."
2954,"As the amount of data generated by today's pervasive environments increases exponentially, there is a stronger need to decipher the important information that is hidden among it. By using complex event processing, we can obtain the information that really matters to our organization and use it to improve our processes. However, even when this information is retrieved, business processes remain static and cannot be changed dynamically to adapt to the actual scenario, diminishing the advantages that can be achieved. In this paper we present CEVICHE, a framework that combines the strengths of complex event processing and dynamic business process adaptation, which allows to respond to the needs of today's rapidly changing environments. We use a simple car rental scenario to show how CEVICHE could be used to maintain the quality of service of a business process by adapting it according to the situation."
2955,"Progress in the field of Web services has resulted in deployment of a significant number of Web services. Furthermore, it is expected that the number of available Web services will constantly grow in the following years. Due to the high number of available Web services, it is a hard task for developers and business analysts to choose which Web services are most suitable for integration. However, despite the increased academic and commercial interest to Web services, there is currently no survey available analysing most relevant Web services. Moreover, to the best of our knowledge, there is no publicly available study analysing the structure and potential synergy between commercial and governmental Web services. In this paper we target these shortcomings by providing a case study of automated Web service composition for semantically annotated commercial and governmental Web services. We propose a method for identifying most applicable Web services and demonstrate it on a case study. We also analyse interaction and potential synergy between commercial and governmental Web services."
2956,"The purpose of this paper is to stimulate a discourse and search for appropriate theoretical foundations for Web services. The complexity of Web services technology demands such a foundation. A theoretical foundation can provide adequate guidance not only to accelerate research related to Web services, but can also promote their acceptance. Based on an extensive review of prior work in SCC and ICWS, we identify theories implicitly used for Web services research, and propose the language-action perspective (LAP) as an important and necessary complement to these. Our proposal follows the observation that there is a close match between the core concerns of Web services and the LAP approach. Our ongoing work is aimed at validating appropriateness of LAP as a theoretical framework for Web services through empirical research."
2957,"With the wide applications of Internet-based services, service reliability has received more and more attention in the research field of Services Computing. Reliability prediction approaches basically compute reliability according to historical records. However, in most current prediction methods, each user is supposed to return a Boolean feedback representing his/her subjective evaluation on service reliability. This binary feedback cannot express the user's evaluation of reliability precisely. Even though some researchers propose to compute reliability according to feedback from similar users or similar services, accuracy of reliability prediction cannot be guaranteed successfully. To address the problems above, this paper presents a fine-grained reliability prediction method based on grouping (RPMBG). In RPMBG, feedback is divided into several dimensions and each user can choose a service degree for each dimension to represent his/her evaluation on consumed service. A quantifying approach of users' feedback on each dimension will be given with the employment of entropy in information theory. At the same time, a grouping methodology including grouping of users and grouping of services is described with the employments of fuzzy c-means theory, PCC, and Levenshtein Distance. Based on grouping, filtered feedback from similar user group and similar service group will be engaged into reliability prediction. Experimental results demonstrate that our proposed RPMBG has significantly increased the accuracy of prediction result compared with other prediction methods."
2958,"The emerging paradigms of service oriented architectures and utility-based computing have the potential to greatly reduce the cost of data management. Data management service providers negotiate with their customers to agree on a service level agreement (SLA) that guarantees performance and reliability. However, these providers have the freedom to cut costs by taking advantage of economies of scale across multiple customers. In this paper, we examine the problem of choosing a QoS level for each table or index in a service provider's backend databases so as to minimize the dollar cost of provisioning storage while satisfying application-level SLAs. This problem is difficult because changes in the access cost of different portions of the database can cause the database to alter its access patterns. We develop an algorithm that optimizes the choice of query execution plans and storage layout simultaneously to meet an SLA at minimum cost. In our experiments we use a part of the TPC-H benchmark as the workload and several models of the incremental cost of placing a volume at a high quality of service. Our results show that significant cost savings are possible through selective use of high storage QoS levels."
2959,"Service-oriented architecture (SOA) ideally provides reliable access to a scalable, agile, and affordable computing environment. In the tactical military context its ultimate goals are greater situational awareness and enhanced operational capability through the efficient sharing of information. Component systems, such as networks and data sources, can provide many of the necessary services to achieve these goals. A structured approach, however, is needed to realize this within command and control systems. This paper describes a hierarchical approach to information integration, called integration layers, and the conceptual foundations for such a structure, using chemical and biological warfare information-system services as examples."
2960,"Managing information security and privacy assurance are fiduciary responsibilities of all government and commercial organizations, but standing up a comprehensive fully-assured environment from the onset may be technically or financially impossible. Many organizations inadequately address this challenge from a 'bottom-up' or piece-meal perspective, certifying and accrediting individual systems or focusing on perimeter systems and portals. A systematic enterprise-wide risk-management approach to information security and privacy is both practical and economically feasible, but must holistically integrate such requirements into both business process management and the technical infrastructure to be effective. The authors' development of the roadmap for information security across the enterprise (RISE) methodology establishes a systematic approach to security and privacy management by leveraging enterprise architecture approaches, and ensures implementation control by integrating the processes and responsibility with enterprise-level portfolio management. RISE defines an iterative threat assessment and response cycle and integrates it with capital planning and investment control (CPIC) for both operational and infrastructure initiatives. This paper describes how RISE ensures risk-informed continuous process improvement and capital planning by maintaining an architecturally founded knowledge base supporting strategic planning and investment review."
2961,"HTTP/2 is the next-generation Web protocol based on Google's SPDY protocol, and attempts to solve the shortcomings and inflexibilities of HTTP/1.x. As smartphones become the main access channel for Web services, we are curious if HTTP/2 can really help the performance of Web browsing. In this paper, we conduct a measurement study on the performance of HTTP/2 and HTTPS to reveal the mystery of HTTP/2. We clone the Alexa top 200 websites into our own server, and revisit them through HTTP/2-enabled proxy, and HTTPS-enabled proxy, respectively. We compare HTTP/2 and HTTPS as a transport protocol to transfer Web objects to identify the factors that may affect HTTP/2, including Round-Trip Time (RTT), bandwidth, loss rate, number of objects on a page, and objects sizes. We find that HTTP/2 hurts with high packet loss, but helps many small objects. The computation and dependencies of fetching Web objects reduce the performance improvement of HTTP/2, and sometimes can even hurt the performance of page loading. At last, we test the server push feature of HTTP/2 to leverage the performance."
2962,"The rush is on to service-enable everything, and those that fail to capitalize on this trend will be left behind in the marketplace. While new applications can be written for this environment, substantial value exists in legacy applications and systems. Successfully service enabling these applications is not easy for software vendors due to the complexity of integration. To address this problem, we focus on the issue of service specification using models which clearly capture the scope, capability and state of a service. Such models make integration of services more certain and can allow previously incompatible services to be combined more easily. We have prototyped Web2Exchange, a platform for modeling, transforming and integrating services. We have publicly demonstrated the platform by showcasing a ""mashup"" of the HP Opencall Media platform with web-based calendaring in November of 2008. Since then, we have developed additional services on the platform, and are beginning to make the platform available to others outside of our organization."
2963,"The trustworthiness of service providers plays an important role when a consumer selects a service. This paper studies the problem of how to efficiently search and select trustworthy service providers for users in social networks consisting of service providers and consumers. A trust value between two participants can be derived by existing methods from the optimal trust path between them in a social network. When more than one trust factors are taken into consideration, the exact optimal trust path selection algorithm is NP-complete. Although several heuristic algorithms have been proposed to find approximate solutions, their time complexities are still too high to be acceptable in practice, especially when they are used in very large scale social networks. Focusing on reducing trust path searching time, this paper proposes an efficient preprocessing-based search strategy. It exploits structural properties of the social networks and builds an advanced data structure from preprocessing, which can be used to simplify and accelerate the trust path searching. Experimental results show our strategy is very efficient and nearly achieves a constant time complexity. The computed trustworthiness based on our method has excellent performance close to that of the best existing heuristic algorithm."
2964,"In the era of knowledge economy, knowledge resources have become the most valuable assets for enterprises. To better understand and reuse knowledge, it is necessary to relate it with the context in which the knowledge is generated and used. This is a process that usually occurs in an experienced knowledge-worker's mind and without efficient supporting tools. This paper proposes an approach for the acquisition and utilization of context for the enhancement of knowledge and with a particular focus on methods to enable context extraction from industrial settings. The approach adopts a knowledge context ontology, to correlate knowledge and its context in the high-level activities of a knowledge worker. Knowledge context are extracted by utilizing a combination of methods including context identification, context reasoning, and context similarity measurement. Based on the proposed approach, a set of services for context aware knowledge enhancement are developed and applied in The Chinese Enterprise Management Tank (CEMT), a knowledge sharing and reusing platform for business management knowledge workers in all around China."
2965,"Many researchers have suggested fuzzy-based methods to derive rankings of services based on the fuzzy degree that each service satisfies a set of weighted quality attributes. Most of these methods assume a complete set of candidate services completely assessed. However, the candidate service set may include services which have not been fully assessed yet with respect to all quality attributes. Unassessed candidates introduce hesitation regarding the ranking of already evaluated services. This paper suggests Intuitionistic Fuzzy Sets (IFSs) to handle these sources of hesitation when assessing quality of services. IFS score functions are used to rank services with regard to each quality attribute. The final ranking can be derived by applying an objective method based on entropy weights for the quality attributes."
2966,"Service Level Agreement (SLA) is gaining more and more interest since the dynamic aspect of the cloud computing can adversely influence the guarantee of the Quality of Service (QoS). Proving an SLA violation is considered to be a complex operation to the cloud consumer. This task gets more and more difficult to the consumers as they use services from multiple providers, each with its own monitoring policies. In order to implicitly help the consumer monitor the SLA and detect violations, it is necessary to implement some advanced SLA management strategies. Ontological representation and reasoning techniques in SLA management may be beneficial to the evaluation of the QoS. In this regard, we propose a monitoring approach with a new semantic SLA modeling for cloud computing. Our approach aims to help cloud consumers monitor the SLA documents and detect violations by considering the semantic meaning of SLA concepts and taking profit from the power of reasoning techniques. This maintains a reliable QoS and respects the SLA parameters. The efficiency and effectiveness of the proposed approach is demonstrated in this paper through a simulation and a case study."
2967,"Low coupling is a service-oriented design and development principle that should be taken into account during all stages. Having loosely coupled services not only increases service reusability, but also prevents the propagation of changes to other services and thus simplifies maintenance of service-oriented systems as well. In this paper, we focus on measuring conceptual coupling as an indicator of how much a service depends on the other services from functional point of view. Latent Semantic Indexing (LSI) is a well-known technique in the field of information retrieval (IR) which has been widely used to measure the degree of semantic relationship between text based documents. In this paper, a metric namely CCS is proposed to measure the degree of conceptual coupling of a service to its environment based on the LSI technique. The proposed metric is evaluated theoretically based on a set of coupling principles."
2968,"Web service discovery has become a daunting task primarily due to its inability for allowing clients to articulate proper service search queries. Improving the quality of service search results could not be achieved unless we determine ways for correctly identifying Web service query goals. In this paper, we identify client goals when performing a Web service search. As part of this work, we introduce the concept of Quality of Web Service (QWS) for our quality-driven discovery mechanism. We determine that client goals in service discovery can be defined as exploratory or informational. We use this information to demonstrate how the knowledge of client goals can become beneficial in improving the way clients conduct service search queries. Results from our experiments are intriguing and show that the performance of informational service queries in terms of precision improve the querying process by 36.26% and 40.39% when compared to Google's PageRank and Yahoo, respectively. We further use our findings to provide insights on improving the service retrieval process."
2969,"Service discovery of state dependent services has to take workflow aspects into account. To increase the usability of a service discovery, the result list of services should be ordered with regard to the relevance of the services. Means of ordering a list of workflows is a similarity measure of the workflow and a query. In this paper different similarity measures facilitating structured workflows and higher level change operations are presented and evaluated based on a pilot of an empirical study. In particular the different measures are compared with the study results. It turns out that the quality of the different measures differ significantly. The best results can be achieved by facilitating n-gram multisets as a workflow as a basis for the similarity measure calculations."
2970,"The home network system (HNS) consists of networked household appliances, intended to provide value-added services. The conventional HNS has been built on the single- vendor system, which severely limits potential of the HNS. To overcome the problem, this paper presents a method that constructs the HNS with multi-vendor appliances. The proposed method first defines vendor-neutral standard services, with which various HNS applications and services are developed. Then, we exploit a dynamic service binding mechanism, which binds each standard service on a vendor- specific API of an appliance during run-time. With this mechanism, common HNS applications and services can be achieved by various combinations of multi-vendor appliances. Moreover, replacing any appliance with another never affects the execution of the applications. We have implemented the proposed method using Apache Axis Web services and Rhino JavaScript engine. The experimental evaluation showed that our implementation works well for a practical HNS with sufficiently small overhead."
2971,"In the design and development of service oriented applications, service choreography models describe the interactions between services at different abstraction levels. These models are usually created and evolved independently by different stakeholders and consequently deviations occur among models such as message not received and incompatible behaviours. It is therefore crucial to detect and resolve the deviations before actual implementation and deployment is undertaken. This paper presents a containment checking approach that verifies whether the behaviour (or interactions) described by the local choreography models collectively encompasses those specified in the global model. Previous studies have not considered the containment relationship between global and local choreography models. The proposed approach performs automated transformation of service choreography models into formal descriptions and consistency constraints for leveraging the analytical powers of model checking techniques for the containment verification. The approach provides more informative and comprehensive feedbacks to the stakeholders for identification of containment problems and their resolutions. The applicability of the approach is demonstrated through use case scenarios of ATM machine, travel booking and order processing systems."
2972,"To facilitate wide adoption of wireless devices for Internet access, it is important to enable the ""develop once and use everywhere"" trademark. This means that a Web page can be developed in HTML for presentation on desktop machines, and then be automatically adapted and displayed on any device including handheld devices. In this paper, we propose a mediator Web service that automatically adapts Web content into suitable formats based on receiving contexts. Our underpinning is a hierarchical semantic meta model and a semantic ontology that guide in partitioning a Web page into semantic segments, which refer to atomic units encapsulating one or more of the four types of semantically coherent elements in a content page: function (functionality related), typesetting (readability related), presentation relationship (space and time related), and literary context (semantics related). We also report our experiences in designing and implementing the mediator service. Our experimental results demonstrate that our service is effective and efficient in automatically transforming Web content into semantic-retained formats suitable to be displayed on handheld devices."
2973,"Service-oriented architecture (SOA) projects typically pose challenging requirements on software developers such as flexibility, speed to market, and adaptability to ever changing business requirements. In this paper, we propose a novel method of rapidly designing presentation module for an SOA-based solution using fine-grained configurable architectural building blocks (ABBs). An ABB-based presentation pattern derived from industry best practices is presented, associated with a tailored ABB configuration framework. We also introduce a formal way of modeling ABBs and their interactions. Real-life experiences of applying our ABB-based model are condensed into sixteen architectural decision points, which are further applied into a variety of SOA solution projects. Experiences accumulated over these decision points are summarized into a set of guidelines to help engineers select and configure ABBs specific to services scenarios and requirements."
2974,"Workflows have gained enormous importance to organize and manage business processes. With the recent advent of smartphones and mobile applications, traditional business process management is shifting. Now, long-running business processes (workflows) have to be executed in large-scale distributed and pervasive environments. Due to the heterogeneity and high dynamicity of such environments, they are vulnerable to frequent communication and device failures and, thus, impose new requirements on the execution of workflows. To increase the availability, we concurrently executed restructured replicas of workflows on multiple nodes. We developed techniques to generate differently structured replicas and propose a metric that identifies the set of replicas that ensures the highest availability during execution. Finally, we presented a distributed algorithm to coordinate and synchronize the concurrent execution of the identified replicas while maintaining the original workflow semantics. Our methods approximately double the availability during execution, while our generation techniques produce almost optimal replicas over a hundred times faster."
2975,Business collaboration is about coordinating the flow of information among organizations and linking their business processes. However unique and challenging security issues concerning business collaboration in the context of SOC have arisen because of its dynamic and loosely coupling nature. In this paper we firstly identify elements for security policy specification. We then discuss different types of collaboration patterns based on which rules for security policy integration can be developed. A framework is developed for handling authorization control for business collaboration. The discussion in the paper is based on the collaboration scenarios in health care.
2976,"In recent years, some researchers have begun to focus on researching CDN-P2P-hybrid architecture, but most hybrid architectures belong to tightly-coupled hybrid model. Cloud computing era calls for a more open content service mode. In order to construct open service relationship, we propose one novel CDN-P2P loosely-coupled SLA negotiation model. CDN firstly needs to provide an open and standard-based agreement interface for the P2P and other applications to negotiate with it. Therefore, we use Web Services Agreement Specification (WS-Agreement) for establishing agreement between two CDN and P2P. This scheme also allows CDN easily integrated with other application. The loosely-coupled CDN-P2P-SLA establishment needs to link with SLA performance monitoring, we design WSDM based CDN-P2P loosely coupled SLA monitoring scheme. The prototype system experiments have proved the feasibility of this model and scheme. Finally, we conclude this paper and analyze the prospective research direction."
2977,"Emerging Semantic Web Services rely on the availability of metadata that describes various functional and non-functional characteristics of computational resources. A number of semantic vocabularies and datasets describing existing services and workflows are publicly available and their rapid growth brings forward a new challenge -- efficient management of semantic data. Many existing semantic data repositories use conventional relational databases to store and query large RDF datasets. The most complex component of this approach is SPARQL-to-SQL query translation. Existing algorithms translate SPARQL queries to SQL using either bottom-up or top-down strategy and result in semantically equivalent but syntactically different relational queries. While it can be expected that relational query optimizers produce identical query execution plans for semantically equivalent bottom-up and top-down queries, is this usually the case in practice? To address this question, we study bottom-up and top-down translations of SPARQL queries with complex nested optional graph patterns that yield SQL queries with left outer joins whose reordering is not always possible. This paper reports our on-going research and performance study featuring SPARQL queries with nested optional graph patterns over semantic data repositories instantiated in Oracle, DB2, and Postgre SQL."
2978,"The previous Quality of Service (QoS) measure method for Web services are not accurate as they often focused on the ambiguity of user preferences but neglected its one-sidedness (i.e., user preferences cannot realize the data distribution characteristics of services set). This paper first points out that user preferences are not only ambiguous but also one-sided. QoS measure should be a co-action of both user preferences and services set, and therefore neglecting the data distribution characteristics of services set have weakened the accuracy of measure results. We present in this paper a novel QoS measure algorithm for Web services employing the subjective and objective weight. The subjective weight is used to quantify ambiguous user preferences and the objective weight is used to correct the one-sidedness of user preference. The two used weights guarantee the measure results can both conform to user preference and reflect the overall performance precisely. The validation of theoretical analysis and experiments based on the QWS real data sets confirmed the efficiency of proposed algorithm."
2979,"With Internet becoming ubiquitous, electronic transactions over the Internet have become an integral part of day to day life. The Internet is used for more and more secure transactions like banking, shopping, submitting tax returns etc. In a way, the need for a secure Internet based electronic voting system is an obvious demand. The task of designing a secure Internet based voting system is a cryptographic challenge. This paper proposes and discusses the design and implementation of secure Internet based electronic voting system using Identity Based Encryption System (IBES). This proposed system satisfies various security requirements like, privacy, anonymity, eligibility, accuracy, fairness, uniqueness, verifiability and receipt freeness. Total user anonymity is achieved using IBES."
2980,"Service-oriented computing (SOC) has emerged as the eminent market environment for sharing and reusing service-centric capabilities. The underpinning for an organization's use of SOC techniques is the ability to discover and compose Web services. In this paper we present a generalized semantics-based technique for automatic service composition that combines the rigor of process-oriented composition with the descriptiveness of semantics. Our generalized approach extends the common practice of linearly linked services by introducing the use of a conditional directed acyclic graph (DAG) where complex interactions, containing control flow, information flow and pre/post conditions, are effectively represented."
2981,"The ability to create virtual organisations (VOs) on demand in a dynamic, open and competitive environment is one of the challenges that underlie grid computing and Web services research. It is widely recognised that to form and manage a VO effectively, the ability to gauge quality of service (QoS) is important. Existing methods for managing QoS in SOC environments tend to adopt a provider-centric perspective, aiming largely at optimising and guaranteeing QoS for service delivery. In this paper we consider QoS management of VOs from a service user's perspective, proposing techniques for supporting service users in assessment and monitoring of QoS during VO formation and operation."
2982,"This study proposes an experience evaluation model (EEM), indicating how service experience can be estimated within interaction-based collaborative service systems with (semi-)automated value co-production between service providers and receivers. A symbiosis theory in ecology is employed to measure the performance of collaboration. The proposed EEM is based on three measurements - proximate response (PR), ultimate response (UR), and evolve dependence (ED) - that measure performance deviation before and after being affected by customer inputs and interaction in service process. The performance can be measured by the fitness of the interaction between service participants effecting of value co-production. The fitness for a provider and receiver can then be determined using life reproducibility success (LRS) based on mutualism performance theory, which facilitates estimation of user's collaborative experience. On one hand, monitoring service performance to explore service experiences, on the other hand, optimizing the interaction-based collaborative service systems through EEM."
2983,"Business processes are prone to variations due to the phenomenon known as concept drift, which refers to the situation where the business process is changing while being executed. Concept drift detection of business processes is one of the most critical problems in process mining. Most of the state-of-the-art approaches use statistical hypothesis testing for change point detection by sliding-window-based analysis and feature extraction, which suffer from bad performance and missed detection of certain types of changes. To attack such challenges, this paper proposes a framework as well as detailed models and techniques for online process concept drift detection without any feature extraction. A streaming scheme is presented to detect, locate, and classify the concept drifts simultaneously without requiring any storage of the event traces that have been analyzed. Four types of process concept drifts which are sudden drift, gradual drift, recurring drift, and incremental drift can be precisely discovered, and noise as well as flexibility in business processes are fully considered. The efficacy of the approach is also validated by simulation experiments on large-scale business process event streams."
2984,"DataGrids are becoming increasingly important for sharing large data collections, achievements and resources. The BSP model is a widely used parallel programming model. The idea of the superstep in the BSP model should be able to help DataGrid access and storage in regular sequence. When services are not isolated from each other in multiuser environments, this should be able to avoid, in the process of data access and storage, the occurrence of four types or phenomena: lost update, dirty read, nonrepeatable read, phantom."
2985,"Trustworthiness has become a critical factor for users to choose the most suitable computing services for business or personal use. However, no standard exists for evaluation and comparison of service trustworthiness. So we try to propose a trustworthiness evaluation framework that can make standard or customized selection decisions based on common sense or user-defined specific trustworthiness criteria. This framework utilizes neural network and fuzzy logic to handle inaccuracy and subjectivity in trustworthiness evaluation. A feedback mechanism is also incorporated to make the entire framework be adaptive to users' preferences on different trust factors. A prototype is implemented to evaluate and compare different types of computing services. Experiments on today's popular email services and cloud-based online storage services prove the effectiveness and accuracy of the proposed framework."
2986,"The grid technology is expected to have the ability of resolving the problem of large-scale information resources sharing in virtual organizations, which deliver a number of grid services to publish large-scale information to end-users. We propose an intelligent client to grid services, which manipulate information resources at semantic level and provides end-users with a universal and intuitive semantic view for various grid services. With a series of semantic plug-ins in semantic browser, end-users can gain richer interaction with grid services and resolve complex domain problems in a more intuitive and friendly environment."
2987,"Proactive management of service contract risks ahead of contract signing is becoming increasingly important for IT service providers due to the cost pressure associated with IT outsourcing. Within an end-to-end risk management process, various risk assessments are performed at multiple stages before a service contract is signed. Based on the risk assessment data, service providers seek to have predictive models that indicate risks of future service contracts. Considering the wide range of risk assessments, the variable frequency in which they are conducted, their sequential nature and the prevalent data scale, naïve statistical modeling approaches, such as linear regression, are not readily applicable to such data sets. It is, therefore, necessary to identify a new methodology for predicting service contract risks based on ordinal risk assessment data. In this paper, we describe an analytical methodology that enables optimal risk prediction for service contracts, along with the lessons learned from implementation within an enterprise-level risk management ecosystem. Such real-world insights can provide guidance to data scientists and researchers both in the service delivery domain as well as other domains with similar data characteristics."
2988,"In the literature, Contextual Transaction Trust computation (termed as CTT computation) is considered an effective approach to evaluate the trustworthiness of a seller. Specifically, it computes a seller's reputation profile to indicate his/her dynamic trustworthiness in different product categories, price ranges, time periods, and any necessary combination of them. Then, in order to promptly answer a buyer's requests on the results of CTT computation, CMK-tree has been designed to appropriately index the precomputed aggregation results over large-scale ratings and transaction data. Nevertheless, CMK-tree requires additional storage space. In practice, a seller usually has a large volume of transactions. Moreover, with significant increase of historical transaction data (e.g., Over one or two years), the size of storage space consumed by CMK-tree will become much larger. In reducing storage space consumption for CTT computation, the aggregation results that are generated based on the ratings and transaction data from remote history, e.g., ""12 months ago"" can be deleted, as the ratings from remote history are less important for evaluating a seller's recent behavior. However, to achieve nearly linear and robust query performance, the deletion operations in the CMK-tree become complicated. In this paper, we propose three deletion strategies for CTT computation based on CMK-tree. With our proposed deletion strategies, the additional storage space consumption can be restricted to a limited range, which offers great benefit to trust management with millions of sellers. Finally, we have conducted experiments to illustrate both advantages and disadvantages of the proposed deletion strategies."
2989,"In this paper we introduce a business collaboration design framework (BCDF), which provides a systematic way for analyzing the requirements and modeling the activities involved in business collaboration development. The ideas presented are illustrated using a complex multi-party, insurance claim handling scenario. The work presented herein emphasizes the relevance of the BCDF for modeling at high level and aligning these high level business requirements to technical realization."
2990,"The descriptiveness of capabilities advertised on service-oriented architectures provides a promising platform for crafting new knowledge. Service mashup has been introduced as an approach for integrating the information provided from multiple Web services into one common operational picture. In the future, scale will be a barrier to these types of approaches. With the entry and exit of large numbers of services on the Internet, it will be difficult to find and suggest the most relevant service candidates for new mashups. In this work, we present an efficient syntactical approach for actively discovering Web service candidates for service mashups. This approach leverages the message naming characteristics of the developers and of the target service repository to inform search algorithms. Favorable precision results are described based on experimentation executed on an open repository of Web service from the Internet."
2991,"In service-oriented enterprise architecture, provisioning business services is made on top of IT processes, which should be elastic amid the availability of computing resources and the variation of user demand. In addition, the provisioning depends on human resources utilized and is constrained by the business objectives (e.g. a goal) plus coarse-grained constraints (e.g. an order in which business services take place). This elasticity and constrainedness can best be witnessed on nonfunctional properties of the business services being provisioned. In this paper, we propose a novel framework for modeling and reasoning about them. The framework features a methodology for formally expressing the aforementioned factors in services provisioning, an engine to find solutions and a simulation."
2992,"The speed of a device may vary since (i) IaaS hardware infrastructures are increasingly heterogeneous and (ii) devices often have a dynamically adjusted speed in order to adapt their energy consumption according to the load. This paper addresses SLA enforcement in a IaaS which includes devices whose speed vary. We show that resource management should rely on an absolute value SLA specification (i.e., a performance metric which is independent from the device speed) and a dynamic translation of this SLA into actual allocations according to the device speed. Surprisingly, while disk or network resource allocations already integrate such a scheme, CPU does not. We propose a CPU resource management system which implements absolute CPU allocation and dynamically translates it into actual CPU allocations according to CPU speed. We demonstrate and evaluate the benefits of this resource management system."
2993,"We present a test-based assurance scheme aimed at incremental security certification. Our scheme assesses the impact of changes at cloud, system, and certification methodology levels on existing certification processes. The proposed solution minimizes the risk of unnecessary certificate revocation and reduces as much as possible the amount of re-certification activities. To this aim, it reuses evidence available in existing certificates to re-validate them when relevant changes are observed."
2994,"This paper presents a specification for a domain-independent, symmetrical, two-party negotiation protocol to reach binding agreements between services based on the principles of contract law. The protocol is necessary as existing specifications, such as WS-agreement, lack the capability to form negotiated agreements and lack sufficient rigor in their design. This leads to ambiguities in their interpretations and, therefore, possible interoperability problems between different implementations. The negotiation protocol fits into the modular, high-level framework also presented here, that can be completed using a variety of existing contract monitoring and management procedures"
2995,"Unpredictability of cloud computing due to segregation of visibility and control between applications, data owners, and cloud providers increases tenants' uncertainty when using cloud services. Adaptation techniques become fundamental to provide a reliable cloud-based infrastructure with definite behavior, which preserves a stable quality of service for tenants. Existing adaptation techniques mostly focus on performance properties and are based on unverifiable evidence, which is collected in an untrusted way. In this paper, we propose a security-oriented adaptation technique for the cloud, based on evidence collected by means of a reliable certification process. Our approach adapts the cloud to maintain stable security properties over time, by continuously verifying certificate validity. It uses the output of verification activities to index a feature model, where equivalent configurations are used as the basis for adaptation. We also provide an analysis of the approach on British Telecommunications (BT) premises."
2996,"With the increment of grid computation, the data secure management and sharing has become more and more complex. NAS can reduce the cost of data management with its specification and scalability, it can accommodate to the requirement of distributed mass data storage management requirement for grid. This paper first presents a secure NAS framework on the grid and a secure storage model, and then analyses the secure policies and each module of framework, the performance analysis and future work are also given up at the last part."
2997,"Quality of Service (QoS) is considered as an important factor to determine the success of a Web Service. Currently, many QoS prediction approaches focus on time series models. However, these approaches only consider linear and nonlinear time series. Analysis of real QoS datasets shows that they are characterized by other behaviors. Incomplete characteristics analysis of existing prediction approaches will result in wrong prediction results. Furthermore, the collected QoS values may miss some data, which will also impact the prediction accuracy. RBF (Radial Basis Function) neural network model can manage the complex linear and nonlinear relationship, with great flexibility and adaptability. Therefore, we propose a novel combinational prediction approach for QoS based on RBF, which chooses the optimal model from the established linear or nonlinear prediction model, and dynamic gray prediction model according to the data characteristics. Next, the predicted results of these models are passed into the RBF training model as the input, and then used for prediction. Using a public QoS dataset and four real-world QoS datasets, we evaluate the proposed approach by comparing it with previous approach. The experimental results show that our approach is better and improves the accuracy and validity."
2998,"Pervasive computing promotes environments where smart, communication-enabled devices cooperate to provide services to people. Due to their inherent complexity, many pervasive applications are built on top of service-oriented platforms, providing a set of facilities simplifying their development and execution. In this paper, we present such a platform, iCasa, extended with an autonomic, service-oriented context module. This module is programmed with a domain-specific service-oriented language built on top of iPOJO, the Apache service-oriented component model. It is validated on smart home applications developed with the Orange Labs."
2999,"Wireless sensor networks are application specific,data centric networks where different applications run on deployed network. Each application interprets the underlying sensor network differently and each changing event has different effect on running applications.In this paper we have proposed context aware middleware architecture called as 'MidSen'. This architecture bridges the gap between multiple applications running at application level and deployed sensor networks.MidSen architecture handles static as well as dynamically changing network and application components.MidSen has adopted a rule based engine to handle system dynamics. Midsen makes applications flexible by allowing them to update their rules against knowledge base. Our experimental results show that proposed context aware service discovery (CASD) algorithm gives better recall value and precision compare to previously proposed discovery algorithms."
3000,"This paper describes a model for capturing service designs and the design of service product lines. The model promotes reuse of service artifacts, enables a balanced view of the service from different stakeholder viewpoints, and provides a foundation for design for service quality. The model leverages a phased approach to design, engineering and management of services. The model has applicability throughout the service lifecycle and provides a standardized vocabulary and structure for designing and maintaining generic services, irrespective of the domain of application."
3001,"A service ecosystem, consisting of various kinds of services and mashups, evolves over time. Existing works on the evolution of service systems focus on either evaluating the impacts of services' changes on the usage of services and the stability of the whole ecosystem, or discovering co-occurrences between services, but fail to disclose any knowledge about the evolution of service composition patterns. Based on our previous work of SeCo-LDA, through scrutinizing the dependencies between different service co-occurrence topics, this paper reveals the latent service composition trends in a service ecosystem. We derive topic dependencies and describe it as a directed topic evolution graph, where four topic evolution patterns are identified. A novel methodology, named Dependency Compensated Service Co-occurrence LDA (DC-SeCo-LDA), is developed to calculate the directed dependencies between different topics, build the topic evolution graph. The evolution trend of service composition could be disclosed by the graph intuitively, and dependency compensation could be adopted to improve the performance when making service recommendation. Experiments on ProgrammableWeb.com show that DC-SeCo-LDA can recommend service composition more effectively, i.e., 2% better in terms of Mean Average Precision compared with baseline approaches."
3002,"Crowd sourcing platforms enable enterprise requesters to leverage the online workforce to process voluminous enterprise tasks on a regular basis. Web services provided by these platforms facilitate requesters to post tasks, retrieve results and incentivize crowd workers. However, service assurance associated with task execution by crowd workers is not provided by these platforms. Owing to the flexible, uncommitted, discretionary working patterns of online crowd workers, service assurance for task execution is considered to be beyond the service assurance offerings of existing crowd sourcing platforms. Enterprises however, require these guarantees to be able to adopt crowd sourcing in a profound manner. In this paper, we propose a Service Assurance Framework as a crowd sourcing platform augmenting service, that provides service assurance for task execution by crowd workers. The framework helps enterprise requesters to identify and engage with workers who possess suitable service assurance attributes. To the best of our knowledge, this work is a first of its kind, in providing service assurance associated with task execution by crowd workers, with respect to enterprise tasks. We implemented the proposed framework and conducted a four-week long, large scale crowd sourcing experiment involving digitization of forms posted by an enterprise requester. Our results validate the efficacy of the proposed service assurance framework for enterprise crowd sourcing and advocate its adoption."
3003,"In their current scope, existing business process modeling methods and techniques lack comprehensive constructs for representing some of the essential business concepts, including business goals, non-functional requirements and resources. Chances are, there will never be a single technique that adequately captures all the business concepts on a single diagram. This paper has two related purposes: First, it evaluates the modeling capabilities of process modeling techniques against the Zachman Framework. Second, it proposes a multi-perspective integration framework for bridging the identified gaps using other requirements modeling techniques such as goal-oriented and data-oriented techniques. A real world case study is then used to demonstrate the integration process. Ultimately, our framework aims at supporting the creation of comprehensive models and facilitating a common understanding of business perspectives regardless of how they are represented."
3004,"The proper operation of service-oriented architecture (SOA) strongly depends on the correct configuration of the service hosting environment that is essentially being shaped on a distributed system. Effective migration management of the distributed system is an absolute requirement to cope with intrinsic-changed nature of SOA. However, due to the large amount of configurations existing in the distributed system services and the complex dependencies among them, to migrate into a distributed system satisfying the operation requirement of SOA becomes an error-prone and time-consuming task. In this paper, we propose a model-based dependency management approach to migrate the distributed system services of the service hosting environment. Our approach has three contributions. Firstly, a model, called system service configuration model is defined for describing configuration parameters and the dependencies of different distributed system services. Secondly, based on the model, dependency management mechanisms are provided for validating the integrity of the configuration discovery process, checking the potentially cyclic dependencies and forming the correct setting sequence. Thirdly, a supporting tool, called system configuration migration (SCM), is implemented to demonstrate our approach, which has been deployed and used in the real migration cases."
3005,"Cloud provider should ensure QoS while maximizing resources utilization. One optimal strategy is to timely allocate resources in a fine-grained mode according to the actual resources demand of applications. The necessary precondition of this strategy is obtaining future load information in advance. We propose a multi-step-ahead load forecasting method, KSwSVR, based on statistical learning theory which is suitable for the complex and dynamic characteristics of the cloud computing environment. It integrates an improved support vector regression algorithm and Kalman smoother. Public trace data taken from multi-types of resources were used to verify its prediction accuracy, stability and adaptability, comparing with AR, BPNN and standard SVR. CPU allocation experiment indicated that KSwSVR can effectively reduce resources consumption while meeting Service Level Agreements requirement."
3006,"Today, enterprise IT environments are complex as never before with individual applications, tiers, or technologies segregated into individual management domains. Typically, the value of business applications and the dependencies between business and IT objects and IT objects among each other is completely unknown or at least not up to date. Thus, ultimately, the business value of individual IT tasks is unknown. Hence it is very hard to perform global management services such as performance optimization in resource-constrained environments. This deficiency is even more deeply felt by an internal or external services provider called in to set up a new optimization or IT management framework.We propose a framework ITBVM for business-value driven IT optimization with particular emphasis on such enterprise environments. A key part is the use of discovery technologies to provide the link between business value and IT objects. As one instance of the framework, we show how discovery can improve a performance-optimization problem in an otherwise blackbox scenario. We validate these improvements through experiments in a controlled setup and through statistical interpretation of fine-grained dependency discovery in a large real enterprise environment."
3007,"Software systems based on Service-Oriented Architectures (SOAs) promise high flexibility, improved maintainability, and simple re-use of functionality. A variety of languages and standards have emerged for working with SOA artifacts; however, service computing still lacks an effective and intuitive model-driven approach starting from models written in an established modeling language like UML and, in the end, generating comprehensive executable code. In this paper, we present a conservative extension to the UML2 for modeling service orchestrations at a high level of abstraction, and a fully automatic approach for transforming these orchestrations down to the well known Web Service standard BPEL."
3008,"Payment arrangement is one of the vital issues in a B2B setting. For example, failing to accurately specify and communicate payment details such as the method of the payment, whether payment is refundable and negotiable, and what settlement model is used, can lead to serious disputes between the provider and requester of a service. However current development and solutions in service description, e.g., WSDL, do not cater for the specification and negotiation of payment related requirements for services. In this paper we present a comprehensive modeling approach for capturing and specifying payment requirements in the context of business collaboration. It will lay a foundation for service providers and requesters to explicitly define and adjust their business and technical requirements for payment in an adaptive and verifiable way. Furthermore, it provides support for terms of payment to be negotiated effectively to reach service provisioning agreements."
3009,"Web services composition is receiving significant amount of interest as an important strategy to allow enterprise collaboration. Current web services composition solutions are rather restricted and inflexible as they are based on pre-defined models of the process environment. These solutions have assumed that the information in the models and consequently the compositions remain static and unchanged throughout the life cycle of the web services composition. However, web services may run in a highly dynamic environment. Therefore, a mechanism is required to support web services composition in dynamic and flexible process environment. The reflective framework presented here aims to improve the adaptability of BPEL-based web service composition. A meta-model was defined to build the self-representation of the web services composition. The meta-model will be modified to adapt to the changing environment, and then, the reflection mechanism utilized in the framework will adjust the web services composition automatically. To ensure the correctness of dynamic adaptation, a set of constraints and a verification approach are proposed. A prototype adaptive service composition environment has been developing to implement our reflective framework and demonstrate its effectiveness on providing adaptive web services composition. In summary, it is stated that the reflective framework provides a suitable solution to the adaptive service composition and reliable control flow and data flow correctness."
3010,"One of the difficulty in applying the concepts of semantic Web technology to a drug discovery is the creation of a software generated URI suitable both for ontology independent and ontology dependent properties of drugs. We propose the use of two types of URI; structure invariants (URI) to identify the ontology independent features and structure semi-invariants (OURI) to identify ontology dependent features. URI are defined by the most fundamental properties (chemical connectivity, bond type, etc.). OURI are defined from context dependent (semantic) properties (binding mode, chemical core)."
3011,"This paper presents a novel information sharing framework using Service-Oriented Database System (SODB) for service registry and repository that facilitates data integration. A SODB is a database architecture composed with reusable services to support searching, querying, deleting, and storing data. As SODB is organized in a service-oriented manner, SODB can be easily changed or maintained by reusing different services. Thus, it can be used to share information in the cloud. The Information Sharing System (ISS) component of the SODB employs domain ontology to share data sources. The domain ontology utilizes mathematical equivalence relations to map data sources into appropriate domain ontology. It also facilitates dynamic query composition across data sources. This paper also presents the design, implementation, and performance evaluation of the ISS component. Our implementation is based on a real application, the Arizona Healthcare Cost Containment System (AHCCCS). This application demonstrates that the ISS can facilitate complex information integration. Finally, this paper presents the performance of the ISS using WAPT (Web Application Testing) for Microsoft Windows XP, and the response time consistently fall in between 0.1 to 0.15 second for each request."
3012,"The Nevada Solar Energy-Water-Environment Nexus project generates a large amount of environmental monitoring data from variety of sensors. This data is valuable for all related research areas, such as soil, atmosphere, biology, and ecology. An important aspect of this project is promoting data sharing and analysis using a common platform. To support this effort, we developed a comprehensive architecture that can efficiently collect the data from various sensors, store them in a database, and offer an intuitive user interface for data retrieval. We employed Arduino-based sensors due to their flexibility and cost-effectiveness. Restful Web Service is used for communication with the Arduino-based sensors, and Google Charts service has been used for data visualization. This framework for sensor data monitoring with Web Service is expected to allow the Nevada Nexus project to seamlessly integrate all types of sensor data and to provide a common platform for researchers to easily share the data."
3013,"Active XML combines XML data and service calls to allow a simple and powerful Web services implementation. Security in Active XML is currently handled by matching the structure of the received data with an XML schema representing the allowed data (including service calls). This solution is not fully satisfactory in case of an open environment where the services do not often know or trust each other. Moreover, the strength of Active XML lies in its simple and dynamic structure, and the modified XML schemas used for security matching can quickly limit the allowed services, or give too much freedom to services that should not be trusted. Given that the result of an Active XML service call is some Active XML data (that may include more service calls), Active XML data is recursive, thus involving more security concerns. We propose a new framework based on the notion of Trust (Trusted Active XML) for handling security in Active XML. In this framework, ""trusted"" services' answers are not restricted to a specific data schema, while ""untrusted"" ones are prevented from performing some unwanted operations."
3014,"Business process models expressed in languages such as BPMN (Business Process Model and Notation) play a critical role in implementing the workflows in modern organizations. However, control flow errors such as deadlock and lack of synchronization as well as syntactic errors arising out of poor modeling practices often occur in industrial process models. In this paper, we provide an empirical diagnostic analysis of such errors for real-life industrial process models. The investigation involved models from different application domains. It turns out that error frequency has non-linear relation with error depth (the maximum depth at which an error occurred) across models from all domains. Error occurrence has statistically significant correlations (p &lt;; 0.0001) with the size of sub-processes as well as with the swim-lane interactions, however only the former correlation is strong (Spearman's ρ = 0.579). We also develop a logistic regression model to estimate error probability in terms of the following metrics: sub-process size, coefficient of connectivity, sequentiality and structuredness; the predictive model fits well with the data (χ
<sup>2</sup>
(4, N = 1261) = 720.68, p &lt;; 0.001)."
3015,"With the development of Web service technology, Web service matching is becoming more and more important to service composition and service Mashup. The research of service matching has made great development and the crucial problem is schema matching. In this paper, we consider the task of matching multiple Web services for discovering the similar function services to the given one. In order to get the accurate matching result, we propose a neural network based schema matching (NNSM) method. While the semantic similarity calculated by individual matchers relies on individual aspects of information about schemas only, which are not sufficient for finding element correspondences between schemas. Our method uses a feed-forward neural network to combine multiple matchers where the back propagation algorithm is used to train the network. This brings significant gains in coverage while yielding modest gains in relevance. The experimental result shows we have a better accuracy compared to individual matchers and related research on service matching task."
3016,"Service composition environments enable people to create, manage, share services, and compose atomic services for their own requirements. Since users and service entity hosts are always distributed in locations in such environments, service responses might be very slow if users invoke services that are physically far from them, which is even slower for invoking composite services. However, those problems cannot be solved using traditional caching technologies in the areas of contents delivery network because service providers in service composition environments always have their own policies about service license issues, and do not allow their service entities to be copied on all service entity hosts. In this paper, we propose the approach of dynamically switching availability of services among service entity hosts based on the invocation request from users, and design several dynamic invocation control mechanisms to improve the response performance of services considering the service license constraints by service providers. Our proposed mechanisms are evaluated by simulation in service composition environments."
3017,"Public transit generates a wide range of diverse data, which include static data and high-velocity data streams from sensors. Integrating and processing this big real-time data is a challenge in developing analytical systems for public transit. We here propose MOBANA (MOBility ANAlyzer), a distributed stream-based system, which provides real-time information to a wide range of users for monitoring and analyzing the performance of public transit. To do so, MOBANA integrates the diverse data sources of public transit, and converts them into standard and exchangeable data formats. In order to manage such diverse data, we propose a layered architecture, where each layer handles a specific kind of data. MOBANA is designed to be efficient. e.g., it identifies the real time position of vehicles by adjusting planned position with real-time data as needed, thus dropping network load. MOBANA is implemented by Distributed Stream Processing Engine (DSPE) and Distributed Messaging System (DMS), which pursue scalable, efficient and reliable real-time processing and analytics. MOBANA was deployed as pilot in Pavia, and tested with real data."
3018,"Despite the availability of services with similar functionality but from different providers in the cloud, using them in a workflow might subject to constraints such as service QoS and service bundling. Service bundling refers to the situation where the subscription of two services have to be done together, such requirement might be imposed by service providers and/or by the alliance group that the providers join in. In this paper, we focus on the service selection problem under the QoS constraints from the user and the bundling constraints associated with the chosen services. We first formulate the service selection problem as a multi-constrained selection problem. Then we propose a recursive heuristic search algorithm that takes the required QoS and bundling constraints into consideration for service selection. This algorithm has two unique functions: (i) utility function to measure the quality of the selection strategy under consideration, and (ii) acceptance function to limit the selection strategy only to those potential service candidates that have higher chance to satisfy the bundling constraints. Experiments show that our proposed solution can find better solutions than the existing ones without too much extra performance overhead."
3019,"Interface mismatches are common among web services interactions. It happens when the interacting services have different message signatures. Semantic Web technologies are proposed to address this problem. If services are annotated with shared ontologies, an interface adapter may be generated by connecting messages that annotated with same or similar concepts. Unfortunately, services are usually annotated with different ontologies. As a result, different ontologies need to be aligned before adaptation. Aligning large ontologies can be time-consuming and inaccurate, which leads to inefficient and unsuccessful web service adaptation. In this paper, we tackle the problem by proposing a sub-ontology extraction method to generate Representation Ontology for each service. The Representation Ontologies can significantly increase alignment speed and accuracy, which consequently improve the process of interface adapter generation. Related tests are conducted to evaluate our work."
3020,"We design a framework for solution validation and couple it with the ability for VM and elasticity management. We implement our design in the form of a ""dashboard which captures all elements of a comprehensive cloud solution management framework. We illustrate the applicability of the dashboard with a cloud solution we have developed."
3021,"Large corporates and government institutions tend to have multi-year service engagements with IT-vendors who provide various kinds of infrastructure and management services including software, servers, IT - security, facility management etc. The engagements are formalized as legal contracts. Given the legal binding and the natural expectation to renew, there is a need to constantly monitor the performance against the stated objectives. Typically, a principle investigator or an account executive hold the responsibilities of deliverables and perform corrective actions as and when required. Given the complexity of the language and various variables involved, it is extremely difficult to proactively detect performance degradation. In this paper, we introduce the notion of contract health and describe Fitcon - a contract mining system that detects service level agreements from contracts, tracks the delivery performance against them and predicts the health of long term contracts. Fitcon is intelligent as it reads these natural language legal contract documents to set its monitoring goals. Fitcon tracks key parameters of the contract and predicts their performance. Comprehensive experiments on real contracts and corresponding past performance data are conducted to test the effectiveness of the system. We test our system across 11 large engagements and the analysis shows promising results. To the best of our knowledge, Fitcon is first such system that has been deployed into production for large scale contract health determination and prediction."
3022,"Web portals are a means for presentation level integration of enterprise applications and services. We describe an approach for enhancing presentation level integration in Web portals by supporting communication between presentation level components. The approach is based on established standards in this area and allows the exchange of structured XML-data between local and remote applications and services. Since the approach is mostly declarative, it can also be used for improving integration of already existing presentation components."
3023,"Component-based applications entail a composition of heterogeneous components often running in different contexts. The complexity and dynamic nature of their contexts result in an increasing maintenance efforts. Autonomic computing came to provide systems with an autonomic behavior based on predefined policies. However, in addition to being knowledge-intensive, the constructed policies may easily become obsolete due to context changes. Decision policies should be dynamically learned to self-adapt to context dynamics. However, currently built autonomic systems are tailored to specific management needs, neither reusable for other management concerns nor endowed with learning abilities. In this paper, we introduce a generic framework that facilitates building self-adaptive component-based applications. Unlike the existing initiatives, our framework provides means to transform an existing application by equipping it with a self-adaptive behavior to dynamically learn an optimal policy at runtime. To validate our approach, we have developed a realistic application and used the framework to render it self-adaptive. The experimental results have shown a negligible overhead and a dynamic adjustment of the transformed application to its changing context. They have also shown less frequent time spent in SLA (Service Level Agreement) violations during the learning phase and a better performing application after convergence."
3024,"Context awareness has emerged as an important element in distributed computing. It offers mechanisms allowing applications to be aware of their environment and enabling them to adjust their behavior to the current context. In order to keep track of the relevant context information, a flexible service mechanism should be available for the client applications. In this paper we present a service architecture to provide context- awareness capabilities to users and client applications. Moreover, the service is able to react depending on the user's preferences and context. The conditions for the reaction and the reaction itself are defined in rules the users submit to the service by means of a convenient rule language."
3025,"A frequent hurdle in applying Business Process Management(BPM) to large enterprises is that, since business processes are not only numerous but also documented in an engagement in multiple representations,it is difficult to work with the documented 'as-is' or 'to-be' state of the business, leave aside make accurate transformation decisions. In this paper, we consider the problem of how to reconcile and organize documented information about processes into groups that convey inter-process similarity. The discovered knowledge can be used for many applications like search, e.g., find all requirements across all available processes that are similar to those of the ""Account Receivable"" process. The method has been tested on a dataset consisting of hundreds of processes documented in Word and Visio, wherefrom it could find process clusters that significantly boosted search performance."
3026,"Service-orientation enables cooperation between multiple organizations and has become a solution of choice to tackle the complexity of ubiquitous computing. The very nature of ubiquitous applications implies a need for dynamic solutions. Service oriented computing provides support for these dynamic applications. However, current solutions overlook important aspects, particularly when dynamically substituting services. Some applications may be mission-critical and therefore the disruption of a particular service could be harmful. Other applications may tolerate the disappearance of a service if the service returns within a predefined amount of time. Hence, guarantees regarding availability of services that compose an application are required. In this paper we propose to take into account service disruptions through service level agreements for dynamic service-oriented applications."
3027,"The objective of the MASSIVE project is to build a grid platform to enable large-scale and distributed scientific and engineering computations. Its focus is numerical simulation and visualization. This paper addresses a prototype grid utilizing resources at the Center for Engineering and Scientific Computation (CESC) in Zhejiang University (ZJU). This grid is capable of taking a model geometry generated by CAD systems, transferring it to an SGI Onyx 3900 supercomputer where it is meshed. The meshed geometry is then transferred to a Dawning PC cluster where it is used to solve computational fluid dynamics and computational solid mechanics problems and it is finally visualized showing numerical results."
3028,"Integrated asset management (IAM) is the vision of IT-enabled transformation of oilfield operations where information integration from a variety of tools for reservoir modeling, simulation, and performance prediction will lead to rapid decision making for continuous optimization of oil production. In this paper, we discuss the similarities and differences of IAM applications and typical e-Science applications. We then propose an architecture for a workflow system for IAM based on the four key requirements: support for creation, orchestration and management of workflows including those involving legacy tools, support for audit trails and data quality indicators for data objects, usability and extensibility. Our architecture builds upon current research in the scientific workflow area and applies many of its learnings to address the requirements of our system. We propose some implementation strategies and technologies and identify some of the key research challenges in realizing our architectural vision."
3029,"Configurable process models can be used to provide information on business processes for different user groups in an appropriate and efficient manner. It promotes the reuse of proven practices by providing analysts with a generic modeling artifact from which to derive individual process models. Before a configurable business process being configured into a concrete business process model, the variability points of the configurable business process need to be identified. The decision on how to bind these variation points boils down to the users' requirements and needs. Given the specified requirements of the users, the configurable business process can be configured. In the paper, we propose a framework for carrying out automatic service-based business process configuration by using SWRL based business rules. We design and implement a variation point ontology, in which the guidelines of variable points are presented by SWRL rules. We also summarize a set of domain-specific business rules too, thus we can use these domain-specific rules to get the specific rules needed to meet users' requirements. We exploit domain ontology as knowledge base and rules as guideline to configure business process, for the purpose of individual configuration. Then we employ a configuration algorithm to configure a configurable business process depending on the reference result we obtain. The approach is validated by a case study from the domain of the urban logistics distribution."
3030,"SOA and Web services are the hottest topics of discussion currently with regards to enterprise architecture. The technologies are maturing from a standardization perspective and the level of understanding of various aspects of a SOA is gradually rising with an increasing number of clients thinking about service implementation and adoption. This trend has generated significant interest in IT and business circles alike, owing to the promise of bridging the gap between business strategists and IT architects. Service Oriented Architecture builds upon decades of distributed computing technologies and advocates the delivery of software applications in the form of an open interface based on strict contracts, leading to loosely coupled systems which are implementation independent. This tutorial will focus on providing an overview of service oriented architecture with emphasis on the evolution of SOA from other technologies such as object oriented programming and distributed computing. Subsequently, the tutorial will delve into exploring SOA from multiple perspectives, such as the relevance of SOA in EAI, SOA features of J2EE and .NET apart from illustrating specific recommendations on migrating to SOA based on some: sample implementations."
3031,"Users focus on their kernel business domain, and have no the ability to use directly the service flow and composite service components. Base on the Web application pattern, the consultation-model-based services application Model (CMSAM) is presented, and the business-model driven services flow framework is proposed in CMSAM. The CMSAM describes the different responsibility amongst the services requester, the consultation services provider, the domain services choreographer, the domain services provider, the content provider and content integrator. The CMSAM maps the service components onto the service executable environment through the business process model, the requirement model, the business process flow model, the abstract service flow model, the executable service flow model and the service instances. In the charging Algorithm, it is the objective function to minimize the service fee under the condition of searching successfully the suitable service for user's business function. At last, an implementation process of part modeling service is given in the product design and analysis domain for the CMSAM as an example."
3032,"Provenance refers to the information about the derivation history of a data product. It is important for evaluating the quality and trustworthiness of a data product and ensuring the reproducibility of scientific discoveries. Much research has been done on storing and querying scientific workflow provenance - provenance that is produced in the execution of data-centric scientific workflows. To address the challenges of big data in increasing volume, velocity and variety, a new generation of scientific workflows, called big data workflows are under active research. As both data and workflows increase in their scale, the scale of provenance naturally increases, calling for a new scalable storage and querying infrastructure. This paper leverages Pig Latin, a high-level platform for creating programs that run on Apache Hadoop, and OPQL, a graph-level provenance query language, to build a scalable provenance storage and querying system for big data workflows. Our main contributions are: i) we propose algorithms to translate OPQL constructs to equivalent Pig Latin programs, ii) we extend OPQL, to support the W3C PROV-DM standard provenance model, iii) we develop and evaluate our system on provenance datasets from the UTPB benchmark, and (iv) we create some visual OPQL constructs in the DATAVIEW big data workflow system to facilitate the easy creation of complex OPQL queries in a visual workflow style. Our preliminary experimental study shows the feasibility of our framework for big-data-scale provenance storage and querying."
3033,"On-time completion is an important QoS (Quality of Service) dimension for business cloud workflow systems. Due to the dynamic and uncertain system performance, failures of on-time completion, namely temporal violations, often take place during workflow runtime and thus temporal verification and temporal violation handling are required. Currently, workflow throughput is employed by temporal verification for a large number of parallel workflow instances as the detector for temporal violations in a collective way. However, based on workflow throughput, it is impossible to locate the exact workflow activities and cloud services where temporal violations occurred. This is a critical problem as to increase workflow throughput of the whole system is very expensive if we do not know where to fix local temporal violations. To address such a problem, based on the results of a throughput based temporal checkpoint selection strategy, this paper proposes a novel temporal violation handling point selection strategy which further utilizes the average response time of a cloud service as the key detector to determine whether temporal violation handling needs to be implemented or not. Comprehensive experimental results demonstrate that while achieving the same on-time completion rate, our strategy is much more cost effective as it can precisely localize temporal violations and hence require a much smaller number of times for temporal violation handling compared with other strategies."
3034,"Increasing legislative and regulatory concerns have fueled an interest in effective and efficient tools for managing business process compliance within organizations. In particular, the key challenge is to understand high-level compliance policies in natural language, and interpret them for a particular usage context. These interpreted policies can then be represented in a formal language, and used to (for example) automatically verify compliance of business process executions against these policies. In this paper, we focus on the first part of this problem: interpreting regulatory policies - called \emph{contextualization}. We employ a natural language parser to extract key phrases from the natural language statements and generate possible interpretations from predefined templates. An analyst chooses interpretations according to the organizational context. These interpretations are then grounded further and represented in a formal language. Via a prototype, we demonstrate our approach on real-life security compliance obligations used within IBM's IT service delivery units."
3035,"This paper proposes to enhance the dynamism and the flexibility of Java Enterprise Edition (EE) servers by introducing a service-oriented architecture (SOA) inside. The purpose is to ease the deployment and offer dynamic server configuration and reconfiguration. Such an approach limits consumed resources and is capable of context adaptation. After defining the properties that must be verified for the service platform, we propose to use OSGi technology as the basis for the architecture. We have experimented with integrating OSGi into Java EE servers. Moreover, this architecture has been chosen for the next generation of JOnAS ObjectWeb's open source Java EE implementation"
3036,"Service-orientation and object-oriented design are common practice in the field of business application development. Business process execution languages help to facilitate the orchestration of Web services in service-oriented architectures (SOA). However, using business processes from within object-oriented and event-driven applications is difficult as asynchronous event handling is missing in workflow-based business process modeling languages. The present article presents an approach for integrating BPEL business processes into object-oriented applications. We propose BPEL remote objects (BPELROs) that can be accessed asynchronously in an object-oriented manner. We present a method how state-based business processes can be implemented using BPELROs. It is shown how to apply BPELROs for software modernization tasks and we also evaluate the performance of BPELROs on different BPEL engines."
3037,"Service providers (SP) business goals require an efficient management of their computational resources in order to perform provisioning, deployment, execution and adaptation which traditionally require human intervention. The semantically-enhanced resource allocator (SERA) is a framework designed to obtain an efficient autonomous service provider management while fulfilling customers' requests. While the efficient dynamic resource management is obtained by means of virtualization and agents, in this paper, we focus on how semantics can help on the task scheduling and resource allocation. In SERA, tasks and resources are semantically described following a resource ontology and resource assignments are inferred by means of a set of rules that define the scheduling policies taking into account the SP business goals."
3038,"Service Level Agreement (SLA) establishment can be viewed as a cross-organizational business process, in which consumers and providers, with varying and potentially conflicting requirements and capabilities, interact with one another in order to try and reach common agreements over the service usage terms and conditions. These interactions are governed by public interaction protocols which define their communicative behaviour, and are guided by private decision making strategies which define their strategic behaviour. In this paper, we propose a novel policy-based approach for orchestrating SLA establishment that enables a loose, run-time coupling between these two behaviours, resulting in a more flexible and adaptive automated SLA establishment process. We have implemented our approach by extending the AutoSLAM middleware [2][3][4] and validated it through a real-world use case scenario of procuring computing resources from Amazon Elastic Compute Cloud (EC2)."
3039,"The number of community detection algorithms is growing continuously adopting a topological based approach to discover optimal subgraphs or communities. In this paper, we propose a new method combining both topology and semantic to evaluate and rank community detection algorithms. To achieve this goal we consider a probabilistic topic based approach to define a new measure called semantic divergence of communities. Combining this measure with others related to prior knowledge, we compute a score for each algorithm to evaluate the effectiveness of its communities and propose a ranking method. We have evaluated our approach considering communities of real web services."
3040,"Service delivery centers are extremely dynamic environments in which large numbers of globally distributed system administrators (SAs) manage a vast number of IT systems on behalf of customers. SAs are under significant time pressure to efficiently resolve incoming customer requests, and may fall far short of accurately capturing the intricacies of technical problems, affecting the quality of ticket data. At the same time, various data stores and warehouses aggregating business insights about operations are only as reliable as their sources. Verifying such large data sets is a laborious and expensive task. In this paper we propose system h-IQ, which embeds a grading schema and an active learning mechanism, to identify most uncertain samples of data, and most suitable human expert(s) to validate them. Expert qualification is established based on server access logs and past tickets completed. We present the system and discuss the results of ticket data assessment process."
3041,"Cloud computing is the latest computing paradigm that delivers IT resources as services in which users are free from the burden of worrying about the low-level implementation or system administration details. On the other hand, within the era of information explosion, some websites may encounter a sharp rising workload due to some unexpected social concerns, which make these websites unavailable or even failure. Currently, a post-action method based on human experience and system alarm is widely used to handle this scene in industry, which has shortcomings like reaction delay. In our paper, we want to solve this problem by deploying such websites on cloud, and use features of the cloud to tackle it. We propose a workload forecasting strategy based on Gompertz curve to predict the sharp rising workload, and a customized resource management framework is also proposed to guarantee the high availability of the web applications and energy saving of the cloud service providers. Our experiment first shows the accuracy of our workload forecasting model by using some workload statistics in the real world, and then a simulation-based experiment is designed to indicate that the proposed management framework detects changes in workload intensity that occur over time and allocates multiple virtualized IT resources accordingly to achieve high availability and energy saving targets."
3042,"Services organization manage a pipeline of sales opportunities with variable enterprise sales engagement lifespan, maturity levels (belonging to progressive sales stages), and contract values at any given point in time. Accurate forecasting of contract signings by the end of a time period (e.g., a quarter) is a desire for many services organizations in order to get an accurate projection of incoming revenues, and to provide support for delivery planning, resource allocation, budgeting, and effective sales opportunity management. While the problem of sales forecasting has been investigated in its generic context, sales forecasting for services organizations entails the consideration of additional complexities, which has not been thoroughly investigated: (i) considering opportunities in multi-staged sales pipeline, which means providing stage-specific treatment of sales opportunities in each group, and (ii) using the information of the current pipeline build-up, as well as the projection of the pipeline growth over the remaining time period before the end of the target time period in order to make predictions. In this paper, we formulate this problem, considering the service-specific context, as a machine learning problem over the set of historical services sales data. We introduce a novel optimization approach for finding the optimized weights of a sales forecasting function. The objective value of our optimization model minimizes the average error rates for predicting sales based on two factors of conversion rates and growth factors for any given point in time in a sales period over historical data. Our model also optimally determines the number of historical periods that should be used in the machine learning framework to predict the future revenue. We have evaluated the presented method, and the results demonstrate superior performance (in terms of absolute and relative errors) compared to a baseline state of the art method."
3043,"The computational fluid dynamics (CFD) grid application platform provides a grid environment for CFD applications using high performance computing. It adopts service oriented architecture (SOA), is based on IBM's Service Domain, extends and customizes it for CFD applications in several modules, such as service requesting, job scheduling, job management, failure recovery, etc. By defining general CFD application interfaces such as data format, the platform integrates different CFD applications, including mesh generation, domain decomposition and resolvers, to follow the workflow customized by end-users to finish a complete CFD job. Furthermore, CFD application development specifications and packages are provided to facilitate migrating most sequential and parallel CFD applications to the platform. Currently, a prototype system is available online to the public for testing."
3044,"The virtual enterprise (VE), which joins many enterprises in order to share their costs, skills and resources in supporting a certain goal, is playing an important role in e-commerce. In a VE, how information is shared among participants is a difficult problem. The emergence of Web services provides us with a new method to resolve it. In this paper, we propose a framework of a Web services based intranet in VE (WIVE). Its focus is on the idea of information retrieval based on ontology. By the use of ontology and agent technology, we bring semantics to Web services in the VE intranet and improve the correctness of information retrieval."
3045,"Service computing is playing a more and more important role in current Internet activities, especially with the rapid adoption of electric markets, more and more individuals are engaging with commercial services. As the potential profit of service computing is becoming clear, malicious users are ramping up unfair rating attacks that can mislead honest service consumers into transacting with dishonest service providers. Moreover, some dishonest service providers may collude with dishonest service consumers to damage the reputation of service rivals. In this paper, we proposed a clustering-based reputation system that is robust to various unfair rating attacks. The model categorizes consumers as either honest or dishonest according to their rating ratio. It utilizes the Dirichlet distribution in determining reputation values. We analyze the profits and costs attained by the attacker and elucidate the conditions under which an attack is profitable. Experiments demonstrate that our clustering-based reputation model is more robust than the state-of-art model against currently successful attacks."
3046,"As business processes continue to gain relevance in different domains, dynamicity is becoming a great concern. Static processes no longer cover the actual needs of constantly changing environments, and process adaptation is a must in order to maintain competitive levels. While creating dynamically adaptable business processes can be a challenging task, undoing these adaptations is a natural functionality that has not been studied in depth. Straight forward approaches for unadaptation can easily end up with corrupted processes, bringing uncertainty to the whole business logic. In this paper we bring forward a solution for efficiently undoing a business process adaptation in event-driven environments, considering also the correlated adaptations that happened afterwards."
3047,"The ability of servers to effectively execute tasks within Cloud datacenters varies due to heterogeneous CPU and memory capacities, resource contention situations, network configurations and operational age. Unexpectedly slow server nodes (node-level stragglers) result in assigned tasks becoming task-level stragglers, which dramatically impede parallel job execution. However, it is currently unknown how slow nodes directly correlate to task straggler manifestation. To address this knowledge gap, we propose a method for node performance modeling and ranking in Cloud datacenters based on analyzing parallel job execution tracelog data. By using a production Cloud system as a case study, we demonstrate how node execution performance is driven by temporal changes in node operation as opposed to node hardware capacity. Different sample sets have been filtered in order to evaluate the generality of our framework, and the analytic results demonstrate that node abilities of executing parallel tasks tend to follow a 3-parameter-loglogistic distribution. Further statistical attribute values such as confidence interval, quantile value, extreme case possibility, etc. can also be used for ranking and identifying potential straggler nodes within the cluster. We exploit a graph-based algorithm for partitioning server nodes into five levels, with 0.83% of node-level stragglers identified. Our work lays the foundation towards enhancing scheduling algorithms by avoiding slow nodes, reducing task straggler occurrence, and improving parallel job performance."
3048,"Foreign exchange (FX) markets see a transaction volume of over $2 trillion per day. A number of standard ways of conducting business have been developed in the FX industry. However, current FX specifications are informal and their business semantics unclear. The resulting implementations tend to be complex and compliance with the standards unverifiable. This results in potential loss of value due to incompatible business processes and possible trades not consummated. This paper validates a formal, protocol-based approach by specifying foreign exchange processes as standardized by the TWIST consortium. The proposed approach formalizes a small, core set of foreign exchange interaction protocols on which the desired processes can be based. The core protocols can be composed to yield a large variety of possible processes. Each protocol is rigorously defined in terms of the commitments undertaken and manipulated by the parties involved. By contrast, traditional approaches as used in the current TWIST specification lead to redundancy in specification and difficulty in understanding the import of the interactions involved. In addition, our approach discovered interesting business scenarios that traditional approaches would have missed."
3049,"In the field of service computing, reputation of a Web service is usually calculated using feedback ratings provided by service users. However, the existing of malicious ratings and different preferences of different service users often lead to a bias towards positive or negative ratings. In this paper, we propose a novel reputation measure method for Web services. The proposed method employs two phases (i.e., malicious rating detection and rating adjustment) to enhance the reputation measure accuracy. We first detect malicious feedback ratings by the Cumulative Sum Method, and then reduce the affect of different user feedback preferences by using Pearson Correlation Coefficient. Extensive experiments are conducted. Experimental results show that our proposed method is effective and can enhance the reliability of service selection."
3050,"In this paper, we present a realistic but high-level service orchestration model where services are dynamically orchestrated with respect to the mobile behavior of nodes that provide them. We present an intermittence analyze model that monitors and quantifies the mobility of nodes. We present an orchestration model that characterizes the temporal behavior of orchestration specification based on probabilistic measures. We merge two different analysis to decide whether the mobile nodes are appropriate for use for service providing. Our contribution targets mobile computing environments but the proposed approach can be generalized in varying degrees in more traditional dynamic orchestration applications."
3051,"Grid monitoring covers two separate areas: application monitoring and infrastructure monitoring. Infrastructure monitoring is different from application monitoring because it puts emphases on testing components in grid environment such as grid middleware (gatekeeper, index service, GridFtp, etc.) and grid services. Testbed Status Monitoring Tool (GTSM) in GridLab provides a reliable information service - Monitoring service, but it is a central architecture and a single failure in key site hosting Monitoring service may bring about inestimable losses. In this paper, a more flexible architecture named WormTest is provided (our inspiration for novel mechanisms of the spread of a worm network comes from living systems), which is aimed to solve the limitations brought by central testing."
3052,"The paper introduces the checkpoint notion for the management of service level agreement (SLA) information within dataflow property graphs. The purpose of checkpoints in an SLA graph database is to assist the automated handling of SLA resources with a flexible and expressive data structure, such as a multi-relational graph. We use an abstract scenario of cloud service provisioning for the analysis of the SLA and checkpoint graph data models. The paper integrates checkpoints into the graph schema by first establishing their relationships to internal SLA elements and to service management operations. In particular, the proposed graph data schema is used for the modeling and processing of a minimal service level evaluation schema. Checkpoints perform data value comparisons and return the service level evaluation status of SLA guarantees at defined time intervals. The SLA graph database is deployed as an RDF quad-store, where checkpoints represent service resources that can be evaluated dynamically and can be defined quite flexibly."
3053,"Since the inception of service-oriented computing paradigm, we have witnessed a plethora of services deployed across a broad spectrum of applications, ranging from conventional RPC-based services to SOAP-based Web services. Likewise, the proliferation of mobile devices has enabled the remote ""on the move"" access of these services from anywhere at any time. Secure access to these services is challenging especially in a mobile computing environment with heterogeneous modalities. Conventional static access control mechanisms are not able to accommodate complex secure access requirements. In this paper, we propose an adaptive secure access mechanism to address this problem. Our mechanism, which consists of two components: an adaptive access control module and an adaptive function invocation module, not only adapts access control policies to diverse requirements, but also introduces function invocation adaptation during access. We have successfully applied the proposed adaptive secure access mechanism to a computer-assisted surgery application called UbiCAS. Performance evaluation shows that with limited overhead, our technique enforces secure access to the services provided by the UbiCAS system in a flexible way."
3054,"Service-oriented architecture has become the standard paradigm for software component integration. However, with the permanently increasing amount of available services and dynamic changes, the complexity of such service infrastructures, their maintenance, and consequently the expenditures spent for their operation increase equally. To deal with these effects, an improvement of service composition and discovery becomes necessary, especially a higher degree of automation. Following the idea of Autonomic Computing, which similarly aims at automating processes and workflows to a high degree, service composition and discovery have to proceed autonomously, which will on the one hand side reduce human involvement to a minimum, but on the other side require certain capabilities on the part of these mechanisms. For these purposes, in this paper we define prime criteria that have to be fulfilled for an autonomic service discovery. Based on that we present a comprehensive survey on existing service discovery approaches and evaluate to which extent they already fulfill these criteria. As a result, the paper reveals that there already exist some approaches that support or even fulfill a couple of the proposed criteria, which principally enables autonomic properties, but what is missing is an holistic approach focusing explicitly on providing autonomic properties."
3055,"The availability of scarce resources in a service-oriented system demands for context-aware selection policies that adapt based on service-level agreements (SLAs). One of the open issues is to prioritize service requests in dynamically changing environments where concurrent instances of processes may compete for resources. Here we propose a runtime monitoring approach to observe the actual state of the system. We argue that priorities should be assigned to requests based on potential violations of SLA objectives. While most existing work in the area of quality of service monitoring and SLA modeling focuses typically on purely technical systems, we consider service-oriented systems spanning both software-based services and human actors. Adaptive request scheduling in such systems is challenging due to the poorly predictable behavior of human actors in performing tasks. Our approach helps to cope with these challenges by prioritizing service requests that may cause violations of SLAs and corresponding objectives that are associated with processes."
3056,"Traditional approaches towards self-healing of services, have emphasized reactive self-healing; viz., stopping a failed service, replacing/reconfiguring it and restarting. Since this is disruptive, we investigate a proactive self-healing approach in this paper. The key issue in proactive self-healing, is the decision by the consumer, based on available observations of the performance of the provider service, to decide when to replace the currently running service (in case of atomic service) before the service fails, or when to replace a component service (in case of composite service), and in a manner that minimizes the cost involved in this replacement. In this paper we address this problem via a Markov Decision Process. We determine the optimal value and define the best action to be taken, when the consumer service is in a particular state of loss due to QoS failure of the provider service. Since the current state is known, our decision will help in proactive self-healing, by initiating service replacement earlier, and thereby minimizing losses to the consumer. We illustrate our ideas using a realistic example in the purchase order domain, and present an implementation prototype for the same."
3057,"XML vocabularies like RSS (really simple syndication) or domain specific syntactic syndication frameworks allow for creation of raw data that can be understood by a wide range of consuming portals and Web sites. Often individual aggregators create content from such feeds however, every single aggregator needs to render UI content separately. Avoiding this via reuse requires creation of presentation-oriented services for dynamic generation of content from same base XML data. Web services for remote portlets (WSRP) provides standards based interfaces for creation of presentation oriented services using Web services technology.However, any WSRP based remote portlet needs to be refreshed every time when even a single data element changes in the portlet. Complementarity, technologies like Asynchronous JavaScript and XML (AJAX) also allow for content aggregation and asynchronous data retrieval. Unlike WSRP, in AJAX only the required control needs to be refreshed, whenever data changes. In AJAX too, every aggregator is required to include AJAX control accessing the same back end service leading to considerable replication of effort. These two technologies complement each other in terms of ease of sharing of aggregated content and better usability of such content. Overall, often there is a requirement to dynamically generate UI content from XML data, customize and share aggregated content with remote consumers and finally allow for partial updates of remote aggregated content for better usability. We take a holistic view of the problem statement and propose an end to end architectural approach that combines usage of WSRP, AJAX along with XML syndication feeds like RSS for creation of standards based, customizable, and dynamically generated reusable UI that has better interactivity, speed and usability."
3058,"Being proactive and vigilant is the best defense against identity theft and the invasion of privacy. This recurrent advice from the public broadcasting attests that security breaches can happen and no identity management system can provide full-proof security. The challenge is even greater in service-oriented architectures where each user has their identities scattered across many services and has no control over management of those identities. Recent research in the area of the user-centric identity management makes user control and consent the key concept for identity management, but there is no consensus on the level of user-centricity. This paper proposes a service-oriented architecture framework called personal identity management that truly puts users in control over the management of their identities. The advantages of this proposal can be demonstrated through a comparison analysis of relevant identity management systems against a set of criteria required for today's identity management."
3059,"Reputation plays a crucial role in the success of e-commerce. In a commercial transaction, it is necessary to present reputation values of web services in a timely and a robust manner so as to counter the unfair ratings of malicious users. To address the time lag problem, most popular web sites use an averaging algorithm with fixed sliding windows, window size is constant and older ratings are dropped upon the arrival of new ratings. Herein, we propose a dynamic sliding window model that is capable of reflecting the reputation values yielded by the latest changes in services. Furthermore, we implement a statistical strategy to filter out unfair ratings by calculating the standard deviation of the ratings after transposing the two-dimensional linear window into the constant one-dimensional window by using linear regression. Experiments confirm the effectiveness of the proposed model, it outperforms the existing reputation system by 40% on average based on the 5 test cases examined, and also show that it can asymptotically converge to the underlying reputation value as ratings accumulate."
3060,"Service composition aims to provide the efficient and accurate model of a service, based on which the global service oriented architecture (SOA) can be realized, allowing the value-added services to be generated on the fly. Because of distributed responsibilities, ownership, and control, often, it is not feasible to acquire all information needed for the service composition, thus there might be no guarantee that the service execution has an anticipated effect. In this paper, we are going to extend current semantic Web service description by introducing the concept of ""service assumption"" which allows reasoning with incomplete information. Furthermore, together with the proposed service assumption, a sequence of rules is developed to describe all permitted behaviors in service composition context"
3061,"Grid and Web services are both hot topics today. In this paper, we present some ongoing work and planned future work at the Cambridge eScience Centre. After an introduction to these technologies in the context of grid applications development, we describe two use-cases: a database of results in computational fluid dynamics (CFD) and a small computational grid for aircraft engineering design. As grid services are moving towards Web services, we continue to make use of the Globus Toolkit v2.4 (GT2.4), without adopting the open grid services architecture (OGSA) wholesale. In our scenario, GT2.4 integrates distributed computing resources including HPC and clusters while Web services wrap the scientific code as a service."
3062,"Cloud computing offers computing, network, and storage capabilities through services that abstract the capabilities of the underlying hardware. Currently, a variety of tools exist that manage the infrastructure provisioning and use scripts to define the final state of the hardware to be deployed in the cloud. However, there are major challenges that need to be addressed to automate the infrastructure management so that they are effectively used in initiatives such as DevOps. In particular, the management of Infrastructure as a Code (IaC) is one of the most important technical challenges to support activities such as the integration, deployment, and continuous delivery of applications. To address this problem, we present a support for the management of DevOps tools, through the definition of a Domain Specific Language (DSL) based on the concept of Infrastructure as a Code, and a tool that supports this language allowing to model the final state of a provisioning infrastructure in the cloud and generating the provisioning scripts for the Amazon Web Services (AWS) platform. The proposed tool reduces the work for development and operations personnel and facilitates their communication."
3063,"One of the most interesting aspects of WSs is the possibility to combine them into Composite Web Services (CWSs). While Business Process Execution Language (BPEL) has emerged as the most popular language to orchestrate services, it introduces serious challenges with respect to the manageability of CWSs. This paper shows an approach to overcome these challenges with a model-aware execution of CWSs based on the Coloured Petri Nets(CPN) modeling language. Although the use of any mathematically grounded formalism enables verification of properties of CWSs, here it is used at runtime to guide the execution of the CWS. This paper presents the architecture and evaluation of using CPN as means for the model-aware execution, monitoring and runtime optimizations of CWSs. Compared to BPEL, the model-aware approach based on CPN significantly improves the performance."
3064,"Current markets of cloud resources are mainly based on a fixed pricing model. In the recent literature several market-based resource allocation models and algorithms have been proposed, showing that dynamic pricing models could result more profitable for both providers and consumers. In this paper we propose a market of resources where the demand and the offer of resources is matched in auction-based sales. Specifically, we looked at this market from the perspective of the provider, who needs a strategy to allocate at best their unused computing capacity. We proposed an adaptive strategy that, suitably customized to the provider's business objective, help them to maximize the revenue in the context of procurement auctions. Furthermore, the impact of resource overbooking onto the utilization level of cloud data centers has been analyzed by means of extensive simulations."
3065,"Optimum Bid price estimation is crucial for Amazon Elastic Compute Cloud (EC2) consumers if they want to secure uninterrupted access to Spot instances at reduced costs. We recently reported that Bid price estimation is an implicit function of seasonal components and extreme spikes in the Spot price history. In this paper we apply time series forecasting to further substantiate this claim. In particular, we benchmark a number of standard forecasting techniques including Naïve, Seasonal Naïve, ARIMA, ETS, STL, and TBATS against Spot markets belonging to different market types based on pricing patterns including the presence of seasonal components, extremes, and trends. We run experiments using different look back and forecast horizons, and evaluate the forecasting techniques using three measures, namely Bid Success Rate (BSR), Bid Price Over/Underestimation (BPO/UE), and Root Mean Squared Error (RMSE). Experimental results confirm that successful estimation of Bid prices in EC2 Spot markets is indeed an implicit function of seasonal components and extreme spikes in the Spot price history. Furthermore, our experiments also indicate that for certain types of markets, it is possible to significantly improve BSR by applying a small correction to the estimated Bid price without causing any major disruptions to the market."
3066,"Software service providers develop custom solutions to meet their customers' requirements. Large customers can typically contract out multiple projects to the same vendor which run the projects assigned to them simultaneously. Large providers are increasingly using a distributed/global model for software development (delivery) wherein they can leverage the skills and cost-advantages of different sites to deliver the projects effectively and efficiently, for which project coordination with accurate information is of central importance. In this paper, we highlight the challenges in effective project coordination for a customer across multiple projects with distributed performing centers. This situation is not addressed by current project manage mentor software development tools. The nature of the problem requires standardized metadata, schedule and resource management, and measurements for project monitoring. We propose a framework for global project management and governance to addresses the identified challenges which also takes current project tools into perspective. Our approach extensively leverages historical data gathered on metrics pertaining to different types of work requests."
3067,"People have a tendency to mimic the actions of the larger group. Herd behavior is one of the most common phenomena in our daily life. Previous studies criticized that homogeneity of herd behavior and blind conformity will cause the bubble to burst. Crowd psychology plays an important role in service marketing. However, is it possible for service operation to thrive in service industry by manipulating the herd behavior? In this study, we proposed and validated that the Cascade-based service innovation (CSI) design which utilizes the attraction effect that highlights the service value and meaning interpretation enable service innovation opportunities. The cascade-based innovation design uses service meaning interpretation as the decoy and applies the attraction effect to drive the cascade-based service innovations. The result indicates that the CSI design could facilitate service providers to manipulate and reshape their service design to achieve sustainable development of service marketing. This study applied hybrid research methodologies that combine empirical research to examine the CSI design for fads operation and qualitative case study, and explore the criteria that enables service providers to operate and manipulate fads phenomenon successfully."
3068,"With the increasing acceptance of cloud data center and virtualization technology by enterprises and industries, the security concern becomes the key hindrance to the development and deployment of cloud computing. Security auditing is a good way to deal with the threats faced by a cloud data center. But traditional auditing is no longer suitable for the new cloud environment. In this paper, we design, implement and evaluate the CDCAS, a novel cloud data center auditing system, which matches the demand of the scalability and efficiency of a cloud data center. In this system, we design one distributed and autonomous agent model which can be controlled by a set of rules dynamically generated to fit its use scenario. We then build the log analysis model which uses the signature based method and correlative analysis algorithm to extract security events from collected log with agreeable false positives. We evaluate our system both on real world and simulation to validate its efficiency. And our system is also deployed by the cloud data center of a well-known financial institution, and performs well."
3069,"In service-oriented software development, configuring and composing software services for users according to their personalized requirements become increasingly important. In our previous work, we have proposed a meta-model framework to describe personalized requirements involving domain-related services, based on which we can create domain models to be reused in dealing with personalized requirements. Faced with users' specified requirements, how to select and customize domain models to satisfy both their personalized requirements and domain restrictions remains an important issue. Towards this end, in this paper, we adopt a rule reasoning approach to customize domain goal models using the Semantic Web Rule Language (SWRL). Two kinds of SWRL rules are defined for recommending and verifying users' selected goals. These selected goals can be used to find corresponding processes or services to satisfy users' requirements within our framework. Experiments and implemented prototype show the feasibility of the proposed approach."
3070,"Managing security risks on information systems is essential to guarantee their security while handling costs. However, the complexity of risk assessments is greatly increased when data is spread on multiple environments. In this paper we present a security risk assessment model for distributing business processes in a multi-cloud environment. We aim at offering the full power of cloud computing to composite applications while shielding companies from the complexity related to security risk assessments in the Cloud. We also want to give them the capability to automatically generate secure and cost-effective applications across multiple clouds. Our approach is based on existing risk assessment methodologies, while using the industry recognized IT standards."
3071,"Web service composition introduces two research challenges to end-to-end integrity and confidentiality of information flow. First, component services need the ability to selectively read or modify information flows. Second, component web services may or may not be trusted by all participants in the same degree. Existing specifications such as WS-security provide fine-grained signatures and encryption for pair-wise interactions, but insufficient support for end-to-end security properties in open environments. Using an electronic prescription application, we illustrate the need for an enhanced framework for providing end-to-end security properties. We then describe a fine-grained, security framework, called WS-FESec, that leverages WS-security to support flexible preservation of end-to-end integrity and confidentiality in web service compositions. Finally, we discuss WS-FESec's support for the lattice model of secure information flow and show how it can be employed to preserve end-to-end security properties in the electronic prescriptions application."
3072,"Component Business Modeling (CBM) designs a framework for defining non-overlapping, independent, reusable, cost-effective business components or service centers that provide business services. CBM is gaining broad acceptance in today's marketplace. On the other hand, compliance with industry standards is becoming an imperative in today's enterprises. We propose a bottom-up method for deriving business components from industry standards, based on the artifact-centric approach. In a climate of constant and unpredictable change, alignment between service centers and industry standards becomes essential to the success of enterprises."
3073,"Service-Oriented Architecture (SOA) is intended to improve software interoperability by exposing dynamic applications as services. To evaluate the design of services in service-based systems, quality measurements are essential to decide tradeoffs between SOA quality attributes. Current SOA quality metrics pay little attention to service granularity as an important key design feature that impacts other internal SOA quality attributes. In this paper we introduce the structural attribute of service granularity for the analysis of other internal structural software attributes: complexity, cohesion and coupling. Consequently, metrics are proposed for measuring SOA internal attributes using syntax code. These metrics will assist in development of optimal service design by considering appropriate trade-offs. An example case study is included to demonstrate proposed metrics."
3074,"An unprecedented volume of data is being generated in healthcare and life sciences, ranging across medical records, claims, lab data, genomics data, medical images, emerging exogenous data, and knowledge. Much of this data is moving to the cloud. In this paper, we describe examples of how the data from systems of record, exogenous data sources and knowledge sources can be combined at cloud scale and speed to create industry-transforming insights to improve health outcomes. We then describe a cloud architecture and building blocks that enable these solutions, and the compliance aspects that are critical to healthcare solutions. Finally, we outline a realization of this architecture and outline further research topics in this domain."
3075,"In this paper, we propose a process calculus, Context-aware System Calculus (CSC), in order to describe context-aware systems that can maintain context models and carry out some operations, including requesting context-aware services, according to the context rules. Besides, a context-aware system can also communicate with other systems or receive context information from context providers by sharing atoms through channels. We present the syntax and semantics of CSC, and demonstrate several case studies to model context-aware systems with CSC, concentrating especially on human factors in one of the examples. We also show its strong expressive power by encoding Pi-calculus as CSC."
3076,"Performance and scalability of location-based services (LBSs) are crucial to the wide deployment of mobile enterprise systems. Location queries are a fundamental capability of LBSs. Conventional approaches to location query processing have been centered on an object-centric architecture where static and moving objects are processed under a unified framework through an object index or query index. In this paper, we identify and exploit the performance benefit of a location-centric framework by promoting a clean separation of location query processing over static objects from query processing over moving objects. We show the performance benefits of treating locations as first class citizens instead of objects. Concretely, we develop a federated location indexing scheme for processing location queries over static objects and maintain a grid-based object index for processing queries over moving objects. Our experimental results demonstrate that the location-centric framework enables highly efficient processing of different types of location queries in a mobile environment, prevails under all types of query workloads compared to the object-centric approaches, and in some scenarios it requires as low as only 6% of the IOs required by a corresponding object index for query evaluation."
3077,"Mobile Internet and cloud computing are changing the paradigm of conventional business processes in service provision and demand. With the increasing user interaction and data transmission between mobile devices and workflow services, conventional business process pattern with centralized workflow engine is suffering from great challenges in service availability, reliability and user experience due to uncertain and changeable user contexts in mobile environments. In this paper, we propose a new paradigm for process services based on dynamic multiple replicas of process instance to improve the reliability and efficiency of process services for mobile users in dynamic and instable environments. Also, we give both the replication and synchronization algorithm for process instances based on task dependency paths annotated with vector clocks, which aims to maximize the number of available process replicas during the replicating and synchronizing process. Simulation experiments indicate that the proposed system can provide much better availability and efficiency for process operations in mobile computing environments."
3079,"Digital asset management (DAM) has increasing benefits in booming global Internet economy, but it is still a great challenge for providing an effective way to manage, store, ingest, organize and retrieve digital asset. To do it, we present a new digital asset management platform, called DAM-Chain, with Transaction-based Access Control (TBAC) which integrates the distribution ABAC model and the blockchain technology. In this platform, the ABAC provides flexible and diverse authorization mechanisms for digital asset escrowed into blockchain while the blockchain's transactions serve as verifiable and traceable medium of access request procedure. We also present four types of transactions to describe the TBAC access control procedure, and provide the algorithms of these transactions corresponding to subject registration, object escrowing and publication, access request and grant. By maximizing the strengths of both ABAC and blockchain, this platform can support flexible and diverse permission management, as well as verifiable and transparent access authorization process in an open decentralized environment."
3080,"This paper presents our latest work on assisting AMS (application management service) clients or accounts to generate effective cross-skill training plans. Specifically, we aim to answer the following two questions: ""Whom to train?"" and ""What to train?"", both of which are very critical to training plan generation. To achieve this goal, we first analyze a given account's service request data and identify a set of candidate categories (which indicate skills) for each account consultant to be trained upon, then we measure the following three important metrics: the importance of each such category, consultant's resource utilization, and the temporal correlation between such category and the categories that the consultant can already handle. Finally, we present all measurements to the account team allowing it to generate very flexible cross-skill training plans based on various goals. So far, we have applied this tool to a couple of real AMS accounts, and have received some initial yet encouraging feedback."
3081,"In this era of the emerging digitized, mobilized, and cloudified enterprises, the concept of the ""compos able business"" is the most critical piece which ties everything together. The digital enterprise is here, and its prime characteristic is that is essentially detaches and segregates existing businesses and reassembles them according to market demands. Every industry, from transportation to eyewear is up for disruption, and developers are in the forefront of this movement. In turn, these developers are under intense pressure to accelerate time to market. The compos able enterprise approach requires a reconsideration of traditional models of the entire IT organization. These organization and their processes need to be broken up into components that follow certain key design principles such as The Minimal Functions with least Dependencies, portability, Shared Knowledge, Predictable Contracts and Maximized Human Value. The last three bullet points encapsulate the very definition of DevOps [3]. The concept of better integration between Development and Operations is a valuable objective. The goal is to foster measurable incremental cultural change to derive most overall value out of the union of people, process and technology. But the cultural issues, reward models, and risk allocation create obvious barriers in attaining those goals. The common industry belief is to use the compos able enterprise framework to build a platform using the right tools and you will have attained DevOps nirvana. In this paper we will explore valuable lessons learned from our mistakes in tool centric adoption of IT Infrastructure Library (ITIL) [8]. We will also show how we applied those lessons to develop a lightweight compos able/contextual DevOps framework that learns and measure itself to avoid those cultural pitfalls."
3082,"Context modeling using a UML profile facilitates interoperability of web service definitions. While model-driven methodologies are a generally accepted practice for generating web service artifacts such as XSDs and WSDLs, they do not guarantee that the artifacts will be interoperable. To avoid interoperability issues, transformations from logical to physical models must be accomplished with an awareness of implementation context. An example case study based on our experience in using the IEC CIM model demonstrates the concept."
3083,"Existing search services are inadequate for machine-based web searches since it is more difficult for machines to perceive the results of a search. This work presents an approach creating a search service that is tailored towards machine-based web search, while avoiding some of the problems associated with search results that are derived from link-based ranking algorithms. In this approach, the content of search results is enhanced by incorporating additional information which is derived from processing a set of web documents that are relevant to the search phrase. Ranking of the web pages is then based on this derived information. The search result is composed in a manner that is designed to promote relevance and aid immediate comprehension of search results."
3084,"The most important topic about the Web recently is semantic Web services. The current trend of the software development is to reduce the development time, but at the same time increase the functionality, reusability and stability of the building block without worrying about how the service is implemented. As the number of services available on the Web increases, the difficulty for both the service requester and provider to look for the most suitable potential counterpart will also increase. Matchmaking therefore is an important aspect of the Web interactions, which enable both service requester and provider trim down the amount of time to meet the most suitable potential counterpart. We describe how to blend the matchmaking algorithm to the typical Web service architecture."
3085,"Software-as-a-service emerges as a new delivery model for the software development and applications. In multi-tenancy applications, tenants can customize and adjust the corresponding services shared by tenants based on their business requirements with strong elasticity and expansibility. As services do not exist independently, the resolving of service dependencies in practical multi-tenancy applications is of great importance. In this paper, we propose an Extended Dependency-aware Hierarchical Service Model (EDHSM) to visually describe hierarchical services and service dependencies. We employ a directed hypergraph to represent the model and resolve service dependencies. Moreover, we apply the service dependencies resolving to dynamic service deployment in optimal placement of tenants and online service migration based on load monitoring for multi-tenancy applications. So application systems can be built quickly and operated stably at a low maintaining cost."
3086,"Service-based software applications, such as pervasive and ubiquitous ones, are increasingly embedded in our daily lives integrating smart communicating devices. Usually, changes in the execution context of these applications occur unpredictably over time, such as dynamic variations in the availability of the used services and devices, or of the user location and needs. This unpredictable variability in the execution contexts makes impossible to know at design-time the exact conditions under which these applications will be used and the services that will be most suited at a given time. Therefore, the architecture of such applications cannot be fully defined at design-time. These applications must be defined in abstract and flexible ways, allowing incremental composition and dynamic adaptation to their execution context at runtime. In this paper, we present a model-driven approach for designing, developing, executing and managing service-based applications. At design-time, an application is mainly defined by its requirements and goals. The application definition can be extended to add specific functional or non-functional concerns, such as dynamic adaptation, deployment or distribution. At development-time, the application can be automatically and incrementally composed, ensuring its consistency with respect to its definition. At runtime, the application execution is supported and controlled by our runtime environment."
3087,"This paper aims to develop a bargaining model for calculation of individual tariffs for mobile service bundles. The paper first looks shortly at the intrinsic drivers of individual tariffs both from sociological and economic perspectives. It proceeds with a bargaining model for individual tariffs which is centered on user and supplier behaviours. The user, instead of being fully rational, has ""bounded rationality"" and his behaviours are not only subject to economic constraints but also influenced by social needs. Individual tariffs are decided through interactions between the user and the supplier. Game theory is employed to provide structured analyses of the interactions and tariff design. Preliminary results, which are based on a music training service, show that individual tariffs can be beneficial to both the user and the supplier"
3088,"This paper deals with the reduction of the number of comatose servers. The characteristic of such a server is to consume electricity while not delivering useful information services. According to recent studies, up to 30% of the servers (including those in datacenters) are comatose. The existence of these servers lowers the interest in clouds for green computing. Our paper assumes a cloud provider whose services are operated on a minimal number of dedicated servers in a datacenter, we also assume that this provider delivers software as a service (SaaS). In order to reduce the number of dedicated servers that could be comatose, we propose to automatically generate and run auto-tuning tasks that, in servers idle time, will learn to calibrate the execution of the software delivered by the provider. Our proposition follows the goal of delivering auto-tuning as an opportunistic cloud-service. For this purpose we introduce an opportunistic auto-tuning service (QTuning) that exploits servers idle time. The service is based on a theoretical model that includes two computational problems: an exploration and exploitation problem. In the former problem, the goal is to evaluate the performance of a software in different configurations. In the latter, assuming a set of performance results for a given software, the goal is to compute the best configurations to run the software with. Our second contribution is to propose and evaluate solutions for the exploitation problem."
3089,"How to classify and organize the semantic Web services to help users find the services to meet their needs quickly and accurately is a key issue to be solved in the era of service-oriented software engineering. This paper makes full use the characteristics of solid mathematical foundation and stable classification efficiency of naive bayes classification method. It proposes a semantic Web service classification method based on the theory of naive bayes. It elaborates the concrete process of how to use the three stages of bayesian classification to classify the semantic Web services in the consideration of service interface and execution capacity. The information gain theory is used to determine the classification influence of different features. Finally, the experiments are used to validate the proposed methods."
3090,"This paper addresses the problem of flexible procurement of multiple services with multiple non-functional characteristics, i.e., quality of service attributes. We consider the one-to-many negotiation approach as a flexible method for procuring multiple services by a buyer agent. We address the problem of coordinating multiple concurrent negotiations and propose a novel dynamic negotiation strategy that considers the behaviors of the opponents of the current negotiation encounter in managing the local reservation values of the common negotiation issues (attributes) of different services. Most previous works consider the problem of negotiation over a single issue. We investigate a more complex situation where a buyer agent negotiates over multiple services given that each service has multiple negotiation issues. The experimental results show an evidence for the effectiveness and robustness of our dynamic negotiation strategy in various negotiation environments."
3091,"In a typical web services environment, a web service framework supports the client and server interaction by, among other tasks, announcing the services interfaces and translating application-level service calls to SOAP messages. Although designed to support inter-operation, research and practice suggest that existing client-side and server-side frameworks, many times, cannot fully inter-operate. The problem is that, as web services are increasingly being deployed to support business-critical environments, interoperability issues may prevent or impact business transactions, potentially resulting in huge financial and reputation losses. In this paper we present an experimental evaluation of the interoperability of 1024 publicly available web services, against a set of diverse and well-known client-side web service frameworks. We have detected at least one severe interoperability issue in over 53% of the services tested and quite different inter-operation capabilities regarding the client-side frameworks. Results clearly show that, although providers frequently claim interoperability capabilities, urgent improvements are required."
3092,"Email is a reliable, confidential, fast, free and easily accessible form of communication. Due to its wide use in personal, but most importantly, professional contexts, email represents a valuable source of information that can be harvested for understanding, reengineering and repurposing undocumented business processes of companies and institutions. Few researchers have investigated the problem of extracting and analyzing the process-oriented information contained in emails. In this paper, we go forward in this direction by proposing a new method to discover business process instances from email logs that uses unsupervised classification techniques. The approach is composed of two clustering steps. The first one uses a powerful semantic similarity measurement method, Word2vec, while the second one uses a similarity measure combing several email attributes. Experimental results are detailed to illustrate and prove our approach contributions."
3093,"Radio frequency identification (RFID) technologies can potentially improve the productivity of retailers. In this paper, we propose a role-based, enterprise-level, RFID-oriented privacy authorization model for supporting the privacy policies in utilizing RFID in retail industry."
3094,"Service-oriented Computing (SOC) has become one of the most preferred distributed computing paradigms which utilize services as elements for designing and developing application systems. In SOC, consumers who have different functional and non-functional requirements might consume services offered by various providers. Faced with the increasing number of Web services and the diversity of service consumers' requirements, how to select and customize service models to satisfy consumers' personalized requirements remains an important issue. To this end, we adopt a model-driven development approach to customize services based on service feature modeling (SFM). Since SFM builds upon feature modeling (FM), it inherits the advantages of the latter. Furthermore, it takes into account service consumers' non-functional requirements with the attributes and attributes type that are not considered in the general feature model technique. A prototype shows the feasibility of the proposed approach."
3095,Humans create efficient social structures in a self-organized way based on a feature called homophily. This paper proposes the use of homophily in Service-Oriented Multiagent Systems to create efficient self-organized structures and provide a decentralized service management.
3096,"Automatically constructing a composite service from a set of basic services is not desirable in practice because the composition assumption often over-simplifies the realistic constraints and states. We try to propose a stepwise methodology to step-by-step construct a composite service, where the event-driven principle and process declaration principle are used to realize process flexibility, and user knowledge aggregation way is adopted to clarify users' demands and intents. For modeling services with flexibility, service decoupling feature based on events is utilized to decouple different parts of a business process, and to independently define process fragments. Process coordination logic among different process fragments is extracted as independent building blocks such that its enactment can be adapted at run-time. Thus, we get a method to model, deploy and run declarative business processes based on events. At run-time, we propose a service construction assistant to learn the complex relation between user clicks and service quality."
3097,"Service discovery of state dependent services has to take workflow aspects into account. To increase the usability of a service discovery, the result list of services should be ordered with regard to the relevance of the services. Means of ordering a list of workflows due to their similarity with regard to a query are missing. This paper presents a pilot of an empirical study on the influence of different measures on workflow similarity. It turns out that, although preliminary, relations between different measures are indicated and that a similarity definition depends on the application scenario in which the service discovery is applied"
3098,"Service-oriented architecture (SOA) is making strides into every industry, and is providing consumer centric approach to the end-to-end solution. In the business world, acquisitions and mergers are very common. Whenever an acquisition or merger happens, it is required to consolidate the business assets. It is possible to use service oriented architecture (SOA) to arrive at an excellent framework for integrating and consolidating these business assets (which typically include business processes and the various software packages). We have designed and built a software framework for integrating various tools used within Global Delivery India Center"
3099,"Microservices is a flexible architectural style that has many advantages over the alternative monolithic style. These include better performance and scalability. It is particularly suitable, and widely adopted, for cloud-based applications, because in this architecture a software system consisting of a large suite of services of fine granularity, each running its own process and communicating with the others. However, programming such systems is more complex. In this paper we report on CIDE, an integrated software development environment that helps with this. CIDE supports programming in a novel agent-oriented language called CAOPLE and tests their execution in a cluster environment. We present the architecture of CIDE, discuss its design based on the principles of the DevOps software development methodology, and describe facilities that support continuous testing and seamless integration, two other advantages of Microservices."
3100,"We present a Web-services framework for publishing, browsing, and analyzing real-time sensor data. Existing work, based on database technologies, assumes a known set of sensors, deployed for a specific application, intended for a group of users who are aware of the application. In contrast, our work provides a new environment for publishing real-time sensor data, and sharing sensor-based applications with unsolicited users over the Internet. Providers register their sensors with a UDDI registry where any user can discover them over the Internet. Once the desired sensors are located, the user can employ two new communication protocols, namely SSCP and SSTP, to control the incoming sensor streams. As the data arrive at the user browser software, a corresponding plug-in such as a data animation module is activated to allow the user to ""see"" the data stream presented in an intelligible way."
3101,"As more and more software systems are used in our day-to-day life from traveling, shopping to consuming health and financial services, our lives and habits ar e becoming highly dependent on the trustworthiness of these systems. It is important for such systems to operate continuously while satisfying users' functional, non-functional (such as performance, availability, and other QoS attributes), and trust (i.e., the degree of compliance of a system to its specification) requirements. However, it is a challenge to design such systems that are specially able to self-adapt to changes in its environments (which we refer as context) in real-time. In this paper, we propose a Bayesian network-based framework to help the development of such self-adaptive software in distributed system domain while ensuring a continuous satisfaction of the QoS and trust values of the systems. We have applied the proposed framework to a case study and the results show the effectiveness of the framework in designing QoS and trust based self-adaptive systems."
3102,"This paper introduces SOAR, a service-oriented architecture for the real-estate industry that embeds trust and security, allows for formal correctness proofs of service interactions, and systematically addresses human interaction capabilities through Web-based user access to services. We demonstrate the features of SOAR through a DealMaker service that helps buyers and sellers semi-automate the various steps in a real-estate transaction. This service is a composed service, with message-based interactions specified in SSDL, the SOAP service description language. The implemented embedded trust and security solution deals with the usual privacy and authorization issues, but also establishes trust in ownership and other claims of participants. We also demonstrate how formal techniques can proof correctness of the service interaction protocol specified in SSDL. From an implementation perspective, a main new contribution is a protocol engine for SSDL"
3103,"This paper presents COLQUIDE an environment for building WEB portals out of existing autonomous and distributed services accessible through Internet. The system provides tools for selecting services with specific functional requirements and for specifying the way their execution must be synchronized within a portal. A portal is a n-tier information system accessible through the WEB, that synchronises transparently the execution (data exchange and communication) of services with user interaction and thereby implements a value added service."
3104,"Various organizations face an explosive growth of data that must be protected and backed up. This challenge is made more difficult by the movement from stand-alone server backup to backup over the local area network (LAN) and by the need to automatically manage multiple backup servers efficiently for concurrent backup jobs. In this paper, we present a novel software approach to backup service, where application servers provide their unused resources to participate in a virtual backup environment. We implement a prototype with the following desirable features: single-instance storage to save storage and communication costs, data replication to enhance data availability, data redirection to avoid job resubmission, and intelligent scheduling to achieve workload balance. The experimental study shows that our approach is promising."
3105,"Customer contact centers have to regularly and proactively evaluate and adapt their customer handling processes to support the business vision of their clients and to maintain a competitive differentiating edge. This work presents the contact collector tool developed at IBM that allows the contact center supervisors to record and analyze the behavior of various agent customer handling processes. The contact collector tool generates quantifiable insights to improve operational strengths and productivity as well as to enhance the quality of the overall interaction. The present work also discusses the advantages of automating the contact collector tool, technical challenges encountered in the automation process so far and performance of the automated components of the tool."
3106,"This paper proposes a model driven approach whereby a composite service specification is generated automatically from a simple declarative definition. It is based on three concepts: services, virtual enterprises and business rules. Using an e-travel application as a running example we explain in detail the steps of the generation process. Particularly, we show how business rules can be used for creating on the fly the control flow relating the services constituting the composite service"
3107,"During the last years we have seen a dramatic increase of new Cloud providers, applications, services, management platforms, data, etc. reaching a level of complexity that implies the necessity of new solutions to deal with such vast, shared and heterogeneous services and resources. Consequently, challenges often related to interoperability, portability, security, discovery, selection, negotiation and description of cloud service and resource may take place. In this sense, Semantic Web Technologies, holding a great potential to cloud computing, have been proven as an efficient means to relive these challenges. This paper examines and explores the role of Semantic Web Technologies in the cloud from a wide variety of literatures. Various approaches, architectures, and frameworks are screened and evaluated based on eight prime research questions. At the end of the review, research opportunities in the form of a roadmap are discussed."
3108,"Innovation in services has become a topic of interest to researchers due to the worldwide shift to ""services"" economics. This comes from the growth of services economies and the shift to services businesses by manufacturing industries, including IT-related industries. This growth of services encourages us to study basic business system changes from product-based to service economies and to define the fundamental challenges to accelerate service innovations. These interdisciplinary activities are called services sciences, management, and engineering (SSME), which includes computer science, operations research, industrial engineering, business strategy, management sciences, social and cognitive sciences, legal sciences, and innovation management. Innovations, in particular, service innovations are difficult to articulate their structures and mechanisms, due to intangibility coming from services characteristics, and a lack of languages to describe them. In this paper, we investigate 1. How can we capture the characteristics of innovations as patterns? 2. What are the categories of patterns for innovations? We introduce innovation pattern categories and patterns from enterprise viewpoints, and apply them to innovation cases. We identify future study areas, such as innovation ecosystem modeling, and innovation lifecycle studies using innovation patterns."
3109,In this paper we present a generic public administration (PA) domain ontology. We define a formal model for a public administration service on the basis of the Web service modeling ontology (WSMO). For this purpose we employ the generic public service object model of the governance enterprise architecture (GEA) providing PA domain specific semantics. We describe the ontology using the Web service modeling language (WSML). This domain ontology is implemented in order to be used in semantic Web services architecture for e-government.
3110,"Enterprise modeling using data dependencies is common in monitoring and business performance management systems. The modern enterprise is a dynamic creature, constantly adapting itself to the changing environment. This adaptation may result in changes in enterprise components and data dependencies between them. An enterprise model must be able to express this dynamism, and business performance management services must be able to react accordingly. In this paper, we briefly introduce ADI (active dependency integration technology), a language for modeling data dependencies between entities. We discuss developments related to support in modeling dynamic environments, where elements may be added or deleted. Dynamism-related developments include the support of automatic dependency instantiation from an abstract dependency. The abstract dependency expresses a general pattern in the ontology, functioning as a template for dependency instances. Another aspect of dynamism is support for changes in existing dependencies rather than only creating new dependencies; for example, adding a new entity to a dependency. Changes in topology do not imply system redeployment. ADI also supports the influence of dynamism on data items and subsequent propagation of this influence through the model."
3111,"Managing service quality in heterogeneous Cloud environments is complex: different Cloud providers expose different management interfaces. To manage Service Level Agreements (SLAs) in this context, we have developed the rSLA framework that enables fast setup of SLA monitoring in dynamic and heterogeneous Cloud environments. The rSLA framework is made up of three main components: the rSLA language to formally represent SLAs, the rSLA Service, which interprets the SLAs and implements the behavior specified in them, and a set of Xlets - lightweight, dynamically bound adapters to monitoring and controlling interfaces. In this paper, we present the rSLA framework, and describe how it enables the monitoring and enforcement of service level agreements for heterogeneous Cloud services."
3112,"Parallel execution and cloud technologies are the keys to speed-up service invocation when processing large-scale data. In SOA, service providers normally employ policies to limit parallel execution of the services based on arbitrary decisions. In order to attain optimal performance improvement, service users need to adapt to parallel execution policies of the services. A composite service is a combination of several atomic services provided by various providers. To use parallel execution for greater composite service efficiency we need to optimize the degree of parallelism (DOP) of the composite services by considering policies of all atomic services. We propose a model that embeds service policies into formulae to calculate composite service performance. From the calculation, we predict the optimal DOP for the composite service. Extensive experiments are conducted on real-world translation services. The results show that our proposed model has good prediction accuracy in identifying the optimal DOPs. Our model correctly predicts the optimal DOP in most cases."
3113,"Social networking sites (SNSs), with their large number of users and large information base, seem to be the perfect breeding ground for exploiting the vulnerabilities of people, who are considered the weakest link in security. Deceiving, persuading, or influencing people to provide information or to perform an action that will benefit the attacker is known as ""social engineering."" Fraudulent and deceptive people use social engineering traps and tactics through SNSs to trick users into obeying them, accepting threats, and falling victim to various crimes such as phishing, sexual abuse, financial abuse, identity theft, and physical crime. Although organizations, researchers, and practitioners recognize the serious risks of social engineering, there is a severe lack of understanding and control of such threats. This may be partly due to the complexity of human behaviors in approaching, accepting, and failing to recognize social engineering tricks. This research aims to investigate the impact of source characteristics on users' susceptibility to social engineering victimization in SNSs, particularly Facebook. Using grounded theory method, we develop a model that explains what and how source characteristics influence Facebook users to judge the attacker as credible."
3114,"Scheduling is an important topic to support data security for workflow execution in clouds. Some workflow scheduling algorithms use security services such as authentication, integrity verification, and encryption for all workflow tasks. However, applying security services to no sensitive data does not make sense as no benefit is gained, yet it increases the makespan and monetary costs. In this paper, we introduce five policies for selection of tasks that handle sensitive data. We also propose a workflow scheduling algorithm based on a multi-populational genetic algorithm for minimizing cost while meeting a deadline. Experiments using four workflow applications show that our proposal can minimize both the makespan and cost, while maintaining the security of sensitive data compared to another approach in the literature."
3115,"The effectiveness of decision-making in agriculture domain can be improved by integrating current local environmental conditions with agricultural information system (AIS). To achieve this objective a series of functional blocks is proposed; which involves collection of environmental parameters from the farm, merging data from multiple sensors, communicating the collected data to the application server, extracting required information, defining ETL process, integration with ancillary data, analysis and presentation services. Web service based architecture comprising of distributed components is proposed to support the computational need. The proposed architecture extends analytical capabilities of end users with the help of multi dimensional expressions and OLAP."
3116,"In this paper, we present a comprehensive study on the threats towards the coordination services for Web services business activities and explore the most optimal solution to mitigate such threats. A careful analysis of the state model of the coordination services reveals that it is sufficient to use a lightweight Byzantine fault tolerance algorithm that avoids performing expensive total ordering of all request messages. The algorithm and the associated mechanisms have been incorporated into an open-source framework implementing the standard Web services business activity specification and an extension protocol that enables the separation of the coordination tasks from the business logic. The performance evaluation results obtained using the working prototype confirm the optimality of our solution."
3117,"With the possibility of system crashes and network failures, the design of robust client/server interactions for collaborative process execution is a challenge. If a business process changes state, it sends messages to relevant processes to inform about this change. However, server crashes and network failures may result in loss of messages. In this case, the state change is performed by one party, resulting in state/behavior inconsistencies and possibly deadlocks. Our basic idea to solve this problem is to cache the response (in a synchronous interaction) if the state of the process instance has changed by the request message. The possible state inconsistencies are recognized and compensated by state-caching and retrying failed interactions. By doing this work, we have learnt the possible failures caused by system crashes and network failures. Our results make it possible to build robust interactions by using cached-based process transformations."
3118,"Stochastic reliability analysis of composite services is challenging, primarily since it needs us to carefully balance accuracy of analysis and its computational complexity: Given stochastic models of service components, we often combine them and define a large complex model to accurately reflect the effects of failures of particular components on the reliability of the entire service. In this paper, we propose a new technique, based on the Markov reward model (MRM) foundation, to substantially reduce the computational complexity without losing accuracy. It evaluates, prior to analysis, the effects of the possible failures and represents them as scalar reward values attached to a single compact Markov model. Thus we can replace the component models with a compact model that retains the complete information for accurate analysis. We demonstrate the effectiveness of this technique for several cases, where failures are correlated with each other in different ways."
3119,"Service oriented architecture approach in general and the Web services technology in particular play significant role in modern collaborative environments. However, it is not enough to have the business functionality of the partners packaged as (Web) services; there is also a need for business-aligned order of interaction between these services (business protocols). Furthermore, it is necessary to guarantee that these protocols are enacted in compliance with the effective policies and regulations. This paper discusses business protocol compliance issues and suggests some techniques for enhancement of business protocols for better compliance. Such enhancements, for example, can support the proposed structured proof of compliance concept and its construction mechanism, which together address the issues of both correct course of collaboration at its critical steps and existence of a tangible proof of correctness of the whole collaborative interaction. Such proof, consisting of individually signed and time-stamped evidence statements, can serve for various audit purposes and, if necessary, in the court of law."
3120,"Incorporating code mobility to inter-operable internet enabled software applications improves efficiency, performance and network traffic reduction. Present mobile agent technology fails to leverage the interoperable web infrastructure developed in a standard compliant manner. Here we describe a development environment and technology that fuse Workflow, Web 2.0, SOA and WS-BPEL and create a distributed computing environment (ACtive E-commerce Framework called ACEF) that permit creation of inter operable, infrastructure leveraging migratable code for design of active internet applications."
3121,"Attempting to reduce the complexity of applying constraints to service deployment and guarantee its correctness, we propose a declarative constraint solution which includes three distinctive components: constraint pattern definition component, pattern-based constraint description component, and pattern-based violation analysis component. Correspondingly, a supporting tool for this solution is implemented and integrated with the deployment topology plan tool."
3122,"In this paper, we present a process-oriented tool allowing the specification of security properties at the service composition level. The tool is based on the notions of abstract and concrete services as well as on the concept of separation of concerns. It provides a framework that allows different people to effectively discuss security issues. Abstract services can be viewed as activities rather than as technical services and are such better understood by non-technical people. Similarly, security is discussed in terms of needs and no complex security technologies are to be specified. The tool relies between these two meta-models specifying orchestration-related concepts and security concepts. Meta-links between the meta-models have been defined to specify the authorized security constraints on the orchestrated services. The tool has been validated on an application specified by Thales."
3123,"IT outsourcing enables companies to contract out IT services, such as infrastructure and application management to external providers. IT services delivery relies on knowledge that is in the collective possession of application and infrastructure specialists. With recent advances in harnessing the expertise of network-connected humans, services businesses have also started to seek strategies to efficiently utilize the collective knowledge of their employees. In this paper, we present the application of collective intelligence to three different service types: 1) automation (translation solution), 2) infrastructure management (asset inventory discovery) and 3) application management (software development). We extrapolate a set of salient properties (i.e., input, output, and size of the collective input) as the key elements for employing collective intelligence within the services business. We discuss some of the differences amongst the disparate collective intelligence services and present the resulting distinctive properties as challenges that may inspire further research in services business."
3124,"QoS-aware service composition is a bi-objectivetask for the generation of a business process: to fulfill functional goals and to optimize the QoS criteria. Planning algorithms are frequently used for the generation of a business process to achieve functional goals. In this paper, we use a planning algorithm, GraphPlan, and a graph search algorithm, Dijkstra's algorithm, to achieve both functional goals and QoS optimization at the same time. Firstly, we analyze graph reachability in the planning graph built by Graphplan algorithm.Taking advantage of graph reachability, we propose an approach of using Graphplan technique combined with Dijkstra's algorithm to solve QoS-aware service composition problem. The experiments show our approach is able to findthe optimal solution for different QoS criteria. Moreover, our approach reduces the possibilities of combinatorial explosion to a large degree when exploring the graph for the optimal path."
3125,Accurately predicting a user's rating to a service is a challenging task in the presence of malicious users that manipulate the ratings to services. Many existing service rating systems lack the ability that counter the manipulation of rating systems. This paper proposed an artificial neural network (ANN) based service rating scheme that counters the manipulation of service ratings. The scheme takes into account of both similarity-based rating and the ratings given by representative users when predicting a user's rating to a service. Some experiments were carried out to compare the prediction accuracy of the proposed scheme with a well-known existing scheme WSRec [26]. The results show that the proposed scheme provides more accurate rating predictions in the presence of a large amount of malicious users.
3126,"Recommending the suitable Web service is an important topic in today's society. The critical step is to accurately predict QoS of Web services. However, the highly sparse QoS data complicate the challenges. In the real world, since QoS delivery can be significantly affected by some dominant factors in the service environment (e.g., network delay and the location of user or service), Web services which are published by the same provider usually have the similar fundamental network environment. These factors can be leveraged for accurate QoS predictions, leading to high-quality service recommendations. In this paper, we expound how Latent Factor Models (LFM) can be utilized to predict the unknown QoS values. Meanwhile, we take the factors of provider and its country into consideration, which imply the latent physical location and network status information, as the latent neighbor for the set of Web services. Hence, the novel neighbor factor model is built to evaluate the personalized connection quality of latent neighbors for each service user. Then, we propose an integrated model based on LFM. Finally, we conduct a group of experiments on a large-scale real-world QoS dataset and the results demonstrate that our approach is effective, especially in the situation of data sparsity."
3127,"Workflow techniques have been widely used as a major computing solution in many science domains. With the rapid deployment of cloud infrastructures around the globe and the economic benefit of cloud-based computing and storage services, an increasing number of scientific workflows have been shifted or are in active transition to clouds. As the scale of scientific applications continues to grow, it is now common to deploy data-and network-intensive computing workflows in multi-cloud environments, where inter-cloud data transfer oftentimes plays a significant role in both workflow performance and financial cost. We construct rigorous mathematical models to analyze the intra-and inter-cloud execution process of scientific workflows and formulate a budget-constrained workflow mapping problem to optimize the network performance of scientific workflows in multi-cloud environments. We show the proposed problem to be NP-complete and design a heuristic solution that takes into consideration module execution, data transfer, and I/O operations. The performance superiority of the proposed solution over existing methods is illustrated through extensive simulations."
3128,"Traditionally, product models often have a rigid nature, both with respect to the manner in which they are initially tailored to clients, and to the way they are maintained over time. Especially when such products are offered at a highly interactive medium such as Internet in the form of Web services, addressing such aspects might be a necessity to obtain a competitive advantage. To cope with these rigidity problems, this paper proposes a dynamic approach to product models, which supports an ongoing interaction process that continuously adapts a product to the background and desires of the client. The generic approach has been formalised and tested in a case study in the domain of car insurances."
3129,In this paper we give graph-semantics to a fundamental part of the semantics of the service modeling language SRML: business configurations. To achieve this goal we use symbolic graph transformation systems. We formalize the semantics using this graph transformation system and illustrating it with a simple running example of a trip booking agent.
3130,"The cost of recovery protocols is important with respect to system performance during normal operation and failure in terms of overhead, and time taken to recover failed transactions. The cost of recovery protocols for Web database systems has not been addressed much. In this paper, we present a quantitative study of cost of recovery protocols. For this purpose, we use an experiment setup to evaluate the performance of two recovery algorithms, namely the, two-phase commit algorithm and log-based algorithm. Our work is a step towards building reliable protocols for Web database systems."
3131,"Despite the importance for expressing location mobility and dynamic composition, channel passing has almost been ignored in the formal work on Web service composition. One important problem here is to ensure that each service in a composition can always get sufficient and correct channels for completing their collaborative work. To support reasoning and verification of those properties of service compositions, in this paper we propose a pair of formal languages that support channel passing on both global and local levels, together with their semantics. Also we present a top-down design methodology that generates local-level processes from global description, and prove that the generated processes must be deadlock-free under the sufficient conditions we proposed."
3132,"In the field of data mining, most of necessary resources, such as datasets, computing resources, might be distributed in different places of the world, which impedes scientists to do research successfully. Service science advocates every available resource as a service and focuses on their collaboration to settle a problem. Here, we propose a service-based architecture for data mining applications, including configuration service, service engine, monitor service, analysis service, visualization service, computing service and data &amp; algorithm provision service. The first 5 services focus on 5 specific aspects of data mining requirements. A demo system has also been developed to validate the feasibility of our architecture."
3133,"Microservices is an architectural style inspired by service-oriented computing that structures an application as a collection of cohesive and loosely coupled components, which implement business capabilities. One of today's problems in designing microservice architectures is to decompose a system into cohesive, loosely coupled, and fine-grained microservices. Identification of microservices is usually performed intuitively, based on the experience of the system designers, however, if the functionalities of a system are highly interconnected, it is a challenging task to decompose the system into appropriate microservices. To tackle this challenge, we present a microservice identification method that decomposes a system using clustering technique. To this end, we model a system as a set of business processes and take two aspects of structural dependency and data object dependency of functionalities into account. Furthermore, we conduct a study to evaluate the effect of process characteristics on the accuracy of identification approaches."
3134,"Modeling is an important and time consuming part of the business process management life-cycle. An analyst reviews existing documentation and queries relevant domain experts to construct both mental and concrete models of the domain. To aid this exercise, we propose the Rapid Business Process Discovery (R-BPD) framework and prototype tool that can query heterogeneous information resources (e.g. corporate documentation, web-content, code e.t.c.) and rapidly constructproto-models to be incrementally adjusted to correctness by an analyst. This constitutes a departure from building and constructing models toward just editing them. We believe this rapid mixed-initiative modeling will increase analyst productivity by significant orders of magnitude over traditional approaches. Furthermore, the possibility of using the approach in distributed and real-time settings seems appealing and may help in significantly improving the quality of the models being developed w.r.t. being consistent, complete, and concise."
3135,"Business Processes naturally involve long running activities and require transactional behaviour across them. The work presented in this paper is a proposal for a novel autonomous failure handling mechanism for long running nested transactions (LRT) and forms part of a general management and compensation model for long running transactions in workflows. The mechanism is based on propagation of failures through a recursive hierarchical structure of transaction components (nodes and execution paths). The management system of transactions (COMPMOD) is implemented as a reactive system controller, where system components change their states based on rules in response to triggering of events such as activation, failure, force-fail, completion, or compensation events. A notable new feature of the model is the distinction of vital and non-vital components, allowing the process designer to express the cruciality of activities in the workflow with respect to the business logic."
3136,"Recovery is the last resort when other components exhibit bugs. It is therefore of paramount importance that the correctness of the recovery protocols be formally verified. Recovery not only needs to cope with database failures but should handle and ideally mask message and process failures in clients and servers. Otherwise, when a reply message is lost the application must be able to determine ""manually"" whether the interaction is to be repeated. This paper develops a statechart specification of a recovery framework that generically guarantees exactly-once execution and applies model checking to prove its correctness."
3137,"Service-oriented Architecture (SOA) for sensor network applications aims at providing composable sensor network services supporting functionality within a specific application domain together with tools for service composition, so more complex functionalities can be composed of component services. In a distributed environment, such a scheme works by having a given component service choose other components that provide the data that it needs to perform its service. In this paper, we propose to use real options theory for selecting component services. Real options are designed to reduce the risk associated with an investment by delaying the investment decision for a certain period of time or by allowing for the substitutions of initial investment. Thus, they enhance managerial flexibility and add to the overall value of a project but at the same time they incur certain costs. It is natural to think about activated services as investments, and we apply the switch options subset of the real options methodology to manage the risks of high cost that may result from the low reliability of sensors and sensor networks. Furthermore, we compare our approach with several service selection methods and show the advantage of the option-based methodology."
3138,"Web bot fraud activity currently accounts for a large number of web accesses. Current resistance methods such as CAPTCHA are not applicable for bot detection at the granularity of each click. In this paper, we propose a service that counters web bots which mimic human clicks by walking random links. We base our defense on systematically applying link obfuscation. The obfuscation is designed as a service that can be applied to websites without changes from web developers and without changing the behavior of human users. The service for resisting web bots is called Decoy Link Design Adaptation (DLDA) and works by transparently modifying every page of a protected website. The modifications are made such that walking web bots cannot traverse valid paths through the website. Specifically, DLDA modifies each original link on the page surrounding it with a group of invalid links. These obsfucated links are carefully styled to be unnoticed or avoided by human users; however, they require significant effort for programs (bots) to identify. Experiments show that DLDA has a very high detection rate for web bots and near zero false positives. DLDA can detect 80% of walking bots ending a session after one minute of inactivity (no clicks). The detection rate increases to 100% when the session is ended where multiple visits of the bots can be grouped into a single session."
3139,"Web services have been extended to give value-added customized services to users through service composition. Service Composition consists of four stages: planning, discovery, selection, and execution. Web service discovery and selection are becoming challenging and time-consuming tasks due to large number of Web services available on the internet. Organizing Web services into clusters of similar services to aid the pruning of the query space is one of the solutions for the issues. Web services can be clustered into similar groups by considering functional attributes to increase the performance of the service discovery. In selection stage, algorithm has to select a single service among the large number of functional equal services by considering the Quality of Service (QoS) values. In this paper, we propose QoS aware service clustering approach to increase the performance of the service selection. Here, service clusters are created using functionality as the first factor, then QoS properties being considered as secondary factors. Our approach considers QoS profit values to compute the QoS similarity. In this paper, we apply a spatial clustering technique called the Spherical Associated Keyword Space which is projected clustering result from a three-dimensional sphere to a two dimensional (2D) spherical surface for 2D visualization. Empirical study of our approach has proved the effectiveness of clustering results."
3140,"The service-oriented architecture (SOA) has become today's reference architecture for modern distributed systems. As SOA concepts and technologies become more and more widespread and the number of services in operation within enterprises increases, the problem of managing these services becomes manifest. One of the most pressing needs we hear from customers is the ability to ""discover"", within a maze of services each offering functionality to (and in turn using functionality offered by) other services, which are the actual dependencies between such services. Understanding dependencies is essential to performing two functions: impact analysis (understanding which other services are affected when a service becomes unavailable) and service-level root-cause analysis (which is the opposite problem: under-standing the causes of a service failure by looking at the other services it relies on). Discovering dependencies is essential as the hope that the enterprise maintains documentation that describe these dependencies (on top of a complex maze and evolving implementations) is vane. Hence, we have to look for dependencies by observing and analyzing the interactions among services. In this paper we identify the importance of the problem of discovering dynamic dependencies among Web services and we propose a solution for the automatic identification of traces of dependent messages, based on the correlation of messages exchanged among services. We also discuss our lessons learned and results from applying the techniques to data related to HP processes and services."
3141,"The global objective of this study is to propose solutions for improving the quality of online search services. We consider the special case where the processing of requests submitted to such services consists of: the querying of several types of sub-services followed by the composition of the output produced the sub-services. This could be the case for instance of a booking service that proposes travel packs including: hotel reservation, car rental and flight booking. We propose to improve the quality of such services with the multi-selection problem. The goal in this problem is to select the subset of sub-services of each type to use in the the processing of search request. The selection must ensure that we maximize the quality of the results we could expect from the search. The multi-selection problem is close to the service selection problem. However, while in the latter, we are interested in a unique sub-service per type, in the former, we want to choose a subset of sub-services. Our paper introduces a theoretical formulation of the problem and demonstrates its NP-hardness. We also propose two approaches for the resolution. The first approach is based on Integer Linear Programming. The second approach combines parallel algorithm portfolio and sampling techniques. Finally, we make a comparative evaluation of the approaches based on practical scenarios."
3142,"Large organizations often have multiple branches situated in different locations, each branch may collaborate and learn from other branches' experience. Their Business processes (BPs) share often similar business goals and are slightly different. These branches are eager to develop new process variants to satisfy new requirements. Process execution logs, so called process event logs, can be used to analyze requirement changing situations and efficiently develop BP variants. However, these logs often have heterogeneous data-sources which prevent an easy and dynamic interoperability between different branches. In this paper, we propose a semantic framework tackling this heterogeneity issue. This framework promotes the creation of a semantic knowledge base from process event logs. Using this knowledge base, we offer BP designers the means to discover suitable BP fragments to assist process variant modeling. We performed experiments on a large public dataset and experimental results show that our approach is feasible and accurate in realistic situations."
3144,"In order to adapt to the inconstancy of business environments and service requirements, the framework of service-oriented product line development is proposed, which is used to analyze, verify and implement dynamic processes and process changes automatically. In this paper, we present a logical feature model which can model the variability of requirements, and introduce variant points in business process templates defined by petri nets to model dynamic processes. Then, we map compound features in a logical feature model to variant points in a template through propositional logics. At last, the configured process can be verified based on petri net theory."
3145,"With the rapid development of mobile internet and wireless network technologies, more and more people use the mobile app to call a taxicab to pick them up. Therefore, understanding the passengers' travel demand becomes crucial to improve the utilization of the taxicabs and reduce their cost. In this paper, based on spatio-temporal clustering, we propose a demand hotspots prediction framework to generate recommendation for taxi drivers. Specially, an adaptive prediction approach is presented to demand hotspots and their hotness, and then combing the driver's location and the hotness, top candidates are recommended and visually presented to drivers. Based on the dataset provided by CAR INC., the experiment shows that our approach gains a significant improvement in hotspots prediction and recommendation, with 15.21% improvement on average f-measure for prediction and 79.6% hit ratio for recommendation."
3146,"The elasticity of cloud computing is able to help Cloud Application Providers (CAPs) adjust the number of rented virtual machines (VMs) for cloud applications according to actual varying demands while enforcing SLAs. In this paper, we design a framework called AERS (Autonomic and Elastic Resource Scheduling Framework). AERS makes full use of the profits of both proactive controllers and reactive controllers in the field of dynamic resource provision. It can not only adjust the number of available VMs for cloud applications with fluctuation of workloads but also react fast when cloud applications break SLAs. In addition, We build a model for the availability of cloud applications. Based on this model, we propose an availability-aware and communication overhead optimized placement strategy integrated in AERS to help CAPs choose proper availability zones for launching VMs. Experiments on Openstack show that AERS is able to provide VMs elastically while enforcing SLAs."
3147,"Web Services Description Language (WSDL) allows a structured way to standardize the description of Web Services, exploiting XML for the exchange of structured information. Nevertheless XML supports little interoperability between services, expected when WSDL documents have to be combined. In this context, the Semantic Web has become a promising research field. Semantic Models (e.g. RDF or OWL) allow knowledge from many different sources to be easily combined so that unexpected data connections can be used. In this paper we provide a description of WSDL documents into a metamodel representation of Semantic Models. It allows interoperability at different levels of abstraction. A Datalog rule based system queries both service description and associated semantic annotations. Such system is built on top of traditional database technology to guarantee an efficient management of Semantic Web Services Descriptions. Finally we present experiments on a real dataset to test the feasibility of the modeling approach."
3148,"Performance, in terms of high quality of service and resource utilization for example, is one of top considerations for cloud applications. However, the response time of most multi-tier applications today frequently present large scale fluctuations (e.g., Ranging from tens of milliseconds up to tens of seconds) during periods of high resource utilization. It is important to find the reasons that cause response time fluctuations when providing good performance and high effective multi-tier systems in cloud environments. In this paper, through extensive measurements of a multi-tier application benchmark (RUBiS), we show that response time fluctuations is real and average response time is not a right measure of multi-tier services system's performance. Through making a probing analysis of requests we find that the large scale response time fluctuations can be caused by concurrent long or mix transactions and evaluate the reason that we find is right. We also propose an effective scheduling policy called CTP(cross-tier-proportion) to smooth response time fluctuations while still achieving high resource utilization in the system."
3149,"Recent studies have illustrated historical financial data could be used to predict future revenues and profits. Prediction models would be accurate when long-run data that traces back for multiple years is available. However, changes in service structures often result in alteration of the nomenclatures of the services, making the streams of financial transactions associated with affected services discontinue. Manually inquiring the history of changes can be tedious and unsuccessful especially in large companies. In this paper, we propose a machine learning based algorithm for automatically discovering service name replacements. The proposed methodology draws heterogeneous features from financial data available in most ledger databases, and hence is generalizable. Our proposed methodology is shown to be effective on ground-truth synthesized data generated from real-world IBM service delivery ledger database."
3150,"We present a value-driven approach capable of determining the global optimization objective of service composition to satisfy high-level value expectations. In traditional service composition methods, global and local QoS requirements and constraints are directly raised by customers, then a set of service components (web services) are selected to achieve the optimized QoS. Yet in practice, there is no mechanism helping customers accurately and comprehensively determine their QoS desires. In our approach, a customer's initial expectations of high-level global values are first put forward, and then five types of dependencies between service values are analyzed. Because values are realized under the facilitation of specific service components, value expectations and value dependencies are transformed into the objective and constraints of service selection and composition, respectively. Another feature of our approach is that, besides web services, other generalized service components such as human activity and physical resources are considered as potential components for fulfilling value expectations. The effectiveness of our approach is demonstrated in a case study of ocean transportation service (OTS)."
3151,"In a service oriented B2B context, companies are aiming to cooperate and interact with others to dynamically create added value services by composing them. The tremendous growth in the amount of available web services impulses many researches on proposing discovery mechanisms to help designers compose services. However, most of the proposed solutions ignore in their discovery queries the upcoming services' interactions within the being designed compositions. In this paper, we present an original recommendation approach to discover a requested service to be incorporated into a specific composition. We propose to take into account the composition context specified through the process fragment surrounding the requested service, and benefit from the modeling and usage of previous service compositions to build our recommendations. Provided experimental evaluations in this paper show that our approach is efficient in realistic situations."
3152,Conference proceedings title page.
3153,"In services and cloud computing, processes need to be continually adapted to changing environments and requirements. Undisciplined process adaptation could easily lead to data flow anomalies, e.g., input missing for some activities in the process. In this paper, we study the problem of data-flow-correctness-preserving adaptation and propose three important criteria that can maintain the data flow correctness in process adaptation. We demonstrate our approach by using a typical process adaptation scenario in BPEL."
3154,"In this paper, we present a top down approach for integrated process modelling and distributed process execution. The integrated process model can be utilized for global monitoring and visualization and distributed process models for local execution. Our main focus in this paper is the presentation of the approach to support automatic generation and linking of distributed process models from an integrated process definition"
3155,"With the rapidly growing number of Web services, how to identify high quality Web services has become a hot topic. Quality of Service (QoS) is a key criterion for the choice of optimal Web services from a set of candidate Web services with similar functions. However, QoS data is acquired through the invocation of services from users. Thus, QoS prediction is critical for building high-quality service-oriented applications. Since QoS is highly related to dynamic factors such as users' or services' status and network environments which are variable over time, it is an important task to predict the unknown QoS values at runtime. In addition, the factors which cause the change of QoS may be various, such as the influence of noise, the change of the network environments. Prediction without taking account of these factors will affect the prediction accuracy. To address the problems above, we propose an online prediction approach for dynamic QoS (OPA-DQ). OPA-DQ extends matrix factorization into an online approach to make the QoS prediction process more efficient. According to the analysis on the factors which will cause the change of QoS, we build a series of processes to make better QoS prediction performance. Experimental results in a real world dataset indicate that our online approach has higher prediction accuracy and efficiency compared with other approaches."
3156,"Cloud storage systems have received extensive attention in recent years. Many individuals and business organizations are beginning to move their data to cloud environments. It becomes increasingly important to investigate secure file storage in cloud environments. In this paper, we present a secure distributed file distribution system in which the customers can directly choose appropriate design parameters and service providers. In our scheme, we use novel coding techniques that almost achieve the Shannon information bound with very efficient coding and decoding process. Our evaluations show the correctness and efficiency of the coding scheme. We show that the problem to find the satisfying file distribution under certain cost and security constraints is NP-hard, and present the Satisfiability Modulo Theories (SMT) formalization to find the satisfying data share distribution with cost and security constraints. The SMT formalization is flexible to be applied to other threshold based cloud file distribution system and can accommodate other constraints. We also analyse the security of the scheme by defining the security metric (compromising probability) for both the eavesdropping and DoS attackers and show that one must carefully choose design parameters to achieve the required security."
3157,"The emergence of the software-as-a-service (SaaS) business model has attracted great attentions from both researchers and practitioners. SaaS vendors deliver on-demand information processing services to users, and thus offer computing utility rather than the standalone software itself. In this work, the author propose an analytical model to study the competition between the SaaS and the traditional COTS (commercial off-the-shelf) solutions for software applications. The author show that when software applications become open, modulated, and standardized, the SaaS business model will take a significant market share. In addition, under certain market conditions, offering users an easy exit option through the software contract will help to increase the SaaS vendors' competitive ability."
3158,"Large-scale distributed systems, such as e-healthcare systems, are difficult to develop due to their complex and decentralized nature. The service oriented architecture facilitates the development of such systems by supporting modular design, application integration and interoperation, and software reuse. With open standards, such as XML, SOAP, WSDL and UDDI, the service oriented architecture supports interoperability between services operating on different platforms and between applications implemented in different programming languages. In this paper we describe a distributed e-healthcare system that uses the service oriented architecture as a basis for designing, implementing, deploying, invoking and managing healthcare services. The e-healthcare system that we have developed provides support for physicians, nurses, pharmacists and other healthcare professionals, as well as for patients and medical devices used to monitor patients. Multi-media input and output, with text, images and speech, make the system more user friendly than existing e-healthcare systems."
3159,"Astonishingly, a prevalent point of view amongst protagonists of SOA is REST to be ""just"" a light-weight approach to web services. Rants pick on concepts and implementations at will, very few argumentations have a clear scientific foundation, some even discuss SOA and REST as an either-or decision. However, REST is just a manifestation of resource orientation. To return to a fruitful discussion and achieve progress, this paper outlines the origins of service orientation and resource orientation, providing a foundation for future agreement and new implementation approaches."
3160,"On-demand computing has transformed enterprise software, lowering risk and cost while increasing user adoption and customer success. To be successful, an application must be designed for on-demand from the ground-up, including core architectural elements such as multi-tenancy, availability, performance, security, metadata-driven customization, integration via web services, etc. As with any new paradigm, initial applications must design and implement all these core attributes, but ultimately platforms emerge that encapsulate core computing services, allowing application developers to focus on innovation and value, and not on reinventing the wheel. With AppExchange, salesforce.com has delivered the first ondemand platform, allowing developers to easily develop and deliver the next generation of on-demand applications. In this talk, Steve Fisher discusses the technical architecture of the AppExchange platform."
3161,"An attractive business model for service brokers and aggregators is the creation of composite services built from services offered by third parties. These providers generate profits by charging their customers a premium above the charges they themselves pay to the third party providers. When multiple providers offer services with the same or similar functional and non-functional properties, selecting services with lowest associated costs is clearly beneficial. However, this is not a trivial task when, as is often the case, third party providers have complex service charging models incorporating discounts based on the context in which their service is used. This context can refer to, for example, which other services a service is composed with, or the time interval in which the service is invoked. We present a service selection algorithm that takes into account time-sensitive intra and inter provider discounts which serves to minimize the expected cost to the service aggregator of offering a composite service within a specified time interval."
3162,"HealthGrids represent the next generation of advanced healthcare IT and hold the promise to resolve complex healthcare problems by integrating health information systems and healthcare entities. Healthcare could benefit from a new delivery approach using HealthGrids to better meet the bio-medical and health-related needs. Specialized services are needed to provide unified discovery of and ubiquitous access to available HealthGrid resources. This paper identifies different types of services available on HealthGrids and classifies them into two-levels, the operational level and the management level"
3163,"We consider geographically distributed datacenters forming a collectively managed cloud computing system. Multiple SaaS providers host their SOA-based, context-aware applications in the cloud. Typically, the context-aware applications serve multiple classes of customers (end users) classified on economic considerations, which determine the Quality of Service (QoS) received by each class. This need for differentiated QoS for each customer class is incorporated into a Service Level Agreement (SLA) negotiated between the context-aware application provider and the cloud provider. A QoS metric that has been explored in large distributed applications is the percentile of response times, this metric provides a form of guarantees on the shape of the response time distribution for the customer. Typical SLAs require the response time of a certain percentile of the input requests from particular classes of customers to be less than a specified value, if this value is exceeded, a penalty is charged to the cloud provider. In addition, the applications we consider are data-intensive with strict temporal order constraints that have to be enforced on requests within the same session of a customer. We propose Data-aware Session-grained Allocation with gi-FIFO Scheduling (DSAgS), a novel decentralized request management scheme deployed in each of the geographically distributed datacenters, to globally reduce the penalty charged to the cloud computing system. Our simulation evaluation shows that our dynamic scheme far outperforms commonly deployed management policies (typically employing static or random allocation with First In First Out, Weighted Round Robin or dynamic priority-based scheduling). We further optimize our solution for dynamic, data-intensive context-aware applications, by proposing a ""context level"" cache replacement policy. Our evaluation shows that, when used in conjunction with DSAgS, the replacement policy decreases the total penalty charged to the cloud."
3164,"Churn prediction is very important to the insurance industry. Therefore, there is a big value to investigate how to improve its performance. More importantly, a good model can be used by a common service provider and benefit many companies. State-of-the-art methods either use 1) shallow models such as logistic regression, with sophisticated feature engineering, or 2) deep models that learn features and classification models simultaneously. In terms of performance, shallow models can memorize better while deep models can generalize better but may under-generalize with insufficient data. Therefore, we propose a combined Deep &amp;, Shallow model (DSM) to take the strengths of both memorization and generalization in one model by jointly training shallow models and deep models. The experiment results show that for insurance churn prediction, joint training can significantly improve the performance and the DSM earns better performance than both shallow-only and deep-only models. In our real-life dataset, the DSM performs better than CNN, LSTM, Stochastic Gradient Descent, Linear Discriminant Analysis, Quadratic Discriminant Analysis, Gaussian Naive Bayes, AdaBoost, Random Forest, and Gradient Tree Boosting. In addition, the DSM can also be applied to other prediction services."
3165,"The unpredictable performance of cloud computing platforms (caused by multitenancy) and the irregular resource requirements of scientific workflows pose several challenges to scheduling and resource provisioning. An efficient scheduling algorithm must be able to detect and react to workload changes and take advantage of cloud elasticity to tackle these issues. This work presents a scheduling algorithm that leverages the structure of scientific workflows and the elasticity of cloud computing platforms to devise schedules that minimize the workflow execution time, while satisfying a budget constraint. Experiments showed that a dynamic provisioning system using this algorithm was able to appropriately respond to performance fluctuations of a public cloud without significant impact to the makespan and cost of workflow executions."
3166,"With the development of IoT and the linking of sensors to the cloud, the demand for event-driven service invocation continues to increase. Moreover, as sensors will be wrapped as Web services, sensor service composition is required in order to detect complex events cost effectively. As the observed environment targeted here changes continuously, iterated sensor service composition will be required to handle all situations. In this research, we first formalize sensor service composition as the constraint-based optimization problem (COP). Moreover, we introduce temporal constraints in the detection of the temporal relationships of complex events and thus realize enhanced sensor service composition."
3167,"Grid computing that has developed with the aim of high-end computing has extended to commercial areas as the enabler of cost and risk minimization for IT resources. To actualize larger benefit from grid by widening grid adoption, the formation of trust relationship among participants is one of the most important components that should be concerned. While trust mechanisms among distributed nodes have been studied, the related works are rather theoretical and conceptual than applicable to practical commercial grid. Based on this observation, we classify trust depending on distinctive requirements in practical grid utilization, and improve the evaluation process of trust by suggesting integrated measuring methods. We also introduce the ways for practical use of evaluated trust by guaranteeing individuals' manageability. Finally, we describe the process for eliciting system constituents' trust requirements and validating suggested architecture, which is conducted in the next phase of this research."
3168,"Considering the inherent heterogeneous, autonomous and dynamic nature of Web services, mismatches usually exist between service signatures and behaviors, and mediation is a common approach for service interactions. Current techniques focus on mismatch analysis and automatic synthesis of mediators at development time, but they do not consider the effect that dynamic changes of services bring about for mediators at runtime. Since Web services are continuously evolving, mediators should be equipped with the dynamic adaptation and re-configuration capabilities to avoid being the bottleneck of the adaptability of service interactions. To address this challenge we propose a dynamic management approach for mediated service interactions. In this paper, migration strategies and correctness criteria suitable for the mediator are provided to realize dynamic adjustment of mediators at runtime, which ensures the agility and adaptability of service interactions based on the mediation mechanism."
3169,"A considerable number of business process modeling approaches are developed for end-users in recent years. The core issue of end-user oriented business process modeling is how to fill the gap between end-user friendly business process model and professional IT process model. In our previous work, a wizard based lightweight event-driven modeling method is designed for end-users to create their own personalized process. We observe that such a process created by end-users often cause execution time errors, which cannot be effectively detected by existing modeling languages. In this paper, we propose a formal model based on extended event-driven process chain (eEPC), which is named lightweight event-driven process chain (lightEPC). We give complete formalization of lightEPC and present two error patterns and corresponding detection algorithms to perform design-time model checking based on this model. These approaches effectively help end-users to create correct business process satisfying their personalized requirements."
3170,"One of the key activities to construct a successful service-oriented architecture (SOA) is the identification of services with the right level of abstraction. Most of existing SOA design methodologies advocate identifying services from the top-down decomposition of business processes. However, the identification quality in these methodologies heavily depends on the expertise and experience of individual designers. The ability to quantitatively evaluate service identification is absolutely needed. In this paper, we propose an approach that supports such evaluation by applying the measurement technology to the service-based business process decomposition. A model to capture related architectural elements with their relationships is presented. A set of design metrics are proposed for measuring various features of identified services in the service portfolio, including service granularity, coupling, cohesion, and business entity convergence. To apply the approach, a prototypical measurement tool for service identification is developed. An automotive work order scenario is used as an illustration example to explain our approach and demonstrate its effectiveness."
3171,"This paper provides a generalized approach for building a QoS based registry for service oriented architecture. This framework enhances the current universal description, discovery and integration (UDDI) standard in order to achieve better search efficiency and improved quality of search based on user specified QoS parameters. The proposed model does not require any changes or modification to the current UDDI standard but provides a middleware repository that is used to store a valid subset of the UDDI data with QoS specifications. The proposed framework is expandable to include additional quality of service parameters depending on the application. The implementation of this framework illustrates the effectiveness of our proposed QoS based registry."
3172,"Outcome-based business (OBB) is a business model that directly links a service provider's revenue or fee to the business value delivered by enabled services to the client. Its financial construct allows for a shared risk and reward model. Using OBB model, the service provider earns its revenue upon meeting mutually agreed benchmarks that affect these key client business outcomes. An OBB engagement typically requires establishing a long-term relationship with the client and reduces the risk for the client during the business transformation. A key component of an OBB business model is its ability to determine the impact of a particular IT solution on the performance of client financial and operational metrics. We outline a systematic method to enable us to do so. We begin with a process to identify gaps in the client's financial or operational performance. Then, we identify the business drivers that can improve those gaps. Next, we map the prioritized business drivers to the underlying IT capabilities that impact these drivers, creating a causal relationship chart called a service value map. Based on the causal relationship between the IT capabilities and the prioritized business drivers, we identify the priorities of the IT capabilities for gap improvement. In the second part of the analysis, we assume that each high level business driver can be measured by a metric (key performance indicator). With the help of a heuristic approach, we estimate the change of the metric over time using the service value map. This is accomplished by: 1) creating two baselines that define the bounds of the metric, and 2) using a maturity analysis to quantify the impact of the IT capabilities over time."
3173,"In many resource constrained web/cloud applications, users are given the ability to generate different kinds of reports after selecting some criteria for each report to be produced. Such criteria could, for example, be filters on certain report attributes. With limited computing resources, it is critical to prioritize the reports requested by users at any certain period of time. Then, that prioritization can be fed into any of the known web services scheduling algorithms. In this paper, we provide a novel cognitive prioritization approach that takes into consideration the free-form user-input text about the criticality of the reports as well as their aforementioned structured attributes/filters. Our method consists of a predictive model that uses the structured data to predict the report completion time as well as another text-mining model that uses the user's text to weight its importance. Then, both outputs are combined with the user profile to come up with the final prioritization. We apply our methodology to real data taken from the report generation of a real-world application of IT service deal pricing and show results that illustrate the effectiveness of our approach."
3174,"Reuse of process knowledge, especially organizational specific process knowledge is of great help to business process management efforts. The amount of such knowledge maybe large and ever growing, so the first step of process knowledge reuse is retrieving. A business goal oriented process knowledge retrieving method is introduced. The reuseable process knowledge is organized into process units. A Process Unit semantic description framework is defined. Business goal is also defined using semantic based model. User defined goal description is transformed into target process knowledge description, and used as input of a PK retrieving algorithm on the PK repository. User requirements on target process's functionality, performance and compatibility with collaborative processes are concerned in the retrieving process. The approach is aimed at providing direct and semi-automatic instructions on business process management operations towards business goal achieving."
3175,"Information technology (IT) service providers competing for high valued contracts need to produce a compelling proposal with competitive price. The traditional approach to pricing IT service deals, which builds up the bottom-up costs from the hierarchy of services, is often time consuming, resource intensive, and only available late as it requires granular information of a solution. Recent work on top-down pricing approach enables efficient and early estimates of cost and prices using high level services to overcome and complement these problems. In this paper, we describe an extended pricing method for top-down pricing using the secondary service level. The method makes use of data lower level services to calculate improved estimates, yet still requires minimal input. We compare the previous and new approaches based on industrial data on historical and market deals, and demonstrate that the new approach can generate more accurate estimates. In addition, we also show that mining historical data would yield more accurate estimation than using market data for services, experimental results are in consistent with our findings in previous work."
3176,"Business process dynamics in services computing has become more and more important to satisfy today's variable enterprise requirements. Existing business process systems lack support for business process flexibility in practice, it has become increasingly difficult to cope with variable enterprise needs. To solve this problem, a dynamic business process model is introduced in this paper, and a dynamic business process generation algorithm is proposed. The verification of the generated business process is based on the Kripke structure to verify the proposed correctness properties specified in Computational Tree Logic (CTL). We implement a dynamic expansion module for dynamic business process engine based on jBPM5, in that way, a black box in the business process definition can be generated dynamically according to the Drools rules. We demonstrate the proposed approach in a real auditing business process system project. Our approach is generic and can reduce the maintenance costs and the risks during the project."
3177,"We provide a novel approach for specifying and relating non-functional properties for distributed component Web services that can be used to adapt a composite Web service. Our approach uses distributed aspect-oriented programming (AOP) technology to model an adaptive architecture for Web services composition and execution. Existing Web service adaptation mechanisms are limited only to the process of Web service choreography in terms of Web service selection/invocation vis-a-vis pre-specifled (Service Level Agreement) SLA constraints. Our system extends this idea by representing the non-functional properties of each Web service - composite and component - via AOP. Hence our system models a relation function between the aspects of the composite Web service, and the individual aspects of the component Web services. This enables mid-flight adaptation of the composite Web service - in response to changes in non-functional requirements - via suitable modifications in the individual aspects of the component Web service. From the end users' viewpoint, such upfront aspect- oriented modeling of non-functional properties enables on-demand composite Web service adaptation with minimal disruption in quality of service."
3178,"Process construction from existing services requires use of appropriate design knowledge. For services that are mapped to underlying legacy applications, this takes the form of enterprise integration solutions. Design knowledge in this domain is available in the form of enterprise integration patterns (EIP). These patterns are, however, difficult to understand; they also use primitives that are different from those used for process representation. As a result, accessing EIP based on process requirements remains a cognitively demanding task for designers. In this paper, we describe a knowledge-base that represents the EIPs, infusing them with semantics derived from speech acts; and a set of heuristics, which can be used to retrieve EIPs for a set of requirements. An example serves to illustrate how the two can work in tandem to assist the designer."
3179,"Managing performance of modern Service Delivery Platforms (SDPs) is a challenging task due to scale, dynamicity and heterogeneity of these systems. With the presence of a plethora of widely divergent applications, it becomes almost impossible to model these SDPs accurately enough for basic management tasks. The dynamicity of these platforms adds another dimension of complexity. Precisely due to these reasons, traditional learning based approaches are not adequate since these approaches are inherently very slow. In this paper, we present a novel technique for performance management in SDP environment to overcome these challenges. In our approach, we construct a multi-dimensional performance model grid called datagrid. Datagrid presents a unified management approach for various system management tasks such as provisioning, fault management and SLA management. According to our understanding, ours is a first system to have a unified approach for large scale SDP environments for multiple management tasks. We have implemented our technique in real distributed system and it shows promising results."
3180,"In order to encourage the growth and take-up in the mobile commerce (mC), trust must be embedded as a core value of mobile products. The future product is likely to evolve within a fast-growing and socially conscious ecology. Product that works with the way people use their phones will develop in a collaborative social network. Providing social networking will facilitate mobile data network for free, and the ecology of soft-product will allow trust in the mC process. In order that this can be effective, a change in the mC supply-chain model would be require. This paper examines an existing model for charging and billing of mobile soft-product and a Mobile Open Billing Gateway (MOPB) model based on the triangle of B2B and B2C was proposed, with the linkage of trust, and compares a sample range of existing product with possible new mobile product. The question posed by this paper 'Is the ecology of mobile product for mC possible'?."
3181,"A service invocation in a composite service is usually defined by its interface for abstraction. Once the interface is standardized and many services which have the same interface become available, a user can bind preferable ones to the composite service only by setting endpoints. Since new services become available day by day in open environment, it is crucial to automatically construct the bindings according to user's requirements. However, runtime binding construction often degrades the performance because searching a vast amount of possible bindings is time-consuming. Moreover, bindings need to be modified even during execution of a composite service when some adaptation process is applied. In this paper, we proposed a framework for constructing bindings, which is based on ATMS (Assumption-based Truth Maintenance System). The framework effectively caches possibilities of bindings and can manage changes of bindings when a runtime adaptation process is applied. We implemented a prototype of the framework and confirmed that it works fast enough for a real scale service composition."
3182,"We present our work-in-progress on making an analytic data fusion service, originally developed and deployed to support high-performance computing and cloud-based applications, available in a enterprise service-oriented architecture (SOA) environment. We posit that not only can SOA-based integration of research software be useful in enterprise business use cases, but also that providing ways to integrate domain-specific and enterprise data is beneficial. We describe use cases driving our work, our data fusion service and its analytic capabilities, and our integration efforts using an open-source enterprise service bus framework."
3184,"In a competitive business landscape, banks are under pressure to innovate, improvise and differentiate their products and services while continuing to reduce the time-to-market for new product introductions. The rise of new businesses and newer ways of doing existing businesses has significantly influenced the need for a better assessment of the business value realized from the IT investments. Client relationship managers, product managers etc. are facing the need to be innovative and flexible in their offerings to the client and they demand an agile delivery infrastructure (process &amp; IT) to support them in this regard. In banks operating multiple lines of businesses, generating a single view of the customer is a critical both from a marketing perspective as well as risk management perspective. This presentation covers the specific applicability of a service based model to the transaction banking space and prescribes reference architecture for this domain"
3185,"This work presents a new service-oriented architecture for enterprise project management that integrates business process, human resources and project management within an enterprise or across the value chain network. In the proposed WS-EPM framework, the project resources include business processes, skilled people, physical resources, control policies, and so forth. The project resources used in WS-EPM are described by WSResource properties language. In addition, the coordination of project resources is guided by a set of operations modeled by business process execution language for Web services (BPEL4WS). At the end of this paper, we present an industry application scenario that leverages the proposed WS-EPM for achieving maximum ROI with limited resources."
3186,"Data management plays an important role in the fields of grid system. In conventional fields of software applications, like e-business applications, the single logical view of various types of distributed data resource and fine-grained access control are the most important requirements. The design and implementation of data management system which is responsible for meeting these requirements in the e-business application is proposed in this paper. The developed prototype of this data management system, as a very important component of the service-oriented manufacturing grid (MGrid) project, provides one single logical view to multiple heterogeneous storage resources so that these resources can be used in a uniform way by higher level applications. Also, fine-grained access control and the better reliability, manageability, location transparency, expandability and performance are also provided"
3187,"In to days economy, collaborative computing grows in importance. Inter-organizational service-based processes are increasingly adopted by different companies when they cannot achieve goals on their own. As a result, conformance problems arise and it must be ensured that the integrity of processes execution remains guaranteed. In this paper, we propose new components, to be deployed along the boundaries of each participating organization, offering external flow control, and notification in case of violation detection, while providing process execution traceability. To achieve our goals, we propose an event-based approach in which inter-organizational exchanges are perceived as events. We define event patterns for filtering the desirable incoming and outgoing messages."
3188,"The paradigm of automated e-service composition through the integration of existing services promises a fast and efficient development of new services in cooperative business environments. Although the ""why"" part of this paradigm is well understood, many key pieces are missing to utilize the available opportunities. Recently ""e-service communities"" where service providers with similar interests can register their services are proposed towards realizing this goal. In these communities, delegating them to already registered services, and orchestrating their executions can process requests for services posed by users. We use the service framework of the ""Roman"" model and further extend it to integrate activity processing costs into the ""ad hoc"" delegation computation. We investigate the problem of efficient processing of service requests in service communities and develop polynomial time ad hoc delegation techniques guaranteeing optimality."
3189,"Current Web service platforms (WSPs) often perform all Web services-related processing, including security-sensitive information handling, in the same protection domain. Consequently, the entire WSP may have access to security-sensitive information such as credit card numbers, forcing us to trust a large and complex piece of software. To address this problem, we propose ISO-WSP, a new information flow architecture that decomposes current WSPs into two parts executing in separate protection domains: (1) a small trusted T-WSP to handle security-sensitive data, and (2) a large, legacy untrusted U-WSP that provides the normal WSP functionality, but uses the T-WSP for security-sensitive data handling. By restricting security-sensitive data access to T-WSP, ISO-WSP reduces the software complexity of trusted code, thereby improving the testability of ISO-WSP. Using a prototype implementation based on the Apache Axis2 WSP, we show that ISO-WSP reduces software complexity of trusted components by a factor of five, while incurring a modest performance overhead of few milliseconds per request."
3190,An important aspect of service-centric systems (i.e. systems composed of services) is the ability to support service discovery at run-time in order to cope with unavailable or malfunctioning services. In this paper we present a framework that supports run-time service discovery. The central characteristic of this framework is the combination of components for monitoring the compliance of service-centric systems with requirements at run-time and components for discovering services at run-time. The framework uses the former components to detect violations of requirements at run-time and uses the specifications of the violated requirements to generate queries for discovering services that could substitute for malfunctioning services. It also uses queries derived from the process specification for service discovery. These queries incorporate both structural and behavioural aspects of the required services.
3191,"Testing Web Services (WS) for robustness is a lengthy and arduous process. After testing a set of services, there is typically a very large quantity and variety of test results to be analyzed, which poses a challenge to the developer that has to manually process all results and identify the out-puts that indicate the presence of bugs in the code. Previous research indicates that well-known automatic classification algorithms can be used to automate this step. However, the applicability of such algorithms is also limited, as they are frequently unable to deal with the large diversity of outputs present in typical WS scenarios, thus producing incorrect results. In this paper we propose an approach that allows the automatic classification of the results of WS robustness tests. The technique integrates rule-based classification (including domain rules) and conventional machine-learning algorithms trained using generic data. The proposed approach was used to classify a large set of results of tests performed over publicly available WS and also over in-house implementations of several TPC benchmarks. Results show the effectiveness of the technique, indicating that it can be integrated in the robustness testing procedure, enabling, in this way, the delivery of a full end-to-end automatic approach for WSs robustness testing."
3192,"In this paper, we propose a novel way of modeling Web services using folksonomies. The key advantage of our model is that it allows a large number of users to participate, easily, in annotating services with tags. This is in contrast to more expressive, logic based models of services, such as semantic Web service models, which require significant expertise for annotation and maintenance. Our folksonomy-based model allows associating semantic constraints on the input and output messages of web service operations using tags obtained from a folksonomy. We show how the model can be used for discovery and composition of services. We also describe a planner that uses this model to compose services and create workflows, automatically. We present performance results for the planner and our experiences in using this model in a sample real-world domain."
3193,"Web services are an ideal implementation platform for integrating disparate legacy systems because they are platform-independent. Enterprise integration patterns (EIP) represent possible design solutions that may be used to construct these enterprise integration solutions. Constructing design solutions with EIP that build on the platform-independence allowed by Web services requires that the former be converted into mechanisms that may be implemented with the latter. One such mechanism is conversation models that may be used to implement interactions among Web services representing different legacy systems. No methodologies exist that designers can use to construct integration solutions using Web services and EIP in this manner. In this paper, we outline such a methodology that generates the design elements in the form of conversation policies for Web services"
3194,"A composite web service often interacts with several partner web services hosted in different servers. These partner web services are represented as activities in a WS-BPEL program describing the composite web service. Invoking the maximal number of these activities concurrently is essential for improving its performance. But complex message coupling and control dependency between activities in a composite web service may prevent them from being arranged correctly and efficiently. In this paper, we propose an approach for detecting problematic activity arrangement in a WS-BPEL program based on analyzing message propagation and activity dependency in the program. The underlying idea of our approach is to check whether activities with dependency are arranged as concurrent and whether activities without dependency are arranged as sequential. Our preliminary empirical results demonstrate the effectiveness of our approach."
3195,"This paper presents a technique developed to forecast workloads in a business process. Business processes such as the process of engaging on a service contract consist of multiple steps that are not necessarily sequential. There can also be multiple routes that work can take in transition. In order to forecast workloads at different steps of such business processes, one needs to predict dynamic movements of process instances within the system as well as the arrival of new instances from outside. By analyzing transition log data, we construct a Markov chain, which models the movement of process instances across different steps of the business process. Our approach takes into account the fact that an instance's prior trajectory may affect its future transitions. Via numerical studies, we demonstrate the overall performance of the proposed forecasting method. We also investigate how the performance of the forecasting method changes as various characteristics of the business process change. The proposed technique is general, and can be applied to a large class of business processes."
3196,"Business process optimization can provide direct benefits to achieve business goals by improving predetermined objectives. In this paper, we study the properties of time performance and develop approaches to optimize it in work flow based business processes. By applying our data-centric business process modeling techniques, we explore the possibility of reconstructing business process that is realized by modifying the execution order of business activities. Accordingly, we develop efficient algorithms to approach optimal time performance for both a single subprocess and an overall business process."
3197,"In the cloud platform, there exist multiple providers offering the same web service, and multiple consumers requiring the same web service. Web service providers (consumers) compete against each other to make a deal with service consumers (providers). Each service provider needs to determine a proper ask to sell the service, while each consumer needs to determine a proper bid to purchase the service. In this paper, in order to address this problem, we assume that the cloud platform runs a double auction marketplace, where web service is traded as the commodity between multiple service providers and consumers. We then use game theory to analyse the Bayes-Nash equilibrium bidding strategies of service providers and consumers on the web service. We also investigate how different service pricing policies adopted by the cloud platform can affect service providers and consumers' Bayes-Nash equilibrium bidding strategies. We show that when the cloud platform sets the uniform market equilibrium price as the service price for all matched service providers and consumers, they will bid close to their valuations on the services. However, when the cloud platform sets discriminatory prices for each matched pair of provider and consumer, service providers will ask more than their valuations and consumers will bid less than their valuations. Moreover, we also investigate the allocative efficiency when the cloud platform adopts different service pricing policies, and show that the cloud platform is very efficient in terms of allocating web services between providers and consumers."
3198,"Product development in mechanical industry can be defined as an engineering process based on the collaboration between different experts using various engineering services. The e-Engineering framework aims to integrate engineering services and mediate between them for the processing of the engineering process. The collaboration within the framework is very similar to a business process, in terms of web service composition. However, the existing web service composition system shows some limitations when dealing with engineering services. This paper presents a QoS-based web service orchestration system for the e-Engineering framework by analyzing the characteristics of engineering services which usually perform numerical analysis. The characteristics of the services are expressed in the form of constraint and selection specification."
3200,"With the development of web service applications, how to improve the efficiency of service discovery is an important research work in modern times. From the aspect of service function, this paper clusters web services to form service clusters based on service ontology which is generated by domain modeling. It can significantly reduce the overhead and enhance the service discovery efficiency. The corresponding service clustering algorithm and experiments are given to verify the feasibility of the proposed method."
3201,"In this paper we present a model for delivering differentiated quality of service based on the value for the provider. The value for the provider is captured by taking into account the (i) client value, (ii) service value, and certain exogenous factors. We enumerate and articulate different dimensions which will affect these components. We present a couple of case studies to demonstrate the usefulness of the proposed model. The first case study is from retail banking based on our real life experience of deploying a service delivery system in a leading bank in India. The second case study is from a business process outsourcing industry. We also define the notion of value plot to visualize the relative importance of customers from value perspective and highlight how the value plot can be used for both operational as well as strategic planning in service delivery."
3202,"To facilitate rapid development of service-based systems (SBS), many service discovering and matching techniques have been developed to find services according to users' functionality requirements. However, users usually also have requirements on non-functional qualities of services (QoS), such as throughput, delay, reliability and security, which are also critical for the success of SBS. In this paper, a QoS-based service ranking and selection approach is presented to help users to select the service that best satisfies users' QoS requirements from a set of services having already satisfied users' functionality requirements. To determine how well a service satisfies users' concerned QoS requirements, a set of functions is presented to normalize services' QoS on various QoS aspects with different metrics and scales, compute services' satisfaction scores on each QoS aspect, and combine each services' satisfaction scores on all QoS aspects together as an overall satisfaction scores. Compared with existing service ranking and selection techniques, our approach has the following advantages: 1) selects the service that best satisfies users QoS requirements instead of the service with the best QoS which may be much overqualified for the users' QoS requirements, 2) improves the flexibility in users' QoS requirement specification, and 3) uses the prospect theory to more accurately model the relation between services' QoS and their satisfaction scores."
3203,"In a typical cloud computing environment, there will always be different kinds of cloud resources and a number of cloud services making use of cloud resources to run on. As we can see, these cloud services usually have different performance traits. Some may be IO-intensive, like those data querying services, while others might demand more CPU cycles, like 3D image processing services. Meanwhile, cloud resources also have different kinds of capabilities such as data processing, IO throughput, 3D image rendering, etc. A simple fact is that allocating a suitable resource will greatly improve the performance of the cloud service, and make the cloud resource itself more efficient as well. So it is important for the providers to allocate cloud resources based on the fitness of performance traits between resources and services. In this paper, we introduce a new cloud resource allocating algorithm, which creates a market for cloud resources and makes the resource agents and service agents bargain in that market. In this way, use is able to be made of the invisible hand behind the market to grantee the efficiency of allocation. The auction model in our algorithm is new to other auction models in that it takes the effectiveness of fitness between resources and services into consideration during the auction procedures. With the idea of fitness introduced, the bargaining process and final price calculation is modified, so that resources and services can not only trade-off between those such as prices, budgets and the required level of QoS, but also on fitness amongst bidders. We study the allocating algorithm in terms of economic efficiency and system performance, and experiments show that the allocation is far more efficient in comparison with the continuous double auction in which the idea of fitness is not introduced."
3204,"Measuring the bottleneck bandwidth of a path is useful to many applications. The existing tools to measure the network bottleneck bandwidth are mostly based on the packet pair technique. But the intrinsic error exists in packet pair technique due to the effect of the data link layer, and the incorrect results obtained by the filter algorithm when the network load is heavy. To resolve these problems, we put forward a new technique called packet tetrad. Because of the differentiate relation between the two packet pairs in a packet tetrad, the disturbance of the data link layer may be eliminated. So the packet tetrad technique has the high accuracy on measurement. The results of simulating measurements under ns-2 environment show that the packet tetrad mode has more robustness than the packet pair technique had."
3205,"We describe the organization of a Server-side platform supporting the execution of Event-Driven Mashups (i.e., composite applications combining services and smart objects through events). To support a large number of concurrent Mashup executions, Mashup Execution Platforms (MEPs) must exploit the processing power of multi-processor computer architectures as well as appropriate concurrency models and programming languages. In order to do so, we describe a MEP based on the emerging Scala programming language which provides an efficient concurrency model - based on the actor model - that is suitable for the execution on multi-processor systems. Since the MEP architecture considered in this paper has been previously implemented in Java we also describe its porting from Java to Scala taking advantage of the compatibility between the two programming languages. Finally, the Java-based and the Scala-based implementations are compared from a performance point of view."
3206,"The lack of a framework that provides for business process outsourcing to the cloud hinders the widespread adoption of this emerging computing environment. Unlike the multiple decision methods related to outsourcing classical applications in a cloud environment, there is no standard dealing with the outsourcing of business processes starting from the enterprise business concerns to select the appropriate business process to be outsourced, to a finer level of decision to select the business process fragments to be supported by the cloud. In this paper, we first present an end-to-end framework that addresses some of these shortages. The presented framework, entitled Business Process Outsourcing to the cloud (BPO2C), covers the outsourcing process lifecyle. Indeed, the BPO2C framework elaborates several phases pertinent to the outsourcing decision, starting from the elaboration of the enterprise business motivations to identify the implied business process in the outsourcing decision, to the identification of outsourceable process fragments to minimize the business process costs, duration and to mitigate cloud risks."
3207,"Accurate and rapid evaluation of web service performance is a key problem of Service-Oriented Architecture (SOA), where services are continuously being (re-)designed and released, and integrated within heterogeneous environments. Unfortunately, pre-deployment testing of services is not suitable to evaluate service performance at both design time and runtime. As a result, often process designers get a reliable assessment of service performance only very late in the lifecycle, once services have been deployed, while customers cannot evaluate service behavior at selection time. In this paper we tackle these problems by proposing a methodology that generates a simulation script that can be used for an early assessment of service performance, and to negotiate and evaluate SLAs on service performance at runtime."
3208,"Many real world business processes are executed without explicit orchestration and hence do not generate structured execution logs. This is particularly true for the class of business processes which are executed in service delivery centers in emerging markets where rapid changes in processes and in the people executing the processes are common. In such environments, the process execution logs are usually natural language descriptions of actions performed and hence are noisy. Despite the lack of structured logs, it is crucial to know the trace of activities as they happen on the ground. Without such a visibility into the ground activities, regulatory compliance audit, process optimization, and best practices standardization are severely disabled. Process monitoring on top of unstructured execution logs has been a relatively unexplored research area. This paper proposes an approach for process trace identification from unstructured logs that applies state-of-the-art text mining techniques. It applies this approach on logs of a real-world business process used in a service delivery center and shows that individual process activities are correctly identified 90% of the time. Also, 65% of the activity traces were identified with zero errors and an additional 24% with a single error. This approach is generic and applicable to a wide array of business processes."
3209,"As businesses scramble to adopt and implement service-oriented architectures (SOA) it is imperative that service-oriented thinking become an integral part of the business itself. We motivate the key concepts for service-oriented business thinking through a range of examples that give a flavor of a range of businesses. In this paper we establish three key concepts - service agreement, service transaction and service function. These concepts are discussed in depth and are related to our ongoing work in the area of modeling business services"
3210,"Business process change is at the very core of business process management, which aims at enabling flexible adaptation to changing business needs. However, the wide variety of drivers for business process modeling initiatives, reaching from business evolution to process enactment, results in multiple models that overlap in content due to serving different purposes. That, in turn, imposes serious challenges for the propagation of changes between these process models. Given a change in one model, this paper introduces an approach to determine a change region in another model by exploiting the behavioral profile of corresponding activities. It, therefore, supports the process of change propagation and eases the synchronization of process models significantly. As a major contribution, our approach can handle changes in pairs of models, even if they are not defined in terms of a hierarchical refinement."
3211,"Today, the success of a cloud system significantly relies on its availability and reliability. To achieve a better availability, fault recovery must be performed in an efficient way. When no backup is available, service re-composition becomes the only option. Although many semantic approaches for service re-composition have been proposed, they all seem to be time-consuming, and service granularity still remains as an open question. In this paper, we introduce an efficient service re-composition approach using semantic augmentation and a fast service granularity selection so that a faulty service can be replaced promptly. A semantic service matching mechanism is also proposed for discovering appropriate candidate services for re-composition. The experiment results from a cloud environment confirm the advantages of our approach."
3212,"Expressive and powerful event-pattern rules are key to successful applications of Complex Event Processing (CEP). However, the definition of complex pattern-detection logic may place heavy demands on business users. In this work we present a model for hierarchical event-patterns, allowing users to compose complex patterns from encapsulated, fully-abstracted sub-pattern definitions with a visual decision graph. Pattern definitions can be reused across business scenarios, thereby encouraging the creation of pattern libraries. In the course of this paper we give a brief overview of the event-based system SARI and describe a number of extensions to its ruling model. We present an algorithm for evaluating hierarchical ruling logic and demonstrate our approach with a use case from the fraud-detection domain."
3213,"The recent growth of connected car technology encourages IT and vehicle organizations to develop advanced vehicle-to-cloud (V2C) services such as driving assistance, infotainment, and vehicle maintenance. However, since their performances highly depend on the resource configuration and regional characteristics, it is almost impossible to examine the service feasibility in every candidate region by constructing real testbeds. To overcome this problem, we present an integrated road traffic-network-cloud simulator for V2C connected car services (IsV2C) with a user-friendly GUI. The IsV2C aims to evaluate V2C services with the user-specified V2C environment and service scenarios in a particular region. In the IsV2C, road traffic, network, and cloud simulation are intimately linked to reflect both the realistic movement of vehicles and service transactions in real-time. To accurately mimic task execution in both vehicles and the cloud, the IsV2C utilizes a rigorous emulation for evaluated services. Simulation results of the IsV2C show whether each simulated application satisfies the service level objectives regarding service time, cloud cost, and data transmission performance. As for the validation, we evaluated three sample V2C applications in an urban area, and the results proved that the IsV2C could offer useful information to both service providers and cloud providers for their service launching and profit estimation. To the best of our knowledge, the IsV2C is the first work that presents an integrated road traffic-network-cloud simulation framework for an end-to-end V2C service evaluation."
3214,"As service-oriented architectures (SOA) mature, an efficient approach for the integration of Web services in portals is required. This holds true especially in medium and large-scale SOA-based systems with a multitude of Web services to be made accessible to the users. The integration scenarios, i.e. the Web service-based features within a portal, are usually composed of complex sequences of user interaction and service communication, aggravating the need for an efficient integration solution. We present an approach for the business process-driven modeling of these scenarios in form of 'user interaction (UI) workflows' as well as a technical framework enabling the model execution within existing portal systems. The integration of Web services in a portal can thus be realized very efficiently by modeling UI workflows and configuring highly generic activity building blocks for dialog construction, service communication and data presentation."
3215,"Focus of this paper is to highlight some of the key considerations in developing services in SOA environment. These are based on lessons learnt and experience gained through SOA engagements. High level view of IBM's service modeling technique known as SOMA (service oriented modeling architecture) is provided here. This technique consists of three key steps - identification, specification and realization of services, components and flows"
3216,"One approach to improve the reusability of business process-based solutions is to represent the business process as a combination of autonomous services via a service-oriented architecture (SOA) centric style. Such an approach would represent variations at service and business process level in terms of two key constructs - variation points (the placeholders in the solution model where variations can be introduced) and variation features (the actual variations that can be introduced at a subset of the listed variation points for that solution). One key research issue that has arisen out of the VOE project is how to actually identify and subsequently justify these variation points and variation features based on changed requirements. In this paper we propose our approach called Variation- Oriented Requirements Analysis (VORA) that addresses this issue. VORA is based on a traceability model that maps requirements to use cases, sequence diagrams, business processes and finally service specifications. We show how this mapping can also be used to (semi-) automatically map requirement changes down to variation points and variation features at service and business process level. We also demonstrate VORA via a simple yet realistic running example."
3217,"Mobile devices and applications have traditionally been seen as data consumers. However, as their computation capabilities are increasing, they are becoming perceived as potential data collectors and providers, and even as hosts for web services. In fact, mobile web services gain in importance when it comes to deliver real-time contextual data, such as current location or real-time heart rate. Some emerging lightweight frameworks to host web services on mobile devices have been developed. They are recognized for their low resource footprint. However, they are barely tested and the potential of utilizing them in real-life settings is not known yet. In this paper, we present an architecture to test web services hosted on mobile devices as well as some preliminary results. We target to evaluate the QoS of these web services such as their response time, availability, throughput, and scalability and to evaluate the overall performance of mobile device host with main focus on the battery consumption."
3218,"In a service-oriented system (SoS), service requests define tasks to execute and quality of service (QoS) criteria to optimize. A service request is submitted to the service selector in the SoS, which allocates tasks to the service that can 'best' satisfy the given QoS criteria. When the selector cannot optimize simultaneously the given QoS criteria, users need to specify priorities and preferences over the said criteria. Accounting for users' QoS priorities and preferences is therefore necessary during service selection."
3219,"Security is today a relevant requirement for any distributed application, and in particular for these enabled by the Web such as e-health, e-commerce, and e-learning. It is thus crucial that the use of Web services, stand-alone or composed, provide strong security guarantees. Web services security encompasses several requirements that can be described along the well known security dimensions, that is: integrity, whereby a message must remain unaltered during transmission; confidentiality, whereby the contents of a message cannot be viewed while in transit, except by authorized services; availability, whereby a message is promptly delivered to the intended recipient, thus ensuring that legitimate users receive the services they are entitled to. Moreover, each Web service must protect its own resources against unauthorized access. This in turn requires suitable means for: identification, whereby the recipient of a message must be able to identify the sender; authentication, whereby the recipient of a message needs to verify the claimed identity of the sender; authorization, whereby the recipient of a message needs to apply access control policies to determine whether the sender has the right to use the required resources."
3220,"Reusable software component repository (SCR) and retrieval management of components have long been in active research for many years. However, building interoperable SCRs is still a challenge for software engineering. Now, domain ontologies have been employed to manage software components, and the method has been deemed as an effective way to achieve the interoperability among different SCRs. Recently, ISO/IEC initiated a new standard (No. 19763) based on OMG MOF (meta-object facility) and ISO/IEC 11179 (MDR, metadata registry) to specify a framework for metamodel interoperability. According to the 3r part of the standard, viz. Metamodel for ontology registration, we developed a grid-oriented platform for ontology-based SCR. In this paper, we present the approach that implements the new standard, and introduce the design of architecture and core component of the platform, which has been successfully deployed in several software enterprises to facilitate producing software systems with software components. Eventually, we put forward the future work to solve the emergent problems when using the platform."
3221,"Application integration is becoming the hottest topic in information technology today. Heterogeneous, isolated content service systems are faced with difficult integration problem. Firstly, we propose a novel Web Services Composition-based Content Service Alliance Platform (CSA-Platform). In this platform, we emphasize on heterogeneous service status monitoring and service scheduling switching issue; we propose a novel WSRF and Multi-agent based content service alliance and management scheme in CSA-Platform. Furthermore, we describe WSDM-based content service monitoring and scheduling model, including Manager Layer, WSDM-Gateway Layer, and Agent Layer. And then we study multi-agent-based content service status collecting strategy. As part of experiment work, we design JMX-MUSE based content service monitoring and scheduling scheme, and carry out the module implementation, and then analyze the experiment results. At last, we analyze the prospective research direction and challenges in this field."
3222,"The semantic Web services composition process arranges several Web services into one composite to realize complex workflows. To do this, semantic metadata of Web services' description are used. The current approaches based mainly on AI planning are immature to be used in practice. In this paper we propose an approach involving users in the semantic Web services composition to help overcome problems occurring in the composition process. The basic idea is to find the users helpful in situations when preconditions are not satisfied or some input data are not available which are in demand to create a composition."
3223,"In the last two decades, research in software engineering has hada focus on software lifecycle management. Rather than a narrowfocus on programming languages environments and softwaredevelopment, researchers are considering the end-to-end lifecycleof software, including design, development, deployment, supportand retirement. Business IT consulting has a similar lifecyclefrom request for proposal, to proposal, delivery, on-goingoperation and retirement. For the past 5 years we have beenworking with IBM Global Business Services to address issues indelivering Business IT services. From this experience, we'veidentified a number of open challenges and have begun workingon solutions and a platform for addressing these challenges. Weare starting with lessons learned in software lifecyclemanagement, and building on them to address challengesparticular to service delivery. For example, like softwarelifecycle management, services lifecycle management requiressupport for end-to-end traceability, coordination between peopleworking on related activities and on hand offs between one phaseof a consulting project and the next. In this paper, we enumeratea set of open challenges for service lifecycle management. Wesuggest how lessons from software lifecycle management can beapplied and give a preliminary report on our implementation ofan open architecture environment to support services lifecycle management."
3224,"In recent years, many approaches have been proposed to facilitate business process design. They attempted to measure the similarity between business processes, merge business process models, mine event logs or recommend activities. In this paper, we present a merging approach that also aims at facilitating business process design. However, instead of merging business process models, we merge process fragments around a particular activity to construct a consolidated fragment for each activity. This consolidated fragment is presented as a configurable sub-process which allows process designers to overview the interactions of an activity and configure them to create business process variants according to particular requirements. The approach has been implemented as an application and tested against a large collection of business process models taken from different domains. Experimental results show that our approach produces concise and efficient configurable fragments."
3225,"Specifying and monitoring service level agreements (SLA) has been the subject of intensive research. However, methods of enforcing SLA have not addressed the specific issues of composite services (CS). Our work focuses on the problem of keeping prearranged SLAs for service workflows including workflows supporting long lived transactions (e.g. WS-BA). As a solution we offer scheduling of component service requests. The latter is based on regaining control over legacy services by means of transparent proxies and later scheduling of their invocations. Various heuristics based policies are evaluated under two different types of service level agreements. The policies vary from the simpler, operating only on the base of business value, to the more complex which also consider Quality of Service requirements, topologies of workflows, utilization of components services, etc. The experiments conducted over the model, which precisely captures the behavior of web services, reveal the benefits of providing schedulers with various types of context."
3226,"Implementing the required degree of isolation between tenants is one of the significant challenges for deploying a multitenant application on the cloud. In this paper, we applied COMITRE (COmponent-based approach to Multitenancy Isolation Through request RE-routing) to empirically evaluate the degree of isolation between tenants enabled by three multitenancy patterns (i.e., shared component, tenant-isolated component, and dedicated component) for a cloud-hosted Bug tracking system using Bugzilla. The study revealed among other things that a component deployed based on dedicated component offers the highest degree of isolation (especially for database transactions where support for locking is enabled). Tenant isolation based on performance (e.g., response time) favoured shared component (compared to resource consumption (e.g., CPU and memory) which favoured dedicated component). We also discuss key challenges and recommendations for implementing multitenancy for application components in cloud-hosted bug tracking systems with guarantees for isolation between multiple tenants."
3227,"With increasing presence and adoption of Web Services on the World Wide Web, to recommend suitable services to users has become an important issue. However, existing personalization approaches, such as collaborative filtering or content based recommendations, are ignoring services' sociability because of the isolation of services without social relationships among them, and lacking of consideration of social influence. Therefore, there is a need for more accurate means to interlink them in a social-enhanced interest network, and to analyze and quantify the social influence. In this paper, we propose a methodology to connect distributed services into a global social service network for social influence-aware service recommendation, called recommend-as-you-go. First, we propose a novel platform to construct a global social service network by linking distributed services with social link using quality of social link, and then we propose a flexible model for effective awareness of social influence to provide a quantitative measure of the influential strength, Next, a novel social influence-aware service recommendation approach is presented based on global social service network, and finally, the experiment results show that our new approach can solve the quality of service recommendation problem well with quick query response, low usage threshold and high accuracy with user preferences by recommend-as-you-go."
3228,"Resource provision for services that have time-varying demands has raised a great concern to service providers aiming at high-standard service quality. We propose a new resource provision approach using service simulation and arrival rate estimation that integrates unsupervised clustering and statistics techniques. We first cluster days that have similar arrival patterns together, where from each cluster we can reveal and separate days having different reasons for time-varying demands of the service. We then adopt the two layer business factor model to estimate multi-interval Poisson arrival distributions on daily bases for simulating stochastic processes. Applying simulation on queuing models with multi-interval Poisson arrival processes, we can observe stochastic changes of customer waiting time, queuing lengths and number of workers under different service strategies. We conduct a case study on an electricity service call center in real industries, showing how to build adequate resource provision and estimation against history data in past years and how the performance improved compared to their previous heuristics in real life operations."
3229,"Predictive process monitoring emerged as a technique to anticipate the outcome of a running instance of a business process. To this end, it first constructs a forecast model based on an encoding of traces of past process executions that are labelled with the prediction target. This model is then used to predict the outcome of a running process instance. However, existing approaches neglect that real-world processes are subject to continuous change, so that prediction models need to adapt to concept drift. In this paper, we take up ideas on incremental learning from general data mining and present a paradigm for predictive process monitoring under concept drift. It is grounded in a systematic experimental study that answers the questions of which encoding of process traces and which incremental learning strategies are particularly suited for predictive monitoring of continuously evolving processes."
3230,"Evaluating alternative solutions for service compositions is done by various properties, each requiring an associated evaluation measure. In this paper, we propose a new measure, namely integration effort, to capture the expected effort a human programmer is expected to invest in integrating composed services into a functioning process. We present several integration effort evaluation measures, which were adapted from the related research areas of schema and ontology matching. These measures are embedded in an extendible framework, allowing application in different levels of refinement. Our measures are empirically validated to be effective proxies of integration effort."
3231,"The use of Web services as an infrastructure of service sharing has made it possible to provide collaboration and interoperability in distributed computing environment. In this environment, service publishing and discovery are required as elementary functionalities for users to be able to locate the shared resources. The mechanism of service publishing and discovery with centralized architecture restricts the reliability and scalability of the distributed computing environment as the services and resources on the Web are fast emerging. The peer-to-peer (P2P) systems and applications, on the other hand, employ distributed resources to perform critical functions in a decentralized manner. This paper introduces Web Services Oriented Peer-to-peer (WSOP) architecture with a combination of centralized and decentralized characteristics, and presents a framework of service publishing and discovery model based on WSOP architecture. The prototype system - WebPeer implemented based on this model demonstrates the WSOP architecture can not only help to overcome the known obstacles in common Web Services infrastructure such as single node failure, but also extend the ability of the pure P2P systems, such as more efficiently locating the resources, increasing the interoperability between different P2P systems."
3232,"Efficient and effective call centers are critical to quality customer service. A framework and a system presented in this paper lower the operational costs and increase availability and scalability of global dynamic call centers in a major corporation. The framework decouples the runtime application perspective from the engine perspective of call flow execution, fundamentally improving the design paradigm of call center scripting"
3233,"Service evolution management imposes complex challenges to a service provider. Identifying which changes are necessary and choosing the ones to implement are some of the decisions that providers have to face. The decision making process involves the estimation of change effects to minimize the impact over the direct and indirect clients of the service portfolio. While most existent works address either shallow changes impact or change management from a technical perspective, this paper proposes a Business Intelligence approach to support providers in decisions related to service evolution. The approach is based on the integration of services usage data and related business data, which can be correlated and presented to providers according to distinct organizational levels. One of the analysis dimensions is represented by usage profiles, which group similar usage patterns discovered from logs of services interactions. Through an illustrative scenario, we demonstrate some analysis possibilities. We also relate decisions to stakeholders and to a change-oriented lifecycle."
3234,"We take the word (or word sequence) that a reader marked in a document as a query and combine global analysis with local analysis to extract the query's context information from the document where query is. By global analysis, we extract keywords from the whole document to reflect the user's research preference. While in the local analysis step, we disambiguate query by extracting keywords from the text that is around the marked query. We use these keywords to reflect the query s context information and suggest interactive query expansion finally."
3235,"The main goal of our key project, CROWN Grid, is to empower in-depth integration of resources and cooperation of researchers nationwide and worldwide. In CROWN, information service is the kernel part which handles resource discovery and management process. Employing existing information service architectures suffers from poor scalability, long search response time, and large traffic overhead. In this paper, we propose a service club mechanism, called S-Club, for efficient service discovery. In S-Club, an overlay based on existing GIS mesh network of CROWN is built, so that GISs are organized as service clubs. Each club serves for a certain type of service while each GIS may join one or more clubs. Performance of S-Club is evaluated by comprehensive simulations. Simulation results show that S-Club scheme significantly improves search performance and outperforms existing approaches."
3236,"With the social development and vehicle increasing, traffic congestion becomes a pressing problem. Carpooling is one solution for traffic congestion. This paper begins with a motivation case and proposes a service-based method to automatically identify vehicles which travel together (i.e., Traveling companion) and provide carpooling candidates for users in heavy urban traffic. This paper focuses on services design and implementation of the carpooling functions in a distributed parallel computing environment. Through a comprehensive evaluation by experiments, our proposal is shown to deliver good efficiency and effectiveness."
3237,"In e-Science, meaningful experiment processes and workflow engines emerge as important scientific resources. A complex experiment often involves services and processes developed in different scientific domains. Aggregating different workflows into one meta workflow avoids unnecessary rewriting of experiment processes and thus improves the reuse efficiency. Remote workflow engines explore the computing power of distributed environment. However, the diversity of workflow description and execution models makes the integration between engines difficult. An agent framework uses ontology based communication language and makes the integration to semantic information of resources seamless, it is thus suitable for coupling distributed engines. In this paper, we present our work in the context of Dutch Virtual Laboratory for e-Science (VL-e) project. A semantic registry for describing workflow engines is implemented, and mobile agents are used to manage distributed workflow coordination."
3238,"Television broadcast production facilities capture, manage, edit, handle, and broadcast audiovisual content by using a wide array of specialized equipment and software. The complex workflow in this environment demands interoperability between devices, but vendor-neutral protocols do not provide access to a significant amount of functionality. This paper proposes the adoption of a Service-Oriented Architecture for controlling broadcasting equipment, addressing difficulties specific to this environment such as the prevalence of non-Web Services, embedded devices, and constrained computational resources. The proposed solution is centered on a semantic service registry, which is able to compose mediators and produces stubs in response to service selection requests. The prototype registry is experimentally evaluated in simulated scenarios, focusing on how size and complexity of the broadcast facility impact on response times."
3239,"Future Internet of Things (IoT) systems will connect the physical world into cyberspace everywhere and everything via billions of smart objects and are expected to have a high economic impact. To date there is little work on trust computation in IoT environments for security enhancement, especially for dealing with misbehaving owners of IoT devices that provide services to other IoT devices in the system. In this paper we classify trust computation models to-date for IoT systems. Our approach is to classify existing trust computation models based on five design dimensions: trust composition, trust propagation, trust aggregation, trust update, and trust formation. We summarize advantages and drawbacks of each dimension's options, and highlight the effectiveness of defense mechanisms against malicious attacks. We also summarize the most and least studied trust computation techniques in the literature and provide insight on the effectiveness of trust computation techniques as applying to IoT systems. Finally, we identify gaps in IoT trust computation research and suggest future research directions."
3240,"In this paper, we introduce a hybrid approach for certifying security properties of cloud services that combines monitoring and testing data. The paper argues about the need for hybrid certification and examines some basic characteristics of hybrid certification models."
3241,"Recent years have witnessed a growing interest in using Web Services as a powerful means for data publishing and sharing on top of the Web. This class of services is commonly known as DaaS (Data-as-a-Service), or also data services. The data returned by a data service is often subject to uncertainty for various reasons (e.g., privacy constraints, unreliable data collection instruments, etc). In this paper, we revisit the basic activities related to (Web) data services that are impacted by uncertainty, including the service description, invocation and composition. We propose a probabilistic approach to deal with uncertainty in all of these activities."
3242,"Summary form only given. Service-oriented architecture (SOA) has been proven to be a flexible and extensible architecture for designing and realizing industry solutions and applications. Enterprise Service Bus (ESB) is a hub for integrating different kinds of services through messaging, event handling, and business performance management. This tutorial will focus on a SOA solution framework; the critical role and value proposition of an ESB in SOA and Web services; ESB (and SOA) analysis and design methodology; best practices for the practical design and implementation of an ESB, including ESB design using the enterprise integration and application integration patterns; ESB and business process integration tools and techniques for ESB implementation; and performance, security and transaction management. This tutorial is based on numerous projects and solution architectures that the authors and colleagues have been engaged in the last 3 years in various industries, including government, financial, retail, electronics and distribution."
3243,"In modern computer system, system logs are important for problem determination in troubleshooting. Especially in the troubleshooting of systems, system administrators need to understand overview of the problems and identify the root causes quickly, and system logs can help the system administrators. However large numbers of unfamiliar system logs when are generated problems occur, and it's difficult to understand and use them. Most of the existing methods for interpreting system logs don't work immediately and are not useful for troubleshooting situations. We have devised a new method for mining log formats and retrieving log types and parameters in incremental log messages. By creating a structured tree using the nodes generated from log messages, we created a method for mining and refining log format continuously in realtime. Our experiments shows that our method can identify the formats of real system logs without prior knowledge."
3244,"We propose a new framework for the analysis of functional properties of e-services supporting the development of cooperative information systems. The framework aims at extending and integrating different approaches providing both a rich domain specification and a suitable operational semantics of the e-service contract, on which we define functional consistency properties. It allows for specifying complex e-services based on the IOPE paradigm, in which the static properties of the modeled system are specified using a description logic knowledge base, as assumed in semantic Web applications. Moreover, it enforces a minimal- change semantics for the axiomatization of the update operator and also includes the ability to reason about update repairing w.r.t. the domain constraints, thus allowing for incomplete service specification. On this foundation, we formally devise several consistency and validity properties of services, providing decidable checking procedures."
3245,"A formal catalog representation of Cloud services is required for automated fulfillment of user requests on catalog items, sharing of building blocks across service offerings and semantic interoperability between IT platforms. The technologies and tools provided by the Semantic Web community are well suited to achieve this formal representation. In this paper, we give an analysis on the commonality and differences of typical Cloud service offerings and of their catalog actions' mappings to delivery operations. We introduce a new ontological representation that leverages Semantic Web technology to formally model the structure and relationships of these service offerings and their operational processes. Our algorithm to select and order the execution of appropriate delivery operations uses this representation together with a new notion of safe sequences. This algorithm realizes the catalog action requests while ensuring system robustness by following sequencing rules we developed to prevent failures during the fulfillment processes for complex offerings. We evaluate our model on concrete Cloud offerings."
3246,"Modern enterprises need their IT infrastructure to be agile, for them to adapt smoothly and swiftly to how the business organization evolves in pursuit of the enterprise goals. Addressing that demand entails pursuing ""business and architecture isomorphism"". The modest offering of a good fraction of enterprise IT installations, however, suggests that such a tenet is easier said than done. At the same time, there is growing consensus that the microservices architecture style has propelling potential to seize that goal, especially now that state-of-the-art technology begins to match its demands proficiently. We have experienced the latter claim to hold true in a Proof-of-Concept implementation and quantitative evaluation performed for a medium-size enterprise seeking guidance on the architecture concept to adopt for their new IT infrastructure. Our project included a fair amount of technology scouting and evaluation, whose principal outcomes, all most encouraging, we report here. The conclusion we arrived at, which we believe applies to numerous other enterprises, is that embracing the microservices architecture style meets with solid technology support, assures architectural agility, and yields satisfactory returns for run-time performance."
3247,"The concept of containerization and virtualization is getting traction in the cloud based IT environments. Docker engine is popular implementation for simplifying and streamlining containerization technology. IT industry realizes numerous automation and acceleration features and facilities through the embracement of the Docker-sponsored containerization paradigm, which is an operating system (OS)-level and lightweight virtualization. However the security issues affect the widespread and confident usage of Docker platform. In this paper, we have discussed important security issues of the Docker containers as well as the related work that is being carried out in this area. Also we have proposed security algorithms and methods to address DoS attacks related issues in the Docker container technology. The preliminary experiments and testing of the security methods are promising."
3248,"Security has been identified to be the principal stumbling-block preventing users and enterprises from moving their businesses to the cloud. The reason is that cloud systems, besides inheriting all the vulnerabilities of the traditional computing systems, appeal to new types of threats engendered mainly by the virtualization concept that allows multiple users' virtual machines (VMs) to share a common computing platform. This broadens the attack space of the malicious users and increases their ability to attack both the cloud system and other co-resident VMs. Motivated by the absence of any approach that addresses the problem of optimal detection load distribution in the domain of cloud computing, we develop a resource-aware maxmin game theoretical model that guides the hypervisor on how the detection load should be optimally distributed among its guest VMs in the real-time. The objective is to maximize the hypervisor's probability of detection, knowing that the attacker is dividing the attack over several VMs to minimize this probability. Experimental results on Amazon EC2 pricing dataset reveal that our model increases the probability of detecting distributed attacks, reduces the false positives, and minimizes the resources wasted during the detection process."
3249,"In this paper, we study a new framework for multi-user Web services selection problem, which aims to select best candidates to meet multi-user's requirements. However, the unavoidable challenges in this problem are the efficiency and effect. Most existing methods are proposed for the single request condition without considering the overload of Web services, which cannot be directly used in this problem. Furthermore, existing methods assumed the QoS information for users are all known and accurate, and in real case, there are always many missing QoS values in history records, which increase the difficulty of the selection. In this paper, we propose a new framework for multi-user Web services selection problem. This framework first predicts the missing multi-QoS values according to the historical QoS experience from different users, and then selects the global optimal solution for multi-user by our fast match approach. Comprehensive empirical studies demonstrate the utility of the proposed method."
3250,"Mobile phones are becoming a new popular platform for business applications. The number of mobile business users increases and so does the need for efficient mobile data exchange and collaboration. However, a traditional business application design approach is not suitable for mobile devices because of the limited memory and connection bandwidth. This paper presents a lightweight mobile SOA-based approach to interaction between business applications running on J2ME enabled devices such as cell phones. The paper includes position statement based on our experience and describes a prototype implementation of the architecture. The important features of our design are: using the knowledge of business processes workflow and XML compression to minimize data transferred; pro-active data push and allowing applications to fully function in a disconnected mode. The above approach results in a lightweight framework which can be used in order to develop a wide spectrum of business-oriented applications."
3251,"A variety of methods have been proposed for selection of service providers as well as adaptation of business processes to manage service providers. However, each method is discussed under a certain set of often implicit assumptions or expectations. It has thus not clearly discussed how selection methods and adaptation methods can be chosen and combined in a consistent way with each other and with the target business process. This paper provides an overview of our study toward an analysis, design and implementation framework to promote consistent use of service selection methods and process adaptation methods with different assumptions."
3252,"Service oriented computing technologies facilitate design and implementation of virtual enterprises. By keeping the systems of business partners as autonomous and lose coupled service components, the service oriented paradigm supports flexible integration of heterogeneous systems or components within virtual enterprises. Taking a real world project in Chinese apparel industry as an example, this paper firstly analyzed the challenges in virtual enterprise construction. Then it presented a service oriented framework that supports IT-enabled heterogeneous resource sharing and dynamic virtual enterprise construction. According to the deployments in apparel companies in China, we conclude that the framework presented in this paper facilitates the construction of dynamic virtual enterprises with process services, participant management and message based inter-organizational process collaborations."
3253,"The deployment of pricing schemes for communication services is driven by the network technologies. QoS is a major factor that affects the choice of a pricing scheme for a given network. In this paper, we explore a pricing scheme that uses service profiles, which define the QoS received from accessing a common resource pool. Through resource sharing between the profiles, maximisation of network resource utilisation is achieved. In this paper, the mathematical model of the proposed pricing scheme is presented, together with QoS evaluation results"
3254,"In this article we focus on the modelization of the Bonjour Grid protocol which is based on the Publish-Subscribe (Pub-Sub) paradigm, a paradigm for asynchronous communication that is useful for implementing some approaches in distributed programming. The aim of this paper is to isolate the generic mechanisms of construction for the publish-subscribe approach then to model and verify, based on those mechanisms, the Bonjour Grid protocol that allows the coordination of multiple instances of desktop grid middleware. We produce models using colored Petri nets in order to describe a specific modeling approach for the Pub-Sub paradigm. Such models are important, first, to formally verify the adequacy of Bonjour Grid in the coordination of resources in desktop grids - for example by proving the absence of a deadlock in the Bonjour Grid protocol, and second, to offer a 'composition'mechanism for integrating any protocol based on the Pub-Sub paradigm. These ideas are illustrated along the Bonjour Gridcase study and they constitute a methodology of building Pub-Sub systems."
3255,"The IT services industry has been undergoing a significant transformation over the last decade primarily driven by the global delivery model - services are provided from delivery centers across the globe based on skill and cost. While extremely effective a key challenge is being able to harness distributed knowledge, especially to drive operational and business process optimizations and a superior client experience. The knowledge about service requirements, client experience and delivery quality is in collective possession of different communities, such as clients, service designers and delivery teams. Current practices to discovering this distributed and unstructured knowledge are semi-automated, and as such they fail to scale and provide accurate insights on demand. Enterprise crowdsourcing provides a mechanism to harness the tacit knowledge from a large group of network-connected humans. In this paper we describe a novel approach to improving operational excellence, with focus on compliance posture, using enterprise crowdsourcing. We demonstrate how enterprise crowdsourcing accelerated deployment of a novel identity access management capability in a global IT service delivery center, reducing the time to discover the required knowledge by 80%. We discuss how the uncovered knowledge networks can be engaged for various on-going operational activities as well as large-scale business transformation initiatives."
3256,"Today, the proliferation of mobile devices coupled with the widespread availability of the Internet is opening up new service opportunities in numerous areas. However, developing mobile applications turns out to be very challenging. Two major plagues are heterogeneity and the need for dynamic adaptation happening at runtime. Dealing with these aspects leads to several problems and impasses. In this paper, we present a service-oriented component model for the development of hybrid mobile applications. This model allows developers to produce mobile applications more rapidly and reduces maintenance costs. An in-depth evaluation on several industrial applications of this model is also presented showing its benefits."
3257,"Service-oriented systems are large, dynamic, and heterogeneous distributed environments that can be plagued with performance problems. These problems are becoming increasingly difficult for human administrators to analyze and indeed rectify, due to the sheer size and complexity of the environments they occur in. To curb this trend, a high degree of performance resilience must be injected into the system, such that it can autonomously overcome problems and resume normal service with minimal human interventions. Attending to this need requires a feed-back loop solution that monitors service-oriented workloads, localizes guilty services using the gathered data, and recuperates the performance of those identified services. Further to previous work on the first two points, this paper seeks to address the last issue through three platform-independent, autonomic means: dynamically switching to another service, automatically restarting the suffering service, and autonomously reversing any incorrect configuration of that service. A strategy is devised to orchestrate these mechanisms to respond to different performance problems. Evaluations in a real- world service-oriented grid show that our approach is effective against two common types of performance problems."
3258,"Web services are software modules that provide interoperability over a network. Web services provide Web service users platform independence while using software. It enables businesses to collaborate by using Web services from Web service providers. Estimating a Cost of Service (CoS) is essential when pricing, selecting, and monitoring a Web service. The concept of cost is not restricted to financial value of technology hardware and software. The cost concept can also include time, usability, and maintenance. Cost of a Web service can be estimated by identifying the attributes of cost from the perspective of different stakeholders such as Web service provider, Web service consumer, Web service repository moderator, and Web service policy maker. In addition, analyzing different roles in Service Oriented Architecture (SOA) will further provide more knowledge about different perspectives of cost concepts in SOA. This paper addresses the essential attributes of estimating cost of a Web service. Moreover, this paper specifies attributes of measuring CoS, defines these attributes, and defines metrics and units of these attributes. Additionally, it provides further hierarchy classification of Web service cost concepts. It also provides a model for evaluating Web service cost based on different cost criteria. By measuring CoS, Web service stakeholders will be able to estimate an accurate value to the CoS."
3259,"In this paper, the problem of commitment in allocating services to different applications and business processes is introduced. We model the problem based on the assumption that the services' instances are not put on hold throughout the lifetime of an application or a business process. Our objective is to find an optimal policy for committing services' instances to different applications to maximize the utilization of available instances of services. We formulate this problem and propose a Markov decision process approach for it. We present the optimal solution for a sample case with two types of services and two classes of application, and compare the performance of this optimal policy with a system with a full commitment policy as well as a No commitment policy system. The comparison results show that the policy obtained outperforms the other two policies. We also evaluate the performance of a system considering beta distribution for the service execution time, and we illustrate the effectiveness of applying the obtained policy on this system as well."
3260,"In large-scale cloud computing systems, even a simple user request may go through numerous of services that are deployed on different physical machines. As a result, it is a great challenge to online localize the prime causes of performance degradation in such systems. Existing end-to-end request tracing approaches are not suitable for online anomaly detection because their time complexity is exponential in the size of the trace logs. In this paper, we propose an approach, namely Magnifier, to rapidly diagnose the source of performance degradation in large-scale non-stop cloud systems. In Magnifier, the execution path graph of a user request is modeled by a hierarchical structure including component layer, module layer and function layer, and anomalies are detected from higher layer to lower layer separately. In each layer every node is assigned a newly created identifier in addition to the global identifier of the request, which significantly decreases the size of parsing trace logs and accelerates the anomaly detection process. We conduct extensive experiments over a real-world enterprise system (the Alibaba cloud computing platform) providing services for the public. The results show that Magnifier can locate the prime causes of performance degradation more accurately and efficiently."
3261,"We observed and grouped common service composition techniques into six patterns with distinct characteristics of their integration intermediary. No pattern is better than others in all business and IT situations. Therefore our analysis suggests a formal methodology in impact and cost analysis to justify the choice of composition patterns in service design. Our contribution leads to an advisory tool consisting of questionnaire, semantic reasoning and assessment map to assist IT architects in making a decision of lasting impact. Through solving the service composition puzzle, we hope further values of service oriented architecture may be demonstrated at a fraction of total ownership cost."
3262,"Substantial amount of research has been done recently to address the shimming problem in scientific workflows, in which a special kind of adaptors, called shims, are inserted between workflow tasks to resolve the data type incompatibility issue. Recently, scientific workflows are increasingly used for big data analysis and processing, which poses additional challenges, such as volume, velocity and variety of data to the shimming problem. One issue is to scale the registration and configuration procedure to a large number of workflow tasks. Another issue is the ease of integrating a large number of remote Web services and other heterogeneous task components that can consume and produce data in various formats and models into a uniform and interoperable workflow. Existing approaches fall short in usability and scalability in addressing these issues. In this paper we 1) propose a new simplified single-component based task model based on extensive experiences and lessons learned from our original multiple-component based task model. The new model separates registration from configuration and eases the process of registering external functional components (such as Web services) into p-workflows, 2) propose a shim generation algorithm that elegantly solves the shimming problem raised by Web service based scientific workflows, and 3) we integrate MongoDB, a NoSQL document-oriented database system for storing and managing large-scale unstructured documents. A new version of the DATAVIEW system has been developed to support the proposed techniques and a case study has been conducted to show the feasibility and usability of our proposed techniques."
3263,Web services are more frequently accessed from a broad range of mobile users. Service composition integrates existing services to fulfill more complex tasks facing more dynamic environment. Pattern based service composition has been proven as an appealing approach to accelerate the service composition. This paper proposes a novel pattern based dynamic service composition framework that integrates elaborately designed pattern generation and pattern enforcement approaches to produce composited services. The pattern generation is formalized as a special graph similarity matching problem and an algorithm is designed to calculate the similarity of these service composition solutions. Patterns are chosen out of the solutions by similarity with specific criteria and applied for fast service composition out of the services in the repository. The pattern enforcement applies pattern to produce service composition instances with dynamic quality based metrics. Benchmark and analysis show this framework accelerates dynamic service composition effectively and efficiently.
3264,"Formal verification has been shown to be a useful technique for performing assurance of electronic business transactions. However the verification efficiency is still a serious bottleneck due to increasingly complex system deployed over multiple data centers. Although great improvements in the state-of-the-art model-checking algorithms, the optimization on the business process level has an advantage in improving verification performance. From a transactional equivalence perspective a method to minimize the target business transactions model and improve the verification efficiency is presented. This work proposes a formal modeling method for a flexible transaction based on an extension of Pi-calculus. An operational semantics is defined as an extension of the classical one for Pi-calculus by introducing membrane activities (a kind of wrapper for processes). Then, a weak bisimulation relation is defined, which is used to reduce the transaction models and apply checking transactions equivalence. A running example throughout the entire paper empirically evaluates the performance of the proposed solutions. The experimental results show a significant improvement in terms of state space and construction time. This will make it possible to provide improvements in the performance efficiency for verifying Service-based applications."
3265,"Knowledge of neuronal circuitry is foundational to the neurosciences, but no tools have been developed for cataloguing this knowledge. Part of the problem is that the concepts used to describe neural circuits are rapidly evolving and vary substantially across different species. The NeuronBank project (http://neuronbank.org) is developing an informatics infrastructure for managing the dynamic, domain-specific knowledge of neural circuitry, providing a reference source, an outlet for publishing new knowledge, and a useful research tool. Our solution is a federation of customizable knowledge bases, each adaptable to store knowledge of the neural circuitry of a single species. The federation is united by a common set of web services and a central portal that provides core functionality across various knowledge bases. This service-oriented architecture provides domain-specific representations of specialized scientific knowledge while maintaining interoperability across a broad discipline."
3266,"When using SOAs in mission critical environments, quality and performance related aspects may not be ignored. Service Level Agreements (SLAs) have to be defined and applied during runtime. In this paper we present a method to enhance an existing SOA container to take into account Quality of Service (QoS) related aspects and enforce these aspects during runtime using feedback controlled priority adaptations. Without major modifications existing business services can use this QoS-component. A use case of the Luxembourg National Benefits Fund shows the benefits of the proposed solution in an operational system."
3267,"The trend towards service business is progressing rapidly throughout the world economics. Responding to the demands for service innovation and service productivity improvement, service science is emerging as a new research field that applies multi-disciplinary studies to service systems. In this paper we propose using control theory as a means to design service management solutions that are robust to system dynamics and uncertainty. We further present several control problems and challenges that are fundamental to service systems, and the opportunities of applying control theory to modeling service systems, controlling service resources, and monitoring service quality. Moreover, we demonstrate how control technologies can be applied to service systems through three case studies: (1) quantifying the complexity and IT management, (2) dispatching consultants in service transition, and (3) monitoring service quality and variability."
3268,"We describe the development of the cross-ontological analytics (XOA) system, which implements a novel Web-enabled approach for computing similarities across gene and gene products with associated specifications of functional genomic relationships. The XOA system enables biologists to leverage combinations of hierarchical and associative data across multiple subontologies in order to reveal relationships between gene and gene products that might not otherwise be exposed by existing methods of computational analysis. We also provide examples of specific proof-of-concept applications in which XOA has been successfully tested."
3269,"With the rapid development of cloud computing technologies, there are increasing number of Web services deployed by service providers on cloud platforms. Invoking Web services is in the form of given orders, by which the end-user can process business and information in anytime and anyway. However, existing Web service compositions mainly focus on single Web services selection and neglect to consider the fact that complex services created and aggregated by service compositions might be low global QoS (Quality of service). In this paper, the importance and necessity to construct service clusters in business process are discussed. Then, how to build up a model to solve the optimal QoS problem of Web service composition using service clusters is proposed. Third, the dynamic programming algorithm is introduced to this model. Finally, a series of experiments are carried out to evaluate our method. The results show that service cluster has advantage to obtain composite service with best QoS for the end-user."
3270,There is a need for protocols to achieve universal interoperability among Web services and to provide a fair and secure environment with non-repudiation. BPEL provides a language for the formal specification of business processes and business interaction protocols. In this paper we propose and verify a non-repudiation protocol using Petri nets for chain-linked business transactions and show that they may be specified in BPEL.
3271,"With the growing adoption of Web services on the World Wide Web, the issue of QoS-based service selection is becoming important. A common hypothesis of previous research is that the QoS information to the current user is supposed all known and accurate. However, the real case is that there are many missing QoS values in history records. To avoid the expensive and costly Web services invocations, this paper proposes an extended Matrix Factorization (EMF) framework with relational regularization to make missing QoS values prediction. We first elaborate the Matrix Factorization (MF) model from a general perspective. To collect the wisdom of crowds precisely, we employ different similarity measurements on user side and service side to identify neighborhood. And then we systematically design two novel relational regularization terms inside a neighborhood. Finally we combine both terms into a unified MF framework to predict the missing QoS values. To validate our methods, experiments on real Web services data are conducted. The empirical analysis shows that our approaches outperform other state-of-the-art methods in QoS prediction accuracy."
3272,"In e-service environment, enterprises need to collaborate to achieve a common business goal in loose-coupling manner. Therefore, it is of great necessity to develop a service interoperation technique that is capable of handling the collaboration among distributed business processes. The Web services architecture defines separate specifications for the choreography and orchestration of Web services. This paper proposes a service choreography solution for business collaboration across organizations boundaries while preserving the local autonomy of their own business processes. A centralized service choreography process is built for specification of the global business process, and then it is transformed to decentralized choreography processes amongst the participants for peer-to-peer interaction. For each participant, a data dependency based process mediation model is developed to facilitate the adaptation between the local pre-established business process and its corresponding decentralized choreography process. This mediation means can ensure the privacy and autonomy of local business process and the adaptability of choreography process. A system architecture is implemented to support our solution in ONCE PI project"
3273,"Cloud data centers are networked server farms commonly composed of heterogeneous servers with a wide variety of computing capacities. Virtualization technology, in Cloud data centers, has improved server utilization and server consolidation. However, virtual machines may require unbalanced levels of computing resources (e.g., a virtual machine running a compute-intensive application with low memory requirements) causing resource usage imbalances within physical servers. In this paper, an agent-based distributed approach capable of balancing different types of workloads (e.g., memory workload) by using virtual machine live migration is proposed. Agents acting as server managers are equipped with 1) a collaborative workload balancing protocol, and 2) a set of workload balancing policies (e.g., resource usage migration thresholds and virtual machine migration heuristics) to simultaneously consider both server heterogeneity and virtual machine heterogeneity. The experimental results show that policy-based workload balancing is effectively achieved despite dealing with server heterogeneity and heterogeneous workloads."
3274,"The quality-of-service (QoS) is a common focus which users and service providers pay close attention to at present. For service providers, it is one of their main targets to find the optimal QoS strategy based on user preferences. However, because of the fuzziness of user preferences and the complexity of service environment, searching an optimal service strategy becomes a difficult problem. In the paper, how the QoS affects a user's satisfaction is analyzed, and then a quantitative relationship between QoS and user satisfaction is built. Based on the relationship, a user utility model of cloud service is established. In order to maximize user utility, a QoS evolutionary algorithm based on user utility model is proposed. In the algorithm, some improvement is designed to balance the contradiction between search scope and search speed in the traditional genetic algorithm. It can be seen through the experiments that the QoS optimization strategy of cloud service output by the QoS evolutionary algorithm is consistent with the target user's preferences, which can effectively enhance the cost effectiveness of service resources."
3275,"In service oriented computing different techniques for monitoring service level agreements (SLAs) are available. Many of these monitoring approaches focus on bilateral agreements between partners. However, when monitoring composite services it is not only important to figure out whether SLAs are violated, but we also need to analyze why these violations have occurred. When offering a composite service a company depends on its content providers to meet the service level they agreed upon. Due to these dependencies a company should not only monitor the SLA of the composite service, but also the SLAs of the services it depends on. By analyzing and monitoring the composite service in this way, causes for SLA violations can be easier found. In this paper we demonstrate how to analyze SLAs during development phase and how to monitor these dependencies using event logs during runtime. We call our approach MoDe4SLA (monitoring dependencies for SLAs)."
3276,"The French project OpenPaaS aims at providing a social platform to help companies initiating and managing their collaborations. Nowadays, data exchange is no sufficient and collaborations need to be supported for every interaction between the actors of the collaboration. This paper concerns a PaaS in charge of supporting the deduction of collaborative business processes that involve subscribing organizations of the PaaS. In order to be applicable to industrial needs, the process deduction should require the minimum knowledge from users: collaborative objectives and a repository of all the capabilities made available by the subscribing organizations. Functional and non-functional gaps are filled simultaneously when building the process: (i) a collaborative ontology allows finding sets of capacities able to achieve the collaborative objectives and (ii) a non-functional assessment builds the optimal process i.e. the sequences of activities and also the set of partners with their corresponding capabilities. This article focuses on the first point and brings a methodology based on semantics to deduce a collaborative process."
3277,"Selective attention is the idea that visual attention is not granted equally to all regions of the visual field, but rather focuses on high-attention regions. In this paper, we propose that selectively improving the performance of video in regions that are given high attention by users can significantly improve the quality of experience (QoE) of video services. We investigate users' attention regions, and choose the subtitle region as a representative high-attention region to present a subtitle-based measurement study. This study concentrates on how and how much the subtitle region affects the QoE of video. First of all, we verify that changes in performance of the subtitle region can cause an illusion about users' judgment on video bitrate. Secondly, we present a logarithmic relationship between video bitrate and the QoE of video. Thirdly, we measure the improved QoE by improving the performance of the subtitle region. Finally, we demonstrate that selectively improving the performance of the subtitle region can significantly improve the QoE of video at little cost."
3278,"With the proliferation of cloud computing, more and more functionally equivalent cloud services with varied quality of service (QoS) have emerged. Service selection for a SaaS (Software as a Service) has become a critical issue in cloud environments, and the transition from single-tenancy to multi-tenancy has made this issue more complicated. Existing approaches suffer from low efficiency in finding optimal solutions, especially in large-scale scenarios. As a result, QoS-aware service recommendation is becoming increasingly important for selecting services for a multi-tenant SaaS that simultaneously serves multiple clients with differentiated QoS requirements. In this paper, we propose a novel service recommendation approach that largely improves the efficiency of QoS-aware service selection for multi-tenant SaaS. Our approach significantly reduces the search space of the service selection problem by selecting representative candidate services based on the diversity and similarity in tenants' QoS requirements for the SaaS. The experimental results demonstrate the effectiveness and efficiency of our approach."
3279,"This paper presents a model for ""help desk chat"" which is distinct from the ""buddy-list"" model for the conventional collaborative chats. Help desk chat includes several distinct capabilities including scheduling and routing functionality, archival of problem resolution sessions, integration with ticketing databases, unification with knowledge management systems, and efficient interfaces for agents to effectively handle and multi-task several chat sessions. The main motivation is to provide an alternate channel to voice calls that increases help desk agent efficiency while improving end user satisfaction. By implementing an end to end help desk chat system and piloting it in a large global enterprise, we demonstrate that help desk chat indeed meets these goals. Analysis results over numerous help desk chat transcripts quantitatively show the effectiveness of chat over voice across key help desk performance indicators including first call resolution, average speed to answer, average call duration, extent of multi-tasking, and end user satisfaction."
3280,"Automated service composition can fulfill user request by composing services automatically when no individual services meet the goal. Unfortunately, most of current automated service composition methods are in-memory methods, which are limited by expensive and volatile physical memory. In this work, we develop a relational-database approach for automatic service composition. Possible service combinations are stored in a relational database on persistence disk instead of volatile memory, and for any composition requests, solutions can be obtained by simple SQL queries. We offer three main contributions in this paper. First, pursuing earlier work, we overcome the disadvantages of in-memory composition algorithms, such as volatile and expensive, and provide a solution suitable to cloud environments. Second, compared with other pre-computing composition methods, we use a single SQL query: there is no need to eliminate spurious services iteratively. Third, we address the quality of services to maximize user's satisfaction in our system. An experimental validation is done, which shows the performance benefits of our system and proves that this system can find a valid composition solution with fewer services to maximize user satisfaction."
3281,"Ensuring cost-effective end-to-end QoS in a multilayer, multi-service, IoT data processing pipeline is a non-trivial challenge. The uncertainties surrounding the 3Vs of streaming data - variety, velocity and volume - impose dynamic QoS-driven resource requirements on each component (or service) of the pipeline and make adaptive resource management a complex task. Our overall research objective is to develop appropriate resource scaling strategies that dynamically adjust the resources allocated to each component in the pipeline so as to ensure end-to-end QoS fulfillment while optimizing the associated costs. To this end, in this paper, we present our work in progress on a model for end-to-end QoS and cost-aware resource allocation for IoT data processing pipelines. We base our model on the well-established unbounded knapsack problem, which offers a simple yet powerful abstraction of constraint-based decision-making. We intend to develop resource scaling strategies on top of this model that can exploit resource and contract heterogeneity to achieve cost-optimal end-to-end QoS-aware resource allocations."
3282,"The cloud computing has become the most popular service model with a proven ability to reduce costs and improve resource efficiency. However cloud platform also suffer from potent attacks from internal and external environment. The cooperation with Managed Security Service Providers (MSSPs) has becoming a compelling solution to the security and privacy risks in the cloud platform. But it is difficult to deploy and manage a variety of security service efficiently and effectively. In this paper, we propose a novel framework, ESI-Cloud (Easy Security services Integration Cloud), to facilitate the integration of multiple managed security services into a cloud computing platform. ESI-Cloud utilizes virtual machine introspection to monitor and analyze the guest VMs execution information. It also provides a client library with rich APIs and a management console to the managed security service providers (MSSPs), with which multiple security services can be easily integrated into the basic cloud platform. We have implemented ESI-Cloud in the Xen hypervisor platform and evaluated the system functionality and performance."
3283,"This paper demonstrates the automatic creation of a Web service that chains together existing Web services to achieve a particular goal. The generated service implements the necessary workflows to convert an instance data of one system into an instance data of another. This paper further demonstrates the reconciliation of structural, syntactic, and representational mismatches between the input instance and the desired output instance"
3284,"Semantic Web services (SWSs) extend current Web services standards to help facilitate their usages. While current SWS approaches have shown some early promising results, they have focused on somewhat unrealistic use-cases that make various problematic assumptions and focus on somewhat farfetched usage scenarios. In this paper we present a category of use case scenarios for SWSs that centers around keeping human users in the automation process while facilitating their activities. We demonstrate our approach with a simplified scenario and highlight some of the details of our architecture and implementation. Finally, we also discuss how our approach could be extended and applied in other domains; namely, the domain of asset-based business approach to creating IT computing infrastructure"
3285,"Despite the large number of software tools and hardware platforms aiming to solve the problems that bioinformatics is facing today, there is no platform solution that can scale up to its demands, in terms of both scope and sheer volume. DiscoveryNet scientific workflow system is here extended into a service-centric component architecture that brings together cross-domain applications through Web and grid services and composes them as novel service offerings. Two case studies implemented on top of the platform, SARS analysis and microarray/metabonomics, are described."
3286,"Most of the existing service discovery methods focus on finding candidate services based on functional and non-functional requirements. However, while the open science community engenders many similar scientific services, how to differentiate them remains a challenge. This paper proposes a trust model that leverages the implicit human factor to help quantify the trustworthiness of candidate services. A hierarchical Knowledge-Social-Trust (KST) network model is established to draw hidden information from various publication repositories (e.g., DBLP) and social networks (e.g., Twitter). As a proof of concept, a prototyping service has been developed to help scientists evaluate and visualize trust of services. The performance factor is studied and experience is reported."
3287,"This paper presents a novel architecture for distributing web service requests on clusters of servers. The architecture facilitates a transparent dynamic distribution of requests according to a range of specified policies. This enables a flexible performance in respect of different objectives, services and platforms (typically based on server workload). The architecture has been successfully demonstrated with a prototype implementation (called ""Jerrymouse""). Our preliminary results with Jerry mouse indicate stable behaviour and worthwhile performance gains (compared with Apache HTTP Server). A specific policy to deliver reduced cluster electricity savings has also been successfully implemented."
3288,"The advent of the internet has paved the means for global communication. In today's world, social networking websites like Facebook, Twitter, SnapChat, and Reddit are an integral part of youth and kids. Being exposed to many strangers on their social network, today's youth and kids are vulnerable to cyberbullying attacks, more so than ever. It has been a growing concern for parents and government as it has steadily increased the mortality rates of youth and kids. Given the degree of freedom one can have on the internet, it's a pain to prevent cyberbullying efficiently."
3289,"Business value evaluation of IT services is critical for an enterprise to develop an IT project portfolio aligned with its business operations. To lessen the dependency on subjective assumptions on how IT capabilities benefit the implementation of business strategies and operations, this paper proposes a business value modeling approach based on value driver trees which hierarchically model causal relationships from IT capability and performance to operational metrics, business metrics, and finally to business strategic metrics. This paper also presents an asset-based modeling framework and an analysis methodology which helps apply the business value analysis to various IT services. Besides depending on subject matter experts' experience and opinion, the methodology also proposes to apply a regression model to historical data of business metrics to discover and refine quantitative relationships among metrics. A case study is presented to illustrate how the proposed value-modeling and the regression method help in real-world business value analyses of IT services."
3290,"Nowadays e-services are gaining strong momentum. The Web is a strongly connected environment and it is moving from being a simple collection of HTML pages toward a container of e-services. In an extremely competitive world, providers of e-service aim at satisfying user needs, providing enriched e-services characterized by high adaptivity to the context and multichannel provisioning. Providers also want to give users the opportunity of executing complex e-services or processes transparently with respect to orchestration issues. The MAIS Project is an attempt to realize a framework able to ensure all these characteristics at the same time. We present our vision of a multichannel adaptive composition framework by focusing on e-service composition and provisioning."
3291,"In this work, we propose a novel e-business architecture that takes into account the specific needs of small and medium-sized enterprises (SMEs) and present a model-driven and highly adaptive approach that facilitates the automation of cross-organizational business processes. Parts of the ebXML standard are leveraged for the initialization of business relationships and for ensuring semantic interoperability, while the Web services stack builds the technical foundation for the actual execution of transactions. The envisioned architecture comprises active, Web services-based adapters at the client side to shield technical peculiarities of local IT installations and to ensure seamless collaboration of the diverse stakeholders, but still relies on a central server that enables users to register themselves, to model supported processes and data as well as to negotiate the exact conditions of an electronic business relationship. The resulting approach can thus be considered as hybrid regarding the degree of centralism involved and with respect to the combination of the Web services stack and the ebXML standard as an infrastructural foundation."
3292,"Cloud data centers and virtualization are being highly considered for enterprises and industries. However, elastic fine-grained resource provision while ensuring performance and SLA guarantees for applications requires careful consideration of important and extremely challenging tradeoffs. In this paper, we present RPPS (Cloud Resource Prediction and Provisioning scheme), a scheme that automatically predict future demand and perform proactive resource provisioning for cloud applications. RPPS employs the ARIMA model to predict the workloads in the future, combines both coarse-grained and fine-grained resource scaling under different situations, and adopts a VM-complementary migration strategy. RPPS can resolve predictive resource provisioning problem when enterprises confront demand fluctuations in cloud data center. We evaluate a prototype of RPPS with traces collected by ourselves using typical CPU intensive applications and as well as workloads from a real data center. The results show that it not only has high prediction accuracy (about 90% match in most time) but also scales the resource well."
3293,"Collaborative Decision Processes focus on long-running, multi-faceted decisions (e.g., the planning and approval process for a municipal development project) made by teams of people. Such decisions can draw on the crowd in a variety of ways, by enabling large-scale on-line brainstorming, crowd-sourced queries, and text mining to determine public sentiments and derive new insights. Such decision processes consist of a combination of structured and highly unconstrained, human-drive activities. This paper describes a framework and a prototype system for support of collaborative decision processes based on declarative, data-centric dynamic artifacts and Advanced Case Management."
3294,"In this paper, we present a model-driven, domain-centric tool allowing the specification of service-oriented applications through abstract services composition. Executable applications, made of concrete services, are automatically generated and deployed. This tool can be customized with domain-specific knowledge. Our contributions in this paper are to slightly extend the tool to be used in the demanding home computing domain. An approach for modelling and implementing dynamic service composition is provided for satisfying the dynamic home environment. This work has been carried out in collaboration with Schneider Electric."
3295,"WS-Policy4MASC is a new XML language that we developed for specification of monitoring and control (particularly, adaptation) policies in the Manageable and Adaptable Services Compositions (MASC) middleware. It extends the Web Services Policy Framework (WS-Policy) by defining new types of policy assertions. Goal policy assertions specify requirements and guarantees to be met in desired normal operation. Action policy assertions specify actions to be taken if certain conditions are met or not met. Utility policy assertions specify monetary values assigned to particular situations. Meta-policy assertions are used to specify which action policy assertions are alternatives and which business value-driven conflict resolution strategy should be used. WS- Policy4MASC also enables detailed specification of additional information necessary for run-time policy-driven management. We evaluated feasibility of the WS- Policy4MASC solutions by implementing a policy repository and other modules in MASC. We examined their usefulness on a set of realistic scenarios."
3296,"This paper introduces the concept of the Logistics Service Map, a modular service construction system with an inherent catalog function. Specialized service providers, so called logistics integrators are to use this concept to integrate subsidiary service providers into logistics networks. The main purpose of the Service Map is to enable and to foster the development of atomic as well as composite logistics services used by the integrator to fulfill customers' requirements. After a brief presentation of the applied research methodology, the current progress of research is outlined and a draft of future research activities and discussion points is given."
3297,"This paper presents the applicability of service-oriented architecture to an e-engineering framework. The existing e-engineering framework models product development as engineering processes based on the collaboration of different experts who are participated in the development. And it aims to provide an integrated design environment to support integrating personnel, design activities and engineering resources during product development process. The applicability of the service-oriented architecture and web services technologies enable the framework to utilize and integrate effectively various engineering resources on the geographically distributed computing environments. And intelligent agent technologies support cooperation and coordination mechanisms for the engineering resources and services. In this paper we describe an implementation of the framework in detail and the application of it to the performance evaluation system for centrifugal pumps as a case study."
3298,"The quality of service (QoS) is a major concern in the design and management of Web service composition. Existing methods for QoS calculation either do not take the probability of path execution into consideration when QoSs are provided for different execution paths, or do not take different execution paths into consideration when a single integrated QoS is provided for the whole composition. In this paper, a comprehensive QoS analysis approach is proposed that calculates the QoS probability distribution by considering both the execution probability and execution conditions of each path in the service composition. Four types of basic composition patterns in the service composition are discussed: sequential, parallel, loop and conditional. In particular, a QoS solution is provided for all types of loop structured service composition."
3299,"Apart from Web services enablement technology and infrastructure, services pricing strategies determine its ultimate outcome for any services delivery in the community-based collaboration environment such as Web 2.0 enabled service market. In this paper, we model pricing strategies by taking services' demands into consideration. We show that the presented different pricing models are complement to each other. They fit the needs of different consumer types such as savvy consumers or causal consumers. One implication of the analysis is to design flexible pricing schemes within Web 2.0 services market as much as possible."
3300,"Business transaction is about coordinating the flow of information among organizations and linking their business processes, associated with the solutions to ensure the eventual generation of the consistent outcomes. One important issue here is how to maintain consistency for each partner in the collaborative business transaction as well as consistency for the transaction as a whole during the long running business process in the presence of failures and concurrent activities. Existing models and protocols by designing static process control can not support the dynamic behavior of the collaborative business transaction in the complex loosely coupled business environment. In this paper, we present a novel transactional model, BTx-Net, which is based on Hierarchical CPN to manage consistency for business transaction. Quite different from traditional CPN, two types of tokens, Management Token and Application Token, are used in BTx-Net. The correlation of both tokens is used for verifying consistency in business transaction. We believe that BTx-Net provides a theoretical basis for describing and managing collaborative business transaction consistencies."
3301,"This paper examines advantages of cloud while supporting real-time service systems using Salesforce platform. We build here a service management platform for the Polish Billiards and Snooker Association (PBSA), based on a real-time system located in a cloud. It allows PBSA managers accomplish tasks in this system on-demand. And, it is deployed as a private cloud to grant an access only to the employees from the snooker organization."
3302,"With the emergence of Business Process Outsourcing and Cloud Computing, enterprises are looking for available business processes outside of their organizations to quickly adopt to new business requirements and also reduce process development and maintenance costs. The process execution needs to be governed as policy enforcement might differ between different clients. Since a process is deployed outside of the organizations and serves multiple process clients, distribution and multi-tenancy have become two requirements for runtime governance of service processes. We address this problem by introducing a policy-oriented aspectual business process framework. The runtime governance from process clients are integrated as aspects through dynamic weaving into process execution."
3303,"Nowadays an increasing number of companies only deploy their enterprise application services over the Internet. Software as a Service (SaaS) in a cloud computing environment allows these companies to focus on providing more competitive services instead of maintenance. As delivery of computing as a service, a trustworthy cloud service widely depends upon its reliability. For this reason, a newly defined Quality of Reliability (QoR) for cloud services is proposed in this paper. To achieve a good QoR, we not only analyze system events from both service consumers and providers, but also provide a layered composable system accounting architecture for cloud systems. A pipelined approach and a dependence estimation algorithm are introduced for pattern recognition and event analysis and prediction. A self-healing layer is also designed to achieve automatic recovery by re-composing services according to their functionalities and non-functional requirements. An implementation of this framework in an education services environment confirms the advantages over extant system accounting systems."
3304,"Recent years have witnessed the development of Cloud Computing. The management of images is a big problem in virtualized environment because there are quantities of Virtual Machine images being stored in a Cloud and most of them are outdated. How to detect the outdated images and patch them efficiently? In this paper, we present a prototype called OPS- Offline Patching Scheme for the Images Management in a Secure Cloud Environment. In OPS, we can detect out the outdated image quickly by a module called Collector. Then a module called Patcher will patch the outdated images. In order to patch an image efficiently, offline patching technology is considered. For the large number of images in the Cloud, parallel scheme is also used. Our experiment results show that OPS can update numerous images efficiently."
3306,"The principle of local activity originated from electronic circuits, but can easily translate into other non-electrical homogeneous/heterogeneous media. Cloud resource is an example of a locally-active device, which is the origin of complexity in cloud scheduling system. However, most of the researchers implicitly assume the cloud resource to be locally passive when constructing new scheduling strategies. As a result, their research solutions perform poorly in the complex cloud environment. In this paper, we first study several complexity factors caused by the locally-active cloud resource. And then we extended the ”Local Activity Principle” concept with a quantitative measurement based on Entropy Theory. Furthermore, we classify the scheduling system into ”Order” or ”Chaos” state with simulating complexity in the cloud. Finally, we propose a new approach to controlling the chaos based on resource's Local Activity Ranking for QoS-aware cloud scheduling and implement such idea in Spark. Experiments demonstrate that our approach outperforms the native Spark Fair Scheduler with server cost reduced by 23%, average response time improved by 15% - 20% and standard deviation of response time minimized by 30% - 45%."
3307,"This paper describes a service-oriented computing kit (SOCK), which is an overarching framework covering the key artifacts in planning, modeling, designing, developing, deploying, and managing service-oriented solutions in the enterprise computing space. Based on a divide-and-conquer strategy, this comprehensive kit is a systematic taxonomy to abstract complexities and organize the major aspects of service-oriented development, so that the roles, responsibilities, accountabilities, skillsets, procedures and deliverables can be clearly defined for the IT teams to effectively construct service-oriented systems. SOCK comprises eight modules - architecture, process, integration, environment, technology, development, management, and roadmap. Each module addresses specific technical concerns in particular areas. SOA provides the service definitions, service catalog, service composition, scope of applications and portfolios, and service architecture modeling methodology. SOP deals with the business process management, process modeling and notations, process orchestration, coordination/collaborations, human interactions, and 1-phase commit compensation for long-lived processes. SOI concentrates on the service interoperability, integration patterns, enterprise service bus, traditional synchronous/asynchronous enterprise application integration, and integration with portals and content management systems. SOE consists of the runtime infrastructure, service registry for discovery and directory, service transformation/routing gateway, service virtualization via grid computing, and quality-of-services compliance with service-level agreements. SOT covers the implementation technologies - Web services, standards and specifications, technical patterns, convergence, and aspect-oriented techniques. SOD is composed of the development lifecycle, programming model, design/development tools, frameworks, reusable utilities/toolkits/components, and model-driven approaches. SOM includes ser..."
3308,"While WS-BPEL is emerging as the prominent language for modeling executable business processes, it provides limited support for designing flexible processes. An important need of adaptive processes is for concurrent activities in the process to respect coordination constraints. These require that concurrent activities coordinate their behaviors in response to events otherwise the process may become inconsistent. We show how the constraints that necessitate coordination maybe represented in WS-BPEL, and use generalized adaptation and constraint enforcement models to provide away to transform the traditional BPEL process to an adaptive one. The final outcome is an executable WSBPEL process without extensions capable of executing on standard BPEL implementations and able to adapt to events while respecting coordination constraints."
3309,"We describe a novel architecture for supporting mobile information access in the occasionally connected computing environment. By utilizing user workflow, profiles, and environmental information, information from various sources published to the system is prioritized, cached and synchronized in the staging server, and eventually disseminated to mobile devices. Furthermore, workflow and user profile can help in predicting near future data needs and therefore can enable intelligent data charging"
3310,"The grid information service (GIS), which is mainly used for resource discovery and monitoring, plays an important role in grid systems. Although many GIS systems have been proposed, e.g. MDS2, no appropriate solutions exist for the OGSA-based grid systems today. The architecture for OGSA-based GIS, i.e. FOSIS (forest structured information service), is proposed in this paper. FOSIS is featured by its forest structure, which is composed of multiple trees of GIS."
3311,"Services discovery becomes a key to accelerating the evolution of Web services as the number of services is expected to increase dramatically. In this paper, we introduce two types of dependency between interfaces in a service to our service matchmaking strategy. Depending on Assignment defines the dependency between the input and output interfaces of an operation, while Dependency on Sequence defines the invoking order among operations of a service. A formal service specification describing the two types of dependency is given. And based on it, a novel service matchmaking algorithm is introduced, distinguishing different matchmaking strategies for different transmission types of operations. Based on extensive experiments, we show that exploring dependency between interfaces is a promising way to realize intelligent service discovery."
3312,"The number of available bioinformatics resources is overwhelming. The most critical issues met by scientists are (1) to identify the resources suitable to express their protocols, and (2) to compose them in an executable workflow. Many advanced projects offer solutions to the composition of services in a workflow, while others address the interoperability of services. We present an approach that allows the organization of bioinformatics resources in a scientifically meaningful framework that leverages existing effort on interoperability and workflow composition and execution. We present ServiceSemanticMap, a system that allows users to explore the wide availability of bioinformatics resources organized with respect to an ontology. The user interacts with the system through a graph-based interface, discovering the resources by exploring the graph. The system exploits a resource catalog where providers may register their bioinformatics tools through a Web resource entry form. We demonstrate the approach with a semantic map devoted to structural bioinformatics. Our approach is collaborative: a group of experts develop and maintain the ontology, while providers and users may register new resources. The first version of the system is demonstrated on line for structural bioinformatics at http: / /bioserv. rpbs . jussieu . fr/SBMap/'index. html."
3313,"In a service oriented system both the internal and the external services may easily interact with each other, which may make the establishment and the enforcement of the privacy policies and the rules across the systems increasingly challenging. In this paper we present our ongoing system architecture and implementation work to enable efficient and flexible methods for auditing such systems by leveraging the middleware platform capabilities. We show how the service process flows that have been executed in the system can be audited both based on their structure and the information that is disclosed by them."
3314,"Most of existing approaches consider composition optimization as a back-end process and rarely expect non-technical end-users to interact directly with the optimization process. Thus these approaches do not enable the end-user to leverage the effects of direct interaction of the process of composition. Indeed, corrective actions can be taken only before and after the execution, not during composition construction. We focus on end-user support and optimized service selection during composition modeling. Of particular interest hereby is whether business users with limited IT skills can leverage our approach to optimize and consume web services and their composition."
3315,"The increasing use of the web as the platform for delivering business processes arises the need to protect both sensitive data exchanged over the Internet and the applications using these data. In this context, authentication, integrity and confidentiality of exchanged messages are requested during interactions between processes, and are commonly called WS* specifications. In this paper, we propose a formal specification of the above security requirements and the corresponding assertions in the exchanged messages, built on the XSB logic programming language. Our framework analyzes the generated models and verifies that incoming messages fulfill the security requirements of a web service. Furthermore, it verifies the compatibility between two policies, which is a significant condition in order to guarantee secure end-to-end SOAP invocations, and it is not currently supported by WS* specifications."
3316,"Today, businesses have to respond with flexibility and speed to ever-changing customer demand, market opportunities and external threats. Service oriented architecture (SOA) is a special way to look a IT systems, focusing on their adaptability - the ability to respond to changing and new requirements. It is more than evident that agile approaches to software development seem to be a natural fit for developing such systems. In this article, we try to harden this evidence. We take a look on the fundamentals of agile software development and assess their suitability for SOA-based systems."
3317,"The cloud computing paradigm provides an environment where services can be composed and reused at high rates. Existing composition techniques focus on providing the desired functionality and at a given deployment cost. In this paper, we focus on the definition of cloud service compositions driven by certified non-functional properties. We define a cost evaluation methodology aimed to provide the composition that minimizes the total costs of the cloud provider taking into account deployment, certification, and mismatch costs, and evaluate it using three different cost profiles."
3318,"We ease the design of collaborative business processes respecting desired business goals by the composition algorithm presented in this paper. The composition of multiple parties' business processes is always done with a specific objective in mind. Not only in the positive case, but also if the objective of a business process can not be fulfilled, all participating business processes need to be in some expected recovery state. We propose a composition algorithm solving the task of designing a collaborative business process while respecting a set of primary and recovery goals. In our model, each business process is described as a finite state machine. The multiplication of all business processes in one single model of possible executions would lead to an explosion of the number of states. Therefore, our composition algorithm directly interprets the multiple finite state machine (FSM) representations and creates a collaborative business process without integrating all FSMs into a single FSM upfront. Our composition algorithm returns an orchestration of the given business processes only in the case that it can be assured that each execution only leads to an expected primary or recovery goal. In order to prove our concepts, we first mathematically define the execution of business processes and orchestrations by providing abstract state machine (ASM) representations for them. Second, we execute the ASMs in the execution engine CoreASM which shows that the generated orchestration steers the execution of the business processes as intended."
3319,"Understanding the real world based on visualisation and prediction is essential for the decision-maker. We build a computational virtual reality environment to improve visualisation, understanding and prediction of the physical world and to guide action. It develops a five-dimensional, computer-generated, computational Virtual Reality Environment for Anaesthesia (VREA). Our online prediction will be calculated based on the correlation and composition computing with respect to the three dimensions: horizontal, vertical and individual. The novel musical notes based anesthetic simulator is proposed to identify the abnormality and visualize the online medical time series. The experiments with the online ECG data will present a real-time case to show the effectiveness and efficiency of our proposed system and algorithms."
3320,"In this paper, we introduce and explore a new computing paradigm we call knowledge as a service, in which a knowledge service provider, via its knowledge server, answers queries presented by some knowledge consumers. The knowledge server's answers are based on knowledge models that may be expensive or impossible to obtain for the knowledge consumers. While this new paradigm of computing is promising, we must establish a solid foundation to ensure its utility. We focus on the security aspect of the paradigm, and particularly on the problem we call knowledge breaching attack, which may allow an adversary to recover the knowledge underlying a knowledge service. Without being able to adequately handling such an attack, the knowledge service providers would never have any economic incentives to develop such a paradigm. Unfortunately, this paper theoretically shows that any interesting knowledge is subject to the knowledge breaching attack, and empirically shows that some knowledge models could be breached after a very small number of queries (e.g., 0.2-]% portion of the domain). Thus we need to investigate technical means that can alleviate such powerful attacks (at least for most practical knowledge models)."
3321,"In recent years, the ability to deliver IT infrastructure services from multiple geographically distributed locations has given rise to an entirely new IT services business model. In this model, called the ""Global Delivery Model"", clients out source components of their IT infrastructure operations to multiple service providers, who in turn use a combination of onsite and offsite (including offshore) resources to manage the components on behalf of the.ir clients. Since the components of services provided can be assembled and processed at any of the delivery centers, a framework for continuous monitoring of quality and productivity of the delivery processes is essential to pinpoint and remedy potential process inefficiencies. In this paper, we describe a framework implemented by a large global service provider that uses continuous monitoring and process behavior charts to detect any potential shifts in its global ITservice delivery environment. Using this framework, the service provider has already improved several of its IT delivery processes resulting in improved quality and productivity. We discuss the major components of the framework, challenges in deploying such a system for global processes whose lifecycle spans multiple delivery centers, and present examples of process improvements that resulted from deploying the framework."
3322,"Carpooling increases the occupancy rate of cars by decreasing the number of empty seats, thereby creating an effective solution to traffic congestion. This paper proposes an intelligent carpool system, BlueNet, which comprises two important modules. These modules are called the Mobile Client module and the Cloud Global Carpool Services module. By using smart handheld devices, users can submit carpool requests and obtain matches within the Mobile Client module via the Cloud Global Carpool Services module. The Cloud Global Carpool Services module generates acceptable matches via the Genetic-based Carpool Route and Matching algorithm. The proposed algorithm furthers the solution to the carpool service problem by dramatically reducing the time required to match a large number of users. In regard to the quality of the matches and processing time, the experimental results show that the proposed Genetic-based Carpool Route and Matching algorithm is able to find carpool route and matching results that are among the most optimal, and operates with significantly less computational complexity to require less services computing time."
3323,"Scientific workflows have become an important paradigm for domain scientists to formalize and structure complex data-intensive scientific processes. The ever-increasing volumes of scientific data motivate researchers to extend scientific workflow management systems (SWFMSs) to utilize the power of Cloud computing to perform big data analyses. Unlike workflows run in traditional on-premise environments such as stand-alone workstations or grids, Cloud workflows rely on dynamically provisioned computing, storage and network resources that are terminated when no longer used. This dynamic and volatile nature of cloud resources as well as other cloud-specific factors introduce a new set of challenges for ""Cloud-enabled"" SWFMSs. Although few SWFMSs have been integrated with Cloud infrastructures that provide some experience for future research and development, a comprehensive study from an architectural perspective is still missing. To this end, we conduct a hands-on study by running a big data workflow in Amazon EC2, FutureGrid Eucalyptus and OpenStack clouds. From this experience we 1) identify the key challenges for running big data workflows in the cloud, 2) propose a generic implementation-independent system architecture that addresses these challenges, 3) develop a cloud-enabled SWFMS called DATAVIEW that delivers a specific implementation of the proposed architecture. Finally, to validate our proposed architecture we conduct a case study in which we design and run a big data workflow towards addressing EB-scale big data analysis problem in the automotive industry domain."
3324,"For an IaaS cloud, the primary task is to satisfy users' demands for resources. Besides, administrators also have to deal with problems such as how to optimize allocation and utilization of resources at the data center level, how to guarantee the application's performance and scalability and how to cut the costs of maintenance and management. This paper focuses on the runtime optimization of IaaS private clouds. In such an environment, administrators usually have more autonomy and control over cloud resources and applications, which brings more space for optimization. In order to achieve appropriate resource allocation for virtual machines and improve utilization, this paper first proposes WarMops: a workload-aware method to optimize the resource configuration of virtual machines and an allocation scheme based on resource reservation and sharing, in an effort to arrive at the proper size of resources to meet the real needs of virtual machines. Integrating the methods mentioned above, this paper puts forward a systematic framework and modules for the runtime optimization of cloud resource management. This paper uses a mainstream benchmark application in this field - RUBiS to test the framework which, in turn, verifies the correctness and validity of the schemes."
3325,"Nowadays, many companies expose their competencies as services on the Internet to facilitate the cooperation with their customers. This situation has created a new marketplace where services have been provided with similar functionality but different qualities such as cost, performance, and reliability. In this scenario, service composition providers have faced the challenge of choosing services that fulfill an expected quality without compromising a planned budget. This challenge is even more stringent when services have different cost behaviors. As a consequence, services can be less expensive than others in a scenario and be more expensive in others. Several approaches have been proposed to address cost analysis of service compositions. However, these approaches have not considered all classes of possible cost behaviors, e.g., Fixed, variable, mixed and step cost. This paper addresses this limitation by proposing a solution to analyze costs of service compositions taking into account service reliability and all classes of cost behaviors. In order to evaluate the proposed solution, we carried out some experiments that show its effectiveness."
3326,"Problem determination in a large and dynamic IT service is a challenging task. In this paper we propose a framework for problem determination based on monitoring the event streams generated by the different components of an IT service. We give a generic representation of a problem through spatial-temporal patterns, which is a graph where the vertices capture the location and the time of the matching events, and the edges represent the spatio-temporal conditions between two matching events. The spatial conditions are based on the underlying system topology graph, and the temporal conditions are based on event timestamps.A practical implementation of the above framework will require fast algorithms for detecting patterns. We present efficient algorithms when the pattern graph is a range and a tree, which are then used as building blocks for a hierarchical heuristic for detecting general patterns. Finally, we show that our algorithms perform well in practice through extensive numerical simulations."
3327,"The target of grid monitoring and management is to monitor services in the grid for fault detection, performance analysis, performance tuning, load balancing and scheduling. This paper introduces the design and implementation of a WSLA-based grid monitoring and management system - GSMon. In GSMon, we use a service-oriented architecture, give monitoring information of the grid service a unique format by using WSLA and develop extended modules in the OGSA container. GSMon conforms to grid service and Web service standards and uses dynamic deployment technology to solve the problem of monitoring new grid services. It is a novel infrastructure for grid monitoring and management with high flexibility and scalability."
3328,"Data validation is one of the most important and, possibly, most under-valued task in an organization. Without clean data, an organization cannot employ sophisticated analysis and optimization tools to strive for excellence in operations, delivery or planning. Organizations have started to realize the value of data and its impact on their efficiency. Typically, they either develop in-house solutions or purchase industry standard solutions. In this work, we propose an alternative of data validation as a service offering. We argue that such a service would be a profitable proposition for both the parties, provider as well as consumer. We present a general framework to enable such an offering. We provide details on one such implementation that we carried to showcase the viability of such an approach. We propose multiple variants of the offering to handle privacy concerns of the consumer. Finally, we present a set of initial results comparing the different variants."
3329,"Semantic web techniques have been leveraged in planning methods for automated service composition. Typically, inputs and outputs of services are described in abstract concepts for efficient and meaningful matching between output of one service and input of another. However, existing methods have not examined concrete data structures, which are essential for successful service interaction in common implementation architectures. To address the problem, this paper proposes a matching method that can be incorporated into existing planning methods to ensure consistency in concrete data structures. The proposed method applies a two-phase matching process to efficiently filter out services that do not match at the abstract level. It also applies a data structure to organize similar services according to their relationships for efficient matching during the planning process."
3330,"Web service clustering is one of a very efficient approach to discover Web services efficiently. Current clustering approaches use traditional clustering algorithms such as agglomerative as the clustering algorithm. The algorithms have not provided visualization of service clusters that gives inspiration for a specific domain from visual feedback and failed to achieve higher noise isolation. Furthermore iterative steps of algorithms consider about the similarity of limited number of services such as similarity of cluster centers. This leads to reduce the cluster performance. In this paper we apply a spatial clustering technique called the Associated Keyword Space(ASKS) which is effective for noisy data and projected clustering result from a three-dimensional (3D) sphere to a two dimensional(2D) spherical surface for 2D visualization. One main issue, which affects to the performance of ASKS algorithm is creating the affinity matrix. We use semantic similarity values between services as the affinity values. Most of the current clustering approaches use similarity distance measurement such as keyword, ontology and information-retrieval-based methods. These approaches have problem of short of high quality ontology and loss of semantic information. In this paper, we calculate the service similarity by using hybrid term similarity method which uses ontology learning and information retrieval. Experimental results show our clustering approach is able to plot similar services into same area and aid to search Web services by visualization of the service data on a spherical surface."
3331,"Identifying similar Web services is becoming increasingly important to ensure the success of dynamically integrated Web-service-based applications. We propose a categorization-based scheme to match equivalent Web services that can operate on heterogeneous domain ontologies. Given the upper ontology for services and domain ontologies, our service matching scheme determines whether a given Web service is a possible replacement using a categorization utility called OnExCat. OnExCat categorizes ontology instances extracted from the service descriptions by a probabilistic categorization measurement that incorporates the concept relationships in the upper ontology for services. In addition to tackling the issue of heterogeneity of domain ontology in service descriptions using categorization, our matching scheme also adapts itself by enhancing the known ontologies with newly discovered ontology instances. Experiments on service matching using our matching scheme based on the OnExCat utility have been performed with promising results, a correct matching rate of over 85%."
3332,"Successful deployment and maintenance of enterprise SOA solutions involves configuring and managing several middleware software stacks. These middleware stacks have several hundreds to thousands of configuration attributes to be configured and managed. Ineffective Management of configuration information increases the cost, labor and time of troubleshooting configuration problems. The reason for it are two-folds: (a) configuration dependencies cut across different software stacks and there is no formal method to capture and consolidate them, and (b) incorrect configuration of attributes results in runtime exceptions. Correlating exceptions obtained during runtime to deployment/maintenance time configuration values is difficult as the context of the error is lost and is often not traceable. Current approaches to managing and troubleshooting problems associated with configuration information involve extensive human labor which comes at a premium. In this paper, we introduce Configuration Map (CM), which is an ordered set of configuration attributes for a deployable middleware component. Such a map when associated with exceptions helps reduce the cost of resolving configuration problems. We present several use cases and show from real life deployment scenarios that CM significantly reduces the cost of labor and time."
3333,"In peer-to-peer (P2P) grids, peers act both as providers and consumers of the services offered in the system. In these systems, fair and efficient match of service request and service provision can be attained by using simple reciprocation-based mechanisms. Under contention, a peer p gives priority to serve the requests of the peers that have the largest difference between the amount of service provided to p and the amount of service consumed from p in the past, i.e. the peers to whom p owes most. Taking a business-driven approach, peers have a cost for the provision of services, while gain utility for the consumption of services, and the peers' profit is given by the difference between the overall utility attained and the overall cost incurred. It has been shown that if a peer may offer multiple services, then the profit that it can extract from the P2P grid is highly affected by the services that it chooses to offer. Thus, the services selection problem can be formulated as an optimisation problem that seeks to maximise the peers' profit. Unfortunately, due to the many uncertainties that characterise this highly dynamic system, it is not feasible for a peer to deterministically decide what is the optimal selection of services that it should make. Thus, services selection algorithms must be based on heuristics. Some heuristics have been proposed, but although they perform well in a number of scenarios, there are many others in which they fall short. This is basically due to their inability to identify situations in which it would be better not to use their resources to provide services that will not yield any profit in the future. In this paper we use a hill-climbing approach to design smarter heuristics that can provide more profitable selections, when compared to previously proposed ones. Our simulation results show that, in the scenarios evaluated, the new heuristics proposed never perform worse than the best of the previously proposed ones, and can outperform t..."
3334,"New value-added services, generated by business process systems that composes services to fulfill user requirements, provide more powerful and flexible features. In most traditional process-centric business process design methods, developing work is largely devoted to data operating in back end business logic and front UI, while control flow design gives less information than implementing a complete business process that may require a much greater level of detail. It's a big challenge to automate these code generations in business process system development. This paper proposes an automatic code generation approach for web based business process software systems based on artifact-oriented business process application development, which can generate front UI and back end business logic code. The approach is applied to developing a practical heating repair business process software system, and the experimental data verify the advantages the proposed approach brought to us in generating code effectively, saving labor force and avoiding coding mistakes."
3335,"With the increasing demand for dynamic web service composition, traditional web service registries are no longer adequate in providing precise service selection. To address this problem, the paper proposes a fine-grain structural concept for traditional service registries, called web service community. Efficient semantic-level service enquiry and community management depend on context information. This paper investigates the issue of context modeling and proposes four categorizations of context information for service composition. Several design issues that relate to these context information are discussed, including community organization, context-aware management and semantic-level web service query."
3336,Service-Oriented Architecture (SOA) enables the re-engineering and migration of legacy software systems into loosely-coupled and interoperable sets of services. Existing approaches focus mainly on defining coarse-grained services corresponding to the key business requirements. An improved migration of legacy systems onto SOA-based systems requires identifying the `optimal' services with an appropriate level of granularity. This paper proposes a novel framework which identifies the key services effectively. The framework approach focuses on defining these services based on a Model-Driven Architecture approach supported by a SOA meta-model. Effective guidelines are proposed for identifying the optimal service granularity over a wide range of possible service types.
3337,"Quality of Service (QoS) can be used to select desired services from among those offering the equivalent function. In language services such as machine translation, one of the QoS metrics is translation accuracy. However, the problems are that evaluating the translation accuracy is too expensive, that the translation accuracy varies with the difficulty of the task, and that the usefulness of the translation to the user depends on the abilities of the user. In this paper, we propose a framework that selects a useful service for a specific user and task by using reputation information of users, which can be obtained at low cost. First, hypothetical reasoning is used to estimate the partial order relation between the accuracy of the language services, the language ability of the users, and the difficulty of the tasks. Second, deductive reasoning is applied to recommend useful services given the user and the task. We propose a reputation-based language service selection system that combines a partial order acquisition system with a service selection system."
3338,"Online Social Network (OSN) services are very popular nowadays. In order to protect the data privacy, Decentralized Online Social Network (DOSN) services have been proposed. In DOSN, the data published by a user and the data replicas are only stored in the friend circle of the user. Although full replication can improve Data Availability (DA), pure DOSNs may not be able to deliver sustainable data availability. This paper proposes a Cloud-Assisted scheme, called Cadros, to improve the DA in DOSN. This paper conducts quantitative analysis about the storage capacity of Cadros as the result of integrating the Cloud into DOSN, and further models and predicts the level of DA that Cadros can achieve. Extensive simulation experiments have been conducted to verify the effectiveness of Cadros."
3339,"By defining workflows, existing services are put into novel contexts and exposed to different workloads, which in turn can result in unexpected behaviors. This paper examines the chaotic behavior of sequential workflows in overload situations and discusses the use of call-contexts as a means of avoiding them."
3340,"Android mobile ecosystem has penetrated into people's every aspect of life. Since more and more sensitive and valuable information is stored in mobile ecosystem, how to configure the permission system to guarantee the security becomes an important issue for the whole community. In order to solve the permission over-privilege problem, a skewness-based permission recommendation framework is presented to identify the over-privilege permission and evaluate the risk for the Android Apps. Specially, the topic model Latent Dirichlet Allocation is employed to build the mapping between functionality and permission. Considering the fact that even the popular applications violate the least privilege principle, a collaborative filtering variant method in which high risk applications and permissions are identified is presented to recommend required permissions. Finally, the permission over-privilege based ap-proach is presented to evaluate the risk of the application. The experiments based on the Apps from Google Play shows that our approach is effective to identify the unexpected permissions for both popular and malicious Apps."
3341,Presents the welcome message from the conference proceedings.
3342,"Composite services are notoriously prone to failure, this is particularly true for long-running, and data-intensive services. Different composition strategies can be employed to make compositions robust. Any service composition strategy does impact performance at the lower network layer and needs to be assessed. Novel approaches are needed to model and evaluate dynamically reconfigurable service composition strategies. We propose an extensible framework to modeling and simulating the behavior of service compositions."
3343,"This paper presents an adaptive failure detection service (ALTER), which incorporates the technique of unreliable failure detection service and the idea of R-GMA. ALTER is organized in a hierarchical structure. It can be adaptive to the system conditions and user requirements with changing the system parameters and system organizations. With experimental evaluation, ALTER shows good scalability and flexibility."
3344,"The service oriented architecture (SOA) provides a methodology for designing software systems by integrating loosely coupled services. Compared to traditional distributed object-oriented architectures, SOA is more suitable to integrate heterogeneous systems, and more adaptable in a changing environment. This paper presents the design and implementation of a SOA framework for financial transaction applications. The framework provides an easy and uniform way for service composition in a controlled environment, and leverages Web service standards with efficient communication mechanisms and durable and/or transactional message queues. Specifically, the work addresses the following issues: 1) the incorporation of existing systems and protocols that are not Web-service compatible. This paper focuses on business processes of equities transactions using the FIX protocol; 2) the configuration and deployment of services and service endpoints in a flexible and dynamic manner; 3) the capability of specifying business processes as Web service compositions and a distributed runtime environment that supports it; 4) the scalability, resiliency and transactional aspects as required in critical business applications. The experience of applying the framework in building a high performance equities transaction system is presented."
3345,"Nowadays, we can conduct many businesses on the Internet, called e-business, like on the physical world. A general tool for communication on the Internet is a Webpage. The question is how one can get evidence to state the event when has disputes on the Internet. Available tools such as cybernotary will generate evidence when one requests. In reality, no one realize to do so, until the dispute occurs. So, a few people keep the evidence while the business occurs. This paper proposes the automatic e-evidence generation system that collects information passed through the Internet proactively. E-evidence is generated at a specific time that allows one who needs it for a later use to solve dispute. A trusted third party is an important mechanism to intercept Web pages between a Web server and browser. However, generating evidence proactively may cause information overload. Data reduction techniques are considered to reduce the huge amount of e-evidence. Data analysis is conducted to find the rate of data reduction."
3346,"A virtual organization (VO) is a dynamic collection of entities (individuals, enterprises, and information resources) collaborating on some computational activity. VOs are an emerging means to model, enact, and manage large-scale service computations. VOs consist of autonomous, heterogeneous members, often exhibiting complex behaviors. Thus VOs are a natural match for policy-based approaches. Traditional policy-based frameworks emphasize reactive behaviors, wherein an external request causes a policy engine to compute a response. However, business service settings require richer policies and call for proactive behaviors. A business not only must respond to explicit requests, but also monitor its environment, collate events, and potentially act in anticipation of events in order to ensure that its policies are satisfied. Autonomous, heterogeneous, proactive entities are best modeled as agents and, therefore, VOs are best understood as multiagent systems. Our main contributions are (1) a proactive multiagent policy-based architecture, (2) a hierarchical model of policy monitoring, compliance checking, and enforcement for VOs, and (3) a formalization of VOs. We evaluate our approach using a real business service scenario"
3347,"Order processing is an important application on services computing. On the conceptual side, business process management helps model such applications. On the practical side, workflow systems help implement such systems. We propose the navigation plan concept for order processing. Business steps in order processing are mapped into process algebra and composed into navigation plans. On the practical side, navigation plans are directly executed in the RiverFish architecture, thus guaranteeing the properties predicted by process algebra. Thus a well-defined order processing application is implemented into a reliable cooperative information system. A widely used application (the DECA system for requesting business TaxID involving several government agencies) demonstrates the usefulness of the navigation plan concept in practice"
3348,"With the flourish of mobile computing, mobile Apps dominate the daily lives of users. Focusing on the social relations among Apps, this paper makes an empirical study on constructing the Global App Network (GAN) in terms of three types of inter-App relations (i.e., intent-based, semantics correlation based, and similarity-based ones), recovering Personal App Network (PAN) in terms of App usage log of each user, and exploring the characteristics of GAN and PAN. The study is based on two real-world datasets: the first one includes thousands of Apps collected from a real-world Android App store, and the second one contains 2-month App usage logs of 40 volunteers. Several interesting phenomena are observed from the study, such as (1) a large portion of implicit inter-App relations that are welcome by massive users are actually ignored by App developers, (2) some explicit relations proactively designed by App developers are actually not frequently used by users, (3) although there is a certain commonness among PANs of different users, each PAN shows a significant personalized pattern which delineates the individualized behaviors of a user. These conclusions are of significance to bi-directional App recommendations, i.e., to recommend neglected inter-App relations to App developers, and, to recommend common and popular inter-App relations to users."
3349,"In order to loose the tight relationship between management system and the managed IT resources, applying SOA in the system a resource management field is a promising way, as web service can increase the interoperability among multiple resources, application and systems. WS-Management is a general Web Services-based protocol for managing system and network resources. Wiseman is an open source project which implements the WS-Management specification. The paper presents three new test evaluation schemes for WS-Management Protocol and its Implementation project-Wiseman, including JMeter &amp; JProbe based Semi-automatic Test Scheme, Grinder &amp; JProbe based Automatic Test Scheme, and Wiseman and WMI Based Test Scheme. The test experiments have verified that WS-Management shows good functionality and moderate performance. After that, we present our two improvement contribution points to the Wiseman project. Finally, we conclude this paper and analyze the prospective research direction."
3350,"The popularity of service-oriented computing makes more and more services available on the Web. Users make use of these services to achieve their personal goals, such as purchasing movie tickets on-line and booking flights. Existing research has proposed various techniques to assist users to select services for achieving user goals. Typically, user choice of services change under different contexts. However, these approaches cannot recommend the desired services based on the changes of user contexts, and are not able to learn from user service selection history. In this paper, we provide an intellectually cognitive personalized assistant framework to achieve user goals. In particular, considering user contexts and historical service selection, our framework interacts with users by asking relevant and necessary questions, and help users navigate through sets of services to identify the desired services. We have designed and developed a prototype as a proof of concept. We perform a case study to evaluate the effectiveness of our framework. On average, our framework, utilizing the learning-to-rank algorithm, namely AdaRank, improves the nine baseline approaches by 12.02%-31.52% in helping users find the desired services. Our user study results show that our framework is helpful in achieving user goals and useful in saving users' time in finding their personalized services faster."
3351,The literature on product-based workflow design considers only static collaborative environments where the product model is static and does not change. However the possible evolution of the product model and thus its impact on the collaborative environment are not considered despite the importance of this evolution for innovative product design. As a result the uncontrolled changes can easily break the cross-organizational process that links the different stakeholders in the collaborative environment. In this paper we present a framework that builds the collaboration contract from the product model and supports a controlled evolution of this contract. We leverage this framework by a set of management operations (services) that enable the contract evolution and investigate how a tailoring operation will not impact the executable cross-organizational process unless this process is maintained executable by the workflow engine. These operations are provided as services by a collaborative platform and displayed through a standard-based user interface.
3352,"Service delivery organizations are constantly under pressure to meet service level agreements (SLAs), operate under tight costs, utilize their resources well and to improve the operational performance. A well-planned task allocation plays an important role in meeting these objectives. This work considers the problem of efficient task allocation to employees in such organizations with the aim of meeting the SLAs by minimizing the number of tasks missing their deadlines and the magnitude of these deadline misses taking into account employees skills, productivity, utilization and fairness. For this problem, it proposes an integer linear programming (ILP) based solution which is SLA-, resource skill-, productivity-, utilization-and fairness-aware. It also proves an approximation guarantee for the problem using the primal-dual technique. Further, in empirical evaluations using real-world transaction processing data from a large services delivery organization, the proposed ILP-based solution outperforms the currently practiced manual allocation in the organization. It is able to reduce the number of deadline violations by 90% and the magnitude of violations by 95% and increases the productivity of resources by 23%-38%. Further, the ILP-based solution never took more than 2 mins to allocate transactions in our evaluations making it a promising solution to deploy in real-time."
3353,"For Component Based Software Engineering (CBSE), an application is a strongly structured and rigid assembly of components. Conversely, Service Oriented Computing (SOC) is very flexible and is a good candidate for supporting dynamic applications. Unfortunately dynamic applications are software applications and as such they need to be clearly structured and managed (as with CBSE), and they need flexibility and dynamism as with SOC. No platform today satisfies both needs. This paper presents the Component-Service model that combines well controlled structure and dynamism, and its implementation into the Apam component-service platform."
3354,"Organizations are increasingly faced with the challenge of managing business processes, workflows, and, recently, Web processes. One important aspect of processes that has been overlooked is their complexity. High complexity in processes may result in bad understandability, errors, defects, and exceptions leading processes to need more time to develop, test, and maintain. Therefore, excessive complexity should be avoided. This paper describes an experiment designed to validate the control-flow complexity (CFC) metric that we have proposed in our previous work. In order to demonstrate that our CFC metric serves the purpose it was defined for, we have carried out an empirical validation by means of a controlled experiment. The explanation of the steps followed to do the experiment, the results, and the conclusions obtained are the main objectives of this paper"
3355,"Due to the shift from software-as-a-product (SaaP) to software-as-a-service (SaaS), software components that were developed to run in a single address space must increasingly be accessed remotely across the network. Distribution middleware is frequently used to facilitate this transition. Yet a range of middleware platforms exist, and there are few existing guidelines to help the programmer choose an appropriate middleware platform to achieve desired goals for performance, expressiveness, and reliability. To address this limitation, in this paper we describe a case study of transitioning an Open Service Gateway Initiative (OSGi) service from local to remote access. Our case study compares five remote versions of this service, constructed using different distribution middleware platforms. These platforms are implemented by widely-used commercial technologies or have been proposed as improvements on the state of the art. In particular, we implemented a service-oriented version of our own Remote Batch Invocation abstraction. We compare and contrast these implementations in terms of their respective performance, expressiveness, and reliability. Our results can help remote service programmers make informed decisions when choosing middleware platforms for their applications."
3356,"In this paper we propose a method for aggregating ranked services. The ranked services are generated from multiple user requests for the same service domain. First, a service search for each individual request is performed and the search results are ranked based on the user's personalized non-functional attributes and trade-offs. Next, the ranked lists of services are then aggregated and top-ranked services are selected for the user. The proposed rank aggregation method produces a consistent ranking after aggregation because it includes the rankings given by other lists for its decision making. We propose two algorithms to deal with both complete and incomplete ranked lists during service aggregation. We also present examples with real-world services to show how the service selection based on rank aggregation works and also analyzes its performance."
3357,"A new type of Web-based applications, known as Enterprise Mashups, has been gaining momentum in the last years. Novel design principles are currently about to emerge allowing to cover the long tail of user needs and to provide individual and heterogeneous enterprise applications in a shorter time. In this paper, we introduce the main components of this new paradigm, and discuss the design principles of the architecture (Enterprise Mashup Stack), upcoming intermediaries and mass collaboration, lightweight composition as well as perpetual beta development model."
3358,"Currently, the number of Web services on the Internet is growing exponentially. Faced with a large number of functionally equivalent candidate services, users always hope to select the optimal one that can provide the best QoS values. However, users usually do not know the QoS values of all the candidate services as the limited historical service invocation records. Different QoS prediction methods are presented to predict QoS values of candidate services. Nevertheless, most of them do not take the physical features of Web services fully into consideration and thus the prediction accuracy is still not satisfying. To this end, we propose a novel Matrix Factorization method, integrating both user network neighborhood information and service neighborhood information with Matrix Factorization model, to predict personalized QoS values. To validate our method, experiments are conducted on a real-world Web service QoS dataset including 1,974,675 Web service invocation records. Experimental results show that our method performs better in prediction accuracy than the state-of-the-art methods."
3359,"Cloud Computing, as a distributed computing paradigm, consists in provisioning infrastructure, software, and platform resources as services. This paradigm is being increasingly used for the deployment and execution of service-based applications. To efficiently manage them according to the autonomic computing paradigm, service-based applications can be associated with autonomic manager (AM) components that monitor, analyze monitoring data, plan and execute configuration actions on them. Since an autonomic manager is composed off our basic components, then autonomic management of new applications lies on determining how many instance of each AM component to use in order to minimize the cost and maximize the management performance. In this paper, we propose both exact and approximate approaches that aim to choose the right number of AM components for managing service-based applications.Experiments shows the efficiency of our approach."
3360,"Deployment is an important aspect of software solutions' life-cycle and is repeatedly employed at many stages including development, testing, delivery, and demonstration. Traditional script-based approaches for deployment are primarily manual and hence error prone, resulting in wasted time and labor. In this paper we propose a framework and approach for faster redeployment of distributed software solutions. In our approach the solution is first deployed on virtual machines using traditional methods. Then environment dependent configurations of the solution are discovered and preserved along with the images of virtual machines. For subsequent deployments, the preserved images are provisioned, and the deployer is provided an opportunity to change a subset of the recorded configurations that cannot be automatically derived e.g. IP addresses, ports. The remaining recorded configurations are derived by executing meta-model level constraints on the solution configuration model. Finally, the virtual machines are updated with new configurations by leveraging the semantics of appropriate scripts. Our framework allows product experts to describe the configuration meta-model, constraints, and script semantics. This product knowledge is specified only once, and is reused across solutions for automatic configuration discovery and re-configuration. We demonstrate with case studies that our approach reduces the time for repeated deployments of a solution from an order of weeks to an order of hours."
3361,"Peer-to-peer applications harness sharing between free resources (storage, contents, services, human presence, etc.). Most existing wireless P2P applications concern merely the sharing of a variety of contents. For magnifying the extent of the sharing in wireless P2P (WP2P) environment, This work presents a P2P mobile service sharing infrastructure that empowers an autonomous peer to propel distributed problem solving (e.g., in the travel domain) through service sharing and execution in an intelligent way. This infrastructure of service composition is not only highly robust to failure but also keenly aware of the surrounding context in wireless environments. We also have implemented this infrastructure into a system platform named UbiSrvInt (that shows promising evaluation results)."
3362,"One reason that enterprises are adopting service-oriented architectures (SOA) is to develop applications more quickly by packaging - and then reusing - applications and data assets as services. Service encapsulation of implementation details is an important feature, and contributes to the loosely-coupled nature of a SOA. From this perspective, SOA data-services seem incompatible with AJAX frameworks which presume a great degree of client-side control of an application's data. For their part, AJAX frameworks promise to increase web-application performance by reducing the number of interactions between the browser and server. Caching server data on the web-client is a well known technique for achieving this goal, but implies that enterprise data is exposed to client-side developers. This paper presents ZAZEN, a SOA that mediates between the need to encapsulate enterprise data as a service and the needs of AJAX developers who want more control of their application's data. We describe ZAZEN's server-side architecture and discuss two APIs to the data-service: a REST API, and an implementation of the DOJO data APIs for relational databases."
3363,"A fault situation occurs to a service needs to be well analyzed and handled in order to ensure the reliability of composite service. The analysis can be driven by understanding the impact caused by the faulty service on the other services as well as the entire composition. Existing works have given less attention to this issue, in particular, the temporal impact situation caused by the fault. Thus, we propose an approach to analyzing the temporal impact and generating the impact region. The region can be utilized by the handling mechanism to prioritize the services to be repaired. The approach begins by estimating the updated temporal behavior of the composite service after the fault situation occurs, followed by identifying the potential candidates of the impact region. The concept of temporal negative impact is introduced to support the identification activity. Intuitively, the approach can assist in reducing the number of service changes in handling the fault situation."
3364,"During business consolidation, multiple organizations need to share data for common interests. However, these organizations may apply different or even conflicting policies, on account of different rules and rule combining algorithms chosen by them. Thus, it is necessary to combine policies from multiple organizations into a global one to manage the access to the shared data. Existing policy combining approaches are unable to automatically combine policies into a global one. In this paper, we propose an approach to address the issue of multiple policies combination. Its key idea is to first decompose the rules in each policy into various classes, and then combine the rules of the corresponding classes to a global compact policy. The latter ensures compliance with each of the original policies at the syntax and semantic levels. To validate our approach, we provide a proof-of-concept implementation of the automated policy combination."
3365,"With the development of services computing technology, more and more voluntary services have been available on the Internet. When using voluntary services, users tend to obtain higher QoS (e.g., get more computational resources to improve throughput of the services) than they actually need because there is no cost. To control QoS of the voluntary services appropriately, it is necessary to design resource allocation mechanism using utilities on both service users and providers. Therefore, we have proposed market-oriented resource allocation where users and providers exchange system resources and QoS based on their utilities. In our proposed approach, service users obtain more utilities if higher QoS is allocated according to their preferences in using the services, while service providers get more utilities if their services are more effectively used by their preferred users. In order to validate the proposed method, we have compared market-based approach with demand-based approach by simulation. The simulation results show that our approach motivated users to give true demands more than demand-based approach."
3366,"The barriers to offering mobile distributed services continue to be prohibitive for most: not only must these services be implemented, but they would also inevitably compete for resources on people's devices. This is in part because of the lack of precise understanding, specification, and analysis of such services. This paper presents MobDisS (Mobile Distributed Services), a model for representing mobile distributed services, allowing them to be carefully studied. Services can be built by composing simpler services. MobDisS is built on the Actor model, which is a well-established model for formalizing concurrent computations in open and distributed systems. The paper presents the syntax and operational semantics of MobDisS, and illustrates its use in specifying services."
3367,"During past several years, we have built an online big data service called CMA that includes a group of scientific modeling and analysis tools, machine learning algorithms and a large scale image database for biological cell classification and phenotyping study. Due to the complexity and “nontestable” of scientific software and machine learning algorithms, adequately verifying and validating big data services is a grand challenge. In this paper, we introduce a framework for ensuring the quality of big data services. The framework includes an iterative metamorphic testing technique for testing “non-testable” scientific software, and an experiment based approach with stratified 10-fold cross validation for validating machine learning algorithms. The effectiveness of the framework for ensuring the quality of big data services is demonstrated through verifying and validating the software and algorithms in CMA."
3368,"The wide use of Web services and scientific workflows has enabled bioinformaticians to reuse experimental resources and streamline data processing in a Web-scale manner. This paper presents a follow-up work of our network analysis on my Experiment, an online scientific workflow repository. The motivation comes from two common questions proposed by bio-scientists: 1) Given services which I plan to use, what are other services usually used together with them? and 2) Given two or more services I plan to use together, can I find an operation chain to connect them based on others' past usage? Aiming to provide a system-level GPS-like support to answer the two questions, we present Service Map, a network model established to study the best practice of service use. We propose two approaches over the Service Map: association rule mining and relation-aware, cross-workflow searching. Our approaches were validated using the real-life data obtained from the my Experiment repository. Empirical statistics of the constructed service network are also reported."
3369,"Many real world business processes are executed without explicit orchestration and hence do not generate structured execution logs. This is particularly true for the class of business processes which are executed in service delivery centers in emerging markets where rapid changes in processes and in the people executing the processes are common. In such environments, the process execution logs are usually a mix of human entered activity log of actions performed and the auto-generated logs by various tools used during the process execution. Process discovery from unstructured execution logs has been a relatively unexplored research area. In this paper, we propose an approach for process discovery from unstructured logs using gaussian mixture models and hidden markov models. We apply this approach to the logs generated by a real-world business process used in a service delivery center and demonstrate that the results obtained are comparable to an approach of manually labeling the logs followed by a best known process discovery algorithm in literature. The approach proposed is generic and applicable to a wide range of business process execution settings."
3370,"The rise of dynamic applications is coming with new development challenges. Indeed, dynamism is a complex concern, difficult to perceive and manage by developers. In the context of a large industrial project dealing with fleet management, we had to deal with important environmental and evolutionary dynamism. To make it easier for the development team, we have used and extended the iPOJO service component model. This paper presents how the dynamism is described in component metadata and how it is managed at runtime. The extensions have been integrated into the Apache Felix iPOJO source code."
3371,"This paper defines a service-oriented solution architecture for n-tier applications (SANTA), primarily for Web-based distributed systems. Most conventional Internet applications have been built on three tiers - Web, application, and database tiers as described in the predominant 3-tier architectural style on both Java EE and .Net platforms. However, a number of leading-edge technologies have matured, which need to be incorporated into the logical solution architecture, such as portal, process choreography, business rule engine, enterprise service bus, Web services, service composition, etc. A new service-oriented model is proposed in this paper, to extend the traditional 3-tier architectural style and position the emerging technologies/products in the right places in the architecture structure. This new architecture model comprises a stack of six interrelated layers, coupled with six vertical pillars. The six layers are access &amp; integration, business process, composite services, services &amp; components, integration &amp; communications, and enterprise resources layer. The runtime infrastructure pillars are composed of the operational management, security, and hosting environment pillar, whereas the development process pillars consist of the application &amp; service frameworks, crosscutting aspects &amp; patterns, and modeling &amp; development tools pillar. This holistic application architecture framework is a systematic taxonomy of major technical constituents of a distributed application in a service-oriented paradigm. Part of this comprehensive model has been extensively utilized in one form or another to design various SOA solutions in different industry sectors"
3372,"In the last few years, the amount of smart devices in domestic environments has incredibly increased. Nowadays, a smart home is usually managed via a gateway offering value-added applications by connecting devices to the cloud. Every new device comes with its own features and protocols or cloud services. There is, consequently, a strong need for constantly modifying the gateway's behavior by deploying, removing or updating applications. However, there is no software architecture ensuring enough flexibility and trust to sustain this need. We consequently propose in this article a framework that allows to easily compose modular and context-aware software architectures intending to host third-party applications. This framework - called AOLOA (Another OSGi-Like On Another) - is based on OSGi and Java permissions. It ensures applications isolation, separates business-logic (higher level) and platform (lower level) layers and allows their trusted management."
3373,"The need for cost-effective platform independent means of application invocation has fuelled technologies like Web services. Web services technology has found its application in a number of interesting ways, one of them being in creation of universal interoperable remote portlets. Web services for remote portlets (WSRP) is a standard that allows aggregator applications to consume remote portlets based on Web services interfaces. XML vocabularies like RSS (really simple syndication) or domain specific syndication syntactic frameworks allow for creation of raw data that can understood by a wide range of consuming portals and Web sites. Usually individual aggregators consume these raw data feeds and render it in some required UI (user interface) format. This approach requires that every single aggregator have a custom rendering application for generation of user specific UI, leading to considerable replication of effort. We take a wholistic view of the problem statement and propose an end to end architectural approach that combines usage of WSRP along with XML syndication feeds like RSS for creation of standards based, customizable, and dynamically generated reusable UI that can be consumed by aggregators like portals"
3374,"We present a method and a system for assessing the competitiveness of IT service solutions. The method enables sales teams to assess the competitiveness of their IT service solutions created during the sales phase in response to request for proposal (RfP). It involves comparing unit costs and unit prices of a given solution with those of client expectations, comparable prior deals and agreed upon rates defined within an IT service provider organization, competition and market. Our contributions include an approach to modeling IT service solutions in a consistent manner to enable comparison. This is a pre-requisite to the second step of comparing IT service solutions. Second, an approach to comparing IT service solutions is proposed. The approach takes into account structured and unstructured attributes as well as qualitative and quantitative values of attributes during comparison. We are currently performing precision and recall experiments at an IT service organization using 200 prior deals. Initial results are encouraging. In the absense of any prior tools to do this, our system is already proving to be a valuable asset to the IT service organization that we are developing the solution for."
3375,"Service-orientation is effective at managing complexity and dynamicity at a programmatic level, but there is still much work to be done in understanding and improving the trust that users place in a system's outputs, and the extent to which they understand the associated risks of decisions recommended by a system. This is crucial if we are to improve the uptake and real-world effectiveness of service-based decision-support systems whilst also reducing the risks (both perceived and actual) of using such systems. This paper presents the current progress of the STRAPP project, which is designing and engineering novel trust and risk assessment mechanisms for services computing and applying these to a number of real-world service-based decision-support systems. A new layered architecture model for trust and risk is introduced and described in detail, and we present our state-of-the-art work in risk-assessment, demonstrating the relationship between provenance data and risk via a mathematical model. We then give a detailed description of our latest software demonstrator, integrated into the Rolls-Royce Equipment Health Management system, and comprehensively discuss the lessons we have learnt from developing such a complex, holistic, and applied real-world system. Finally, we describe the future work we plan to complete."
3376,"This paper proposes the use of service-based architectures as a way to bring the benefits of ubiquitous computing solutions to real world problems. We compare this services approach to the traditional view of computer applications as user tools, noticing that service providers can bring significant economies of scale not only in monetary terms but also in computational simplifications based on multiple user data aggregation and large scale data information processing and gathering. We argue for such ubiquitous services by analyzing the evolution of the World Wide Web in the last two decades and the success of service providers such as Google, eBay, and Mapquest in changing the way people live; and by showing how a services approach can make feasible many solutions to real problems such as urban congestion, preventive health care, and global trade."
3377,"High resource utilization is an important goal in achieving high return on investment in cloud environments. Guaranteed quality of service (QoS) is an important goal for web-facing applications such as e-commerce. Achieving both high utilization and high QoS simultaneously is a significant challenge, since high utilization often implies more QoS failures such as long response times. In this paper, we adopt a profit model based on response time (i.e., decreasing or negative revenues for increasing query answer response time) to represent the QoS requirements. Our data shows that such a profit model often yields different analytical results compared to traditional performance metrics such as average throughput. Using extensive experimental measurements (on the same hardware platform and software stack) of the standard RUBBoS n-tier benchmark, we study the impact of different allocations of software resources such as the size of thread pools in various servers in an n-tier system. First, the profit model emphasizes the importance of appropriate allocations, showing a difference of up to 48.6% when system utilization is high (over 80%). Second, our experiments show that over-allocation of thread pool may lead to unnecessary consumption of critical resources (e.g., CPU) that reduce profits by up to 84.8%. Third, we found that under-allocation of thread pool in one server may lead to under-utilization of several servers downstream in an n-tier system, also reducing profits by up to 52.8%. Our data shows that the best allocation depends on several system parameters, including resource availability. We designed an adaptive algorithm to find the best allocations and show its effectiveness through our experiments and analyses."
3378,"Software services are, just like any other software system, subject to permanent change. We argue that these changes should generally be transparent to service consumers. However, currently consumers are often tied to a given version of a service and have no means of easily upgrading to a newer version. In this paper we propose a WSDL-driven classification of Web service change types and discuss a versioning mechanism for service-oriented systems that considers revision management on registry- and client-side. We use the concepts of service version graphs and selection strategies to provide transparent end-to-end versioning support, and show how this approach is implemented in our service-oriented computing runtime VRESCo. Furthermore, we illustrate the advantages of our approach in comparison to the current state of the art using a realistic case study."
3379,"Prior service science work defines a service system as dynamic configuration of people, technology, organizations, and shared information from both service providers and clients that co-creates value [1]. Although such an abstraction is important, it does not directly address core issues we face in today's service industry, such as sustainable service excellence. This paper expands such prior service science work by analyzing how service systems evolve over time and what factors are critical to sustain service excellence. The paper identified and analyzed three factors attributing to the service system health throughout its lifecycle phases: the instilling of the value co-creation concept in the service system, the balancing of innovation and commoditization dynamics, and the configuration of core resources in the service system, i.e., people, technology, organization, and shared information. The analysis is derived from many real-world case studies in the information analytics service area. Key recommendations and lessons learned are presented as well."
3380,"Increasing the productivity of Service-Oriented Software Engineering through a model-driven methodology needs to go beyond business service modeling and the automation of transforming models that adhere to the SOA style. Commonly, business analysts focus on modeling business processes. Then, IT developers implement software architectures that are automatically generated from the process models. In this paper, we look forward to maintaining the alignment between business services and their supporting IT assets when business settings evolve. We introduce an approach for an incremental synchronization between business process models and component configuration models that follow SOA architectural principles. We automate the change forward propagation by enforcing in-place translations of the updates on the models in order to preserve their consistency."
3381,"The Internet of Things (IoT), which connects things to the Internet, is being ushered in recent years. On top of that, the Web of Things (WoT) presents a new World Wide Web (WWW) with vast contents from thing resources. To date, these resources are being exploited only in business applications where data mining processes are run. This paper addresses a different use of these resources in which, end users are the resources' principal consumer. We propose a Browsing as a Service for the IoT that allows building representations of connected things on the Web or homepages for things. Browsing these homepages let end users to interact directly with the things, such as view things' data or carry out actuations. Our evaluation shows that the proposed homepages for things can be served well by constrained devices with computation resources as low as 16kB of memory and up to 18-MHz system clock. Using our cloud system, we present a demonstration that show how homepages for things can be designed and deployed by manufacturers and developers."
3382,"The Web environment has shifted from a distributed source of information to a distributed source of service. Service-oriented has become mainstream in software development. Because of this, research into the new theory and methodology of software should be carried out. As a new paradigm, Web components aim at providing support to service-oriented development by enabling automatic discovery, composition, invocation and interoperation of services. We discuss various aspects of Web components and present a concept model of Web components. By means of parameterized contracts, we discuss the introspection of Web components in detail and present an approach to tailoring the service."
3383,"Modeling with graphical notations is characterized by a wide range of notations, stakeholders, and tools. In the course of this paper, a generic web-based model encoding and editing protocol is outlined to introduce a novel approach to modeling utilizing open web standards only. It eases tool integration, aids collaborative model editing and is ultimately supported by browser-based editing environments."
3384,"Over the last decade, grids have become a successful tool for providing distributed environments for secure and coordinated execution of applications. The successful deployment of many realistic applications in such environments on a large scale has motivated their use in experimental science [L. C. Pearlman et al., (2004), K. Keahey et al. (2004)] where grid-based computations are used to assist in ongoing experiments. In such scenarios, quality of service (QoS) guarantees on execution as well as data transfer is desirable. The recently proposed WS-Agreement model [K. Czajkowski et al. K. Keahey et al. (2004)] provides an infrastructure within which such quality of service can be negotiated and obtained. We have designed and implemented a data transfer service that exposes an interface based on this model and defines agreements which guarantee that, within a certain confidence level, file transfer can be completed under a specified time. The data transfer service accepts a client's request for data transfer and makes an agreement with the client based on QoS metrics (such as the transfer time and confidence level with which the service can be provided). In our approach we use prediction as a base for formulating an agreement with the client, and we combine prediction and rate limiting to adoptively ensure that the agreement is met."
3385,The goal of the ShanghaiGrid is to provide information services to the people. It aims to construct a metropolitan-area information service infrastructure and establish an open standard for widespread upper-layer applications from both communities and the government. This paper introduces the Information Service Grid Toolkit in detail.
3386,"We are concerned with reliably harvesting data used to monitor a service-based system hosted in a mobile ad hoc network (MANET) environment. These data are time-bounded, time-sensitive time-series data recorded by individual hosts in the network. Harvesting is used to gather the data for global time-series analyses, such as fault localization. The MANET environment challenges data harvesting, due to the inherently unstable and unpredictable connectivity and the resource limitations of wireless devices. We present an epidemic, delay tolerant method to improve the availability of time-series monitoring data in the presence of network instabilities, asymmetries, and partitions. The method establishes a network-wide synchronization overlay to incrementally and efficiently move data to intermediate nodes. We have implemented the algorithm in Java EE and evaluated it in the CORE and EMANE MANET emulation environments."
3387,"The emergent IT clouds as the future of datacenters enable considerable opportunities for the services creation, deployment, management and usability. Users all over the world, from individuals to businesses have been taking advantage of the new cloud services automation and scalability benefits. However, the services creation and business support are still dominated by intensive manual labor. Offerings with similar infrastructure requirements and dependencies are mainly built from scratch as separated entities, making the service development inefficient and error prone. We propose a graph based solution for semi-automated service creation, which expresses the mapping between a business support system and an operations support system. We first identify and expose, at the leaf level of our graph, the meaningful IT operations in the form of basic services. Then, we extend our graph by representing existing services offerings in terms of these operation level service definitions as well as simpler services offerings. At service creation time, an offering manager can re-combine existing building blocks to define new services, besides implementing new blocks down to the operations support system. Our solution takes into consideration the constraints and costs of the service offering sub-components as far as their mapping down to datacenter resources for optimizing the service placement into data-centers. We present a study of the Desktop Service use case."
3388,"Service compositions are established to implement complex functionality based on elementary services in different application areas. The execution time of a service composition is crucial in many application domains. A service composition is said to be time-critical when its execution time is restricted. Because of uncertain response times, the execution time restriction of a time-critical service composition can be violated. Several reconfiguration approaches dealing with service failures and deviating QoS values were recently proposed, but because of their general purpose, they do not consider specific characteristics that occur in the presence of uncertain response times. In this paper, a time-sensitive reconfiguration is proposed that takes into account the uncertainty of response times and the temporal conditions found at execution time of a time-critical service composition. It is shown that a reliable execution of time-critical service compositions is achieved with only slightly increased cost."
3389,"Late-binding and substitutability offered by the service-oriented approach improve adaptability but increase the need for fast and efficient algorithms to select services. In this paper, we proposed to use the Formal Concept Analysis (FCA) approach as a classification tool to select services at runtime, according to user specifications. We propose to classify existing services and generate a decision tree to help user select the most appropriate service(s). One of advantages of using FCA is the ability to select without additional cost an equivalent service in the case of a service must be replaced at runtime. Our approach have been implemented and validated on pervasive use cases within a European collaborative project."
3390,"Service-oriented Architectures (SOA) and Web services leverage the technical value of solutions in the areas of distributed systems and cross-enterprise integration. The emergence of Internet marketplaces for business services is driving the need to describe services, not only from a technical level, but also from a business and operational perspective. While, SOA and Web services reside in an IT layer, organizations owing Internet marketplaces are requiring advertising and trading business services which reside in a business layer. As a result, the gap between business and IT needs to be closed. This paper presents USDL (Unified Service Description Language), a specification language to describe services from a business, operational and technical perspective. USDL plays a major role in the Internet of Services to describe tradable services which are advertised in electronic marketplaces. The language has been tested using two service marketplaces as use cases."
3391,"In peer-to-peer (P2P) environments, a peer needs to interact with unknown peers for the services provided. This requires the trust evaluation prior to and posterior to interactions. This paper presents Trust/sup 2/: a novel and dynamic peer trust evaluation model, which aims to measure the credibility of peers' recommendations, and thus to filter noise in responses and obtain more accurate and objective trust values. In our model, prior to any interaction, the trust value results from the evaluations given by other peers. Posterior to interactions, the trust values results from both other peers' evaluations and the requesting peer's experience. In the aggregation of trust evaluations, the weight to the requesting peer becomes higher and higher. Meanwhile, during this process, the credibility of each responding peer's recommendation can be measured round by round. This leads to the filtering of low credibility peers and the improvement of trust evaluations."
3392,"Social networking has become a frequent activity with many users having accounts on multiple social networking sites. Profile attribute inference has gained popularity due to its usefulness in applications such as link prediction and recommender systems. This paper proposes to infer ego user attributes, by propagating the known attribute values of followers or followings within certain circles. With the ability to follow or be followed by any user, the possibility of many weak links being formed is high. We utilize tie-strength to address this and differentiate each users' influence in the ego user attribute inference. We show that our approach based on followers sub-network, following sub-network and all links sub-network yields better accuracy than friends sub-network, which is the converted undirected network obtained from a directed social network. We conduct extensive experiments and achieve better accuracy than the previous best method for attribute inference."
3393,"This paper proposes a rapid application development framework for creating mobile Web services supported by a dynamic workflow engine that displays an interface depending on a specified workflow. The model leverages on existing reference architectures, and open source standards, to create an interoperable, flexible and easy to implement development framework for creating mobile Web services. Our model is generic in nature to allow the development of Web services for a diverse range of scenarios. The dynamic workflow engine, residing on the client allows for easy updates on the server, and again caters to a diverse set of applications on the client side. The rapid application development model for mobile Web services proposed in this paper has been applied to a field service solution workflow and details on the implementation at both the client and the server are provided."
3394,"With the prosperity of Cluster Computing, Cloud Computing, Grid Computing, and other distributed high performance computing systems, Internet service requests become more and more diverse. The large variety of services plus different Quality of Service (QoS) considerations make it challenging to design effective allocate and scheduling algorithms to satisfy the overall service requirements, especially for distributed systems. In addition, energy consumption issue attracts more and more concerns. In this paper, we study a new energy efficient, profit and penalty aware allocation and scheduling approach for distributed data centers in a multi-electricity-market environment. Our approach efficiently manages computing resources to minimize the processing and transferring energy dollar cost in an electricity price varying environment. Our extensive experimental results show the new approach can significantly cut down the energy consumption dollar cost and achieve higher system's retained profit."
3396,"The complex requirements of E-commerce application call for selecting a set of Web services to reuse theirs business logics, where Service Oriented Architecture (SOA) provides a promising solution to the problem of cross-platform services integration. To this purpose, how to discovery services is a key to support the quick services composition, which has been a challenging task in Web application engineering. However, the traditional approaches have limitations in recall ratio and precision ratio because the keyword and semantic annotation query modes are hard to verify the target Web service, such as interaction behaviors and control flows of composite service. In this paper, it proposes a probabilistic model checking based service discovery method. First, it focuses on service reliability and time constraints to formalize the probabilistic behaviors of service process. Second, the quantitative verification properties of user requirements are specified in the form of temporal logic formulae. Third, the service discovery is to verify service process model against expected properties for identifying candidate services. Finally, in order to reduce model checking tasks, the correlation based service recommendation is introduced to explore more suitable services for user. Our framework improves the efficiency of service discovery without changing any existing service infrastructures."
3397,"The evolution of the web triggered XML and web services which pave the way for world-wide information integration and service integration. Web 2.0 concepts and technologies further enriched community based web services. Now with debut of virtual world, there come the opportunities of new types of services with which services providers and services consumers could interact in immersive services environments. Virtual services or 3D services provide new service opportunities that existing web services cannot provide. In this paper, we compare the interaction patterns for web services and virtual services. We propose an architecture for virtual services and list its implications for new business models."
3398,"As a common delivery model in cloud computing, SaaS applications are becoming increasingly popular. With the increasing of user's individual and diverse requirements, multi-tenancy has been the main delivery model for SaaS applications in future. Meanwhile, in order to adapt to the complex application development, application components had become modularization and fine-grained. Thus, new applications can be built through assembling those components quickly and agilely. So service composition for multi-tenant is the key to build application flexibly. This paper proposes a service granularity space for multi-tenant service composition based on granularity computing, which provides a semantic basis for multi-tenant service composition. The service granularity space supports the characteristic of hierarchy, inheritance, evolved correlation and versioning, effectively responding to the challenge of service composition for cloud computing SaaS applications. On the one hand, the service granularity space makes it possible to change disorder services into the hierarchy and ordered clustering services, and on the other hand, it makes it easy to develop multi-tenant SaaS applications by combining customization and evolution according to individual and diverse requirements. Our final experiments further demonstrate the feasibility and the efficiency of our proposed approaches."
3399,"The capability to analyze systems and applications is commonly needed in data centers to address diverse problems such as root cause analysis of performance problems and failures, investigation of security attack propagation, and problem determination for predictive maintenance. Such analysis is typically facilitated by a hodgepodge of procedural code and scripts representing heuristics to be applied, and configuration databases representing state. As entities in the data center and relationships among them change, it is a challenge to keep the analysis tools up-to-date. We describe a framework that is based primarily on the principle of interpreting declarative representations of knowledge rather than capturing such knowledge in procedural code, and a variety of techniques for facilitating the continuous update of knowledge and state. A metamodel representing data center-specific domain knowledge forms the foundation for the framework. A model of the data center topological elements is an instantiation of the metamodel. Using the framework, we present a methodology for conducting a variety of analyses as a model-driven topology subgraph traversal, governed by knowledge embedded in the corresponding metamodel nodes. We apply the methodology to perform root cause analysis of performance problems in the domains of 3-tier Web and InfoSphere Streams applications."
3400,"Designing the qualities of services (QoS) and verifying their ensuring at runtime are key issues addressed by today information systems. In this paper, we present our solution to these issues through a case study related to the provisioning of media streaming services. Our approach is based on the main concepts of the service oriented architecture and exploits the service level agreements to express QoS and adaptive mechanisms to monitor and change QoS at runtime."
3401,"Navdriver is a freely-available Software-as-a-Service technology that leverages hotlink assignment algorithms. This paper provides a formal description of the hotlink assignment problem, as originally described over a decade ago, as well as its representation inside Navdriver. This paper reconciles the description of Navdriver found in [1] with the mathematical statement of the hotlink assignment problem used in [2]. Furthermore, this paper describes, for the first time, details of the Navdriver algorithms that had not been disclosed before. The paper also discusses the applicability of Navdriver in real websites as well as limitations, in general, of inserting hotlinks dynamically in production websites."
3402,Component business modeling (CBM) serves as a powerful analytical framework for reasoning about the business as a set of business components that collaborate through the provision and consumption of business services. This paper proposes and illustrates a method to calculate the relative importance of the entities that make up a componentized enterprise architecture. The proposed method includes a formal definition of the importance of each entity in the business architecture calculated from the high level business values.
3403,"Service oriented architecture (SOA) helps dynamically construct composite services out of a set of low-level atomic services to satisfy customer requirements. For the purpose of capacity planning and resource provisioning, it is important to understand these services' demand for system resources, e.g., CPU. In this paper, we propose a black-box method for estimating CPU demand of service requests based on linear regression between the observed request throughput and resource utilization level. A key advantage of our method is that its input data (i.e., request-processing throughput and resource utilization) can be easily obtained without intrusive software instrumentation. Moreover, we observe that, in an SOA environment, the service composition relationship (i.e., how low-level atomic services are connected into a composite service) is either known in advance or can be discovered through various means. We leverage this composition relationship to further improve the quality of CPU demand estimation. By analyzing the dependency between a composite service and its constituent low-level atomic services using linear algebra, our method can eliminate the collinear problem introduced by the service composition relationship. Moreover, our method can further reduce the number of unknown variables in the linear regression problem, and hence reduce the time duration needed to collect input data. In a dynamic SOA environment, this translates into faster response to changing workloads and more accurate estimation. We demonstrate these advantages of our method over a baseline method through extensive evaluation."
3404,"Summary form only given. The Business Process Execution Language for Web services (BPEL) has emerged as the de-facto standard for cross-organizational business processes and business interaction protocols for connecting Web services. At the beginning of BPEL, people mainly focused on automated business process execution. Although intended as a language for connecting Web services, BPEL has been applied by ambitious people to realize a wide variety of process-aware intra-organizational application, which gives the heavier burden on BPEL than ever. Faced with emerging wider scenarios (complex human activity, various non-Web service APIs, etc), original BPEL has difficulties for fulfilling the new requirement. Human activities have complex and dynamic impact on business process execution. BPEL was designed for automated service-to-service interactions, and did not define mechanisms interacting with people in its current version. Considering most modern business processes commonly involving some extent of human-intensive workflow, inability of specification is actually serious, and in some cases fatal. To complement the lack of BPEL on human-driven workflow, and coordinate work performed by human-driven workflows and automated processes, this paper presents a novel four-step procedure. In the first step, we distinguish human activity from automated process among business process, and treat them as external process and internal process, respectively. While the former supports a more event-driven, flexible and dynamic human-driven workflow with unstructured data, the latter, as lower part of the former, supports sequential, predefined and static approach with structured data. In the second step, we utilize state machine to model the human-driven workflow. The state machine provides a flexible and efficient way to model the more loosely defined nature of human workflow. In the third step, we transform the state machine-style workflow to the BPEL-style process. In the fourt..."
3405,"Testing Web services for robustness is a difficult task. In fact, existing development support tools do not provide any practical mean to assess Web services robustness in the presence of erroneous inputs. Previous works proposed that Web services robustness testing should be based on a set of robustness tests (i.e., invalid Web services call parameters) that are applied in order to discover both programming and design errors. Web services can be classified based on the failure modes observed. In this paper we present and discuss the architecture and use of an on-line tool that provides an easy interface for Web services robustness testing. This tool is publicly available and can be used by both web services providers (to assess the robustness of their Web services code) and consumers (to select the services that best fit their requirements). The tool is demonstrated by testing several Web services available in the Internet."
3406,"A precise definition of services is necessary in order to discover, publish, and deliver them. Services, when provided, should satisfy their binding contractual obligations, be seen as trustworthy by the users, and correctly fulfill the needs of the context in which they will be used. The FrSeC architecture proposed in this paper aims to fulfill these needs. The architecture is formally describable, supports the specification, publication, discovery, selection, and composition of services with context-dependent contracts."
3407,"This paper describes characteristics of information and service design by exploring the needs and motivations of tourists. Tourists are expected to be important and demanding users of location-based services. They will need customized means to filter their experience of destinations, as well as ways to meaningfully participate in the creation of narratives and histories about different places. Mobile technologies will also allow tourists to be more discriminating in their patronage of different service offerings, especially as they gain greater knowledge of so-called ""backstage"" processes. These demanding needs will require choreography between services offered by many different commercial, cultural, educational, and community providers. The paper suggests approaches to deliver tourist location-based services based on low barrier of entry principles of Web architecture. The paper concludes with a discussion on how the erosion of backstage/front-stage distinctions in service systems impacts service innovation."
3408,"Most approaches for resource management employ simplistic behavioral models of heterogeneous resources. To fully support the rich interaction between resources and business processes in a real business environment, in this paper we introduce a novel approach where resources are modeled in much the same way that we model business processes. Resources are systematically modeled as full-fledged business entities to capture resource behavior and also the interactions between resources and business processes. A service-oriented resource management architecture is developed to manage resources and support a range of resource allocation patterns and policies. This architecture can be flexibly integrated with BPM applications to enable monitoring resource utilization and optimizing resource allocation. This new approach to modeling resources sheds light on the interplay between business processes and resources."
3409,"Service oriented architectures (SOAs) have emerged as a preferred solution to tackle the complexity of large-scale, complex, distributed, and heterogeneous systems. Key to successful operation of these systems is their reliability and availability. In this paper, we propose an approach to creating fault tolerant SOA implementations based on an architectural pattern called Rich Services. Our approach is model-driven, focuses on interaction specifications as the means for defining services and managing their failures, and is technology independent. We leverage an enterprise service bus (ESB) framework to implement a system based on the fault tolerant Rich Service pattern. We evaluate our approach by measuring availability and reliability of an experimental system in the e-business domain."
3410,"In SOA, service composition and selection can be viewed as delegation behaviors. Due to multi-levels service workflow, the delegation relationship among services is often confused and the overall trust of component services by the requester cannot be evaluated quantitatively. In this paper, we propose a model to depict the delegation relationship and quantitatively measure the implied trust degree among services in SOA. Firstly, we use Hierarchical Elementary Net System (HENS) that is based on the Petri Net to model the service workflow. Then, a Delegation Tree is abstracted from the HENS. Based on the Delegation Tree, we can derive the delegation relationship among services, calculate the trust degree among services which have delegation relationship, and find out the Critical Trust Path in the Delegation Tree. This model is important because it can quantify service delegation in SOA; it can also be used to guide delegation process thus reducing service risk due to delegation."
3411,"Organizations adopting service oriented architecture can derive higher business value only if the functionality provided by existing systems is reused. ""Servicising"" existing systems (locating and exposing available functionality as a set of service based operations), is an efficient way forward, to the evolution of existing systems to SOA. To facilitate the location of service(s) already implemented in the systems, a helpful step is to locate components that realize the required functionality. In this paper, a static and semi-automatic approach to locating components realizing service is proposed. The approach consists of three steps: retrieval of links between a service and the components in the source code, filtering of links using static structural analysis and finally categorizing them as functional and technical components. The results of the approach applied to two open source systems are presented. The results indicate that the approach is quite effective in identifying the links between the service descriptions and source code components and categorizing them into functional and technical components."
3412,"The nature of the relationship between the IT function and the lines of business shapes the policies for managing business services. In particular, the selection of lines of authority and financial model for the IT function dictates which business objectives drive the management of competing business demands for shared IT resources and services. This paper proposes to model business service management policies in a three-layer hierarchy and studies the impact of IT governance models on those policies. A software embodiment for the management of those policies is also highlighted"
3413,This paper describes a provenance-based access control system for the cloud. The system combines both provenance and access-control models together with a rule-based mechanism. The system allows rule propagation (in the cloud) and a central execution engine for enforcing security and trustworthiness.
3414,"Summary form only given. In response to pursue of achieving business agility and technology flexibility through SOA, this paper provides insight and discussion regarding to what SOA is, what it really means to an enterprise, where it is right now, where it is leading to, how to practice it, the expected benefits, and how to calculate associated ROI. It includes discussion in SOA concepts, technologies, and best practices based on practice experience, survey from public sources, as well as initial ideas and contributions. SOA enables agile businesses through composable business processes and services that are supported by flexible and composable IT services. The commonly accepted standards ensure interoperability, shareability, and reusability. SOA can be applied to the full spectrum of enterprise business and IT, which include business service specification, IT strategic planning, enterprise architecture, solution development, and business operation. Also, SOA can be considered as a practical modeling approach for enterprise architecture (EA) development. It can help to bridge EA with solution architecture and implementation by layered service descriptions across business modeling, application modeling, and technology implementation; so that it can help bring EA into reality. The ROI for SOA should consider the full spectrum of SOA benefits. A ROI reference matrix can be constructed based on the value proposition and IT strategic planning, which can provide guidance for iterative ROI assessment and performance measurement. The concept of SOA is not new, which can be traced back to the common object request broker architecture (CORBA). The popular component-based and service-oriented architecture has extended its scope to business domain, which is reflected in federal enterprise architecture (FEA). Web services enable the SOA concept being applied in Web environment. The content in this paper includes SOA conceptual model, federated SOA service infrastructure, enterp..."
3415,"Cloud computing offers lots of attractive benefits to individuals and organizations. However, the adoption rate is a far cry from what it really deserves. Lack of well defined Service Level Agreement (SLA) is one of the key factors. This research aims to provide better understanding of the status quo of the current public cloud service level agreements and to recommend best practices. We conducted a content analysis on around thirty SLAs to identify commonly stated attributes and commonly missing attributes. A follow up case study interviewed a selection of cloud customers further investigates the importance of SLA attributes and its relationship with potential business. The result reveals that some of the commonly missed attributes such as customer support, data protection plan are considered as very important attributes and most SLA attributes are directly related with business values."
3416,"As more and more applications are based on SOA (service oriented architecture), effective service discovery is an urgent requirement for such service applications in the cloud environment. Some existing work focus on taking service content as text and using document search technique to implement service discovery. However services are designed to implement some specific functions or objectives, which leads to the underlying ""topic"" or ""semantic"" of services. In this paper, we study the top-k service retrieval problem from both the text perspective and the semantic aspect, which is to find a set of k services that can best answer a query and the result set is to balance between the content relevance and the topic diversity among the returned services. Both service content and service topic are considered to identify the candidate services. We propose the objective function which is sub-modular, and we design the search algorithm with a approximation guarantee of factor 1 - 1/e for the (best-first) greedy search algorithm. Experiments on a large TREC benchmark and services collection show the effectiveness of our approach."
3417,"Service creation specifies and generates the model of a SOA application, which drives the subsequent activities in the SOA development lifecycle. Contemporary service creation is mostly done manually. Ontology technology can enhance the degree of automation and semantics of service creation, however, most works on ontology focus on ontology usage in fields other than software service development. This paper proposes an ontology based service creation process framework to apply ontology really for software service development. An ontology system that facilitates reuse in service creation process was designed, a four-phase service creation process was proposed, and a shipping case was carried out on our service creation tool to show the increasing service reuse rate under the ontology based service creation process framework."
3418,"Claim based identity management denotes an open identity model which uses the notion of claims to describe identity attributes. A claim is an identity attribute named with an abstract identifier (e.g. a URI), which applications and services can use to specify the attributes they need. Open and extensible formats for the exchange of identity attributes ensure interoperability among different identity systems. For this reason, claim based identity management lays the ground for Identity metasystems, which provide an identity layer on top of existing identity systems and promise an easier management of digital identities among the Internet.However, the Internet grew into an environment of mostly isolated domains for a good reason. Service providers find it hard to accept identity information from any other than the own domain. While claim based identity management provides the means to specify identity information on a per attribute basis, trust is usually defined in a general manner. Service providers state the issuers of identity information, they trust, but do not restrict for what. In this paper, we argue that for a truly decentralized management of identity information, trust should be defined on the same granular level as identity information. We propose a model which considers trust on a per-claim basis. In our model, trust into a claim is defined as the assumed correctness and integrity of a claim in dependence of the issuer. As a proof-of-concept, we implemented a small flight booking scenario which uses claims augmented with an expected trust level to show how we can achieve more flexibility for the user in his choice of an identity provider when considering not only whom to trust, but for what."
3419,"Marine logistics service is one of the fundamental production services supporting the import and export businesses of other industries. The current marine logistics service eco-system formed spontaneously, and during its development, various types of service providers were stepwise imported and complex business collaborations formed between them. Based on our investigation on marine logistics services between South Korea and Weihai, China, we found that some deficiencies existed in such a decentralized service eco-system and led to non-optimized quality of marine logistics service. To solve these issues, we designed a bi-lateral resource integration oriented solution, in which a third-party service agency is established to help cargo owners and various logistics providers set up mutual service relationships, integrate bi-lateral resources, and monitor and evaluate quality/credit of services, thereby improving QoS and customer satisfaction. Business operation mechanisms, especially profit patterns, are briefly discussed. Benefits and disadvantages of BIRIS-based solutions are analyzed. Business and technological lessons learned from the solution are briefly put forward. Finally some possible directions for future innovations and improvements are discussed."
3420,"The dynamic unpredictable nature of service-oriented environments demands service-based systems to be inherently flexible and adaptive. In this paper, we propose a constraint-based framework for supporting dynamic business process adaptation. In our framework, process adaptations are performed in a modular way based on process fragments. Process fragments are standalone fragments of processes that can be reused across multiple processes. Processes are composed of a series of fragments. The relationships among fragments are specified in a constraint-based way. The adaptation logic to select concrete fragments is separated from business logic in a process and can be defined in a policy-based approach. Processes are dynamically generated based on the constraints and adaptation policies according to the operating environments, so it is flexible to adapt to the ever-changing operating environments and business requirements. We have implemented a prototype to demonstrate and evaluate our approach."
3421,"Mashup is a hallmark of Web 2.0 and attracts both industry and academia. It refers to an ad hoc composition technology of Web applications that allows users to draw upon content retrieved from external data sources to create entirely new services. Compared to traditional ""developer-centric"" composition technologies, e.g., BPEI and WSCI, mashup provides a flexible and easy-of-use way for service composition on Web. It makes the consumers free to compose services as they wish as well as simplifies the composition task. This paper makes two contributions. Firstly, we propose the mashup architecture, extend current SOA model with mashup and analyze how it facilitates service composition. Secondly, we propose a mashup component model to help developers leverage to create their own composite services. A case study is given to illustrate how to do service composition by mashup. This paper also discusses about some interesting topics about mashup."
3422,"Accepting recent proposals to supporting dependable e-health enterprise systems via the adoption of emerging autonomic grid computing and services-oriented architecture (SOA); this paper presents the motivations for a novel semantics/knowledge plane for a service-oriented e-health middleware and applications. The paper starts by a detailed description of a developed sensor and actuator framework and overlay. This is followed by, and illustrated using a remote healthcare monitoring scenario. A case-study of pregnancy monitoring is used to illustrate the design and usability of such a grid overlay. Finally, the paper concludes with general remarks and a statement of further work"
3423,"The quality of data contained in the enterprise information systems has significant impact, both from the internal business decision-making perspective and the external regulatory and shareholder obligations. This paper addresses data quality assessment in business processes by proposing a modeling framework to quantify the data quality in an information processing system. We present a business process modeling framework for data quality analysis and develop the mathematical formulation for error propagation. This is overlaid with a business controls framework where the placement and effectiveness of the controls alter the propagation of errors. This framework enables the estimation and management of data quality when faced with changes in various aspects of the business process. It also allows the formulation of optimization problems that trade off the cost of business controls with the level or cost of the resultant data quality. We illustrate the modeling framework and analyses with a revenue management process"
3424,"In the service-oriented architecture, component services can be composed to provide value-added services. The notion of composite service skyline facilitates obtaining a QoS-optimal service combination. State-of-the-art approach to computing composite service skyline assumes that candidate services of different tasks are independent. However, real-word services are usually QoS-correlated. To this end, we present a novel approach to computing the composite service skyline in the presence of QoS correlations. Advanced pruning techniques are proposed to accelerate our approach. We conduct extensive experiments to evaluate the effectiveness and efficiency of our approach."
3425,"This work proposes a novel model where multimedia contents with their related services (business processes and functions) are packaged together as mobile agents. This is intended to enable content providers both to encapsulate their contents and to provide value-added services, for flexible content editing, delivery, and presentation. In addition, agents encapsulating contents/services can contain other agents in themselves (synthesis of agents). A mobile agent compound integrating multiple contents/services can be thus dynamically formed, in which multiple agents work cooperatively and migrate together as a unit. This paper also describes our proposed MAFEH/WS framework for the model. Agents in the model can be developed by incorporating simple parameter settings for synthesis control into business process descriptions in BPEL4WS."
3426,"In SOA, one challenging problem is the QoS evaluation for the newly composed service. Most existing work on service composition focuses mainly on the static QoS behavior of individual components. Many other factors such as execution profile (e.g. service task and network dynamics) and ""best results"" with slow selection time vs. ""good enough"" results with fast selection time are often ignored. To address the practical aspect of service selection and to perform more appropriate tradeoff, we propose a service QoS model and evaluate two selection algorithms in this paper. Experimental results show that our model and mechanism can provide good tradeoff between service selection time and QoS metrics."
3427,"Virtual machine (VM) placement for Datacenter (DC) consolidation is a challenging problem, particularly in the face of VM workload fluctuation. In this paper, we present a stochastic model for optimization of DC consolidation and propose intelligent strategies for statistical VM multiplexing on physical machines (PMs) to ensure optimal use of hardware resources, while providing a service guarantee. We have provided an optimal strategy by modeling and solving the problem as a stochastic integer programming problem followed by a more scalable strategy based on a greedy heuristic. Extensive simulation based experimental results show that the strategies are more efficient in resource utilization while providing bounded service guarantees, than the traditional way of VM placement without any consideration to workload fluctuation."
3428,"Migration of IT infrastructure to the Cloud transforms enterprise data, applications, and services to one or more other Cloud environments. Cloud migration engagements often rely on an in-depth discovery of the client's (source) IT environment, which is rather costly and can take up to six weeks before any meaningful conversations with customers can begin about the migration itself. There is a demand for a more agile approach to enable sales teams to perform rapid qualification of cloud fitness and reason about the benefits of Cloud using minimal information from the clients. The existing, consulting based approach typically relies on a number of discovery and analysis tools, yet the entire process is manual, expensive, and time consuming. In this paper we present a suite of cloud transformation analytics (CTA) services designed to streamline the process of premigration and migration analysis, such as cloud fitness validation and consolidation recommendations. CTA supports reasoning about diverse target clouds, as well as various transformation methods to match clients' needs, such as image migration, workload migration, and cross platform migration. We discuss our key insights and lessons learned from employing cloud fitness validation capability on datasets of up to 2000 servers to enable and accelerate the process of migration."
3429,"With the increasing number of computers on the Internet, there is a growing interest in harnessing the unused and inexpensive computational resources over the Internet. However, current approaches such as the grid computing paradigm are not sufficient. We present our preliminary work that uses extends peer-2-peer (P2P) computing with a framework that allows grid computing over the Internet."
3430,"The tendering process is a key business process that helps businesses find a suitable contractor. In the building and construction industries, clients invite tenderers to submit an estimate of prices, detailing the costs associated with completing a building. In this way, the client can base their decision on the result of the tenders to select the most suitable contractor. Currently in Hong Kong, many of such tendering processes are still mainly manual and paper based. The tenderers need to collect the tender's booklet, price it, and bring it back to the client's office before the deadline. In this paper, we present a design and implementation of an e-tendering system by using Web services for the automation of such tendering processes. We also show how e-tendering reduces the problems that occur in the manual process."
3431,"In big data analytics, clustering plays a fundamental and decisive role in supporting pattern mining and value creation. To help improve user experience and satisfaction level of clustering algorithms, one important key is to let users define the quality of the aggregated clusters (e.g. In terms of the homogeneity and the relative population of each resulting cluster) they prefer instead of to fix the number of clusters to be obtained before the clustering process. In this paper, we first propose a new measure, called the Clustering Performance Index (or CPI), that takes into consideration of homogeneity, relative population, and number of clusters aggregated. Then we propose a new hierarchical clustering algorithm by adopting homogeneity as its key similarity. Experimental results show that our proposed clustering algorithm can achieve a good balance among CPI, the number of clusters aggregated, and the time cost of the algorithm."
3432,"Hospital emergency departments are complex systems, characterized by uncertainty and variability, often operating with limited resources and high demands. Simulations of these departments have proven to be efficient tools in improving their performance in certain circumstances, however the human aspect is often overlooked in the application of modeling and simulation to this field. This paper describes an ongoing project which is applying agent-based modeling techniques to the simulation of hospital emergency departments. The work performed so far, including system analysis and a preliminary model, are shown and the advantages of this technique explored."
3433,"The need for a robust data center that is fault tolerant can never be overemphasized, especially nowadays that the advent of big data traffic, internet of things and other on-demand internet applications are on the increase. The rate at which these data are transferred across the internet is worrisome, and a thing of concern to the data center developers. The emergence of ubiquitous computing has also aided to the increase in traffic across the internet, because computing occurs more now by use of any device, in any location, and in any format. These issues have compounded the management of Cloud Data Center used for storage, transfer, and analysis of data across the cloud; as a result, the data center network devices become prone to failures, which automatically impacts on its performance. Nevertheless, several researchers have come up with solutions, though not sufficient to mitigate these issues. Therefore, on our part, we realised that architectural design of data center network is the bedrock of having a fault tolerant, reliable, robust, and congestion free network. So, this paper, which is an extension of our previous works, based on an improved version of Fat Tree (called Z-node); we proposed a Hybrid fat tree design and compared it with Single fat tree design, for client to server communication pattern such as HTTP and EMAIL applications. The simulation results obtained with different device failures and traffic rate patterns, show that the Hybrid fat tree design performed better than the Single fat tree design, hence will be the best bet for the transfer and analysis of big data in cloud data center network."
3434,"In a collaborative situation, involving several partners, a BPM approach may be useful to support the design of a Mediation Information System (MIS), in charge of ensuring interoperability between partners' IS. For such an objective, there are two main barriers, which are: (i) building collaborative business process cartography, and (ii) once the collaborative business process cartography defined, fill the deep semantic gap between business activities and technical web-services. This articles aims at presenting state of the art, results and current works concerning both these critical issues."
3435,"In the practice of process-centric systems implementations, business process modeling is becoming more crucial. This paper introduces an approach to automatically generate test suites for the process paths to check the consistency, which is very important, between the business process model and the implementation of Enterprise System. The paper also presents a supporting tool to apply this approach in SAP implementation. A large-scale enterprise transformation program is discussed as a case study."
3436,"The shape of entities used in web services are mainly defined with the idea of a specific business case in mind. As soon as any third party uses these web services with another perception of the process, the shape of the entities may not be optimal any longer. Our paper proposes a solution that involves the service consumer party by letting the service consumer re-define the entities. Two architectural approaches are shown that provide an unobtrusive solution towards dynamic shaped web service entities. Furthermore we discuss the principles of declarative web services and the usage of context oriented programming for our concept of virtual endpoints."
3437,"This paper describes our latest work on designing and developing a benchmarking system to be applied to AMS (Application Management Service) domain. Benchmarking allows an AMS client or account to understand where it stands in relative to others in terms of its operational performance, and helps it set a realistic target to reach. We elaborate the entire system process in the paper which consists of the following modules: account data collection, cleansing, sampling, mapping and normalization, benchmarking pool formation, KPI (key performance indicator) design for account performance measurement, KPI implementation, evaluation and visualization, and finally, a post-benchmarking analysis. So far, we have applied this benchmarking tool to a dozen of real AMS accounts, and have received some initial yet encouraging feedback."
3438,"Enterprise IT environments are complex: business applications rely on distributed middleware running on diverse hardware with components depending on each other in many unexpected ways. Discovery of applications' dependency on IT is a critical step in managing application and IT infrastructure together. Many tools and practices have emerged to discover and report IT assets and applications' dependency on the IT assets. However, our experience in the field shows that there are significant challenges in effectively deploying the tools. There is a critical need to research and develop flexible processes, methods, and practices, and architecture-level support for them in the tools to enable successful discovery using a ""built-to-order"" approach. In this paper we discuss our experiences with an advanced application-data relationship discovery tools in large scale enterprise environments and based on these experiences we identify three main challenges of effective discovery. They are: deployment process and related security issues; unavailability of software and administration-related information; and tool integration. To address these challenges, here we demonstrate a holistic approach that includes flexible processes, methods, and practices in the tools for achieving the necessary built-to order capability."
3439,"Many are anticipating that grid computing infrastructure, utilities and services will become an integral part of future socio-economical fabric. However, factors such as cost of access, reliability and dependability of the technology undoubtedly influence its adoption and widespread use. IBM are promoting autonomic computing as a solution to alleviate such concerns by delegating software systems' management, security, reliability and performance tuning to the software itself. Based on an on-going research work, focusing on high-assurance concerns of widely distributed systems, this paper outlines a service-oriented approach and associated service infrastructure description languages, which are used to facilitate the construction and management of ad-hoc federated software services."
3440,"Database related spending is always a significant cost sink for most CIOs. One component of this spending may be related to maintenance, upgrades, physical hardware costs and other routine items that may not be avoidable. However, a large chunk of the spending may be due to inefficiencies in the data storage mechanism, redundancies in the storage of critical data elements, replication of data access logic and proliferation in the specific data storage technology used. Some of these issues may be addressed by a comprehensive data architecture strategy that attempts to tackle these inconsistencies and redundancies. In this paper, we present architecture for shared data services that may be implemented at an enterprise level to tackle issues associated with data scatter, redundancy and replication. We discuss in detail the various components of such architecture and their applicability towards specific problem areas"
3441,"With the incredible popularity of cloud computing, the adoption of mass customization (MC) is significant for building a cloud computing system that could provide services provisioning in a manner of multi-tenancy. Because of lack of a standard architecture that supports MC development for cloud services, the existing metadata or model driven approaches have insufficient abilities to realize personalized requirements with mass production when applied to product development in large-scale enterprises. Aiming at these problems, this paper presents a novel MC-based development approach for enterprise-level business cloud services based on the specification of the Cloud Computing Reference Architecture (CCRA), and shares the practice about how the approach is applied to building Kingdee K/3 Collaboration Development Cloud (CDC). Successful practice has proved that by adopting our MC development approach, we can develop platforms and tools on the cloud at a low cost and more effectively."
3442,"Choreography changes performed by one party may affect other parties. The changes and the implication for other parties can be determined. However, the required changes on the orchestration are difficult to determine since a choreography is an abstraction of the orchestration and thus information is lost. In this paper an approach is proposed to enrich the orchestration with the observed changes using a syntactical representation of the orchestration language and semantic invariants of the orchestration language."
3443,"Information assets in service enterprises are typically available as unstructured documents. There is an increasing need for unraveling information from these documents into a structured and semantic format. Structured data can be more effectively queried, which increases information reuse from asset repositories. This paper addresses the problem of extracting XML models, which follow a given target schema, from enterprise documents. We discuss why existing approaches for information extraction do not suffice for the enterprise documents created during service delivery. To address this limitation, we present the Asset Harvester Assistant (AHA), a tool that automatically extracts structured models from MS-Word documents, and supports manual refinement of the extracted models within an interactive environment. We present the results of empirical studies conducted using business-process documents from real service-delivery engagements. Our results indicate that the AHA approach can be effective in extracting accurate models from unstructured documents and improving user productivity."
3444,"Template-based composition of web services allows users to define templates with specified activities and to construct the control flow among the activities. And appropriate web services can be discovered and selected for each activity. However, choosing compatible services requires precise formal specifications of activities and templates because supporting systems lack ""memory"" of previous cases of using certain services together. In this paper, we propose to alleviate this problem by combining a template-based service composition framework with a recommender system and intelligent services. The combination not only supports users in defining the templates but also resolves issues of compatibility with limited computational power and less formal specification of templates using users' feedback from previous usage and services' feedback from intelligent services' communication ability."
3445,"Businesses are increasingly realizing the value of creating a {\it single view} of its customers and partners by integrating information residing in 'siloed' datasets within and outside the enterprise. However, the task of {\it augmenting} data available within the enterprise with data purchased from third-party providers or that residing in a public domain such as Web often results in warehouses that contain databases having incomplete and/or inconsistent data. Hence, before the data can become useful, one must eliminate the inconsistency in values appended to the enterprise data. In this paper, we present {\it Data Augmentation as a service (DAaS)} that can help business in creating a consistent and usable single view of entities of interest. Specifically, our service will enable business rule writers to quickly create data augmentation rules by using our approximate functional dependency driven rule generation scheme. An accompanying challenge comes from having to manage a large number of rules and ensuring that new rules do not negate already existing rules. To mitigate this problem a rule-management and evaluation system that uses the Ripple Down Rules (RDR) framework is provided as part of our service. Using several large real-world datasets, we show our ability to learn rules for imputing attribute values with high accuracy and scalability necessary for enterprise users, how conflicts can arise within rules, and finally our ability to effectively handle those conflicts with high accuracy."
3446,"IT outsourcing service providers are increasingly required to balance cost-reduction challenges with quality-improvement goals. Such efforts can be aided by an improved methodology for services solution engineering - which involves requirements analysis, mapping requirements to service provider capabilities, documenting the service design and defining transformations of the customer's environment to facilitate more efficient service delivery. One such improved methodology leverages reuse in the form of a taxonomy of standardized service offerings and a repository of standardized service designs they can be mapped to. In this paper, we describe a prototype system that provides automation for such a methodology, by encoding design policies that are used to assist solution architects in gathering relevant information from the customer and cross checking design decisions. This paper presents the system architecture, discusses sources and models of knowledge, and illustrates how this knowledge can be used with specific examples from the services field."
3447,"This paper describes a traffic analysis platform that is implemented as a service, so different government authorities or geographic locations can utilize its functionalities more efficiently and effectively. In particular, most of the research in traffic analysis is solely focused on numerical analysis, while the conceptual relationships among moving objects are rarely captured. In this paper, we investigate into traffic relationships by a unified conceptual model that captures traffic dynamics under changing contexts to facilitate service implementation. By using the unified model, we can capture domain knowledge in a robust and consumable format for our proposed services for different users and on different geographic databases. The knowledge can also be mapped into a service computational model for other value-added traffic analysis. Instead of using the previously proposed spatio-temporal ER model, we define our model by augmenting the traditional ER model with symbolic contexts to better capture spatio-temporal context variations and their relationships with moving objects. We demonstrate the expressive power of our model by using Beijing taxi trajectory data to show that our approach is effective in representing traffic relationships for analysis."
3448,"This paper aims at effectively reproducing the results of previous scientific workflow executions. We first identify the information that needs to be recorded in a workflow execution log, based on which we then determine the data flow among experiments. To effectively record a workflow execution log in a relational database management system (RDBMS) when the workflow design is absent, we propose a generic relational storage schema. Then techniques have been designed to automatically discover the minimal set of experiments that must be performed in order to reproduce a scientific result by posing appropriate SQL queries. Although such SQL queries can be evaluated using an off-the-shelf database system, we investigate the unique characteristics of the workflow log data and optimization techniques for evaluating such SQL queries efficiently."
3449,"We present an informatics infrastructure for biocuration, based on a combination of techniques from information extraction (IE) and knowledge engineering (KE). We describe the high-level design of this infrastructure which we base on the concept of 'experimental type'. Here, we treat each experiment as a specific type of knowledge statement determined by the experiment's design. We provide a preliminary, detailed example of the use of the infrastructure to support the construction of a database pertaining to neuroanatomical tract-tracing experiments. This work generalizes to provide support for other experimental types and could be used to make biocuration efforts more efficient. We also discuss how the process of annotating text for IE directly supports designing schema for databases. We envisage how this architecture could support small-scale, laboratory-centric knowledge bases that each support service-oriented functionality."
3450,"Service-Oriented paradigm enables the composition of loosely coupled services provided with varying nonfunctional properties in terms of Quality-of-Service (QoS). Given a composition template, finding the set of appropriate services that guarantees users' functional requirements under given QoS constraints has been widely acknowledged to be a challenge. The problem of optimal service selection and composition is usually addressed by considering timeinvariant, stochastic, or bounded QoS of candidate services in their QoS analysis models and scheduling algorithms. Our proposed work, instead, considers time-varying QoS of services and leverages its run-time fluctuations for generating dynamic and predictive service composition schedules, by using an ARIMA-based time-series prediction model and genetic algorithms. Extensive case studies based on multiple service composition templates and real-world QoS datasets clearly suggest that our proposed method outperform traditional ones in terms of response time and throughput."
3451,"The composition of services allows for achieving complex business requirements on top of already existing functionality, potentially procured from third party providers. Facing a growing market of services, enterprises have to select the most appropriate services to support their service compositions. Beside functional attributes the service selection is driven by nonfunctional properties like the price or the quality of a service. Taken into account quantity discounts, bundle effects, and subscription-based charging, complex service charging models are discussed in the literature of service marketing. Dynamic and complex charging models are expected to play an important role in the future service industry. Therefore, a service selection has to deal with those complex charging models in order to obtain a cost minimizing composition of services. In this paper, we present a cost minimizing service selection model based on mathematical programming that considers complex service charging models found in the literature. In addition to a pure cost minimizing point of view, the proposed model also takes into account end-to-end constrained quality of service (QoS) attributes like the execution time of service compositions. The conducted computational experiments demonstrate that the approach is applicable to solve problem instances of reasonable size."
3452,"The service sector is the dominant part of the economy, and yet services pricing is not well studied. The pricing of services, especially IT services, differs from the pricing of products to a certain extent. This paper proposes a feasible value based pricing method. The pricing process includes: business value modeling and analysis, customer segmentation, and fully distributed cost pricing. A practical case is presented to show the applicability of the new pricing method. The pricing solution followed the proposed methodology can lead to higher profits and more- satisfied customers than possible with uniform pricing."
3453,"In this paper, we investigate the question of QoS prediction of Web Service Composition (WSC) implementing a business process. We focus on the graph reduction technique and the prediction of the Service Response Time. In the graph reduction technique, we assume that a Web Service Composition can be represented as a graph. The main thesis is that the QoS of such a graph graph can be obtained from a composition of the ones of its nodes. Multiple graph reduction algorithms have been proposed in the literature. Our contribution is twofold. We propose first a fast algorithm based on graph reduction for the prediction of the Service Response Time of a Web Service Composition. In comparison to those existing in the literature, this algorithm uses less memory space and has a better time complexity. The obtained improvements are in particular significant on very large Web Service Composition where the number of services is huge. Our second contribution is an analysis of the graph reduction technique for QoS prediction that takes into account the unfolding of services. In such cases, we show that the prediction of QoS can lead to a NP-complete problem. We also provide an integer programming model for predicting the Service Response Time in this case."
3454,"Volatile requirements should be managed such that changes can be introduced into the system in a quick and structured way. This paper presents directive-oriented pattern analysis (DoPA), a requirements engineering approach that handles volatile requirements by managing the coupling between business intentions and service integration. The key insight is to utilise services as commodities via service choreography patterns. DoPA captures differentiating enterprise intentions as directives, while using patterns to handle common business needs. This enables the notion of declarative configuration of services to achieve business agility."
3455,"Performance unpredictability is one of the major concerns slowing down the migration of mission-critical applications into cloud computing infrastructures. An example of non-intuitive result is the measured n-tier application performance in a virtualized environment that showed increasing workload caused a competing, co-located constant workload to decrease its response time. In this paper, we investigate the sensitivity of measured performance in relation to two factors: (1) consolidated server specification of virtual machine resource availability, and (2) burstiness of n-tier application workload. Our first and surprising finding is that specifying a complete isolation, e.g., 50-50 even split of CPU between two co-located virtual machines (VMs) results in significantly lower performance compared to a fully-shared allocation, e.g., up to 100% CPU for both co-located VMs. This happens even at relatively modest resource utilization levels (e.g., 40% CPU in the VMs). Second, we found that an increasingly bursty workload also increases the performance loss among the consolidated servers, even at similarly modest utilization levels (e.g., 70% overall). A potential solution to the first problem (performance loss due to resource allocation) is cross-tier-priority scheduling (giving higher priority to shorter jobs), which can reduce the performance loss by a factor of two in our experiments. In contrast, bursty workloads are a more difficult problem: our measurements show they affect both the isolation and sharing strategies in virtual machine resource allocation."
3456,"Current semantic trend makes OWLS be replete with composite services regardless of how the compositions originate, but it provides no way to verify correctness. To alleviate this deficiency, we propose a hierarchical CPNets based model for the specification of process composition so as to raise the composition reliability. The model defines a composite process formulated in OWLS file in a three-level specification: interface net, orchestration net and composition net, and specifies the control and data flow by mapping control constructs of OWLS into CPNet formalism. Furthermore, we provide the techniques to analyze and validate the CPNets and explore corresponding meanings of the properties in the context of Web services composition. Based on the model, the behaviors of a composite service can be simulated and validated to allow correcting the composition errors in advance, thus effectively preventing it from runtime failure. Finally we implement an example scenario to illustrate the effectiveness of our approach."
3457,"Semi-structured processes arise extensively in various industries such as government, insurance, banking and healthcare and are characterized by their flexibility and data-driven nature. Case management systems are evolving to handle the growing demand for providing operational support to knowledge workers involved in these processes. While enabling flexibility in human actions is an integral part of these systems, this capability increases the potential for errors in case handling. We propose a new paradigm of case health aimed at providing knowledge workers and supervisors a case health service to continuously and objectively assess both the current state of a case instance and provide a prognosis for the future state of the instance. The case health service is composed of constituent services each of which provide case health indicators and can be combined in flexible ways to determine the overall health of an instance. We have implemented case health with respect to a simulated credit card dispute scenario modeled in IBM Case Manager and evaluated the utility of computing the case health of running instances of the credit card dispute scenario. Results indicate the utility of incorporating case health in a case management system in terms of early detection of undesired outcomes and abnormal execution patterns in running case instances."
3458,"Summary form only given. Services businesses have become very exciting growth opportunities for the industry. How to design, model, and implement business services using IT technology is becoming a challenging issue. With the introduction of service-oriented architecture (SOA) and Web services, componentizing enterprises and services based on patterns has paved a way to running a successful services business. In this talk, the author describes the services ecosystem, methodology and supporting techniques used to build, operate, and manage business services."
3459,"Data Mashup is a special class of mashup application that combines information on the fly from multiple data sources to respond to transient business needs. In this paper, we propose two optimization algorithms to optimize Data Mashups. The first allows for selecting the minimum number of services required in the data mashup. The second exploits the services' constraints on inputs and outputs to filter out superfluous calls to component services in the data mashup. These two algorithms are evaluated and tested in the healthcare application domain, and the reported results are very promising."
3460,"This tutorial presents the foundational knowledge for the researchers and practitioners on Service- Oriented Architecture (SOA) and Web services. The traditional ""triangle"" SOA and variations that better support SOA services and solutions will be examined. Critical Web services infrastructures will be covered, such as WSDL, BPEL, WSRF, Discovery, Composition, Registry, and Web services invocation and relationship binding. How Web 2.0 and SOA can benefit with each other will also be explored. An IEEE SOA Solution Reference Architecture standardization initiative will be introduced in this tutorial to illustrate how different pieces of technology components can be used to build reusable, flexible, and extensible SOA solutions. Finally, the presenter will depict research and development challenges and directions in the field of SOA and Web services. The target audiences are all-level researchers, practitioners, and students. This tutorial material is created for the IEEE Body of Knowledge initiative on Services Computing, which is sponsored by the IEEE Computer Society Technical Committee on Services Computing."
3461,"Service oriented architecture (SOA) is an architectural style to reuse and integrate subsystems in existing systems for designing new applications. Each application is designed in an implementation independent manner using abstract concepts: network services and connections between network services. In SOA, the non-functional aspects of services and connections should be described separately from their functional aspects because different applications use services and connections in different non-functional contexts. This paper proposes a UML profile to graphically design the non-functional aspects in SOA and maintain them in an implementation independent manner. This paper presents the design of the proposed UML profile and describes how it is used in service-oriented application development"
3462,"A Service-Based Application (SBA) is built by defining a workflow that composes and coordinates different Web services available via the Internet. In the context of on-demand SBA execution, suitable services are selected and integrated at runtime to meet different non-functional requirements (such as price and execution time). In such dynamic and distributed environment, an important issue is to guarantee the end-to-end Quality of Service (QoS). As a consequence, SBA provider is required to monitor each running SBA instance, analyze its runtime execution states, then identify proper adaptation plans if necessary, and finally apply the relative countermeasures. One of the main challenges is to accurately trigger the adaptation process as early as possible. In this paper, we present a two-phase decision approach that can accurately analyze the adaptation needs for on-demand SBA execution model. Our approach is based on the online prediction techniques: an adaptation decision is determined by predicting an upcoming end-to-end QoS degradation through two-phase evaluations. Firstly, the end-to-end QoS is estimated at runtime based on monitoring techniques; if a QoS degradation is tent to happen, in the second phase, both static and adaptive strategies are introduced to assess whether it is the best timing to draw the final adaptation decision. Our approach is evaluated and validated by a series of realistic simulations."
3463,"Data Web service composition is a powerful means to answer users' complex queries. User preferences are a key aspect that must be taken into account in the composition scheme. In this paper, we present an approach to automatically compose Data Web services while taking into account the user preferences. User preferences are modeled thanks to fuzzy sets. We use an RDF query rewriting algorithm to determine the relevant services. The fuzzy constraints of the relevant services are matched to those of the query using a set of matching methods. We rank-order services using a justification of Pareto dominance, then compute the top-k service compositions. We propose also a method to improve the diversity of returned compositions while maintaining as possible the compositions with the highest scores. Finally, we present a thorough experimental study of our approach."
3464,"Predicting inventory shipments can be useful for lean inventory management such as inventory planning. In this paper, we propose approaches to predict inventory shipments based on the data extracted from the inventory management module of Oracle EBS systems of a GPS-manufacturing company. First, we introduce the process to extract the inventory shipment data from the Oracle EBS system. Then, we adopt time series forecasting algorithms (i.e., ARMA) and Primitive KNN algorithms to predict the future shipments for a group of inventory items. At last, by discovering the patterns of parameter settings when optimal prediction accuracy is achieved, we develop new algorithms to reduce runtime at different levels with trade-offs in prediction accuracy."
3465,"The financial crisis has recalled the importance of proper financial planning. Companies which are organized as a multitude of legal entities are in particular affected by planning irregularities. To optimize the financial planning process and to cope with the challenges, we propose our service model ""Financial Planning as a Service"" (FiPlaaS). This approach allows companies to redesign their planning processes according to SOA principles and to achieve substantial improvements in performance."
3466,"Near Field Communication (NFC) is a new short-range wireless communication technology that enabled simple, security and intuitive peer-to-peer communication between NFC-enabled devices. The advent of NFC has given rise to several interesting applications under short-range radio technology. Among the interesting applications, mobile payment is the first application in the marketplace. In the research, we proposed NFC-Micro SD technology, a new technology in NFC field. The new technology creates a more convenient environment and lower threshold for mobile payment to enter in. The collaborative mobile payment solution follows the original security mechanism of the traditional credit cards operation, and improve the value of the partners in the collaborative value chain, included the hardware produced and software development. The two modes, passive mode and active mode, provided by NFC-Micro SD technology also provide the reliable transaction for the mobile payment. In summary, the NFC mobile payment solution via NFC-Micro SD technology improves the higher acceptance, using willingness, and loyalty of the partners and users. It also makes the value chain as the collaborative solution. Thus, the NFC mobile payment via NFC-Micro SD technology can be seen as the better mobile payment solution and will lead the development of the mobile payment market."
3467,"Web services have become a primary mechanism for consuming resources available on the Internet. As more and more services are published on the Web, automated service discovery is critical to consumers to identify relevant and reliable services efficiently. In this paper, we enhance the Web Service Crawler Engine (WSCE) framework by introducing comparison measures to allow for more accurate identification, discovery and ranking of relevant Web services. To discover services effectively, we need to be able to measure and compare the similarity among services. Most ontology-based and IR-based discovery techniques assume that service input/output are simple data types when calculating service similarity. However, real-world services published on the Web usually have complex data types input/output parameters. Furthermore, a good match of parameters does not guarantee good usability and good reliability. The relevant services must be further evaluated by users' past experiences, based on both objective and subjective measures, to make optimal solution selection possible. This paper proposes a service matchmaking algorithm that considers the complex data types of service input/output parameters, as well as experience-based objective and subjective measures for ranking. Experiments show that our approach performs better than previous works that only consider simple data types."
3468,"This paper presents a solution framework and implementation of a virus detection and vulnerability remediation (VDVR) system based on service- oriented architecture (SOA). VDVR system is an end to end, and Web based solution that searches/analyzes the output of a network detection system. VDVR system is also responsible for identifying affected machines, providing a workflow for managing the isolation of the machine from the network, remediating the infected machine, and reactivating the machine on the network."
3469,"When designing scientific workflows, users often face the so-called shimming problem when connecting two related but incompatible components. The problem is addressed by inserting a special kind of adaptors, called shims, that perform appropriate data transformations to resolve data type inconsistencies. However, existing shimming techniques provide limited automation and burden users with having to define ontological mappings, generate data transformations, and even manually write shimming code. In addition, these approaches insert many visible shims that clutter workflow design and distract user's attention from functional components of the workflow. To address these issues, we 1) reduce the shimming problem to a runtime coercion problem in the theory of type systems, 2) propose a scientific workflow model and define the notion of well-typed workflows, 3) develop three algorithms to typecheck workflows by first translating them into equivalent lambda expressions, 4) design two functions that together insert ""invisible shims"", or runtime coercions into workflows, thereby solving the shimming problem for any well-typed workflow, 5) implement our automated shimming technique, including all the proposed algorithms, lambda calculus, type system, and translation functions in our VIEW system and present a case study to validate the proposed approach."
3470,"Modern business applications are typically required to support collaboration and coordination of several business partners. Web services have become the de facto technology for implementing such applications. One approach for specifying Web service calls and interactions is ActiveXML(AXML), a data-oriented workflow language. A problem with state-of-the-art workflow engines for AXML is that the execution is poorly optimized and thus too time-consuming.In a previous short paper, we proposed an optimization approach for workflows specified in AXML. Here, we elaborate on this proposal in more detail, providing the formal descriptions to show the general execution of the optimization rule, the algorithm to group the Web service calls, and an analytical argument for the resulting performance gain."
3471,"Mashup Programming is one of the latest trends in web application development. However, different sources still regard that mashup technology is in its infancy despite the mushroom growth of mashup-based applications and technologies. This calls for an in depth analysis of the support available to end users for mashup development. In this paper, we propose a multi-dimensional framework to evaluate existing mashup tools and platforms. The main idea behind this evaluation is to study the state of the art of mashup development from the end-user perspective and critically evaluate the efforts in this regard against a set of criteria to identify the research directions in this field. The idea of evaluation of mashup tools has been tackled through different approaches in past, however, in this research we attempt to synthesize additional concepts regarding mashup development and analyse this area against previously ignored factors, such as support for capturing users' requirements and users' goals. This study also includes platforms that have not yet been covered by previous evaluation studies. The framework presented in this study can be used as a guideline to inform future design of mashup development environments for both academic and industrial research. The results of the study reinforce the need of a more user-oriented approach to deal with the inherent issues of end-user mashup programming. The study concludes with the proposal of use of goals as a guiding mechanism to derive the mashup applications."
3472,"With the rapidly growing demand for the cloud services, a need for efficient methods to trade computing resources increases. Commonly used fixed-price model is not always the best approach for trading cloud resources, because of its inflexible and static nature. Market-based trading shows promise for more efficient resource allocation and pricing in the cloud. However, most of the existing mechanisms ignore the seller's costs of providing the resources. In order to address it, we design a single-sided market mechanism for trading virtual machine instances in the cloud, where the cloud provider can express the reservation prices for traded services. We prove that the proposed mechanism is truthful, i.e. the buyers do not have an incentive to lie about their true valuation of the services. We perform extensive experiments in order to investigate the impact of the reserve price on the market outcome. Our experiments show that the proposed mechanism yields near-optimal allocations and has a low execution time."
3473,"A contract is a legal agreement involving parties, activities, clauses and payments. The activities are to be executed by the parties satisfying the clauses, in accordance with the associated terms of payment. An e-contract is a contract modeled, specified, executed, controlled and monitored by a software system. In this paper, we study payment issues in e-contracts. Payments are meant for, and so are closely related to, the execution of activities specified in the contract. We consider (i) the payment amount for the execution of an activity, (ii) the time of payment relative to the execution and (Hi) tracking the payment against the execution of the activity. For the first two, we use an execution model that enables representing a variety of states encountered in the generally complex executions of the activities in a contract. For tracking, we use a multi-level composition model for the activities. Our study brings out various issues that need to be addressed in designing a payment monitoring system."
3475,"Shanghai Transportation Information Service Application Grid (STISAG), an intelligent transportation system (ITS), is used to deal with the serious traffic jam in the city. It's based on service-oriented architecture (SOA), Grid and Web service technology. The system design and some key technical problems are discussed in this paper. The content in this paper not only gives a case study for SOA, but also provides a new and effective solution for ITS."
3476,"Summary form only given. The emerging concept of semantic Web services aims at more sophisticated Web Service technologies: on basis of semantic description frameworks, intelligent mechanisms are envisioned for discovery, composition, and contracting of Web services. The tutorial explains the current state of the art in semantic Web services on basis of the Web service modeling ontology WSMO and related initiatives. Commencing from the vision and arising challenges for semantic Web services, the tutorial in detail explains the specifications of recent frameworks for semantic Web services and presents the Web service execution environment WSMX as the WSMO reference implementation. The tutorial consists of three main sections that subsequently provide a complete overview of semantic Web services and the latest status of WSMO. The tutorial addresses academic as well as industrial researches and developers that are working with Web services and are interested in semantic Web services."
3477,"Privacy becomes an increasing concern in modern society because personal information is being collected by more and more online services on the Internet. Although many privacy-aware models and methods were proposed, the protection technology of privacy is underway. This paper aims at addressing privacy-aware access control in composite services. We introduce an automaton-based monitoring solution for privacy-vital information flow for a single execution of a composite service. In addition, this paper also gives a policy consolidation algorithm based on orchestration structures and attribute composition, which can also be used to combine privacy policies."
3478,"In service-based systems, one of the most basic requirements for service matching is to find services with functionalities similar to the expected functionality of a given service discovery request. In this paper, a functionality-compatibility-based service indexing and matching approach is presented. This approach indexes a set of services using functionality-compatibility defined on their parameters and conditions, and filters the indexed services to identify those with compatible functionalities to the service discovery request efficiently. This approach is evaluated to show the effectiveness and efficiency."
3479,"Previously, Amazon EC2 Spot prices were always driven by short-term trends in supply and demand, requiring consumers to have an in-depth understanding of Spot markets and the bidding process in order to make ""intelligent"" time-vs-money-vs-value trade-offs. However, with the newly announced streamlined access model for Spot instances, Amazon states that the Spot prices will adjust more gradually based on long-term trends instead of reacting to short-term fluctuations in demand and supply. Therefore, consumers are no longer required to understand Spot markets and bidding, and yet can save up to 90% off the On-Demand prices. In this paper, we study the pricing patterns before and after the introduction of the modified model using standard statistical approaches including econometric inequality indices (the Gini coefficient and the Theil index), logistic regression, a hybrid forecasting technique based on Naïve, and Principal Component Analysis. Our findings confirm the announcements made by Amazon including less frequent Spot price changes, disappearance of sudden spikes, and smooth (but not necessarily gradual) adjustments in the Spot prices. Rather surprisingly, with the introduction of the new model, the median Spot prices have risen in the majority of the Spot markets. In addition, even in the changed access model Spot price forecasting can still yield valuable insights into the evolution and structure of a given Spot market, although there may no longer be a need for sophisticated bidding strategies."
3480,Presents the welcome message from the conference proceedings.
3481,"A web service can be composed of multiple component web services in a loosely-coupled environment. Traditional Role Based Access Control (RBAC) is inadequate for the authorization management of composite services since the administration of the component web services has not been taken into consideration. In this paper, we propose a novel conceptual model, named as Service Oriented Authorization Control (SOAC) to facilitate the administration and management for both service consumers and component web services. A set of administrative functions are also provided for managing the elements of SOAC. This research will be the first step towards managing service-oriented authorization."
3482,"More and more companies are currently migrating business processes to the Cloud in order to handle customer service in an efficient and cost effective way. Cloud Computing's elasticity and flexibility in service delivery makes it an ideal solution for companies to deal with highly variable service demands and uncertain financial environment to ensure the required QoS while using resources and reduce their expenses. Elasticity management is witnessing a lot of attention from IT community as a pivotal issue for finding the right tradeoffs between QoS levels and operational costs by working on developing novel methods and mechanisms. However, controlling business process elasticity and defining non-trivial elasticity strategies are challenging issues. In this paper, we propose an elasticity strategy description language, called Strat. It is defined as an extensible Domain-Specific Language to allow business process holders to describe elasticity strategies that are evaluated using our formal evaluation framework. Given a usage behavior and a business process, the evaluation consists in providing a set of plots that allows the analysis and the comparison of strategies. Our contributions and developments provide Cloud tenants with facilities to choose elasticity strategies that fit to their business processes and usage behaviors."
3483,"Recent composition languages like BPEL allow for composing single Web Services as higher-level business workflows. Runtime engines obtain the task to deploy and to publish these processes as regular Web services that can be located and used by any client. These runtime engines, however, do not enable end-users to flexibly adapt the constructs that represent a workflow in a personalized manner. The claim of this work is that end-user adaptability (tailorability) of personalized workflows will be increasingly significant for future applications. Based on our experience with component-based tailoring methods, we propose the multi-tier architecture TailorBPEL that supports the tailoring of personalized BPEL-based workflow compositions."
3484,"Today's networking and storage providers can typically offer a limited range of services to their customers because of their reliance on a small, preferred range of infrastructure technologies. The providers do this in order to deliver effective, well-understood services for their customers and minimise the risk associated with developing solutions from a variety of different technologies. They each deliver a particular solution for their customers. However, from the customers' point of view, the provisioning of innovative, customised services that go beyond the services of a single infrastructure provider is very attractive. This paper presents a service-oriented architecture based on a well-established business model - the virtual network operator (VNO) - that allows the providers to meet their customers' complex requirements. Our proposed architecture uses virtualisation techniques to abstract the infrastructure services in such a way that new customised, value added services can be composed using existing infrastructure and/or VNO services, and then offered to customers. We illustrate our approach through two case studies of infrastructure services: storage and networking, and present the Web Services technologies we have developed for infrastructure virtualisation."
3485,"A key research challenge in Web services concerns (semi-) automatic discovery and composition of Web services, in order to construct new Web services with desired properties or capabilities. This talk provides a survey of key developments that work towards this ambitious goal. The fundamental work in this area has centered on three models, each coming with a different approach to the composition problem. The OWL-S model for Web services focuses on how Web services interact with the ""real world,"" represented abstractly using (time-varying) first-order logic predicates and terms. A representative composition result here uses a translation into Petri nets. The ""Roman"" model for services focuses on an abstract notion of ""activities"" (without modeling how they impact the world), and in essence model Web services as finite state automata with transitions labeled by these activities. A powerful composition result is obtained using a reduction to propositional dynamic logic (PDL). The conversation model focuses on messages passed between Web services, and again uses finite state automata to model the internal processing of a service, with transitions labeled by message sends and message reads. A key result here concerns determination of the ""local"" behavior of individual services, if they are required to conform to a given ""global"" behavior. The talk also discusses two ongoing efforts to unify the three models just described. One activity, by the semantic Web services intiative (SWSI), is to develop a semantic Web services ontology (SWSO). This is based on the Process Specification Language (PSL), a first-order ontology for sharing descriptions of manufacturing processes, which recently became an ISO standard."
3486,"In scientific workflow environments, scientific discovery reproducibility, result interpretation, and problem diagnosis primarily depend on provenance, which records the history of an in-silico experiment. Resource Description Framework is frequently used to represent provenance based on vocabularies such as the Open Provenance Model. For complex scientific workflows that generate large amounts of RDF triples, single-machine provenance management becomes inadequate over time. In this paper, we research how HBase Bigtable-like capabilities can be leveraged for distributed storage and querying of provenance data represented in RDF. In particular, we architect the ProvBase system that incorporates an HBase/Hadoop backend, propose a storage schema to hold provenance triples, and design querying algorithms to evaluate SPARQL queries in the system. Using the Third Provenance Challenge queries, we conduct an experimental study to show the feasibility of our approach."
3487,"Service-Oriented Computing (SOC) is a wide and complex research area. Despite the huge effort in both industrial and academics initiatives, several challenges need to be addressed in order to effectively realize the SOC vision. One of the most relevant issues is the need of effective, flexible, reliable, low cost solutions for dynamic service brokering and composition. This paper presents results of an ongoing work on the design and development of a service- and message-oriented middleware for atomic and composite service brokering, named SAI middleware. The SAI middleware offers a set of features for service brokering and dynamic composition, while also guaranteeing loose coupling between service providers and consumers and relaxing the prerequisites for service providers to publish their capabilities in an interoperability domain. SAI dynamic composition is based on an Artificial Intelligence planning approach and on the adoption of an ontology-based functional profile encoding information for enabling automatic information extraction and combination in the service composition chain. Our main contribution consists in addressing these issues in a holistic way, as required to effectively support the SOA vision in real application scenarios, while not optimizing single aspects yet."
3488,"Service oriented computing has gained a considerable momentum as a new paradigm for building enterprise information systems. Notable efforts have been made recently from both researchers and industrials to support the construction of service-based applications; nevertheless several issues still need to be tackled including service definition and adaptation, and services orchestration. This work proposes Pyros, an environment for building and orchestrating open services. An open service is represented by a workflow that coordinates calls to service provider methods. Thereby component activities and the way they are synchronized are rendered visible. In order to finely orchestrate services, they are associated with entry points. An entry point acts as a gateway for inserting and getting information about the progress of service execution. The paper details the approach adopted by Pyros for building and orchestrating services, and presents associated architectural choices. Furthermore, it reports an experimentation that we conducted for implementing an eTrader application using Pyros."
3489,"Along with the rapid growth of heterogeneous cloud services and network technologies, more mobile devices use cloud storage services to enlarge the capacity and share data in our daily lives. We commonly use cloud storage service clients in a straight forward fashion, since we may easily obtain most client-side software from many services providers. However, when more devices and users participate in heterogeneous services, the difficulty increases to manage these services efficiently and conveniently. In this paper we design and implement a novel cloud-oriented file service, Wukong, which provides a user-friendly and highly-available facilitative data access method for mobile devices in cloud settings. By using the innovative storage abstraction layer and a set of optimization strategies, Wukong supports heterogeneous services with a relatively high performance. By evaluating a prototype in a systematic way on the aspects of the supporting interface, system performance, and the system resource cost, we find that this easily operable file service has a high usability and extensibility. It costs about 50 to 150 lines of code to implement a new backend service supporting plugin. Wukong achieves an acceptable throughput of 179.11 KB/s in an ADSL environment and 80.68 KB/s under a country EVDO 3G network with negligible overhead."
3490,"Effective design and implementation of any IT Optimization process relies on critical technical and business insights about IT environment. The essential information, which relates to hardware and software assets, is often in collective possession of the infrastructure and application specialists. In this paper, we describe the Knowledge Harvesting &amp; Information Synthesis System (KHISS), which engages enterprise online communities to capture the information required for IT optimization activities. We describe our experience in deploying KHISS principles to manage enterprise IT infrastructure and application portfolio, as a first step in on-Cloud migration -- a large business transformation activity. We discuss the challenges in attracting and maintaining enterprise online communities, as well as in assuring the quality of knowledge collected through this collaborative mechanism to guide the strategic decisions about enterprise IT environment."
3491,"The concept of Service Oriented Architecture (SOA) enables flexible and dynamic collaborations among different service providers. Backed up by SOA, scientific workflows can bring together various scientific computing tools and resources all offered as services to answer complex research questions. However, studies conducted on my Experiment show that although the sharing of service-based capabilities opens a gateway to resource reuse, in practice, the degree of reuse is very low. This motivates us to propose ServiceMap to provide navigation support through the network of services in building scientific workflows. In this paper, we propose an extension of ServiceMap, i.e., ReputationNet that incorporates the reputation of services/workflows and their publishers to reinforce its capability in terms of service and workflow recommendations. We develop a novel model of the reputation aspects of the services/workflows, and we propose heuristic algorithms to provide service recommendations based on reputations. Experiments have been conducted with workflows on my Experiment to evaluate the effectiveness and validity of the ReputationNet approach for service recommendations."
3492,"The cost and effort for developing personal healthcare systems with IoT devices can be substantially high mainly due to complex inference capabilities. To tackle the problems, we proposed INFerence-as-a-Service (INFaaS) in our previous work. INFaaS is a cloud service which provides a core set of common functionality required to develop various context-aware applications. Since INFaaS is designed for generic purpose, its applicability and genericity should be verified in that sense. In this paper, we present a comprehensive case study of developing an IoT-based Personal Healthcare System with the INFaaS cloud framework. This system requires various inference algorithms to diagnose diseases with the acquired medical contexts, which will increase developers' overhead. We believe that a cloud service providing inference services such as INFaaS yields a tremendous benefits in terms of cost benefits."
3493,"It is considered that Web services have had a tremendous impact on the web as a potential silver bullet for supporting a distributed service-based economy on a global scale. However, despite the outstanding progress, their uptake on a web scale has been significantly less than initially anticipated. Isolated service islands without links to related services have hampered service discovery and composition. In this paper, we propose a methodology to drive innovation from isolated service islands into the global social service network to connect the service islands. First, we propose Linked social service-specific principles based on Linked Data principles for publishing services on the open web as linked social services, and suggest a new platform for constructing a global social service network. Then, an approach is proposed to enable exploitation of a global social service network, providing workflow as a service. Finally, experimental results show that Linked social service can solve the service composition problem by enabling providing workflow as a service based on the global social service network, and has the potential to be the next wave of services."
3494,"International standards and references about ITSM, like ISO 20.000 and ITIL, deal with the management of service quality, define SLAs (e.g. availability, response time, etc.), and how to manage them from a process perspective. These definitions can be applied both to a service provided by a single provider under a single Service Level Agreement and, in more complex scenarios, to a service made up by the combination of several services provided through multiple contracts. The latter scenario brings to light issues and aspects relevant to the business and to the IT staff. The paper proposes an approach to model SLAs in complex service systems by means of check points. The model supports Service Level Managers in dominate SLAs both from technical and contractual perspective and facilitates communication with business. A case study is used to motivate and exemplify the model."
3495,"Developing contemporary software architectures requires the consideration and adoption of the Service-oriented Architecture (SOA) principles. Distributed applications are a very common domain in which SOA guides design decisions in particular. For a long time, SOAP and its related stack of standards have been the only technological choice for implementing SOA-based systems. With the increased adoption of the REST concept, an alternative to SOAP is gaining traction. Security considerations have been part of the SOAP-based standardization work since the very beginning. As a result, a mature and comprehensive set of security-related standards is available for building SOAP-based service systems. REST-ful service systems, however, cannot take advantage of such a fully developed security framework yet. This paper therefore revisits the SOAP-based web services security stack in order to identify commonalities, differences and gaps in the security available for REST-ful services. From these findings a desired REST-ful web services security stack is proposed together with related research, development and standardization challenges."
3496,"The linkage between healthcare service and cloud computing techniques has drawn much attention lately. Up to the present, most works focus on IT system migration and the management of distributed healthcare data rather than taking advantage of information hidden in the data. In this paper, we propose to explore healthcare data via cloud-based healthcare data mining services. Specifically, we propose a cloud-based healthcare data mining framework for healthcare data mining service development. Under such framework, we further develop a cloud-based healthcare data mining service to predict patients future length of stay in hospital."
3497,"In crisis management context, coordination of partners is a crucial requirement. However, defining, orchestrating and maintaining collaborative behavior of stakeholders of crisis management are arduous tasks. This article presents an approach dedicated to support these three missions in crisis management context. This objective is achieved according to three abstraction levels in charge of covering the three previous expectations. Besides, three prototypes have been developed (based on SOA and EDA) and are introduced in this article also through a simple use-case."
3498,"In e-Science, many scientific workflow management systems have been developed to integrate distributed computation resources, data sets, and mining algorithms. Users usually modify and rerun a workflow while repeating procedures: preprocess of data, selection of features, modification of data, selection of mining algorithms, generation of models, and evaluation of the models. These procedures are continued until the domain knowledge is acquired. However, as the size of the data increases, the execution time of the workflow becomes longer and longer, which drives up the cost of rerunning the modified workflow. As a result, it becomes hard to quickly obtain the analysis result. In this research, we avoided the rerun of the workflow by storing service invocation results on a platform and realized data-centered service composition by adding and deleting rules to be fired. To validate the effectiveness of our proposed platform, we created two rule-based services to analyze real-time data: stream message data and sensing data."
3499,"Cloud computing enables users to perform their computation tasks in the public virtualized cloud using a pay-as-you-go style. Current pay-as-you-go pricing schemes typically charge on the incurred virtual machine hours. Our case studies demonstrate significant variations in the user costs, indicating significant unfairness among different users from the micro-economic perspective. Further studies reveal the reason for such variations is interference among concurrent virtual machines. The amount of interference cost depends on various factors, including workload characteristics, the number of concurrent VMs, and scheduling in the cloud. In this paper, we adopt the concept of pricing fairness from micro economics, and quantitatively analyze the impact of interference on the pricing fairness. To solve the unfairness caused by interference, we propose a pay-as-you-consume pricing scheme, which charges users according to their effective resource consumption excluding interference. The key idea behind the pay-as-you-consume pricing scheme is a machine learning based prediction model of the relative cost of interference. Our preliminary results with Xen demonstrate the accuracy of the prediction model, and the fairness of the pay-as-you-consume pricing scheme."
3500,"The Business Process Model Notation (BPMN) provides a standard graphical language that can be used by business analysts for modeling business process choreographies. A challenging task is to formally verify that constructed choreography models are logically correct with respect to safety, liveness, and various application-specific correctness requirements. To aid with this important task, we present a model checker based framework to automate the verification process. The main component of our framework is the BPMN verifier, a tool that can automatically convert BPMN choreography models into PROMELA, the input language of the SPIN model checker. We describe the implementation and functionality of the BPMN verifier, and how the tool eases the task of expressing Linear Temporal Logic (LTL) correctness requirements, through its LTL Manager component."
3501,"Today, Cloud services are mainly traded on provider platforms such as on Amazon's EC2 On-Demand marketspace. Thereby, consumers and providers neither negotiate the price nor the characteristics of the services. The recent years underpin a trend to more dynamic Cloud markets. So e.g. the Cloud provider Virtustream released a revenue model where consumers are charged based on consumed μVMs while Amazon extended it's spot market with spot blocks and spot fleet management. Hence, multi-round bilateral negotiations are a promising approach for trading Cloud services on future Cloud markets. Such negotiations are based on an alternating exchange of offers and hence, they are termed Bazaar-negotiations. Specifications such as the WS-Agreement Negotiation - which is maintained by the Open Grid Forum - foster the development of Bazaar-based markets. To ensure integrity and transparency of negotiations with untrusted negotiation partners - which is a precondition for the adaption of Bazaar-based Cloud markets - blockchains are a promising approach. In this paper we introduce a concept of a blockchain for Bazaar-negotiations whereby we assume that the offers exchanged during negotiations follow the structure defined in the WS-Agreement Negotiation specification. We implemented it within a CloudSim based simulation environment which is able to simulate such Bazaar-based markets to show its technical feasibility."
3502,"Service Level Agreement (SLA) establishment can be viewed as a complex business process in which consumers and providers, with varying and potentially conflicting preferences, interact with one another in order to reach mutually acceptable agreements over the service usage terms and conditions. These interactions are governed by public interaction protocols which define their communicative behaviour and are guided by private decision-making strategies which define their strategic behaviour. Time plays a crucial role in decision-making, necessitating support for modelling temporal constraints in interaction protocol specifications. In this paper, we propose two temporal constraints, namely the deadline constraint and the validity constraint and use the Amazon EC2 Spot Bid Request lifecycle to illustrate the need for supporting them. We extend our previous state based model for SLA interaction protocols [4] to support time sensitive conversations. We have implemented our proposed approach in Auto SLAM [3], a policy-driven framework for automated SLA establishment, and validated it through a real world use case scenario of procuring computing resources from Amazon Elastic Compute Cloud (EC2)."
3503,"The mobile ambient is a formal model for mobile computation in which processes reside and move in a hierarchy of named locations organised as a tree-like structure, but the real-time property of the mobility has not been well described. In this paper, we extend mobile ambient with time, and then present discrete time mobile ambient calculus (DTMA). Based on DTMA, we investigate the modelling for Web service composition orchestration that has some mobility and time constraint. The service orchestration case which is used for the service composition example is formalized based on BPEL4WS basic actions modelling, and this work is a foundation for the model checking of the real-time mobile service orchestration."
3504,"With the popularity of cloud computing, many cloud service providers deploy regional data centers to offer services and pplications. These large-scale data centers have drawn extensive attention in terms of the huge energy demand and carbon emission. Thus, how to make use of their spatial diversities to green data centers and reduce cloud provider's costs is an important concern. In this paper, we integrate service reward, electricity cost, carbon taxes and service performance to study cost-effective request scheduling for cloud data centers. We propose an online and distributed scheduling algorithm CESA to chieve the flexible tradeoff between these conflicting objectives. The time complexity of CESA is polynomial, and it can be implemented in a parallel way. CESA requires no prior knowledge of the statistics of request arrivals or future electricity prices, yet it provably approximates the optimal system profit while bounding the queue length. Real-trace based simulations are conducted which verify the effectiveness of our CESA algorithm."
3505,"Web services have become important building blocks of distributed applications and have matured to the point where service lifecycle issues such as version management are now paramount. However, there is a lack of versioning support in relevant standards and tools. We present an approach which leverages the existing WSDL service definition model to build a versioned service hosting solution. We distinguish between a Web service interface (published) version and its implementation version (private). We introduce the concept of a service interface proxy. This proxy, which can be generated automatically, implicitly defines the service interface version, and is published as the logical service endpoint. Client requests are routed dynamically by the proxy to appropriate implementation versions. We have implemented a prototype of our approach to demonstrate its applicability."
3506,"Service-oriented collective intelligence, which creates new value by combining services provided by various organizations via services computing technologies, has been gaining in importance with the development of services computing technologies. Because collective intelligence needs many participants, it is crucial to build a framework where a wide variety of policies of service providers are satisfied. In this paper, we propose an architecture which handles a comprehensive process of service selection, adaptation, and coordination to satisfy policies of service providers. First the system selects services, and then adapts the services to the given policies if any of available services cannot satisfy the policies. To achieve this, we formalized this problem as an extension of constraint satisfaction problem and showed a solution. Moreover, the system often needs to force a composite service to follow protocols given by service providers. Therefore we proposed a method which uses meta-level control functions for composite services in order to change order of service execution."
3507,"Recently, trust has been recognized as an important factor for grid computing security. We develop a trust management architecture for trust enhanced grid security incorporating a novel trust model which is capable of capturing various types of trust relationships that exist in a grid system and providing mechanisms for trust evaluation, recommendations and update for trust decisions. The outcomes of the trust decisions can then be employed by the grid security system to formulate trust enhanced security solutions. We design several algorithms to demonstrate how one can derive the trust enhanced security solutions for both user and resource provider protection with the proposed trust management architecture. Leveraging on trust knowledge and forming it as part of the security decisions, the proposed architecture possesses several desirable emerging properties that enable it to provide an improved level of security for grid computing systems."
3509,This article describes our novel methodology to ascertain the IT Cloud customer's perceived service value in terms of non-functional requirements and our experience applying this methodology to clients using a Virtual Desktop Service running on an IT Cloud.
3510,"The purpose of this article is to introduce the system that analyzes external business environment utilizing RSS or web news. The proposed system will visualize positioning of companies in an industry. The positioning analysis shows the company priority issues in changing market. Conventionally, such information is collected and regenerated manually by analysts. Our technique retrieves RSS or web news, applies characteristic analysis for each sentence in the content, evaluates the sentences with business rules, and performs macro analysis based on the Five Forces analysis. This technique will grasp competitive powers of companies in the specific industry and will discover the differentiation factors of the company. The authors apply the proposed system to the optical network industry, and the system revealed that rivalry among the existing competitors was the most influential force in the industry."
3511,"Service Oriented Computing (SOC) is the fast emerging successor of the Object Oriented Computing design paradigm. Web Services technology, which is considered as an implementation of the SOC model, has had an incredible acceptance but tumultuous growth. This apparent chaos is due to its inherent lack of flexible cooperation strategies, generic service model and weak semantic description, mainly when deployed for Enterprise Applications with distributed computing transactions. Agent Based Web Services(AWS) is an appropriate approach to implement the Enterprise-Agent Design Model, which aims to overcome these inherent limitations of Web Services. In this paper, we report our ongoing efforts to use AWS in a Network Management System(NMS) we have built. Our NMS employs both SNMP and Mobile Agents(Aglets). We are developing an intelligent software agent, which is integrated with the Drools Rule Engine so that appropriate rules are triggered when pre-defined events occur. Our work is a novel approach covering a multitude of technologies such as SOC, AWS, Rule Engine, SNMP and Mobile Agents, with our Software Agent acting as a `glue' imbibing this synergy."
3512,"Service oriented computing (SOC) has provided a promising architectural for business collaboration. However challenging security issues concerning business collaboration have arisen because of its dynamic and loosely coupling nature. An important problem is how to specify security policies that belong to different application domains and integrate them to satisfy the requirements of the collaboration. In this paper we will provide analysis on authorization policy requirements for business collaboration, collaboration patterns and various security comparability and integration issues. We believe it is a first step toward a framework for modeling, specifying and handling authorization control for business collaboration."
3513,"With the increasing complexity of IT outsourcing environments thousands of servers and their configurations are increasingly managed by globally distributed teams. This requires a flexible identity access management process in place to efficiently provision necessary access rights for a given system, only if users need it, when they need it and for only as long as they need it. In this paper we present a novel approach to discovering required role permissions by integrating system data and enterprise crowd sourcing (a process where a group of experts solve problems through collaboration). By mining server registries, compliance repositories (such as user revalidation records), we derive a set of servers and the respective access rights for each team member. This data is then validated and updated by one or more team members using the principles of crowd sourcing. We show that this approach improves the role discovery process and accelerates the deployment of the security service infrastructure."
3514,"""Cloud of Things"" (CoT) is a concept that provides smart things' functions as a service and allows them to be used by multiple applications. In the CoT, a single smart thing instance should efficiently host multiple applications, called multi-tenancy. However, since multiple applications may simultaneously access the same smart things, they may contend for uses of the same smart things, which is called resource conflicts. Moreover, smart things inherently form complex dependencies, examples of which are include a group of smart things in a room, a group of smart things owned by a person, etc. Since handling resource conflicts and complex dependencies at an application level is typically ad-hoc and error-prone, it results in exacerbating readability of application codes. To address these issues, we propose a middleware for Cloud of Things called ECO. The ECO middleware manages organizations to handle dependency among/between smart things and virtualizes physical smart things to enable isolation between/among multiple applications using shared smart things yet internally controls smart things's sharing to resolve resource conflicts. Also, it provides consolidation by harmonizing different smart things's execution contexts of multiple applications for efficient utilization of the shared smart things. As a result, ECO middleware facilitates development of multiple applications over heterogeneous smart things with efficient sharing. The ECO middleware is implemented with heterogeneous device frameworks like UPnP, ZigBee, and CoAP over 6LoWPAN. We show that ECO middleware provides efficient sharing controls and access controls with negligible virtualization overhead."
3515,"The introduction of mobile clients and context-aware behaviors into Web Service compositions may generate faults and inconsistencies. We introduce an extension of a composition model where context-awareness is made explicit and a number of correctness properties are verifiable. In particular, our extended model enables the verification of properties commonly used to validate context dependent applications. We also propose a set of algorithms to verify these properties efficiently."
3516,"As service behavior plays a key role in service interaction, behavior-based service discovery has been increasingly recognized as a critical activity for service-based systems. However, little attention has been paid to the retrieval of behavioral models, which is critical for behavior-based discovery. This paper proposes an approach for extracting service behavioral models from WS-BPEL, which has been emerging as the prominent language for service orchestration. Our approach identifies all participants involved in a WS-BPEL process and abstracts the interaction of the process with its participants through behavioral models. Furthermore, we distinguish three types of ordering constraints between activities in behavioral models. Based on this, we present a mechanism to automatically adapt WS-BPEL processes to find and use other similar services when no service satisfies exactly the specified requirements. We have implemented a prototype to demonstrate and evaluate our approach."
3517,"Mobile Web services enable the IT industry and the mobile industry to create products and services that meet customer needs within the existing Web services framework. Transaction support is crucial in mobile Web services. The existing mobile transaction models are application-specific and not suitable to investigate the generic theoretical model. We present a new formal method called PMTM (P system-based Mobile Transaction Model) to formalize the behavior of mobile transactions. There are two kinds of transition rules in PMTM. The first group is object rules, which describe the transitions in membranes. The second group is membrane rules, which define the structural modification of membranes. Finally, we use mobile dining philosophers problem to illustrate the usage of PMTM."
3518,"Cloud computing has attracted much interest recently from both industry and academic. However, it is difficult to construct perfectly secure mechanisms, in face of complex and various attack behaviors in cloud computing. In this paper, a stochastic game model (SGM) is proposed to describe the attack-defense behavior in cloud computing, the physical machine, attack-defense behavior and their attributes are modeled by using SGM, thus forming the attack-defense game model of cloud computing. On this basis, the Nash equilibrium of attack-defense process of physical machine is computed in order to get the best defense strategy. The related theories of Petri net are used to verify the correctness of proposed method. The computation formula and the actual meaning of performance index are given. The enforcement algorithm is also proposed. Both case study and simulation results show that the proposed method can adapt quickly to the changes in cloud application, thus improving the security of cloud computing."
3519,"The emergence of mobile devices brings high potential of business value. These devices include sensors, actuators, smartphones, tablets, and other complex devices. They have various computational resources, and are usually connected by wireless connections. On top of the available resources and provided services, complex tasks can be performed. Researches have focused on offloading the resource-intensive computation to stronger cloud infrastructure, or providing services on top of connected mobile devices. It remains a challenge to exploit the various resources on heterogeneous devices. In this paper, we propose a mobile environment model to describe the connected devices and study the structural and behavioral characteristics of the environments. Based on the model, we design the routing protocols and a language to support the composition of environments. We propose a framework to provide a unified, flexible and scalable service for task/process deployment and execution on top of the heterogeneous and dynamic mobile environments."
3520,"Jeff Barr is focused on furthering awareness among software developers of the opportunity to innovate and build businesses using Amazon Web Services. Launched in July 2002, Amazon Web Services exposes Amazon.com technology and product data that enables developers to build innovative and entrepreneurial applications on their own. Barr meets regularly with developers in the U.S. and internationally to introduce Amazon Web Services and to help them build businesses and applications with the program's services. He joined Amazon in August 2002 as a Senior Software Developer on the Associates team."
3521,"Merging orchestrations is a crucial issue in the development process of service-based applications. However, merging orchestrations with overlaps is a manual and tedious process today. In this paper, we present a case-study on the Bronze-Standard, a medical imaging application built from Web-Service based orchestrations. We introduce the OMSM, an orchestration model supporting merging, that we designed to assist this process. Through a detailed analysis of the use-case, we show how our model helps the developer to obtain a proper composition of the application. There is still room for generalizing the approach to a broader set of orchestrations as discussed."
3522,"Computational thinking has recently been widely recognized as a fundamental skill that should be cultivated for everyone and in every field. Although there is an increasing interest in research in teaching and learning computational thinking in recent years, an engaging, effective online learning system is yet to be built for teaching, learning, and applying computational thinking online. To this end, we have developed DATAVIEW, an online social learning system for learning computational thinking concepts and skills online. The main contributions of this paper are: 1) we developed a new, effective online learning model for computational thinking based on our previous widely-applied R2D2 model. A signature characteristic of this new model is being interactive and learner-centered, thus ""i"" is carried out through the entire learning experiences, which provides teaching facilities to instructors and interactional tools among instructors and learners, 2) we implemented, validated, and refined iR2D2 in our DATAVIEW computational thinking online service, 3) We propose to use the DBR (design-based-research) approach to study the relationship between technology and teaching in the context of computational thinking, generating research results and findings applicable to online teaching in other domains as well."
3523,The paper presents work-in-progress on the cloud service provisioning across multiple cloud providers. The work assumes the emergence of Cloud Brokers between customers and cloud providers. The brokers split user requests and ensure provisioning from multiple providers. An exact splitting algorithm is developed to efficiently split the cloud requests among the multiple cloud platforms with the aim of decreasing the cost for customers. This splitting is formulated as a Mixed Integer Program and this is combined with Open Flow and NOX technologies that achieve flow based inter-cloud networking. A new controller module is developed and integrated in NOX to configure the Open Flow switches for inter-cloud path establishments.
3524,"The as a Service paradigm reflects the fundamental idea of providing basic coherent functionality in terms of components that can be utilized on demand. These so-called services may also be interconnected in order to provide more complex functionality. Automation of this service composition process is indeed a formidable challenge. In our work, we are addressing this challenge by decomposing service composition into sequential decision making steps. Each step is supported by a recommendation mechanism. If composition requests recur over time and if evaluations of composition results are fed back, a proper recommendation strategy can evolve over time through learning from experience. In this paper, we describe our general idea of modeling this service composition and recommendation process as Markov Decision Process and of solving it by means of Reinforcement Learning. A case study serves as proof of concept."
3525,"In service computing, the quality of service (QoS) has been used to distinguish different services. Many service recommendation schemes predict how a customer might rate the QoS of various services. Based on the predicted ratings, they recommend services to the customer. Most of these schemes do not consider the unfair rating problem. As the QoS rating of a service can determine whether the service is chosen by a customer, malicious users and services might explore the weakness of the existing schemes in handling unfair ratings to gain commercial advantage. This paper proposed a service recommendation scheme that is robust against unfair rating. When predicting a customer's QoS rating for a service, the proposed scheme takes into account of the ratings given to the service by the users that are similar to the customer, the ratings that the service gained from the typical users and the own experience of the customer. Experiments with the proposed scheme show that (a) the scheme has good prediction accuracy, and (b) it can counter the manipulations by the malicious users and services effectively."
3526,"The lack of accuracy is the main issue in language service research. One reason is the negligence of accommodating users and their cultures in discovering, composing, and organizing services. On the other hand, a community behavior is mostly mirrored in jargons, common sentences, mottos and so forth. Moreover, culture is inseparable to its contents. Most culture disputes are due to cultural contents that can be represented in images, audio and videos. These two features of culture analysis and contents are currently not available in language services. Our main contribution is that we provided a necessary framework of service discovery, composition and organization for cultural language service to solve this problem. In service discovery, we implemented semantic similarity in cultural language services by utilizing language service ontology. In service composition, we presented hybrid language service architecture to provide multi level analysis in cultural text and culture contents in multimedia. In service organization, we provided multi-objective constraint optimization that is able to accommodate the characteristic of language services organized in multi culture environment."
3527,"There is an increasing demand for context-aware adaptive services that can evolve at runtime in response to unanticipated changes in their environments or functionalities. Enabling the runtime evolution of such context-aware adaptive services is still a major challenge. In this paper, we introduce a novel approach to tackle this challenge. At the development time, our approach explicitly models a context-aware adaptive service from three aspects: functionality, context, and adaptive behaviour. As such, these aspects and their relationships can be clearly captured and easily manipulated. The approach also generates the executable artifacts of the composite service from their models. These artifacts are engineered with the ability to be changed at runtime (i.e. evolvable artifacts). To cope with unanticipated changes, we adopt the models@runtime concept for keeping the service model alive at runtime. This model is then manipulated by the software engineer to take into account such changes. To apply the model changes to the running service's composition, differences between the running service model and its evolved model are computed. Then, adaptation actions corresponding to the differences are generated and applied to the service's evolvable artifacts. To demonstrate the approach applicability, we have used it to develop and evolve two case studies: travel guide and electronic exam services."
3528,"Web services choreography is used to design peer-to-peer applications where each peer is potentially a Web service. It defines the required behavior of participating Web services along with their interactions through message exchanges. Implementing a complex system described by a choreography requires selecting actual Web services whose individual behaviors are compatible with the overall behavior described by the choreography. Although the selected Web services implement the specified behavior, they may not be able to interact due to the policies they enforce to protect their resources. A Web service' resource can be an operation or a credential type to be submitted to be able to invoke an operation. In this paper, we propose a novel approach to determine at design time whether a choreography can be implemented by a set of Web services based on their access control policies and the disclosure policies regulating the release of their credentials. We model both Web services and Web services choreography as transition systems and represent Web services credential disclosure policies as directed graphs. We then verify that all possible conversations of the Web services choreography can be implemented by matching credential disclosure policies of the invoker Web service with the access control policy of the Web services being invoked. We propose a resource release graph to enable this verification."
3529,"Process behavioral similarity calculation is widely used in many activities of business process management, such as process mining, process clustering and process retrieval. However, most existing process behavioral similarity measurement methods do not consider variable constraints (e.g. time constraints) in process models. Since the variable constraints of tasks, especially time constraints, are crucial to quantify the differences between the executions of tasks with the same or similar labels in process models, it is significant to calculate exact process behavioral similarity with the consideration of their impacts in process models. In this paper, we formally define the process behavioral similarity with time constraints and propose a time-aware method to efficiently compute the process behavioral similarity based on the ordering relations with time constraints. Experiments illustrate that our proposal can significantly improve the rationality and accuracy of process behavioral similarity measurement."
3530,"An IT services businesses must continually manage the assignment and movement of practitioners to or between projects balancing multiple objectives (1) to minimize their idle bench, while aiming (2) to increase revenue from new project opportunities. The key to simultaneously achieving these objectives is a well managed Service System for Workforce Deployment with distributed decision making. In this paper, we discuss the decision support needs for this Service System, and present a decision support tool that computes an 'optimal' assignment of practitioners to projects,and improves the effectiveness and efficiency of this assignment process. The tool comprises of a matching module that matches practitioners to project openings, an optimization module that produces globally optimal recommendations, and a user interface for displaying the recommendations. We describe the decision support tool, our experiences with developing and deploying it in an IT services company, and our efforts to promote a common shared perspective among decision-makers enabling multiple decision makers to independently arrive at near-optimal decisions using the tool recommendations."
3531,"A composite service can have its overall quality of service (QoS) measure computed with the QoS measures of its constituent services. In the stochastic case of QoS modeling, accurate computation for the probability distribution of the composite QoS measure is NP-hard because of the inherent complexities of probability value calculation for the function of discrete random variables. However, given reasonable assumptions on the monotony of the composite QoS function and on the independence of constituent QoS measures, we have proposed a lower bound approximation algorithm that computes the approximate value of the composite QoS distribution for admission test purpose in much lower-order complexity of time even in the worst case. The effectiveness of the proposed method is verified and compared against the naive algorithm using simulative trace data."
3532,"The recent proliferation of API hosting frameworks has dramatically eased the development of interesting web mashups and provided monetization opportunities for enterprises offering high value APIs. Most of these mashups are based on request/response REST model that is widely used in the web world. However, REST is not the primary vehicle for communication oriented services such as Internet telephony, chat, presence or live video communications. Session Initiation Protocol (SIP) is commonly used for communication services and has well defined Java APIs for use by applications and application developers. In this paper, we present ""Talking Cloud"" - a API hosting platform on the cloud, for enabling the composition of asynchronous, communication-oriented services to create high value mashups such as object detection in real-time video. Our platform is based on a unique combination of HTTP and SIP, and provides a number of useful features such as service customization, media signaling and routing, social network interactions, elasticity and scaling, besides standard API management functionalities such as usage metering. Our platform provides a flexible framework that allows applications to place hooks within the service invocation workflow, and invoke application logic that is executed remotely, enabling flexible and context sensitive handling of calls/media. We describe our platform architecture and show how communication mashups can interact with it using a combination of REST and SIP interfaces that are exposed by the platform. We describe a prototype implementation and present two usecases - nearest available helpdesk agent and real time object detection within live video streams."
3533,"It is important in service oriented architecture (SOA) to separate functional and non-functional requirements for services because different applications use services in different non-functional contexts. In order to maximize the reusability of services, a set of constraints (e.g., dependency and mutual exclusion constraints) among non-functional requirements tend to be complicated to maintain. Currently, those non-functional constraints are informally specified in natural languages, and developers need to ensure that their applications satisfy the constraints in manual and ad-hoc manners. This paper proposes a model-driven development framework, through the notion of feature modeling, to explicitly and graphically specify non-functional constraints in SOA. The proposed framework allows developers to validate non-functional constraints in their applications in an automatic and consistent way. This paper also describes how the proposed framework is implemented and effectively used for service-oriented application development."
3534,"This paper addresses the issue of selecting Web services residing in a community. Since these Web services have similar functionalities, this selection depends on their Quality of Service (QoS). Existing approaches only consider the satisfaction of users' requirements and neglect the satisfaction of Web services' requirements and the community to which they belong. This paper proposes an approach of selecting Web services based on the satisfaction of all three parties - user, Web service, and community. The approach consists of first, formalizing the selection process and then, using integer programming to define a score function, which can be maximized to find the best selection based on three satisfaction factors. Experiments using real Web services and measurements are conducted to demonstrate the influences of the approach."
3535,"Process mining aims at distilling useful knowledge from the execution logs of process models. It has become a vivid research area in recent years. In this paper, a novel approach for process mining based on two event types, i.e., START and COMPLETE, is proposed. Information about the start and completion of tasks can be used to explicitly detect parallelism. The algorithm presented in this paper overcomes some of the limitations of existing algorithms such as the a-algorithm (e.g., short-loops) and therefore enhances the applicability of process mining in practical situations. Based on the completeness of the given event log and the behavior theory of Petri nets, the correctness of the algorithm can be proved theoretically."
3536,"In the cloud based service provisioning industry, one of the main challenges that providers face involves keeping existing tenants engaged while attracting new ones. To address this, providers need to gain insights about customer satisfaction. In that regards, support ticket data, understood as the main way of communication between both parties, can be mined to obtain an estimation of customer satisfaction by means of the polarity of the sentiment extracted from the report descriptions. To that end, in this work we propose a model which can learn a feature representation for sentiment polarity changes, from the sequence of tickets emitted by a given customer during the period associated with the service subscription term. Then, that resulting feature representation, combined with other handcrafted features related to contract and ticket data, is passed to a classifier which estimates the likelihood of service subscription renewal by the customer. Experiment results using real data from a service provider shows that learned representation of sentiment polarity changes from support ticket data in combination with other handcrafted features improves the accuracy in predicting subscription renewals. Moreover, our architecture is flexible enough incorporate and integrate several feature representations and give more expressive power to the prediction."
3537,"Current Web users usually have their own files, work documents, communications and personal contacts distributed in the storage systems of many widely-used Internet services(e.g. Google Docs, Gmail, Facebook, Zoho). Therefore, they face the challenge of being not able to have an integrated view for their related data objects (e.g. mails, pics, docs, contacts). Recently, most of the major Internet services provide standard APIs that allow developing software applications that can read and write data from their underlying data store after providing the credential access information of registered accounts. The MyDeepWeb system is designed to let the Web users interact with their Internet services normally while, behind the scene, the information of their objects will be extracted, consolidated, linked and then populated into a single private cloud-based data store where the user can have integrated access to their data objects from anywhere through multiple devices."
3538,"Cloud platforms and services usually provide an APIlayer as decoupled, language agnostic interface for both front-end client integration and back-end data and/or function access. The availability and performance of the APIs have significant impact on the quality of end user or client experiences due to its nature of interaction endpoints. However, the extreme dynamics, complexity and scale of the current cloud platforms challenge the applicability of the existing performance monitoring and anomaly detection approaches from timeliness, accuracy, and scalability perspectives. This paper presents a novel approach to API performance monitoring,which recognizes performance problems by response time deviation from a baseline response time / throughput model that are created and continuously updated through online learning. In the postdetection phase, an MIC (Maximal Information Criteria) based correlation algorithm is used to group alerts into a higher leveland more informative hyper-alerts for end user notification. We prototyped our solution for a large-scale commercial cloud platform,evaluated it using three months' API performance metrics data,and compared with a couple of existing representative algorithms and tools. The results show our approach is able to detect API performance anomalies with a high F1-score. Compared to existing Granger based approach, our approach has achieved nearly onetime increase in F1-score. Moreover, the alert reduction ratio of our approach outperforms several state-of-the-art approaches."
3539,"Dynamic collaborations are the means by which a group of autonomous entities collaborate to achieve common objectives through sharing their own resources (e.g. specialised applications, controlled information and data, and storage and networking infrastructures). With the emergence of ""cloud computing"", it is now possible to share such resources as services. However, sharing such resource services requires that the participants agree to the terms and conditions of their responsibilities, as well as access to information and policies regulating their behaviour within the collaboration. This is done through the use of an eContract that captures the contributed resource-oriented services, as well as their respective service level agreements. One of the unique characteristics of the contract in dynamic collaborations is that each autonomous participant must agree with the services contributed by other participants against a set of its own policies. Within this context, this paper presents an efficient contract negotiation algorithm, called conflict neighbouring algorithm (CNA). We model and verify the algorithm using SPIN model checker, and compare its performance results with other alternative algorithms."
3540,"Grid computing is becoming a mainstream technology for sharing large-scale resources, accomplishing collaborative tasks and integrating distributed systems. Information service plays a very important role in grid computing environment. Through information service grid applications are able to exploit the knowledge about current grid status and configuration to adapt themselves to changes in heterogeneous, dynamic environments. We propose a framework of information service based on the testbed ShanghaiGrid. In GT3, the index service in MDS can not provide a reliable indexing, while in our framework, using the notification method to contact grid services to index service to provide a reliable indexing. The information that information service should provide according with the ShanghaiGrid is discussed in detail."
3541,"This paper describes ways to connect ledger cost behavior of a service delivery project with cost estimations derived at the time of contractual agreement. The purpose of this connection is to improve the management of the service life cycle, providing long range forecasting of the profitability of various service offerings. We emphasize cost, but our methods apply also to revenue, and consequently to profit. In a perfect financial world the connection between the cost estimate and the actual costs would be maintained in a tight feedback loop. The complex real world of multi-year and multi-country service delivery projects requires accommodation of (1) historical changes in accounting practices and (2) missing data. We describe the content of a cost case, the content of the ledger, and methods for forecasting profitability of parts of delivery projects using both the cost case and historical ledger experience of other similar projects. This paper reports a work in progress. We limit discussion of accuracy measurements to one benchmark, and we discuss potential improvements we have not yet implemented."
3542,"People-driven service engagements involve communication over channels such as chat and email. Such engagements should be understood at the level of the commitments that the participants create and manipulate. Doing so provides a grounding for the communications and yields a business-level accounting of the progress of a service engagement. Existing work on commitment-based service engagements is limited to design-time model creation and verification. In contrast, we present a novel approach for capturing commitment-based engagements that are created dynamically in conversations. We monitor commitments identifying their creation, delegation, completion, or cancellation in the conversations. We have developed a prototype and evaluated it on real-world chat and email datasets. Our prototype captures commitments with a high F-measure of 90% in emails (Enron email corpus) and 80% in chats (HP IT support chat dataset) and provides promising results for capturing additional commitment operations."
3543,"Over the last years, the complexity and variety of services available on the Internet increased. This fact is leading to the search for efficient techniques of routing client requests to the best server available. A known technique is the application layer anycast (ALA). The main goal of this work is to elaborate efficient ways to provide ALA with quality of service in the context of cloud computing. To achieve this goal, we proposed a new algorithm (GALA, Global Application Layer Anycast). It inherits characteristics from another existing system (GAA, Global Application-layer Anycast), and uses geolocation as a differential. The results of the experiments show that GALA, compared to the inherited algorithm, maintains the client requests efficiencies and substantially lowers their latencies."
3544,"In the development process of service-oriented systems, business process models are used at different levels. Typically, high-level business process models that describe business requirements and needs are stepwise refined to the IT level by different business modelers and software architects. As a result, different process model versions must be compared and merged by means of model version control. An important prerequisite for process model version control is an elaborated matching approach that results in precise mappings between different process model versions. The challenge of such an approach is to deal with syntactically different process models that are semantically equivalent. For that purpose, matching techniques must consider the semantics of process modeling languages. In this paper, we present a matching approach for process models in a versioning scenario. Based on a term formalization of process models, we enable an efficient and effective way to match syntactically different but semantically equivalent process models resulting in precise mappings."
3545,"Distributed software systems are the basis for innovative applications. Agility is the key for achieving survivable and maintainable distributed systems because the non-deterministic nature of distribution would otherwise leave the system uncontrollable, especially in emerging mobile ad-hoc networks. A mobile ad-hoc network (MANET) is based on a self-organizing and rapidly deployed network of mobile services that collaborate without using any preexisting fixed network infrastructure. Survivability is defined as the capability of a service to fulfill its mission in a timely manner, even in the presence of attacks, threats, or failures. Any failure of a single system may affect a business process; however, the monitoring of many mobile services simultaneously is not an easy task. This paper bridges the analyses of business processes and security threats with an alert mechanism. We propose an integrated approach to engineer a survivable distributed system through dynamic workflow generation in the context of Web Services Business Process Execution Language (WSBPEL), and Really Simple Syndication (RSS)."
3546,"This paper proposes the content grid architecture for pervasive content delivery. Leveraging the advantages of the Web, P2P, and application networking systems, this new grid infrastructure can provide better quality of content and application delivery. We implemented the content grid architecture using Globus to illustrate its feasibility and potentials"
3547,"Most social media analyses such as sentiment analysis for microblogs are often built as standalone, endpoint to endpoint applications. This makes the collaboration among distributed software and data service providers to create composite social analytic solutions difficult. This paper first proposes a system of systems service architecture (SoS-SA) design for social media analytics that support and facilitate efficient collaboration among distributed service providers. Then we propose a novel Twitters sentiment analysis service implemented on top of this design to illustrate its potentials. Current sentiment classification applications based on supervised learning methods relies too heavily on the chosen large training datasets, approaches using automatically generated training datasets also often result in the huge imbalance between the subjective classes and the objective classes in the sentiment of tweets, making it difficult to obtain good recall performance for the subjective ones. To address this issue, our proposed solution is based on a semi-supervised learning method for tweet sentiment classification. Experiments show that the performance of our method is better than those of the previous work."
3548,"Current composite Web service development and management solutions, e.g. BPEL, do not cater for flexible and adaptive business collaborations due to their pre-defined and inflexible nature that precludes them accommodating business dynamics. In this paper we propose a rule driven approach for adaptive business collaboration development in which rules drive and govern the development process. We firstly introduce the business collaboration context framework (BCCF), which provides enterprises with the context required for business collaboration. We then explain our model driven approach with which enterprises can capture this context in business collaboration models. Subsequently we demonstrate how we utilize rules to drive the development and management of such models, as such facilitating flexible and adaptive business collaboration"
3549,"Advancement of container technology (e.g. Docker, LXC, etc.) transformed the virtualization concept by providing a lightweight alternative to hypervisors. Docker has emerged as the most popular container management tool. Recent research regarding the comparison of container with hypervisor and bare-metal demonstrates that the container can accomplish bare-metal performance in almost all case. However, the current literature lacks an in-depth study on the experimental evaluation for understanding the performance interference between microservices that are hosted within a single or across multiple containers. In this paper, we have presented the experimental study on the performance evaluation of Docker containers running heterogeneous set of microservices concurrently. We have conducted a comprehensive set of experiments following CEEM (Cloud Evaluation Experiment Methodology) to measure the interference between containers running either competing or independent microservices. We have also considered the effects of constraining the resources of a container by explicitly specifying the cgroups. We have evaluated the performance of containers in terms of inter-container (caused by two concurrent executing containers) and intra-container (caused between two microservices executing inside a container) interference which is almost neglected in the current literature. The evaluation results can be utilized to model the interference effect for smart resource provisioning of microservices in the containerized environment."
3550,"To realize human-centered coordinating services in ubiquitous computing, we require a framework that coordinates physically distributed heterogeneous services according to users' intentions and locations. How can we coordinate the services to assist the user in receiving a coordinated service to maximize the user's satisfaction in an environment? In order to solve this issue, we have been developing a sort of agent-based coordination framework, called ""location-mediated Web service coordination,"" that orchestrates Web services wrapped in the agents. In this paper, we show a prototype application of the framework, context-aware information assist services in a museum."
3551,"CIT (continuous integration testing) has been widely studied in the testing research field in order to start some levels of integration test as early as possible. One challenge of CIT lies in how to simulate the behavior of those unavailable components. Existing methods like stud and mock fail to provide the advanced component simulation capabilities required by CIT from perspectives like diversified program artifacts, behavior transitivity, and configurability. This paper proposes a new simulation apparatus, namely surrogate, to address this problem. The surrogate generator generates platform specific code skeleton from definition of the component to be simulated. The generated code communicates with surrogate engine and returns simulated platform specific behaviors. The surrogate engine simulates component behaviors including both output and possible invocation on dependent components. Moreover, it provides platform independent interfaces and configuration model. Early implementations of surrogate generator and surrogate engine are introduced in detail. To validate the value of surrogate technology in CIT, a case study has been carried out with careful analysis. The result shows that this technology really helps identify some bugs at early stage of development."
3552,"This paper presents a structural model and an infrastructure for resource management and service discovering in SOA. The structural model is composed by a set of metaclasses which allow representation and descriptions of services, as well as different types of service components and granularities for composition and reuse. The model includes procedures to find and retrieve candidates to build composite services for meeting specific needs and purposes. Candidate services are found and retrieved by means of intelligent discovery mechanisms on the basis of a set of descriptors. The discovery procedure adopts a Case-Based Reasoning approach, in which the services are considered as cases that are kept and indexed in a repository."
3553,"There have been many studies on the automated composition of Web services. Most of previous works build a composite Web service by chaining the input and output of a Web service without considering the functional semantics of the Web service itself. However, they cannot guarantee that the service composed will provide a requested functionality. Furthermore, they have high time-complexity since every possible combination of available services should be considered. This paper proposes a composition method that explicitly specifies and uses the functional semantics of Web services. First, the proposed method constructs a graph model, which represents the functional semantics of Web services as well as the dependency among inputs and outputs. Second, composition paths with core services, which satisfy the functionality requested by a client, and value-added services, which support the transformation between I/O types, are searched in the graph model. Finally, possible composite services are built from the paths searched. The proposed method improves the semantic correctness of composite services by considering the functionalities of Web services, as well as the composition speed by only considering functionally related services."
3554,"In this paper, we propose a skyline computation system UCOS (User Clustering based Online Skyline), which divides the computation into offline and online stages. Based on the truth that QoS similarity implies the skyline similarity, the offline stage of UCOS system dose user clustering according to the historical user-service QoS records by given distance metrics. Then, we compute the representative skyline for each cluster standing for the general characters of the users' skylines. Benefit from those offline results, the online stage is able to give a rapid prediction for online skyline request and achieves good online computation performance by doing refinement on the predicted results."
3555,We present two phenomenon Web semantics and grid computation. These two concepts could be seen as some state transition system and could use coalgebra concept to unify the two concepts and get their final objects.
3556,"Researchers are beginning to realize the potential of Web services that can use the Web as a place for information publication and access as opposed to the traditional Web- services paradigm that merely uses the Web as a transport medium. Traditional Web services can be difficult to discover, can have complex invocation APIs, and require strong coupling between communicating applications. In previous work, we presented ontology-based techniques in which users make service requests using free-form, natural- language-like specifications. This paper shows how we can use these ontological techniques to automatically create ontology-based Web services that (1) are easy for software agents to discover because they are created based on machine-processable formalisms (ontologies), (2) have invocation APIs requiring only simple read and write operations, and (3) require no a priori agreements regarding types and data formats between communicating applications. Experiments with our prototype implementation in several domains show that our approach can effectively create Web services with these characteristics."
3557,"In computational grid, resource backup is the most important method for improving the availability of applications. It can be implemented by making advance reservation of redundant resources. But in order to make the grid applications more cost-effective, it is absolutely necessary to devise multiple policies for resource backup in order to provide flexible resource backup services. In this paper, we give the detailed discussion of several resource backup policies, which were briefly presented in our former work, and evaluate them with a grid resource backup simulator, GReBSim, which is designed as one of the simulating tools in our grid project. The simulating results show that combination of multiple policies is quite important for trading off in resource utilization and time to recovery for failed applications."
3558,"In service oriented computing paradigm (SOC), services can be composed dynamically to fulfill complex needs. In the literature, the problem of service choreography has been widely studied. The existing approaches assume that only one instance of each involved service is required. This assumption is very restrictive since in a choreography, multiple instances of each service may be required. To handle this issue, in this paper we propose a novel instance-aware service choreography approach. The aim is to enable automatic service composition through a choreography where the service instances are defined dynamically."
3559,"Problems with ontology-dependent approaches for dynamic web services composition occur when ontologies are poorly maintained or unavailable in a domain, or the costs of maintenance exceed their benefits. To resolve such problems, this paper proposes the use of Google Distance as an ontology-independent method for the semantic similarity matching stage of web services discovery. Further, it provides an improved method for matching QoS parameters in the operational similarity matching stage of web services selection. The methods are embedded within a simple architecture for dynamic web services composition. We provide comparisons between our and existing QoS-enabled service discovery approaches in two major categories of web composition methods: workflow-based and AI-based. Our implementation exercise clearly shows the performance tradeoffs that help implementers to better assess an overall QoS-enabled dynamic composition method for its suitability to application-level services workload characteristics."
3560,"This paper presents Abacus, a service-oriented programming language designed for the development of grid applications. Abacus considers that all the grid resources constitute a unified logical address space, where each memory cell holds a resource in the form of a service, and a grid application solves a problem by operating on these memory cells. Abacus allows programmers to concentrate on the logic of their applications, such as service implementation logic, service invocation logic, and glue logic. Low-level details such as resource distribution, resource binding, and service deployment are supported by the compiler and the runtime system. With such virtualization techniques, Abacus helps to enhance the productivity of programmers in developing grid applications."
3561,"In this paper we identify the importance of the problem of discovering dynamic dependencies among Web services. The approach we take is to automate the identification of traces of dependent messages, based on the correlation of messages exchanged among services. We infer service dependencies based on the correlated message traces."
3562,"This paper proposes a service automaton to proactively provide services to users in a context aware pervasive system. First, a context space (CSP) model is proposed based on conceptual space theory, and this model deals with context, situation, services and user preferences in a unified way. In CSP, situation is used to depict the specific scene faced by a user; user preferences can be deemed as a basis to predict the user desired goal situation; to derive a service composition that achieves a given goal, we formalize semantic web services as automata with an emphasize on the relations among contexts and services. Afterwards a composite service automaton (CSA) is defined to synthesis services into a global automata and support the service composition to achieve the goal situation. At last, a temperature control case in smart homes is illustrated to validate the model and approach. The example shows that the service automata can conveniently reflect the state transition of a context aware system from a global perspective and help user achieve the goal situation."
3563,"Service-oriented Architecture (SOA) is becoming the prevalent architectural style for the creation of agile and flexible enterprise IT infrastructures. Services, as the key elements of Service-oriented Computing (SOC), can be composed in very efficient way to support flexible business processes and to adapt application systems quickly to new business requirements. Virtualization technologies offer cost-effective and rapid resource scalability as well as greater flexibility in providing hardware resources to service demands. However, also service-oriented applications need to be tested, monitored and maintained; thus the management aspect of Service-oriented infrastructures is often neglected. We propose a Service Management System that enhances the management of infrastructures for SOC and describes a first step towards the self-management of services in infrastructures with virtualized hardware resources. This paper provides a core set of components and required management functions that must be considered in providing a Service Management System for SOC."
3564,"Modelling and enacting cross-enterprise business processes (CBPs) is a key ability for successfully setting up and managing virtual organizations, e.g. supply chains. In this paper we present and compare approaches for modelling CBPs based on the service-oriented architecture paradigm. By embedding our overall approach into a model- and architecture-driven development perspective, we show how service-oriented systems realising CBPs can be derived from business-level modelling"
3565,"The key to successful adoption of SOA is not only reusability but also the ease with which services can be developed, deployed and maintained in a service providing environment. One key guiding principles of SOA is implementation abstraction, that fosters complete encapsulation of implementations, exposing only the primitives as interaction points with the service. Granularity of service abstractions defines the different level of primitives exposed to prospective service consumers. The primitives being the only external representation for a service, their design is extremely important as the granularity not only determines the ease with which the services are identified, but also the ease with which services realization can be achieved. So far there has been no systematic approach to apply design principles such that right level of abstractions can be defined. We present a tool called Service Abstraction Threshold Estimator (SATE). Given the available implementation artifacts, SATE allows varying levels of primitives to be defined, elicits the complexity associated with different levels of primitives and provides a threshold estimation engine that displays the optimal level of abstraction that can be achieved in a given service development environment. We demonstrate SATE on a real-world example, and also evaluate SATE against design decisions taken by software architects. The results of our evaluation show that SATE can provide more accurate threshold estimations, and also helps in reducing the time and effort taken to maintain already developed services."
3566,"Traditional research in service composition has assumed perfect functional matching of service capabilities against stated requirements. In real life, however, this is a myth, as borne out within several SOA development and deployment organizations such as IBM in customer engagements. In particular, the variations in data, functional and nonfunctional requirements present a serious hurdle in reusing existing available services and creating service compositions at run-time. Current research in semantic Web services seeks to address this problem by creating meta-models that capture the domain and later on grounding the requirements and capabilities to this meta-model. Our research project, Variation-Oriented Service Composition and Adaptation (VOSCA), investigates the key research issues at modeling-time and run-time described above. Before the VOSCA vision is realized, however, some basic notions need to be first defined and clearly articulated. Hence this paper provides an initial first step by focusing on defining service variants and providing algorithms for service matching."
3567,"Scalability is one of the main challenges of social media analyses such as sentiment analysis. Micro logs as emerging opinion sharing platforms require new approaches that are more scalable and accurate. In this paper, we propose, implement, and evaluate SSSA, a Semantic Scoring Sentiment Analysis service, which matches the demand of scalability and efficiency of a sentiment analysis system for a large volume of micro logs. We first build an automated tweet tagging system to alleviate the cost of building a dataset for training. Our approach then calculates semantic scores of words in tweets. These scores are the baseline for the proposed subjectivity classification and feature selection steps. Finally, we build a polarity classifier to estimate the polarity of subjective tweets on top of them. Experiments show that SSSA, compared to conventional approaches that based on emoticon, hash tag, or semantic scoring, has improved the scalability and accuracy of micro log sentiment analysis."
3568,"Business process modeling and enactment approaches for services compositions provide a way to coordinate the activities performed by de-centralized entities such as web services. As business needs change, the defined processes supporting the business also need to change and adapt, giving rise to the need for flexible business processes. However, a service composition is a collaborative environment where the service providers, consumers as well as the aggregator have business goals to achieve. Safeguarding such goals can be a daunting task upon numerous runtime modifications to business processes, but it is necessary to ensure the viability of the composition with respect to the business goals of all the parties. Therefore the flexibility needs to be controlled but without unnecessary restrictions. In this paper we propose a novel architectural approach to model, enact and manage business processes and their changes based on explicitly represented service relationships of a service composition."
3569,"Business process is collection of standardized and structured tasks inducing value creation of a company. Nowadays, it is recognized as one of significant intangible business assets to achieve competitive advantages. This research introduces a novel approach of business process analysis, which has more and more significance as process-aware information systems are spreading widely over a lot of companies. In this paper, a methodology of business process clustering based on process similarity is illustrated. The purpose of business process clustering is to analyze accumulated process models in order to assist design of new processes and reengineering of existing ones. The proposed methodology exploits structural similarity metrics of business processes. The methodology is illustrated with synthetic process models of insurance processes."
3570,"In the context of global manufacturing and service outsourcing, production-related services are organized as manufacturing services network. Service partner selection and production distribution planning are two key difficulties that arise in the optimal design of this network. This paper seeks to develop an effective methodology, which includes a mathematical model and an efficient genetic algorithm to solve problems. The mathematical model describes the characteristics of manufacturing services network. An efficient genetic search algorithm is developed to find out the optimal solution with minimum operating cost. The performance of the proposed algorithm is evaluated by solving a set of randomly generated problems."
3571,"The Simple Object Access Protocol (SOAP) is an XML based protocol that is widely used over the Internet as it supports interoperability by establishing access among Web servers and clients from the same or different platforms. However, SOAP Web services suffer the bottlenecks and congestions as a result of Web messages being bigger than the real payload in addition to the potentially increasing demand of the requested Web services. Aggregation of SOAP messages is an effective solution that has been developed to significantly reduce network traffic by aggregating SOAP messages at the server side and then multicast them to the Web clients. The major problem of the aggregation techniques is that they require efficient similarity criteria that can compute the similarity of SOAP messages as group-wise and not just pair-wise. In this paper, a new unsupervised auto class Fractal clustering technique is proposed for clustering SOAP messages into a dynamic number of clusters according to their Fractal similarities. The experimental results showed that the proposed Fractal clustering technique can improve the performance of Web services significantly better than other clustering standards such as the K-means and PCA combined with K-means by enabling the aggregation model to aggregate the most similar messages in one group resulting in better messages size reduction. Furthermore, the proposed Fractal clustering technique potentially reduces the required processing time in comparison with other standards."
3572,"With the extensive adaptation of Web service based applications in dynamic businesses applications including on demand computing, highly configurable virtual solutions and cloud computing based systems demand automated tools for composing and managing these services in composite systems. The use of standard protocols for publishing, discovery, invocation, process definition and SLA definition etc. (e.g. UDDI, WSDL, SOAP, BPEL, WS-Agreement, WS-Policy) has made it possible to compose highly usable composite systems with minimal efforts. On the other hand the existence of a number of functionally equivalent service provide the system designers with the flexibility of choosing the most appropriate services for the system. Hence, automated negotiation among Web services provides an effective way for the services to bargain for their optimal customizations and allow the discovery of overlooked potential solutions. In this paper, we present a Web service negotiation framework that would be used by both the customer and providers of Web services for conducting automated negotiations for quality of service properties of Web services. Our proposed framework is highly flexible, protocol independent and supports participant polices for communication, negotiation and service level agreement creation. We further extend our approach by extending WS-Negotiation and WS-Renegotiation to model multi-round negotiation for multiple attributes of negotiated services in a multi-service and multi-party negotiation scenarios. We also describe a semantic web rules based approach for converting compatible participant policies for mutual comprehension of Web service. We describe in detail the process of negotiating service level agreements using the semantic web rules based approach."
3573,"Emerging Internet of Things (IoT) system consists of heterogeneous physical devices that are interconnected via the Internet Protocol (IP) networks. In order to integrate the front-end physical things in the IoT management systems, Business Process Management Systems (BPMS) have gained attention as a viable option. However, enabling the participation of the resource constrained IoT devices in the business process execution raises the question, when considering the agility and energy conservation, whether the device should embed a standard workflow engine or it is better to execute a program representing the workflow. This paper aims to provide a guideline to developers by investigating the two business process workflow execution approaches for resource constrained IoT devices via the comparison of performance and resource usage. The experiments show the comprehensive comparison between the two approaches together with the discussion of the lessons learned."
3574,"This paper introduces a method realizing dynamic provisioning of services in a distributed environment. Depending on a particular state of infrastructure the call of a service can lead to a new instance in the infrastructure or to using an existing instance. Hence, the dynamic deployment allows optimized distribution of service instances within a certain infrastructure. The paper introduces a context model for services that are registered in a distributed runtime environment. Furthermore, algorithms are introduced determining the need for instantiation as well as the best location for deployment. Hence, the best location is determined by correlating the context model, the certain state of infrastructure as well as data transfer costs."
3575,"The Web service level agreement (WSLA) is a Web Service specification language developed to provide a SLA template to capture all the information negotiated and agreed upon by a service provider and service consumer. Though this specification is suitable for many business-to-business collaborations involving two parties, it does not fulfil the requirement of a new business model involving multiple parties, where a party provides multiple services to other parties and consumes services provided by other parties. To address this requirement, this paper proposes an extension to the WSLA language to capture multi-party collaborations, called WSLA+."
3576,"The dynamic composition of Web services from independent service providers for complicated distributed applications is emerging as a key technique in the Internet world for future critical software systems. In such an environment, a user may need to compose some Web services to perform critical tasks along a given execution path that needs high reliability to avoid unacceptable failures due to service crashes. To achieve high reliability, the active parallel replication and standby parallel replication strategies can be used for managing the redundancy existing in each task of an execution plan. However, more replicas of Web services used during a task execution means higher price a user has to pay for. In this paper, we propose an approach to select necessary Web services from different vendors for the purpose of their composition in a way that satisfies the user-specified reliability, price and response time, while trying to minimize the price and delay of an execution plan. We provide simulation results to study the performance of the proposed approach and compare it with four variations that use different heuristics for service selections."
3577,"As a kind of tamper resistant hardware, Java Card is regarded as a trusted third party that can operate safely on an untrusting host. At the same time, with the development of e-commerce, network management and mobile agent technology, multimobile agents' cooperation can not only improve the efficiency of distributed compute, but more importantly, it has a comprehensive applicative value in solving the security issues of mobile agent system. Aiming at settling the bottleneck of Java Card on mobile agents' congestion, this paper, proposes a multimobile agents' separation scheme in destination host. By a specific example, the paper analyzes the detailed process of this scheme for its security and validity. Furthermore, it provides the architecture of multimobile agents and other applications on how multimobile agents resist attacks primarily from the malicious executing hosts and draws some significant conclusions."
3578,"The use of Semantic Web Services (SWS) increases the agility and adaptability of process execution in many settings. On the basis of available semantic descriptions, SWS allow the automatic selection,composition and mediation of the most adequate Web services to accomplish a specific process activity. However, we claim the need of a comprehensive semantic layer to cope with process, activity and domain-concept descriptions, beyond the scope of current SWS technologies. Therefore, we propose a semantic model that provides a starting point for the design and implementation of SWS-based processes across different settings and applications. The proposed Situations &amp; Goals (S&amp;G) Semantic Model bases on a user-centric view of processes. Particularly, the following main elements have been identified: situations define the possible states of the world; situation descriptions capture, from a user perspective, the context in a particular situation; goals link between a situation and the following and are defined in terms of initial and target situation descriptions. The S&amp;G semantic model is the epistemological core of a Situation Driven Processes (SDP) Ontology, which defines a general pattern for representing processes and can be derived for concrete settings and applications. As a result, Web applications can exploit the knowledge captured by the model to opportunely invoke SWS and thus address adaptive and flexible process executions."
3579,"Care processes in healthcare organizations are very complex and difficult to define precisely in advance. Furthermore, a specific process for the same disease can vary according to the characteristics of a patient and his or her situation at hand. In order to provide personalized healthcare services to patients, it is essential to identify the context of the patients. This paper presents an integrated architecture of context-aware business process management system based on ubiquitous computing technologies. By detecting the current health status of a patient using various ubiquitous devices such as RFID and smart sensors, the proposed system helps healthcare professionals provide personalized healthcare services."
3580,"Electronic contract systems deal with the lifecycle automation and management of contract documents from their establishment to expiry. Many existing contract systems are built as monolithic, vertical stand-alone applications that are inflexible and difficult to scale and interoperate with other enterprise systems. This paper proposes a service-oriented electronic contract system that leverages a number of common middleware services to provide an open, extensible and interoperable solution. The basic common services include the content repository services, the workflow and document routing services, the security services, and the notification services. A standard data model is defined to specify the metadata properties for managing, tracking and storage of contract documents in a central repository. Integration services are also introduced to enable the system to collaborate and share contract data with external applications such as contract authoring and fulfillment tracking tools. The implementation of an enterprise multi-party electronic contract system using standard commercial middleware components will be presented to illustrate the simplicity, reusability, interoperability and flexibility of this service- based approach."
3581,"In the Service Oriented Architecture paradigm services are self-contained software units of functionality. Several services can be composed to assemble a composite service that provides an overall functionality. This process is called Service Composition. The automation of the service composition process aims at decreasing the human intervention during the service discovery, matching, ranking, filtering and reasoning about the resulting composition candidates. The paper presents an innovative approach to automatic service composition. The discovery and matchmaking is based on semantic annotations of service properties, e.g. their inputs, outputs and goals. The approach uses a graph-based search algorithm to determine all possible composition candidates and applies certain measures to rank them. Filtering and reasoning is accomplished by validating the composition candidates using goal-based expressions."
3582,"Text categorization (TC), as an important domain of machine learning, has many unique traits, such as huge number of features, serious redundant features, dataset imbalance, etc. In this paper the various ensemble methods of naive Bayes classifiers and SVM classifiers are experimentally compared on the TC tasks. Besides, a new type of classifiers, moderated asymmetric naive Bayes classifiers, is proposed. Its advantages over the conventional naive Bayes classifiers in performance and computational efficiency are demonstrated."
3583,"SaaS(Software as a Service) has recently made a great impact on the software industry and changed the traditional lifetime of software. To obtain the benefit of SaaS, more and more software products have been converted into SaaS model, including business workflow management systems (BWfMS). Compared with the traditional BWfMSs, SaaS oriented BWfMSs introduce a new set of features which demand a new architectural design for SaaS oriented BWfMSs. Although several SaaS oriented BWfMSs have been developed, a study from an architectural perspective for SaaS oriented BWfMSs is still missing. The main contributions of this paper are: 1) according to the four SaaS maturity levels, it depicts four different scenarios where SaaS oriented BWfMSs work, 2) based on the four different scenarios, it proposes a reference model for SaaS oriented BWfMSs at each SaaS maturity level; 3) to validate the feasibility of the proposed reference model, it implements a specific BWfMS named WAAS."
3584,"Web services are increasingly being used to provide critical operations in business-to-business and safety-critical environments. In these environments the exploitation of security vulnerabilities may result in major damages in the services infrastructures, financial or reputation losses to the organizations involved, and other catastrophic consequences for the users and the environment. Web services frameworks are the basis for developers to create and deploy web services, and must provide a robust and secure environment, so that an application can deliver its service, even when in presence of security attacks. In this paper we study the behavior of well-known web services frameworks in the presence of security attacks targeting the core web services specifications, i.e., those enabling basic message exchange functionalities. Results show that frameworks are quite resistant to attacks. However, they also indicate that even very popular and highly tested frameworks can be vulnerable to attacks, with potentially catastrophic consequences for the services being deployed."
3585,"This document gives an overview of a research project realized by the Centre de Recherche Public – Gabriel Lippmann, with the aim to restructure the overall IT environment of the National Family Benefits Fund in Luxembourg. The focus of this text is on the practical experiences gained by introducing a Service Oriented Architecture. After a rough overview of the initial heterogeneous application landscape, the proposed solution relying on integration and workflow management standards is exposed. The conclusion analyzes the benefits of the proposed solution and the problems encountered during the development and setup phase."
3586,"Service discovery and composition are challenging issue of service computing to provide value-added service. Existing approaches by keyword or ontology matching have limitations for locating realistic services discovery and composition considering non-functionality or sociality. On main reason in that approaches are based on isolated services. The isolation hinders efficient discovery and composition of services. Therefore, in the past research, they suggest social linked service network considering relationships of functional and nonfunctional properties, and social interaction based on complex network theory, where they can locate related services through sociability. However, it would be difficult to create social linked service network because services portable devices and sensors has been increasing with progress of Big Data technology. In this paper, we propose creating social linked service network to improve performance of network construction as considering distributed process on Big Data infrastructure. First, we propose an algorithm that creation network graph using Map Reduce parallel programming model. Finally, experimental results show that our creating network using Map Reduce approach can solve the heavy computation load for many calculations of network elements."
3587,"In e-commerce and e-service environments, transaction context is important when evaluating the trust level of a seller or a service provider in a forthcoming transaction. However, most existing trust evaluation models compute a single value to reflect the general trust level of a seller without taking any transaction context into account. In the literature, a trust vector approach has been proposed to resolve the above problem. In particular, the trust vector contains different sets of trust values (termed as CTT values) so as to outline a seller's reputation profile. As a result, buyers can identify the potential risk existing in a forthcoming transaction (e.g., value imbalance, i.e. a malicious seller may build up a high level of trust by selling cheap products and then deceive buyers by inducing them to purchase more expensive products) and thus avoid monetary losses. In computing CTT values, some approaches are proposed that store the precomputed aggregation results over large-scale ratings and transaction data of a seller, so as to deliver prompt responses to a buyer's query. Though these approaches allocate relatively small space to each seller for storing the aggregation results, if applied in a system with millions of sellers, space consumption will be intolerable. In this paper, we propose a novel model for CTT computation with fixed storage space, which provides a trade-off between aggregation detail and storage space. It is particular suitable for CTT computation where a request is regarding a seller's trust in recent time period, e.g., the latest six months, rather than six months plus one day. We have conducted experiments on both an eBay dataset and a synthetic dataset to illustrate its good efficiency in responding to buyers' CTT queries."
3588,"Cloud Computing represents a new trend in the development and use of software. Many organizations are currently adopting the use of services that are hosted in the cloud by employing the Software as a Service (SaaS) model. Services are typically accompanied by a Service Level Agreement (SLA), which defines the quality terms that a provider offers to its customers. Many monitoring tools have been proposed to report compliance with the SLA. However, they have some limitations when changes to monitoring requirements must be made and because of the complexity involved in capturing low-level raw data from services at runtime. In this paper, we propose the design of a platform-independent monitoring middleware for cloud services, which supports the monitoring of SLA compliance and provides a report containing SLA violations that may help stakeholders to make decisions regarding how to improve the quality of cloud services. Moreover, our middleware definition is based on the use of models@run.time, which allows the dynamic change of quality requirements and/or the dynamic selection of different metric operationalizations (i.e., Calculation formulas) with which to measure the quality of services. In order to demonstrate the feasibility of our approach, we show the instantiation of the proposed middleware that can be used to monitor services when deployed on the Microsoft Azure© platform."
3589,"Service composition in sensor networks combines elementary services with a specific functionality to create a service with higher level functionality. The previous efforts in automating composition were sending full information about all services across the entire sensor network, creating a security risk and imposing significant communication overhead. Furthermore, learning based composition or error detection methods do not consider global information, leading to inefficiencies in the generated composition graphs. In this paper, we propose a probabilistic context-free grammar (PCFG) based modeling technique to construct service compositions. The successful compositions created for the given application are treated as statements belonging to an efficient composition PCFG of this application. The given set of such compositions is used to derive this PCFG automatically. Future composition could be then easily constructed with the help of such PCFG. We present our methodology for achieving such modeling and provide examples of its use to demonstrate its advantage over previous work. We also evaluate the resulting improvements in performance of compositions and in the costs of their creation."
3590,"In this paper, we present C-ABSC, a cooperative privacy preserving attribute based signcryption mechanism. It consists on performing the combined signing and encrypting processes of a set of data devices' inputs in a secure collaborative manner. The main idea behind C-ABSC relies on the distribution of the signcrypting operation among different devices, with respect to selected sub-sets of a general access predicate, such as an untrusted aggregating entity is capable of decrypting the received aggregated data only if a sufficient number of IoT devices cooperates. The C-ABSC scheme is multifold. First, it provides a selective access to authenticated aggregated data contents. Second, it provides a privacy preserving signcrypting process, such that a curious aggregator can neither infer the used IoT device's attributes for signing nor deciphering single data chunks. Third, C-ABSC relies on low computation and communication processes, mainly for resource-constrained devices."
3591,"In this paper, we present a scalable authorization service, based on the concept of fine-grained access control (FGAC), for large-scale grid infrastructures that span multiple independent domains. FGAC enables participating resource owners to specify fine-grained policies concerning which user can access can their resources under which mode. We argue that such an authorization service must be integrated with the resource broker service to avoid scheduling requests onto resources which do not authorize the user request. For this reason, we develop a novel resource broker service that integrates access control with resource scheduling. In our system, both resource owners and users define their resource access and usage policies. The resource broker schedules a user request only within the set of resources whose policies match the user credentials (and vice-versa). Since this process of evaluating authorization policies of resources and user, in addition to checking the resource requirement, can be a potential bottleneck for a large scale grid, we also analyze the problem of efficient evaluation of FGAC policies. In this context, we present a novel method for policy organization and compare its performance with other strategies. Preliminary results show that the proposed method can significantly enhance performance."
3592,"To achieve effective security in the increasingly dynamic computing environment of the mobile workforce our perspectives on security have to change, requiring the development of new approaches. We discuss a new security paradigm, shrink-wrapped security, in which security services are tightly coupled to a user's situation, and present a framework to help enable its realization."
3593,"Service-oriented architectures allow content providers to enable easy and transparent access to multimedia content for their clients as well as outsource non-strategic streaming services to third-party providers. We design a QoS framework and associated algorithms that can provide prioritized and fair media streaming based on service level agreements. We formulate the QoS support as a multiple choice multiple knapsack optimization and design several low-complexity algorithms to provide approximate solutions. We bound the performance of these algorithms, and evaluate them in terms of the number of clients supported and QoS provided over realistic simulation scenarios"
3594,"During a standard software development process, organizations create text-based documents that describe software requirements, design, and implementation. These text-based specifications describe the functionality of future applications as they relate to an existing IT infrastructure. We suggest that these documents also implicitly describe core underlying service-based capabilities of the organization. In this paper, we describe an approach that (when provided with software specifications from multiple organizations) can recommend services shared by the multiple organizations represented. These approaches leverage the syntactic similarity of the specification text and semantic information as inferred from WordNet. Experiments show the effectiveness of this approach when processing real software requirements specifications in operational environments."
3595,"Service-oriented architectures and web services have been used to foster the development of loosely coupled, interoperable, and distributed applications. Mission-critical and business process systems can be implemented with them, requiring a high level of quality. Model-based testing allied with state models is a promising candidate due to its efficiency, effectiveness, and flexibility. In this paper, we propose a model-based testing process to verify service-oriented applications. Finite state machines are used to model and support the test case generation. We evaluated the applicability of our process with a case study using a prototype tool."
3596,"The paradigm of the cloud computing model brings the expectations of the services consumed from the cloud to be always on and available for global consumption around the clock. Hence, the challenge for the service providers becomes to accommodate the planned maintenance windows to keep them to a minimum duration in order to reduce the impact on the consumers of their services. In this paper, we assess the continuous delivery methodology called Blue/Green deployment which targets to enable service updates with zero maintenance windows, and thus with no disruption to the end users. To overcome current issues we proposed an optimized Blue/Green technique based on automated service discovery, dynamic routing and automated application deployment. Our experiments indicate that the service discovery based Blue/Green deployment has a better performance for service continuous delivery compared to the available technologies."
3597,"Software organizations wanting to implement a systematic reuse program face the challenge of organizing and cataloging their software assets so that they can be retrieved in different contexts of usage across their divisions. With the advent of services-oriented architectures (SOA), of which Web services is an example, software components are readily available as services on the web using standard protocols. However, service descriptions are usually only those that are provided by the service producers and unless a service producer has thought about all the contexts in which its service may be used, there is no guarantee that the service can be retrieved with high recall. In this paper, we investigate how different contexts of asset consumption may be used for better asset modeling and discovery. We introduce an extensible set of consumption factors where the service descriptions may be provided by the service producer or other roles, implement a prototype that can evolve with the modeled factors, and demonstrate that the solution enables improved precision and recall for services available in a large-scale software organization."
3598,"With the emergence of grid computing and service-oriented architectures, computing is becoming increasingly less confined to traditional computing platforms. Grid computing promises the accessibility of vast computing and data resources across geographically dispersed areas. This capability is significantly enhanced by establishing support for mobile wireless devices to deliver access to high-performance computing under demanding circumstances. Access to the grid from mobile devices can be very effective in business environments where users can access the vast computing power and data repositories on the grid while working out on the field. This also enables and encourages collaborative working environments. A grid demonstrator system for distributed aircraft health monitoring already developed and implemented in the UK E-Science Grid project, DAME, is introduced. In this demonstrator, CBR technology for decision support is implemented in a practical framework that enables Grid-based, proactive mobile computing."
3599,"This paper examines the problem of Web service selection in a community. Web services reside in communities due to the similiar functionalities they offer. The selection in communities is important because each Web service provides a different level of service. Selection methods thus far only consider the satisfaction of users and neglect the satisfaction of Web services and the community to which they belong. In this paper we propose a method of selecting a Web service based on the satisfaction of all three parties - user, Web service and community. The proposed solution consists of first formalizing the selection process and then using linear programming techniques to define a score function, which can be maximized to find the best selection based on the three satisfaction factors."
3600,"BPEL is a popular standard language in SOA enterprise for specifying business process. A developer has to use specific programming technologies to construct process structure and specify Web services inputs and outputs in the process. It is time-consuming to specify every business process from low-level Web services. This paper proposes a semantic extended BPEL model for SOA process generation, and implements the model in a practical BPEL IDE. The new BPEL IDE supports efficient visual design of SOA processes, and helps developers focus more on business logic. A real multimedia conference SOA application is given to demonstrate the efficiency of SOA process generation with semantic extended BPEL model on the new BPEL IDE. And a Load Runner evaluation is given to validate the efficient BPEL process generation has no negative effect on execution efficiency of the generated BPEL code."
3601,"When Service Oriented applications and services are modeled, the term Quality of Service (QoS) is used to refer to the collection of constraints and quality requirements for a service. It is important that QoS attributes are specified in early stages of the development process, and modeled in a way that can be recognized and understood by all stakeholders. Regarding Service Oriented Architecture (SOA) modeling, QoS usually includes security, performance and availability. There are many options to realize business processes with services, such as collaborations with partners, internal services, third parties services (SaaS) among others. In previous works we have proposed the automatic generation of services specified in the Service Architecture Modeling Language (SoaML) from business processes, and the associated code. As SoaML allows to model functional requirements only, we have extended our proposal to also take into account QoS modeling and generation from SoaML service models, enriching the specification of services with quality characteristics."
3602,"This paper proposes a new automatic approach for the detection of SQL Injection and XPath Injection vulnerabilities, two of the most common and most critical types of vulnerabilities in Web services. Although there are tools that allow testing Web applications against security vulnerabilities, previous research shows that the effectiveness of those tools in Web services environments is very poor. In our approach a representative workload is used to exercise the Web service and a large set of SQL/XPath injection attacks are applied to disclose vulnerabilities. Vulnerabilities are detected by comparing the structure of the SQL/XPath commands issued in the presence of attacks to the ones previously learned when running the workload in the absence of attacks. Experimental evaluation shows that our approach performs much better than known tools (including commercial ones), achieving extremely high detection coverage while maintaining the false positives rate very low."
3603,"One of the most important characteristics of the traditional approaches of searching a missing object is a societal and infrastructural participatory crowd sensing where the crowds/participants 'consciously' opt to register the information in mind, and decide when, where, what, and how to sense and report. While the alert may extend to a broad community, the effectiveness of the crowd sensing doesn't reach its potential, due to its active participation factor in sensing, reporting, and processing methods. Unless there are clear incentives and privacy protection methods for the crowd sensors, it is considered very hard to achieve its practical deployment in reality. In this paper, we propose to automate the crowd sensing process and radically improve the effectiveness in the IoT System through opportunistic crowd sensing (OCS). Sensing a missing object's presence in proximity is automatically conducted by smartphones, as the application runs in the background and opportunistically collects and reports the data without active involvement of the user. We address the major technical challenges of privacy, security, sensing effectiveness, and user resource utilization, and validate its operation and effectiveness with the prototype system."
3604,"Quality of service (QoS) has been considered as a significant criterion for selecting among functionally similar Web services. Recent approaches focus on computing the skyline over a set of QoS attributes. This can completely free users from assigning weights to QoS attributes. However, these approaches are not sufficient in a dynamic Web service environment where the delivered QoS by a Web service is inherently uncertain. In this paper, we tackle the problem of skyline on uncertain QoS. We represent each QoS attribute of a Web service using a possibility distribution and introduce two skyline extensions on uncertain QoS called pos-dominant skyline and nec-dominant skyline. We then develop appropriate algorithms to efficiently compute both the pos-dominant skyline and nec-dominant skyline. Finally, we present our experimental results that show both the effectiveness of the introduced skyline extensions and the efficiency of the proposed algorithms."
3605,"Web services run in a highly dynamic environment (the Internet) which makes the composite service will face multiple exceptions in its execution. Thus, it needs to take effective actions to deal with the errors causing the exceptions. Then, by such actions, composite service can adapt to the dynamics and complexity of its execution environment. Since accurately identifying the error source which causes the exception is important, the problem of diagnosis for composite service becomes one of the key issues in the adaptive service composition. This paper proposes an approach for diagnosing composite service based on error propagation degree. After analyzing the error propagation relation between the services, the uncertain casual relation between the exception and the service is formed by the way of computing the error propagation degree. And then the diagnosis algorithm which is based on the error dependent matrix is established. The diagnosis can be achieved by fuzzy reasoning. This approach can preserve the efficiency and accuracy of the diagnosis for composite service under the uncertain environment by the computation of the error propagation degree and fuzzy reasoning, which can support the self-adaptation of the composite service effectively."
3606,"Cloud performance diagnosis and prediction is a challenging problem due to the stochastic nature of the cloud systems. Cloud performance is affected by a large set of factors such as virtual machine types, regions, workloads, wide area network delay and bandwidth. Therefore, necessitating the determination of complex relationships between these factors. The current research in this area does not address the challenge of modeling the uncertain and complex relationships between these factors. Further, the challenge of cloud performance prediction under uncertainty has not garnered sufficient attention. This paper proposes, develops and validates ALPINE, a Bayesian system for cloud performance diagnosis and prediction. ALPINE incorporates Bayesian networks to model uncertain and complex relationships between several factors mentioned above. It handles missing, scarce and sparse data to diagnose and predict stochastic cloud performance efficiently. We validate our proposed system using extensive real data and show that it predicts cloud performance with high accuracy of 91.93%."
3607,"In service oriented computing, authentication factors have their vulnerabilities when considered exclusively. Cross-platform and service composition architectures require a complex integration procedure and limit adoptability of newer authentication models. Authentication is generally based on a binary success or failure and relies on credentials proffered at the present moment without considering how or when the credentials were obtained by the subject. The resulting access control engines suffer from rigid service policies and complexity of management. In contrast, social authentication is based on the nature, quality, and length of previous encounters with each other. We posit that human-to-machine authentication is a similar causal effect of an earlier interaction with the verifying party. We use this notion to propose interaction provenance as the only unified representation model for all authentication factors in service oriented computing. Interaction provenance uses the causal relationship of past events to leverage service composition, cross-platform integration, timeline authentication, and easier adoption of newer methods. We extend our model with fuzzy authentication using past interactions and linguistic policies. The paper presents an interaction provenance recording and authentication protocol and a proof-of-concept implementation with extensive experimental evaluation."
3608,"An important IT service outsourcing business is to resolve incidents related to IT infrastructures our clients contract our company to support. Incidents are recorded as structured and unstructured data in tickets, which contain various characteristics about the incidents including timestamps, description and resolution Analyzing such incident tickets becomes a critical task in managing the operations of the service in order to keep the operations within the agreed upon service level agreement. Ticket analytics is essential to identify anomalies and trends, as well as detect unusual patterns in the operations; such analysis is hard to do manually especially for large accounts with complex organization and scopes. This paper focuses on ticket analytics and some key statistical techniques applied in the analyses. Finally, we use real-data examples to demonstrate these techniques and discuss major challenges of ticket analyses."
3609,"The cloud computing paradigm moves data across administrative domains to be managed by various outsourced database (ODB) service providers. There needs a means for a data owner to verify the correctness and completeness of query results returned from these ODB services. The process is called query authentication. Existing ODB services do not provide such assurance to customers. This is mainly due to that most techniques in literature are expensive and limited in functionality. This paper proposes to use independent external observers for query result verification. The proposed method requires the ODB service to return a verification object (VO) for each query result set, as most existing methods do. Differing to existing methods, we propose a complement set based VO construction method. The cost of computing complement set based VO is low compared to hash tree based ones commonly seen in existing methods. The proposed method does not require the data owner or the ODB service provider to maintain complex authentication data structures, instead, the workload of data state tracking and VO validation can be distributed among a set of observers. Furthermore, the proposed VO construction method is capable of supporting multi-attribute queries. The method is implemented as a thin wrapper of an ODB service with the support of additional observer services. We evaluate the performance of the system in Amazon EC2 and examine its cost of achieving trustworthiness."
3610,"A multitude of issues affect the broader adoption of Cloud computing, with security arguably being amongst the most significant. To address security concerns, the process of threat analysis is advocated to assess potential attacks that can undermine the security goals. However, conducting threat analysis for the Cloud is a non-trivial task given the plethora of attack surfaces entailed in the multiple layers of the operational stack and the resource/customer interfaces. Consequently, contemporary Cloud threat analyses approaches primarily focus on specific services/layers without analyzing the malicious behaviors over the complete multi-layered Cloud ecosystem. Hence, the need is of a comprehensive Cloud threat analysis approach that can (a) analyze the spectrum of malicious behaviors stemming from the vulnerable service interactions across the multi-level operational stack, and (b) correspondingly enumerate the attack surface exploitability by varied types of attackers. We achieve such a holistic Cloud threat analysis via a novel multi-level modeling of Cloud operations to obtain a comprehensive behavioral profile of its underlying services. Our proposed approach, using Petri Nets, targets the identification of core operational states to enumerate the normal sequence of Cloud operations along with the triggers that provide the state transitions. The obtained states transition enumerate comprehensive multi-level state space baseline of ""normal"" sequences and also constitutes to identify multi-level vulnerabilities not recognizable by the traditional single-level threat analysis."
3611,"The reconfigurable composite services (CWSs) can repair itself if any execution problems occur, in order to complete successfully its own execution. The research problem in which we are interested in is how to ensure the correctness of reconfigurable CWSs. This paper proposes an incremental design approach for modeling and verifying reconfigurable CWSs by using EventB. We start by abstractly specifying the main requirements of reconfigurable CWSs and then refining them through several steps by introducing the concept of dependencies and dynamic transactional patterns (DTPs) to model reconfigurable CWSs. The consistency of each model and the relationship between an abstract model and its refinements are obtained by formal proofs. Finally, we use ProB model-checker to trace possible design errors."
3612,"With the sweeping progress of services technology and crowdsourcing, individuals are offering their capability as a service and companies are orchestrating them for problem solving over the web. As a consequence, recent years have witnessed a rapid development of a human service ecosystem. In contrast to web service ecosystem, human service ecosystem is more complicated by the fact that humans grow capability overtime and their collaborations typically imply human involvement. It is thus worthy to understand the modeling of evolution of human capabilities and collaborations for optimization and more effective management. This paper proposes a three-layer time-aware heterogeneous network model, and based on it, a novel method is developed to study how human service providers and consumers develop their service provision and orchestrating capabilities as well as how they collaborate with each other. Exploratory analysis uncovers some evolution patterns which open a gateway for building possible applications such as human service recommendation for consumers, human capability development, and mechanism design for platform management."
3613,"Multimedia streaming means delivering continuous data to a plethora of client devices. Besides the actual data transport this also needs a high degree of content adaptation respecting the end users' needs given by the form of content preferences, transcoding constraints, and device capabilities.When it comes to content editing (like mixing in subtitles or picture-in-picture composition) relying on third party service providers may be necessary. For improved efficiency all adaptation should be done in a service-oriented way, because a lot of adaptation modules can be reused within different adaptation workflows. In this paper we discuss the extensions of Web service frameworks, and present a first implementation of a service-oriented framework for media streaming and digital item adaptation, concentrating on the technical realization of the services. Our experimental results show the practicality of the actual deployment of service-oriented multimedia frameworks."
3614,"Enterprises are witnessing increased thrust on collaboration and integration of existing applications to provide value-added services across the entire supply chain. Traditionally, enterprise applications are built as point solutions with context-specific built-in assumptions hard-coded in their implementation. Enterprise application integration discipline deals with the mechanism for integrating such isolated applications into a consistent whole. By modeling an enterprise application as a 3-tuple comprising of its data, service and process models, EAI problem can be visualized as view-integration problem over data, service and process models. This paper presents a pragmatic approach to analyze the process model of an existing application with respect the process model of desired application to identify and mitigate the conflicts in the built-in assumptions of two process models. A formal technique to analyze the process model at various levels of granularities and a set of operators to mitigate the conflicts are proposed. The proposed approach maximizes the reusability in the context of EAI"
3615,"This paper reviews the main efforts in integrating metadata records in distributed digital libraries environment. Each of these solutions does meet some of the requirements of distributed library services somehow, but further study are still needed in some issues, such as limitation on the capacity of collections, retrieval of content-related records built in different metadata formats. In order to alleviate the problem, a novel super-peer based infrastructure for integrating metadata records in distributed and heterogeneous environment is introduced with respect to three situations, namely, sharing a common schema; in different schemata but in same community; in different schema and community. This paper sequentially describes feasible solutions according to these scenarios before coming to the conclusion."
3616,"This paper proposes a fully distributed scheduling algorithm to process MapReduce data-intensive applications across geo-distributed clusters in federated clouds. The proposed algorithm, called FedSCD, takes advantage of data locality while reducing both VM cost and data transfer cost (between clusters) subject to Deadline constraint. This work is compared to conventional partially distributed scheduling algorithms in federated multi-cloud environments. Performance evaluation proves that the proposed algorithm FedSCD can reduce the MapReduce job cost by an average of 40% and ensure optimal resource allocation."
3617,"With the ever-increasing number of unmanned customer interaction channels (e.g. Internet and mobile banking), financial platforms are becoming subject to an expanding collection of system attacks due to their unique potential for large scale monetary theft. Significant challenge therefore remains in keeping fraud policy repositories current to prevent financial loss and customer inconvenience, due to the rapid rate at which fraudsters re-engineer their methods and deploy new system attacks. Accordingly, fraud policy development based upon mining of an institution's transactional data provides only an isolated view of industry behaviour with no support for the exchange of fraud policy data with other institutions that are likely to experience similar behavioural attacks as fraudsters horizontally permeate their efforts through analogous sector organisations. This paper proposes a policy distribution service for the automated collection, analysis and distribution of emerging fraud policy definitions through a collaborative policy sharing model. A key aspect of the service is the creation of an industry wide policy knowledge base within a centralised mediator component and distribution of policies using a common modelling formalism which maybe mapped into multiple disparate target fraud management implementations."
3618,"Service Assured task execution is an exemplary need for Enterprise Task Crowdsourcing, even when crowdsourcing tasks on open and public crowdosourcing platforms, which are characteristic of non-committed and discretionary task execution by crowd workers. Service Assurance (SA) has broader semantics than SLA fulfillment in the context of enterprise task crowdsourcing, as explained in our previous work on Service Assurance Framework for Enterprise task crowdsourcing. The framework is designed for open crowd platforms and enables to map a given requester SLA to worker SA requirements and select crowd workers who evince high probability of SA requirement fulfillment. Even with these select crowd workers, it might not be possible to implicitly assume the sustenance of workers' expected SA (which is computed based on workers' prior task executions) in the day to day task executions on the Enterprise Requester tasks, owing to the unbinding task execution facilitated by the open crowd platforms. This might result in defaulting in requester SLA fulfillment, rendering the worker selection (by the framework) futile. Hence, in addition to selecting the required SA fulfilling workers, it is essential to employ suitable SA sustaining strategies in place for sustaining the SA expected of workers. The SA Sustaining Strategies and the Service Assurance Framework together constitute for the Service Assurance Sustaining Enterprise Task Crowdsourcing Service (SAS Service)for Enterprise Tasks, which is deliberated in this paper. The SAS Service's performance is validated using appropriate crowd experiments."
3619,"With the increasing number of functionally similar services deployed over the Internet, QoS (Quality of Service) plays a more and more important role on deciding which component services should be selected to satisfy the quality requirements for a composite service. Although there exist some service composition approaches to select component services with regard to global QoS constraints, none of them facilitates handling global constraints for general flow structures in real-time distributed systems. In this paper, based on a hierarchical Colored Petri Nets, an approach named LOSGC (Local Optimal Selection with Global Constraints) is proposed to address this problem efficiently and effectively. The new approach is applicable to real-time distributed systems with general flow structures, including sequential, while, conditional choice and parallel operations. The global constraints are decomposed into local constraints according to flow structures, and then the distributed local selections are utilized to select optimal services that satisfy local constraints. A case study is performed to show that LOSGC can effectively apply to complex systems. Meanwhile, the results of experimental evaluation indicate that LOSGC can achieve close-to-optimal results with a significant improvement in terms of computation time."
3620,"Although the use of Cloud services is proliferating, the notion of Cloud security remains ambiguous. This typically arises from two causes, namely (a) the limited awareness about security details by the average Cloud customer which results in the customers being unable to clearly express their security requirements, or (b) the lack of interfaces/tools that can meaningfully capture the customer requirements. In general, the Cloud customers are only able to provide qualitative requirements due to their inability to express precise security requirements. Nevertheless, Cloud customers still need to assess and benchmark various security services provided by different providers in order to select the most suitable Cloud provider that can satisfy their ""imprecise and uncertain"" security requirements. This paper proposes a methodology for enhancing the security aspects of Cloud services by quantitatively comparing the customer security requirements with the security offered by Cloud providers. The novelty of our approach is based on the usage of a fuzzy logic schema to manage the uncertainty of those qualitative requirements. We validate our framework by applying it to real-world data that leverages the standardized Cloud service level agreements structure proposed in the ISO/IEC 19086 standard."
3621,"Complex information technology (IT) installations within an enterprise raise contextual challenges when delivering IT service management (ITSM) that maintain and enhance software for the business. Enterprise Architecture (EA) tools are now available to represent context as business processes, services and enabling IT components. However, few methods exist to leverage these representations for the management of complexity that exits in the EA. In this paper, we contribute towards this emerging area of research by posing fundamental questions: 1) ""what makes an IT service complex to deliver and by what measures?"" and 2) ""what is the complexity management?"" To answer these questions, we begin by examining related work in measures of software complexity. Next, we use a graph-based framework to identify complexity patterns that exist in the real world. Using the framework and case study method we show the relationships between the difficulty of service delivery and complexity patterns in the EA context. Generalizing from this, we propose principles for the complexity management."
3622,"During their lifecycle, business processes are keen to change. Changes either concern the process model structure or the accompanying rules, e.g. Compliance rules (laws and regulations). In the context of business process collaborations, several process partners collaborate together, and changing one process might result in knock-on effects on the other processes, i.e., Change propagation. Since business processes are often subject to restrictions that stem from laws, regulations or guidelines, i.e., Compliance rules, changing them might lead to the violations of these rules (non-compliability). So far, only the impacts of process changes in choreographies have been studied. In this work, we propose an approach that analyzes and evaluates the impacts of process changes on the different compliance rules and inversely, the impacts of compliance rule changes on the process choreography."
3623,"Web 2.0 provides the technological foundations upon which the crowdsourcing paradigm evolves and operates, enabling enterprises, universities and eGovernments, to access scalable networks of knowledge experts on-line. However, there is no existing practice allowing for coordination of crowdsourcing tasks, and their integration with existing business processes and embedding these services into the Web fabric. In this paper, we examine two applications of enterprise crowdsourcing service in the domain of IT Service Delivery: 1) IT Inventory Management and 2) End-User Support. We illustrate how a) expert discovery mechanism, b) virtual team building capabilities, c) task management and d) provisioning of task-based services, enable enterprises to effectively build knowledge networks, which are able to execute complex and transformative knowledge-intensive tasks. Finally, based on the application analysis, we propose PeopleCloud, an on-demand service system, which spawns and manages scalable virtual teams of knowledge workers by either (1) building on the wisdom of crowds within an enterprise or across a value chain or (2) creating a marketplace for accessing specialists on-line."
3624,"We present Swift, a system that combines a novel scripting language called SwiftScript with a powerful runtime system based on CoG Karajan, Falkon, and Globus to allow for the concise specification, and reliable and efficient execution, of large loosely coupled computations. Swift adopts and adapts ideas first explored in the GriPhyN virtual data system, improving on that system in many regards. We describe the SwiftScript language and its use of XDTM to describe the logical structure of complex file system structures. We also present the Swift runtime system and its use of CoG Karajan, Falkon, and Globus services to dispatch and manage the execution of many tasks in parallel and grid environments. We describe application experiences and performance experiments that quantify the cost of Swift operations."
3625,"In this paper we describe a framework that supports runtime service discovery in both pull and push modes. Our framework supports service discovery based on structural and behavioural models of services and applications, as well as quality and contextual constraints. In the approach, we use a proactive push mechanism in which services are identified in parallel to the execution of the system based on subscriptions of services and queries. A prototype tool has been implemented in order to illustrate and evaluate the framework."
3626,"Future e-business models will rely on electronic contracts which are agreed dynamically and adaptively by web services. Thus, the automatic negotiation of Service Level Agreements (SLAs) between consumers and providers is key for enabling service-based value chains. The process of finding appropriate providers for web services seems to be simple. Consumers contact several providers and take the provider which offers the best matching SLA. However, currently consumers are not able forecasting the probability of finding a matching provider for their requested SLA. So consumers contact several providers and check if their offers are matching. In case of continuing faults, on the one hand consumers may adapt their Service Level Objects (SLOs) of the required SLA or on the other hand simply accept offered SLAs of the contacted providers. Thus, this paper proposes an analytical forecast model, which allows consumers to get an assessment of the probability to find matching providers. Additionally, we present an optimization algorithm based on the forecast results, which allows adapting the SLO parameter ranges in order to find at least one matching provider. Not only consumers, but also providers can use this forecast model to predict the prospective demand. So providers are able to assess the number of potential consumers based on their offers too. Justification of our approach is done by simulation of practical examples checking our theoretical findings."
3627,"With the advances of e-Science, scientific workflow has become an important tool for researchers to explore scientific discoveries. Although several scientific workflow management systems (SWFMSs) have been developed, their support of exception handling is still limited. In this paper, we introduce our approach of exception handling in the VIEW scientific workflow management system. We propose an exception handling language for scientific workflows based on our workflow model. Both syntax and semantics rules of our language are presented. Different exception handling primitives, such as retry, alternative, and repeat, are supported in our language with flexibility for their composition to provide a sophisticated and flexible exception handling mechanism. Moreover, two exception handling algorithms and the architecture design for exception handling in VIEW are also presented."
3628,"A dynamic cooperation model for different agents' message exchanging is proposed. It is thought that the curve of surrounding words' relevant influence approximatively fits normal distribution with a mean of 0 and a standard deviation of /spl sigma/. Following those mechanisms, agent-based selective information retrieval based on topical crawling, high-performance indexing and an efficient search algorithm is implemented. The agents' cooperation model expresses the procedure of agent organization and communication. Topical crawling based on anchor word concept analysis can guarantee a higher performance. The topical crawler makes it easy to find as many relevant pages as possible and as few irrelevant pages as possible."
3629,"Business process modeling is an important topic of enterprise modeling. Modeling process can help us to define new process, evaluate and improve existing ones. As the development of Internet and B2B commerce, business partnerships are being fostered in order to broaden the scope of their market. Business process thus has been scatted over multiple organizations. At the same time, each participant of the cooperation has their own private process. So how to define and model cross-organizational process while keep each participant's autonomy poses new challenge to us. In this paper, we will focus on two problems:!) How can we guarantee the private process of each participant is consistent to the coordination protocol specified by message sequence charts? 2) How can we formalize the execution of the interaction protocol, which means the execution of private processes conforms to the interaction protocol? An approach based on object Petri net and behavior inheritance will be proposed in this paper to solve the above two problems."
3630,"With the rapid proliferation of wireless networks and mobile devices, Mobile Peer-to-Peer (MP2P) networks have attracted a huge amount of users for resource sharing. However, in an MP2P network, peers frequently join and leave the network, which makes the network topology dynamically change. Thus, it is difficult to establish long term and effective trust relationship among peers, making trust management become a challenging task. In this paper, we propose a dynamic grouping-based trust model DGTM, to classify peers. A group is formed according to the peers' interests. The experiments illustrate that our proposed dynamic grouping-based trust model DGTM always achieves the highest successful transaction rate under different circumstances."
3631,"Information technology (IT) service providers typically compete in a tender kind of process to win highly valued service contracts. The process starts with the client submitting a request for proposal (RFP) document. The service providers prepare s solution that would fulfill the requirements from the RFP. However, this solution is typically prepared manually, requiring intensive resource preparation that can take weeks or months. In this work-in-progress paper, we propose a two-step automated end-to-end solution methodology to prepare a competitive solution. The first step involves taking the client's requirements and mapping them to the optimal set of the provider's offerings and their attribute values that cover such client requirements at a minimum cost. In the second step, market benchmarks are applied to compute the pricing of chosen offerings. Sometimes these benchmarks are unknown for particular offerings provided in some geographies world-wide. Therefore, we propose an approach for inferring these unknown benchmarks along with a confidence score. We apply our overall methodology to real data of one of the world's largest IT service providers and show that it is both more efficient and more effective than manual solutioning, thus increasing win rates for the provider."
3632,"In spite of the enormous attention that automated service composition has attracted in the recent past, and the large number of approaches that has been developed, these approaches are rarely put into practice and deployed for real-world problems. In light of the increasing demand for an automated construction of services in various domains, this is a somewhat surprising observation. In fact, in domains such as automated machine learning and cloud gaming, both users and providers of software are highly interested in running services that are created in an on-the-fly manner. This paper presents PROSECO, a framework that aims at closing this gap by creating running services from simple queries specifiable by end-users relying on predefined prototypes. To show the applicability of PROSECO, we instantiated it for three highly heterogeneous domains: automated machine learning, cloud gaming, and information integration."
3633,"This paper develops a combined simulation and optimization model that allows to optimize different service pricing strategies defined on the social networks under uncertainty. For a specific reference problem we consider a telecom service provider whose customers are connected in such network. Besides the service price, the acceptance of this service by a given customer depends on the popularity of this service among the customer's neighbors in the network. One strategy that the service provider can pursue in this situation is to stimulate the demand by offering the price incentives to the most connected customers whose opinion can influence many other participants in the social network. We develop a simulation model of such social network and show how this model can be integrated with stochastic optimization in order to obtain the optimal pricing strategy. Our results show that the differentiated pricing strategies can increase substantially the revenue of a service provider operating on a social network."
3634,"In this paper, we present an autonomic service bus, named Cilia, dedicated to pervasive computing. We begin by presenting a use case related to pervasive health and by recalling the main requirements of that domain. In particular, we explain that autonomic features are absolutely necessary in a field characterized by important environmental dynamism and by the absence of skilled human administrators. We then present the main features of Cilia, including the Java embedded DSL and the runtime environment. Cilia autonomic abilities are then detailed. The framework provides a set of touch points to dynamically monitor and adapt mediation chains under execution. The framework also includes adapters implementing local control loops allowing runtime selection and substitution of devices. Cilia has been validated in a collaborative project, named Medical."
3635,"In this paper, we propose a novel model for finding experts on a given topic using social influence analysis in enterprise social network. In enterprise social networks, employees usually talk about some topics relevant to their tasks. With the integration of social technology in BPM (Business Process Management), more expertise characteristics are reflected by their actions in social networks. Social networks became an important place for sharing expertise. We explore the potential of enterprise social networks, such as Yammer and IBM Connections, as a source of expertise evidence. In this work, we utilized influence analysis approach to find experts in enterprise social network. Generally, not all experts have the habit of sharing their expertise in social networks. So expert finding approaches, such as simply using link analysis, are of limited use. Our approach can address this problem. The experimental results show that the proposed approach can find real experts, not just managers with higher influence. Empirical results have also been presented to demonstrate the effectiveness of the proposed models."
3636,"The performance of service system is especially low when demand is stochastic and nonstationary. This paper considers an inventory control problem with multiple-period replenishment lead time and nonstationary stochastic demand, and provides an optimal policy for this kind of problem. The problem is formulated as a stochastic dynamic programming model, then a solution algorithm which can obtain the optimal solution is developed. Numerical comparisons verify the effectiveness of the proposed policy."
3637,"These days IT service providers are rapidly embracing an automated services delivery model in order to keep pace with advances in technology and demanding market pressure to reduce and maintain quality. Application development and maintenance is a good example of a service system in which a sizable volume of tickets are raised everyday for different issues to get resolved with a view to deliver uninterrupted service. An issue is captured as summary on the ticket and once a ticket is resolved, the solution is also noted down on the ticket as resolution. It will be beneficial to automatically extract information from the description of tickets to improve operations like identifying critical and frequent issues, grouping of tickets based on textual content, suggesting remedial measures for them etc. In particular, the maintenance people can save a lot of effort and time if they have access to past remedial actions for similar kind of tickets raised earlier based on history data. In this work we propose an automated method based on background knowledge of tickets for recovering resolutions for fresh tickets using unsupervised learning and the traditional kNN (k-nearest neighbor) search. In absence of domain ontology we use ticket description to extract ontology which is grounded in WordNet. The experiment of our dataset shows that we are able to achieve a promising similarity match of about 48% between the suggestions and the actual resolution which shows an improvement over clustering without background knowledge."
3638,"The increasing popularity of modern virtualization-based datacenters continues to motivate both industry and academia to provide answers to a large variety of new and challenging questions. In this paper we aim to answer focusing on one such question: how to improve performance and availability of services hosted on IaaS clouds. Our system, structural constraint-aware virtual machine placement (SCAVP), supports three types of constraints: demand, communication and availability. We formulate SCAVP as an optimization problem and show its hardness. We design a hierarchical placement approach with four approximation algorithms that efficiently solves the SCAVP problem for large problem sizes. We provide a formal model for the application (to better understand structural constraints) and the datacenter (to effectively capture capabilities), and use the two models as inputs to the placement problem. We evaluate SCAVP in a simulated environment to illustrate the efficiency and importance of the proposed approach."
3639,"In service-based systems, service evolution might raise critical communication issues since the client cannot be aware of the changes that have occurred on the black-box services side. In this paper, we propose an automated process to adapt the client to the changes that have occurred. Our approach relies on a compatibility measuring method, and changes the client interface to ensure the system compatibility. This solution is fully automated inside a prototype tool we have implemented."
3640,"Resources are the basis of a grid, and their description is important for a grid user to discover and access resources. Grid resource describing is divided into two parts: resource description and resource specification. In this paper, an XML-based Grid Resource Specification Language is defined. A grid resource registry meta-service is then designed based on GRSL. A resource registry meta-service is implemented and tested. The test and tryout indicate that GRSL results in a powerful extensible framework for grid resource specification and discovery."
3641,"Service-oriented architectures (SOAs) are fundamentally changing the way in which we conceptualize and design business applications. A SOA-based application typically composes various distributed functions, including some possibly provided by external parties such as independent businesses. The key advantage of SOAs is the resulting dynamism, since the composed parts can be readily swapped out in favor of others of like functionality. SOA environments thus reflect the dynamism of human socioeconomic environments where businesses interact, collaborate, and expose services to each other in order to jointly create value. This paper presents a multiagent model for Web services and catalogs architectural styles that are key for SOA applications. It conceptually evaluates the styles by showing the kinds of service usages and the resulting dynamic interactions that they enable."
3642,"As the practitioner uses the Internet and the cloud infrastructure to facilitate online media and entertainment streaming services in large scale, it is urgent to have the quantitative analysis for the service design. The purpose of the quantitative analysis is to facilitate a high-standard service quality of such cloud-based streaming services. Naturally the service level is related to the following factors: the processing ability on the server side, the bandwidth allocated for the customer, the traffic condition of the Internet, and the processing ability of the end device on the client side. However, there is a lack of formal quantitative analysis, nor theoretical exploration, of the relationship between the service quality and these four factors. This study addresses this theoretical gap with a proposed service framework. Its queuing models for the cloud-based streaming services lead to closed form expressions for service quality of system and the corresponding computer resource required. A simulation procedure is also proposed to catch the dynamics of the presented cloud-based streaming services, estimating operation characteristics such as lag time and interruptions that a customer may experience under different environment settings."
3644,"Typically, BPMN Designer only needs to consider the business process without knowing the detail of invoked service, which helps them to simplify the design procedure. However, in some data centric workflow scenario, if designer didn't know about the data model of the invoked service, the BPMN workflow execution will be inefficient due to data conflict. There is lack of dynamically data modeling capability in BPMN, which means some data conflicts might happen in the designed workflow. To solve the problem, this paper introduced a hybrid model combining process and data, which is called process-data (PD) model. PD model defined several data conflict scenarios, which transformed the conflicting problem into parallel collection constructing problem. A novel collection generating method is introduced for the parallel collection creation. Based on the output of method, user can find a way to optimize the data conflict and increase the performance of the workflow."
3645,"Today, more and more software are augmented with service-oriented packaging. At the same time, more and more business and government services are provided and offered in the form of software. However, there are debates on whether and how much service engineering has in common with software engineering. Can we design service engineering models and frameworks in a similar spirit as the way software has been engineered in the past two decades? Or should software be designed, engineered and offered in a way similar to services that existed even before computer age?"
3646,"With the rapid development of Web APIs, selection of the suitable Web APIs from the service repositories for users to build Mashup applications becomes more and more difficult. Even if the existing methods show significant improvements in Web API recommendation, it is still challenging to recommend similar, diverse, and relevant Web APIs with high accuracy. In this paper, we propose a novel Web API recommendation method, which integrates tag, topic, co-occurrence, and popularity factors to recommend Web APIs for Mashup creation. This method, firstly exploits the enriched tags and topics information of Mashups and Web APIs derived by the relational topic model to calculate the similarity between Web APIs and the similarity between Mashups. Secondly, it uses the invocation times and category information of Web APIs to derive their popularity. Thirdly, multi-dimensional information, such as similar Mashups, similar Web APIs, co-occurrence and popularity of Web APIs, are modeled by factorization machines to predict and recommend top-k similar, diverse, relevant Web APIs for a target Mashup. Finally, we conduct a set of experiments, and experimental results show that our approach achieves a significant improvement in terms of precision, recall, F-measure, compared with other existing methods."
3647,"Many researches have been done for stock market prediction. While there are many free and premium data sources available today, and there many new machine learning algorithms have been proposed, most of these researches do not focus on utilizing these data sources and algorithms. This paper proposes a stock market prediction service framework that allows users to choose different data sources and machine learning techniques. In particular, while most existing prediction approaches are based on neural networks, support vector machines or Naive Bayes, we illustrate the flexibility of our framework by including metric learning based methods to predict stock movement."
3648,"Business processes, especially those in knowledge intensive environments, often emerge rather than following predefined steps. Supporting emergent processes is one of the key issues for collaborative knowledge sharing. This paper first introduces a component-based workspace metamodel used to support emergent processes. A loosely-coupled collaborative process management model, WorkPath, is proposed based on the workspace structure to support flexible process evolution management and coordination among processes. Key elements that construct workspace and WorkPath, such as role, action, artifact, workspace and reference relation, are described in detail. An implementation prototype and future work is also discussed at the end of the paper."
3649,"With the increased number of richness service resources, and maturity of (Service Oriented Architecture) SOA-based solutions, it is critical to develop an innovative software engineering methodology for user-centric and on-demand service oriented computing. This paper proposes the RGPS (Role-Goal-Process-Service) requirements meta-modeling framework for service requirements elicitation, analysis, and modeling. Meanwhile, the standardization on service interoperability is a key driver to realize on-demand services based on various service resources. We have developed the ISO standards series: MFI (meta-model framework for interoperability) and MFI (Ontology-RGPS). We combine these two meta-model frameworks, which provide a foundation to bridge the gap from SOA to SSOA (Semantically-aware SOA), and further to On-Demand SOA."
3650,"With the advent of cloud computing, a significant number of web services are available on the Internet. Services can be combined together when user's requirements are too complex to be solved by individual services. Since there are many services, searching a solution may require much storage. We propose to apply a compact data structure to represent the web service composition graph. To the best of our knowledge, our work is the first attempt to consider compact structure in solving the web service composition problem. Experimental results show that our method can find a valid solution to the composition problem, meanwhile, it takes less space and shows good scalability when handling a large number of web services."
3651,Ensuring that the IT/business functions of an organization realize its business objectives has long been recognized as a critically important question. This paper reports on a project that seeks to overturn established management orthodoxy by establishing that business objectives can be adequately modeled by leveraging a domain ontology and that methodological and tool support can be provided for the task of correlating the objectives of an organization and its service offerings. This paper presents an interim report from this project that describes how to leverage a domain ontology in i) building business objective/goal models in a top-down manner (required to be able to refine these to a level where there could be an ontological match between the languages used to describe objectives and services); ii) assessing the degree of ontological match between low-level objectives and business services as a step towards an automated framework for establishing strategic service alignment. We provide a brief illustration of our in-progress implementation within a toolkit called ServAlign.
3652,"Topological dynamics of mobile peer-to-peer are exacerbated by the changes of mobile user's location and interest. We observe that the velocity representation of mobile nodes provides a characterization of the mobility groups. We attempt to predict the future availability of wireless links based on group mobility model. Intuitively, mobile peers within the same group tend to have a high probability of keeping stable interconnection topology. We use the velocity-group model, which helps us predict the future availability of wireless links and leads to fast amplifying the system's total streaming capacity using its self-growing. The extensive simulation results show the efficiency of the proposed model in mobile peer-to-peer media streaming system."
3653,"Summary form only given. Web services and Enterprise Java Beans are evolving technology that allows the development and publication of software components. These components can be integrated together in plug and play manner. It provides a generic component based framework on the Web for large and complex distributed application development. This tutorial provides a detailed explanation of how you can build different types of EJBs to implement your distributed application. It will also provide details of publishing, accessing and innovation of Web service functions. Moreover, it will give you some dos and don'ts for developing Web services and EJBs. This tutorial is valuable for those involved in designing and deploying distributed B2B solutions such as Web developers, architects, consultants, engineers and analysts and programmers. The basic programming experience is assumed."
3654,"In this paper we introduce a Web service promotion model - Web services community. This service community supports three types of IT resource management networks - participant network, service network, and resource network. We then analyze a quality of service model for a set-vice request from multi-perspectives for the purpose of evaluating the service fulfillment. Finally an optimization approach based on the three types of networks is developed for optimizing the service fulfillment in this service community."
3655,"Business process (BP) can be supported by a large number of resources with evolving contents. In order to receive the support from these resources, the BP must satisfy the authorization policies of these resources. On the other hand, a BP also has its own authorization policies that users must satisfy in order to interact with the BP. Meanwhile, execution policies need to be applied to manage the sequence of tasks invocations in a BP. Therefore, without proper coordination among these policies, BP may not be able to perform correctly, e.g., imperative support from a specific resource could be missing or unauthorized user access can occur. An effective authorization management bringing all types of policies together becomes a must for a BP executing correctly without breaking any authorization and business rules. In this paper, we propose a process model, SOAC-Net that is incorporated with an authorization model, Process-Aware Service-Oriented Authorization Control (PASOAC). PASOAC is an extension of Role Based Access Control (RBAC), which takes both resource and user into account. A set of authorization constraints are designed in PASOAC to coordinate the user access and the resource support in a process environment."
3656,"Service Oriented Architecture (SOA) has evolved from business-to-business interactions, into the new API model that enables open partnerships and interoperability with just about anyone. In SOA, terms of service (ToS) and service level agreements (SLAs) were agreed upon on one-to-one basis, as the Web service interfaces were defined. In contrast, API ecosystems appeal for self-serve and instant gratification, being able to access, try and buy an API with a single click. As business teams and citizen developers access API ecosystems, they will want to quickly identify APIs that conform to their legal and usage terms requirements. For example, a large, established enterprise would not be as keen to give up their brand permission easily through use of a 3rd party API, whereas a citizen developer would probably not be concerned too much. Therefore we see a need to be able to automatically assess API terms of service to facilitate comparison and selection of multiple APIs from different providers. To enable API consumers to navigate this flood of APIs, we present a system that simplifies API terms of service creation and assessment. The core of the proposed system is a common API terms of service data model, which captures legal and entitlement capabilities. The system enables profile-based search, thereby allowing users to specify their terms of service requirements relevant to different roles that they may have in the ecosystem (e.g. citizen developer, enterprise procurement officer, etc.)."
3657,"The Web service is a main working pattern and a significant application model for next generation Internet application. The service-oriented architecture is a very promising architecture for practical implementation of the next generation geographical information systems. This paper investigated the service-oriented architecture for constructing a distributed and Web service enabled geographical information platform. The GIS platform architecture is designed as a multi-layer architecture that integrates the Web service, Servlet/JSP functions and GIS APIs based on the framework of J2EE infrastructure. The Web service framework was applied to the GIS system design and implementation. GIS Web services were designed to provide the hosted spatial data and GIS functionality to integrate the customized GIS applications to perform basic geo-processing tasks, such as address matching, map image display, and routing, without maintaining GIS tools or the associated geographical data. The system architecture, junctions, system integrations, and some key technical problems were investigated. It has an important application prospect in the GIS tools development and application."
3658,Presents the welcome message from the conference proceedings.
3659,"The implementation of business processes with services allows applying the design principle of separation of concerns by defining, on the one hand, the specification of the business process in a model, and on the other hand, software services to support specific automated parts of the model. Both technologies and platforms for services implementation and execution have evolved considerably over the years, but is not the same for services design. The Service Oriented Architecture Modeling Language (SoaML)is a standard that allows the specification of most service concepts -by extending UML elements-and code generation from these elements. In this paper we present an approach for generating service models in SoaML and the code to support the execution of specific parts of BPMN 2.0 business processes. We extended previous work by means of adding new mappings regarding Service tasks and basic workflow patterns in orchestrations for internal use of services."
3660,The service-oriented computing (SOC) paradigm promotes the use of basic composition units - services - to support the rapid development of distributed applications. Service composition is today a manual long-time activity. This paper presents DoCoSOC - a multi-domain development environment that automates service composition - that uses domain SOA models to automate service composition.
3661,"Copyright and Reprint Permissions: Abstracting is permitted with credit to the source. Libraries may photocopy beyond the limits of US copyright law, for private use of patrons, those articles in this volume that carry a code at the bottom of the first page, provided that the per-copy fee indicated in the code is paid through the Copyright Clearance Center. The papers in this book comprise the proceedings of the meeting mentioned on the cover and title page. They reflect the authors' opinions and, in the interests of timely dissemination, are published as presented and without change. Their inclusion in this publication does not necessarily constitute endorsement by the editors or the Institute of Electrical and Electronics Engineers, Inc."
3662,"In the past a few years, provision of an Internet-scale multicast with the use of application-level routing components has generated a lots of interests both within the research community and in the commercial world. Overlay multicast is a concept representing such multicast scheme. Overlay multicast offers accelerated deployment, simplified configuration, and better access control. In this paper we present an overview of the existing overlay multicast solutions and classify them according to different criteria. Also we compared the performance between topology-independent and topology-aware end-system multicast and performance difference of topology-aware end-system multicast and proxy-based multicast."
3663,"Nowadays, users are relying on mobile service ecosystem for their daily life, while there is a lack of support for users to efficiently migrate their tasks across different terminals. Furthermore, the same application running on different terminals may even have different layouts to guarantee the user experience, which makes the migration more difficult. To facilitate the switching among multiple terminals, in this paper, we present a service-based framework to support the service migration in multi-terminal environment. First, we propose the application state migration meta-model to formally describe the application migration between different terminals, including a workflow-based application model to model the application's process and a migration model to support the dynamic migration. Based on the meta-model, we then develop the application framework to support the entire migration process of applications, consisting of the application encapsulation tool and the migration engine to enable the multi-terminal switching for the existing applications. The experiments including two real cases show that our approach can effectively simplify user operation and significantly reduce the switching time. Our ATM can greatly improve the quality of experience (QoE) of users."
3664,"This paper introduces a grid-enabled and SOA-based SaaS application platform, which is named GridSaaS. The major contributions of the GridSaaS platform are: construction of a value-added supply chain of SaaS applications; adoption of grid technologies to deliver SaaS applications in high level maturity model; rapid development and delivery of new SaaS applications; well-designed solutions for the requirements of integration in UI layer, process layer and data layer."
3665,"Account services are playing important roles in building IT ecosystems, such as Google Accounts, Apple ID and Microsoft Your Account for the forthcoming windows 8. In this paper, we propose a novel mechanism to build a cloud oriented account service for SME SaaS Ecosystem (SSE), a public cloud ecosystem among small and medium sized enterprises of China. Different from the account services listed above, our account service has been established not only for individuals, but also for enterprises. Characteristics of the account service are as follows: 1) multi-tenant oriented implementation through virtual directory Information Tree (DIT) views based on a hybrid data structure; 2) partition based distributed architecture enables the service sufficiently calable and available; and 3) GUI based interfaces for configuration management, RESTful and SOAP based interfaces for the third party services invoking."
3666,"Data processing in environmental science is essential for doing science. The heterogeneity of data sources, data processing operations and infrastructures results in a lot of manual data and process integration work done by each scientist individually. This is very inefficient and time consuming. The aim is to provide a view based approach on accessing and processing data supporting a more generic infrastructure to integrate processing steps from different organizations, systems and libraries. We propose an approach modeled in Colored Place/Transition Nets which has been implemented in a Web Service infrastructure."
3667,"We propose architectural properties required for supporting automatic service composition. After defining composable service architecture, we proceed to examine the role of trust and reputation systems in such environment. Based on the proposed infrastructure we give several options for achieving automatic service composition, under the assumption that previously defined requirements are architecturally supported. Finally, we discuss the impact and outlook for automatic composition."
3668,Computational grids provide significant computing power by sharing resources across administrative domains. In secure environment the performance of girds arrives at maximization. For this purpose we propose a distributed secure structure which emphasis communication of hosts and active security issues in a grid. We also discuss the fault tolerance and recovery mechanism for enhancing system reliability.
3669,"Cloud Computing is radically changing the way of providing and managing IT services. Big enterprises are continuously investing on Cloud technologies to streamline IT processes and substantially reduce the time to market of new services. The current Cloud service model enables companies, with a low initial investment, to easily test new services and technologies, like IoT and Big Data, on a ""ready to go"" virtualized infrastructure. However, large organizations are still facing multiple challenges in migrating business-critical services and sensitive data to Public Cloud environments. To investigate the current adoption of Public Cloud services, we interviewed IT managers and cloud architects of over sixty multinational organizations. The survey assesses both business and technical issues and requirements of current and future Cloud strategies. Our analysis shows that Cloud Service Providers (CSPs) are not yet perceived as fully able to address critical points in security, regulatory constraints and performance management. Hence, to control their public cloud services and to overcome such limitations, multinational organizations must adopt structured SLM approaches."
3670,"We present a cost-effective service composition algorithm MC4MR aiming at mass customized QoS requirements. If a group of customers raise personalized QoS constraints on a given service, the algorithm looks for a finite number of composition solutions to meet these requirements and realize maximum benefit. To pursue higher cost-effectiveness, we combine multiple individualized requirements together and find a limited number of composition solutions that jointly satisfy them. The algorithm adopts the recursive and greedy strategies to reduce the computation complexity. Mass requirements are ordered according to the ""potential benefit (PB)"", requirements with higher PB are handled earlier, and subsequent ones are handled based on previously obtained solutions using three heuristic policies. The MC4MR algorithm is further simplified as an OC4MR algorithm (to look for one solution for multiple customized requirements) and an OC4OR algorithm (to look for one solution for one requirement). Experiments are conducted to compare performance and cost-effectiveness between MC4MR and traditional approaches, and some factors that impact the quality of solutions are explored."
3671,"In this paper, we describe a new methodology for automatically quantifying the relative work required for a programmer to port an application from one web API to another, i.e. ""porting effort"". Our approach defines a simple language (based on Python) with which API developers specify the semantics of API operations, a tool set that consumes and extracts semantic similarity of API operations from annotations expressed in this language, and a metric that facilitates ranking of porting effort for API operation pairs. We evaluate our approach using both randomly generated and real-world web APIs and show that our metric can correctly categorize the relative difficulty that developers associate with porting an application from one API to another."
3672,"The growth of web services has been increasingly noticed. Monitoring these services ensures the Quality of Service. Monitoring is the basis for violations verification, but it can also be used to predict a possible violation. The paper proposes a self-adaptive fuzzy architecture for web service monitoring, which attempts to predict a possible failure of the provider. Initially two non-functional features were monitored: response time and availability. The prediction is based on services historical data that are analyzed by a fuzzy system. With this prediction, there is a self-configuration on the system that changes service priority, making the provider processes high priority before low priority services. This prediction also helps the self-optimization of the system. It can be observed by the decrease of average response time and by the increase of average availability. It is not always possible to predict a failure, and after a failure, the system is capable of self-healing using recovering actions. Results show improvements of e-contracts accomplishments. With the fuzzy system, we observed an increase of 40.41% in e-contracts accomplishments, and a decrease of 42.64% in average response time (according to the environment), showing that the proposed architecture is promising."
3673,"A major goal of the On-The-Fly Computing project is the automated composition of individual services based on services that are available in dynamic markets. Dependent on the granularity of a market, different alternatives that satisfy the requested functional requirements may emerge. In order to select the best solution, services are usually selected with respect to their quality in terms of inherent non-functional properties. In this paper, we describe our idea of how to model this service selection process as a Markov Decision Process, which we in turn intend to solve by means of Reinforcement Learning techniques in order to control the underlying service composition process. In addition, some initial issues with respect to our approach are addressed."
3674,"Services are successful for applications with high levels of dynamicity and interoperability. Ironically the actual Service-Oriented Computing (SOC) technologies are such that interoperability is problematic if different platforms are used simultaneously, and dedicated tools for the support of most engineering tasks are essentially missing. We address the interoperability issue through the definition of a Service Abstract Machine (SAM) that observes a set of heterogeneous service platforms running on different machines and dynamically builds a reliable and homogeneous model of the global state. Conversely, changes in the state model, through SAM API are dynamically translated into the corresponding action(s) in the actual underlying SOC platforms. Evolution, extensibility and engineering support are addressed by dedicated software engineering environments that fully rely on the state model, as if SAM were a real SOC platform. The paper discusses the challenges we faced and the limits of our approach. We show how the system has been implemented, and we draw the early lessons of SAM second year of use."
3675,"Building service-based applications requires providing the ability to handle, maintain or upgrade the services that compose these applications. As same services may be used by a wide variability of applications, the management of the heterogeneity at runtime is required. This is crucial to reconfigure applications in case of service failures. The DAMASCo framework reduces the complexity of modeling services focusing on the discovery, composition and adaptation of context-aware services. But currently, it does not support the dynamic reconfiguration of service-based applications. In this work, we follow a Dynamic Software Product Line approach to extend DAMASCo for providing reconfiguration to support specific situations of fails at runtime. We propose a novel approach of grouping services in families facilitating the selection and usage of similar services in case of fails. We apply our approach to an intelligent transportation system case study where DAMASCo composes and reconfigure the necessary services to provide a dynamic route for a driver's request."
3676,"The emergence of Web services and service-oriented architecture (SOA) makes application development easy. However, since the computing environments on which applications are running are becoming complex, it is harder for users to set up security properly. Considering such complex security environments, this paper describes a tooling framework to generate Web services security configurations using model driven architecture (MDA). According to the MDA concept, users simply add security intentions to an application model, and then detailed security configurations are generated, employing transformations over UML constructs and a security environment model. In order to demonstrate that the framework is practically useful, we also illustrate how to generate configuration files for a commercial product."
3677,"Social networking sites (SNSs), such as Facebook, have become part of everyday use. While many individuals and organizations use SNSs to maintain contact and to do a variety of services, attackers may see them as a prime target for performing different types of attacks. Phishing is one of the most common attacks, and one of the most challenging problems in SNSs. Existing human behaviours literature related to social capital, habitual usage, and risk perception shows a strong indication that it is possible to predict Facebook users' susceptibility to phishing victimization based on their demographics, anonymity, social capital, and risk perception. Using quantitative survey, this paper aims to predict Facebook users' susceptibility to phishing victimization based on these factors. Among the hypothesized factors, we found that it is possible to predict user's susceptibility to phishing victimization based on the user's anonymity status, the number of all friends the user is connected to, the number of strangers that the user is connected to, and the number of close friends that the user is connected to."
3678,"The increasingly growing supply and demand of infrastructure as a service (IaaS) makes cloud trading possible and desirable in open cloud exchange marketplaces. The automation of cloud services trading in such marketplaces is an essential next step in the cloud market evolution, and it requires a proper market mechanism to effectively and efficiently trade IaaS. Due to the high problem complexity, associated with complex cloud services configurations, the research conducted so far has not resulted in an efficient and effective market allocation schemes for trading large public clouds. In this paper, we make the first step towards effective and efficient market mechanisms for cloud services exchange by addressing the problem of cloud services allocation in double-sided cloud markets. We consider realistic cloud exchange marketplace scenarios, and propose a family of combinatorial greedy mechanisms together with the two types of sorting criteria functions for homogeneous and heterogeneous resource allocation. We perform extensive experiments in order to analyse the impact of the mechanisms' parameters on the allocative performance and identify the sets of parameters that lead to the maximization of the mechanisms' efficiency based on the desired objectives. Our study demonstrates that the proposed greedy mechanisms are effective and efficient for the large-scale double-sided allocation problems. The experimental results also show that the sorting criteria for homogeneous goods is more efficient when it heavily relies on the contract's surplus, and the allocation mechanism based on resource scarcity factor is more efficient when the mechanism favours better-balanced contracts."
3679,"Various prior studies have leveraged cloud computing and big data techniques to promote adaptive micro open learning. However, this novel way of open education resource (OER) delivery and access suffers from the cold start problem of learner information. In this paper, we introduce a service oriented solution to assist OER providers and instructors to deal with the sparsity of data in OER recommendation using an ontological approach. Learners' features are predicted by spreading activation and demographic similarity based inference. An evolutionary algorithm is provided to realize the OER recommendation in terms of heuristic rules."
3680,"To implement abstract business processes, elementary services are selected for each abstract task. Because of uncertainties of Quality of Service (QoS) values during execution, services may become faulty and cause the violation of end-to-end awaited constraints. Additionally, due to the dynamic nature of service systems, several environment changes may occur at run-time. In fact, services can join or leave the system or change their offerings. To deal with possible changes and maintain the feasibility of the selected solution, enabling dynamic service selection during execution is essential. This is not a trivial task especially in the presence of several constraints and dependencies between services namely QoS and temporal constraints. Existing approaches do not consider the specificities of temporal properties and usually handle violations after they have occurred. In this paper, a novel proactive dynamic service selection approach is proposed to deal with changes during execution while considering both QoS and temporal constraints. Experiments show that, by using our approach, faults can be successfully handled in a reasonable time while guaranteeing overall constraints."
3681,"The advent of cloud, big data, and mobile creates fast-growing demand of storage. Cloud service providers and data centers are looking for cost-effective storage solution alternative to traditional high-cost embedded-system based storages to meet the need of newly emerging applications, such as messaging, video streaming, data analytics, etc. In particular, they are facing the challenge of lowering cost by accommodating multi-workload on a single instance of storage without compromising workload performance requirements. Software-defined storage (SDS) is a new generation of storage system. Unlike the traditional embedded-system based storages, the SDS uses a software-stack above commodity hardware to provide more valuable and cost-effective features. To meet the challenge the cloud service providers and the data centers are facing, the architecture of a new SDS platform called Federator is proposed in this paper. This paper argues that the architecture of a SDS platform should have three main characteristics: 1. The separation of the control and data path, 2. Self-configuration of storage resources, and 3. Restful APIs for new business extension. A new approach for self-configurable SDS is designed within Federator. This approach includes two types of neural network, which provides optimal storage resource configuration for any type of application. With the clear separation of the control and the data path, the intelligent self-configuration technologies, and the standard Restful API, Federator is expected to better meet the requirements of the new applications in ever-changing computing environments."
3682,"SOA brought significant progress in information systems development, but the full exploitation of its advantages is sometimes hindered by specific problems. One of them is concerned with paradigm of Model Driven Development (MDD), where it is necessary to control correctness of the whole design within many models on different levels of abstraction. This is the place, where we would like to contribute. We propose a new approach to application of SOA design patterns in the development process. Patterns are transformed into a machine acceptable form using category theory and attributed graphs. This form is language independent, leads up to automated detection of pattern or antipattern in the description/model of SOA based system and in the future can facilitate also correctness checking."
3683,"Service-Oriented Computing is known as a new computing paradigm that utilizes existing services as fundamental elements for developing distributed applications based on the so-called ""use, not own"" manner. In this paper, services are classified into three levels in accordance with different business requirements. Services are assembled by choosing lower-level services or other services at the identical level. Tenant applications are implemented by way of choosing services in composite business level. A hyper graph-based service model is used to represent hierarchical services and multi-tenancy applications. A dependency is a relation between services wherein a change to one of the services implies a potential change to the others. We propose algorithms based on directed hyper graph which can represent dependencies between services and applications to verify the correctness of dependencies during the application construction period and we implement a SCA-based platform for the large-scale multi-tenancy application construction."
3684,"Service-oriented computing (SOC) is viewed as the computing paradigm of the near future, allowing for the dynamic interaction of services provided by distributed business partners. Being a declarative knowledge representation model, ontologies serve as a foundation for SOC. Due to the heterogeneous nature of independently designed ontologies, it is problematic for partners to understand the concepts adopted in ontologies from other sources. In order for partners to achieve seamless collaboration of services, they need to reconcile their ontologies with each other. During the alignment process and the following service interactions, compatibility is an important measurement that has been neglected in most research work. We extend a vector system to encode ontology compatibility. In addition, we present a new model - probabilistic center ontology - for better recording and maintenance of ontology alignment results. Our precise and efficient approach is verified by both theoretic proofs and experimental results"
3685,"Mashup is presenting new kind of application in web 2.0 world. Mashup is not simply about the AJAX technologies, rather, it is typically related to reuse the data and other services from other web side and web applications. There are many ways to build up the mashup. This half-day tutorial will focus on using XML and JSON format of data and service and will introduce the following to the participants."
3686,"Service-oriented architectures provide more hopes for reusable components. However, there are still many challenges for services to become a new paradigm to support reusable components. This paper analyzes the difficulties of software reuse and reusable components, discusses the fundamental processes to develop software with services, points out the major challenges for reusable services and proposes initial solutions."
3687,"In this paper, we present a semantic web services composition framework based on a distributed architecture and on semantic and uniform based- ontology service descriptions. Central to our work is the use of the concept of community of services. Community constitutes a unified, homogeneous and consistent access interface to existing and heterogeneous e- services which propose similar functionalities for services belonging to this community. Moreover, based on OWLS language, our framework supports a rich expressivity in services and user constraints descriptions. It takes into account the nondeterministic behaviors of e-services instances, in particular when their internal state cannot be predicted in advance. In our framework, we use an operational semantic given par Statecharts formalism to make composition execution strategies model."
3688,"In present day globalised trade, the ability to support collaboration through B2B (Business-to-Business) services is crucial. One of the key challenges is to pervasively connect partners across the entire value chain, with the appropriate service offerings. Thus, it is vital to quickly identify potential partners to form new B2B collaborations or to support formed collaborations with swift decision making abilities. This paper proposes the use of Description Logic (DL) based reasoning to ensure completeness and decidability in reasoning for context-aware B2B services. A B2B Context Model is realised through a DL-based representation language called OWL (Web Ontology Language). The expressivity of OWL was extended through the SWRL (Semantic Web Rule Language), to represent business rules identified in the model. The performance scalability of an ontology-based knowledge base using OWL was then investigated. Results showed that such a knowledge base is not scalable."
3689,"Healthcare workflow has recently become an enabling technology in the healthcare industry to automate processes, improve care quality, and enhances patient safety. Although some work has been done on healthcare workflow for some departments of a hospital, automating processes and improving patient safety in the Sterile Processing Department (SPD) is still an open problem. The main contributions of this paper are: i) we identify the main factors and issues in the current practice of a sterile processing department that compromise patient safety; ii) we propose to address these issues using healthcare workflow and identify the key architectural requirements for a healthcare workflow system for SPDs; iii) based on these requirements, we propose a service-oriented architecture and validate it with our SPD workflow prototyping system, SPDFLOW. To our best knowledge, SPDFLOW is the first effort in exploring healthcare workflow technology in the SPD domain."
3690,"A grid-based zero-latency data stream warehouse (GZLDSWH), built upon a set of OGSI-based grid services and GT3 toolkit, overcomes the resource limitation issue for data stream processing without using traditional approximate approaches. However, due to its ""automated event-based reaction"" characteristic, the GZLDSWH requires a mechanism which allows the grid services to be able to work together to fulfill the common tasks. This paper describes the collaboration model for the grid services which enables the automation of the GZLDSWH in capturing and storing continuous data streams, making analytical processing, and reacting autonomously in near real time with some kinds of events based on well-established knowledge base."
3691,"Web services have become one of the most popular technologies of Web application. But the quality of Web services is not as stable as traditional software components' because of the uncertainty of network. Based on non-probability-set, the convex method is used to judge the range of performance affected by uncertain-but-bounded attributes. This method only requires the highest and lowest value of the uncertain attribute values and need not to know their probability distribution. This paper proposes the metric algorithm of the change of web service quality, and proposes the Web service decisionmaking algorithm based on the theory of multiple attributes decision by TOPSIS (technique for order preference by similarity to idea solution) in operations research."
3692,"The seamless integration of software applications supporting business activities and field devices belonging to the plant floor is one of the great challenges of the IT world. This requires to build Internet scale distributed systems in complex, heterogeneous environments characterized by stringent requirements regarding security and evolution. In this paper, we argue that software engineering tools are needed in order to facilitate the work of application developers. This paper presents a model-driven engineering approach for the development of a specific tool, dedicated to the development of industrial services for power distribution industry"
3693,"Sensing technologies provide system applications with the awareness of environmental conditions, customer behaviours, object movements, etc. Further, with such capability, system applications can be smart to intelligently adapt their responses to the changing conditions. With regard to business operations, these system applications ensure that business processes can run more intelligently and adaptively. These features will undoubtedly improve customer experience, enhance the reliability of service delivery and lower the operational cost for a more competitive and sustainable business. To enable context awareness to business process management, this paper proposes a conceptual method of depicting the context of a business process and the related mechanism of perceiving the contextual dynamics. A running example demonstrates the applicability of the proposed method and the improvements to process performance are evaluated using process simulations."
3694,"Web service composition is emerging as a promising technology for supporting large-scale, sophisticated business process integration in a variety of complex e-science or e-business domains. Particularly, semantics have been proposed as a key to automatically solving the discovery and composition problem. However, most of semantic composition approaches still remain at a stage of low efficiency because of the performance issues brought by the involved ontology reasoning and manual processing. To address this problem, in this paper, we present a QSQL-based service composition algorithm towards a fully-automated service composition. QSQL (Quick Service Query List) is an efficient service query index list which can achieve about the same semantic service discovery effects as other existing semantic composition methods, but with much less reasoning. With our proposed QSQL-based service composition algorithm, composition plans can be created to meet a user's query in an automatic, efficient and semantic manner. In particular, with our algorithm, most existing composition plans in QSQL can be founded and ranked by exploiting a weighted Petri net representation; which will facilitate the execution verification. The final experiment is conducted to further demonstrate the feasibility of our proposed composition approach and its efficiency."
3695,"GeneGrid is a collaborative industrial grid computing R&amp;D project initiated by the Belfast e-Science Centre (BeSC) under the UK e-Science programme and supported by the UK Department of Trade &amp; Industry (DTI). GeneGrid includes commercial partners who provide real business engagement. The project aims to provide a platform for scientists to access their collective resources, skills, experiences and results in a secure, reliable and scalable manner through the creation of a ""virtual bioinformatics laboratory"". GeneGrid provides seamless integration of a myriad of heterogeneous applications and datasets that span multiple administrative domains and locations across the globe, and presents these to the scientist through a simple user friendly interface. This paper presents each of the five main GeneGrid components, and discusses how these components are integrated to form an advanced service oriented architecture for automatic and dynamic in silico experiment execution. A commercial use case as provided by our industrial partners is also discussed."
3696,"Service oriented architecture is widely adopted, accepted and appreciated for both horizontal and vertical integration of enterprise applications. The success is hugely aided by web service composition, which is a temporal collaboration of independent and loosely-coupled web services to execute a business process at runtime. In the Future Internet, the present practice of composition, the most popular variant of which is service orchestration, is expected to face a lot of problems due to its inherent centralized orientation. As a result, service choreography is widely viewed as an ideal replacement candidate. However, achieving a decentralized collaboration of autonomous, 'non-aligned' and loosely-coupled web services is a challenge. To systematically enact a web service choreography, the present infrastructure has to address several complications. In this paper, we present several atypical issues faced by service choreography and the technological advancements required for its enactment. Based on the proposed solution, we develop a prototype with 'stateless' RESTful web services. The entire prototype is deployed in-house (within the Institute) on a virtualized platform."
3697,"In a message-driven SOA, message exchange patterns (MEPs) define a reusable notion of conversational contracts between a service consumer and a service provider from the provider's point of view. They enable a common understanding regarding a message flow between both parties. In contrast to version 1.1 of the Web Service Description Language (WSDL) the current version 2.0 has introduced a template for defining such patterns that allows to define and reference patterns beyond the standard input/output ones defined in the specification. Although reasonable, this approach brings several disadvantages which we point out in this paper. Since WSDL 2.0 MEPs and WS-BPEL processes describe interaction behaviour from the same perspective BPEL makes a perfect candidate as a language for formalising MEPs, especially because it provides a powerful mechanism for describing control flow and correlation of related messages. In this work we propose a way to formalising MEPs using a WSDL-less BPEL dialect called BPEL light. We introduce a new abstract BPEL profile for defining reusable and machine-readable MEPs that is capable of expressing arbitrary message exchanges. With this approach we pave the way for more flexible interaction styles and reduce the impedance mismatch between imperative programming and message orientation."
3698,"The files to be transferred should be specified by the URLs, it is not convenient for users to get the wanted file through fragmentary file related information and lack efficient ways to search where the file is. Here proposed an intelligent data transferring system (IDTS) based on semantic Web services, modeling data transferring functions as semantic Web services by OWL-S, and register the services by UDDI. We map service profile into UDDI records for performing semantic matching. The IDTS use semantic Web services to fulfill automatic discovery, execution, and composition and interoperation of data transfer services."
3699,"Web rule languages with the ability to cover various types of rules have emerged to make interactions between Web resources and broker agents possible. The chance of describing resources and users of a domain through the use of vocabularies is another feature of Web rule languages. Combination of these two properties makes Web rule languages an appropriate medium to make a hybrid model of representing both contexts and rules of a policy-aware system, such as a Web service. In this paper, we describe how REWERSE rule markup language (R2ML) can be employed to bridge between different policy languages using its rich set of rules, vocabulary, and built-in constructs. We show how the concepts of the KAoS and Rei policy languages can be transformed to R2ML and then from R2ML to the other policy languages. Following these mappings, we have implemented transformers, which enable us not only to share policies between KAoS and Rei, but also to transform policies onto other rule languages (e.g., F-Logic) for which transformations from/to R2ML are already developed."
3700,"Business transaction recovery is the fundamental approach of handling exceptions during execution as well as ensuring transaction features. In this paper context is applied to recovery of the web service business transaction and exceptions are classified into four categories: network exception, physical exception, service exception and user exception. Based on the classifications we define different start points of recovery process to simplify the recovery process of business transaction. Also evaluations on contexts of exceptions are performed to extract the feasible instances of compensation paths and the sensibility weights of path costs are utilized to select the optimal compensation instance. An illustration to recovery scenarios for travel agent is presented to verify our approach and algorithms."
3702,"Contemporary public administrations all over the world are exploring new service provision models that adhere to citizens needs. Recently, the ""life-event"" concept was introduced as a guiding metaphor for presenting and providing integrated public services. A life-event includes all public services that are related to a specific situation that citizens face. Although the concept of life-events is increasingly important for public administrations, the relevant scientific literature is limited. In this paper we present the main classes of an ontology for modeling life-events. It is anticipated that the use of this ontology will facilitate domain experts in public authorities to model life-events in a straightforward manner."
3703,"In manufacturing organizations is difficult to reach the requirements of the new business models (agile and dynamic adaptation to changes) due to technological and conceptual constraints between elements located at different levels of the organization, which prevents the integration of business and manufacturing processes. In this paper, a new industrial machinery model that achieves this integration has been proposed. This model, named IMaaS, shows the industrial machinery as a set of business processes, removing the conceptual constraints, and exposed as services, removing technology constraints."
3704,"In this paper, we introduce a service-oriented design method by integrating the concept of aspects, which is called service-oriented design with aspects (SODA) in order to utilize services and aspects as fundamental and abstract elements in the design phase of software lifecycle. The service model is represented in the structural and behavior views using UML with its extension mechanism and Petri net respectively. Aspects are used to capture service-specific concerns required for delivering high-quality and user-friendly services. By weaving services and aspects, we can generate various versions of a service system as well as their Petri net based service semantics that also facilitates the verification of service design results. We exemplify a service design result of SODA by applying it to a supply-chain management application."
3705,"This paper proposes a multiagent based distributed control system for an intelligent robot that is an integration of many functional modules. According to the whole requirements of the system, many agents and a distributed blackboard system are designed so that the system realizes a very flexible robot control from the low level such as servo control to the high level control such as motion planning. It especially analyzes functions of agents and presents results of preliminary experiments with real robot system."
3706,"Stock market is an important and active part of nowadays financial markets. Stock time series volatility analysis is regarded as one of the most challenging time series forecasting due to the hard-to-predict volatility observed in worldwide stock markets. In this paper we argue that the stock market state is dynamic and invisible but it will be influenced by some visible stock market information. Existing research on financial time series analysis and stock market volatility prediction can be classified into two categories: in depth study of one market factor on the stock market volatility prediction or prediction by combining historical price fluctuations with either trading volume or news. In this paper we present a service-oriented multi-kernel based learning framework (MKL) for stock volatility analysis. Our MKL service framework promotes a two-tier learning architecture. In the top tier, we develop a suite of data preparation and data transformation techniques to provide a source-specific modeling, which transforms and normalizes a source specific input dataset into the MKL ready data representation. Then we apply data alignment techniques to prepare the datasets from multiple information sources based on the classification model we choose for cross-source correlation analysis. In the next tier, we develop model integration methods to perform three analytic tasks: (i) building one sub-kernel per source, (ii) learning and tuning the weights for sub-kernels through weight adjustment methods and (iii) performing multi-kernel based cross-correlation analysis of market volatility. To validate the effectiveness of our service oriented MKL approach, we performed experiments on HKEx 2001 stock market datasets with three important market information sources: historical prices, trading volumes and stock related news articles. Our experiments show that 1) multi-kernel learning method has a higher degree of accuracy and a lower degree of false prediction, compared to exist..."
3707,"Composite services are often long-running, loosely coupled, and cross-platform, so they bring additional complexity to the system in the presence of failures. Transaction is a basic concept in building reliable distributed system. However, Web transaction cannot be easily rolled back as conventional ACID transactions, so a main problem that remains is how to cancel a long-running Web transaction. This paper introduces a calculus based on pi-calculus, which deals with the faults of Web service composition in long-running business processes. The calculus is equipped with operational semantics to guarantee both appropriate installation and activation of compensation of Web Transaction."
3708,"Electronic identity (eID) systems enable electronic services and applications to identify users reliably. In an eID system, unique data, i.e. an eID, is assigned to each user. The eID unambiguously identifies the user within the eID system. In most cases, the user's eID is extended by additional attributes such as name, address, or date of birth. The assigned eID and associated attributes are used by electronic services and applications to identify users unambiguously and to obtain required information about these users. In practice, required user attributes potentially need to be exchanged between different eID systems. Unfortunately, each eID system uses its own ontology to represent and organize eIDs and associated attributes. This diversity of ontology definitions prevents an easy exchange of eIDs and attributes between eID systems. To address this issue, we propose an ontology-alignment solution that provides interoperability between eID systems. We show the feasibility of the proposed solution through a Web service (WS) based implementation. This WS enables eID-based applications to retrieve eID attributes from different eID systems. Experiments conducted show that the proposed solution and the resulting WS works with arbitrary ontologies and hence provides interoperability between eID systems."
3709,"Incomplete automation and lack of collaborative work can be frequently found in the traditional governmental information system. When using the system, a citizen has to discover the involved governmental services to satisfy his requirement and contacts with each of them. Semantic Web services (SWS) in e-government can allow the automated Web service discovery, selection and execution making the automatic composition of governmental services a reality. A citizen can gain the response conveniently after submitting the request to the platform of SWS. In this paper, SWS requirements in e-government are presented, and a conceptual architecture for SWS in e-government is proposed, furthermore a prototype system is designed to implement SWS in e-government."
3710,"This paper presents a social network-based peer-to-peer search service for identifying right collaborators in the context of Web 2.0. We present a three-layer hierarchical social network, in which we identify two important relationship ties – knowledge relationship tie and social relationship tie. These relationship ties are metric used to measure the collaboration strength between pairs of participants on a social network. The stronger the knowledge relationship tie, the more knowledgeable the participants; the stronger the social relationship tie, the more likely the participants are willing to share their knowledge. By analyzing and calculating these relationship ties among peers using our computational model, we propose a systematic way to discover collaboration peers according to configurable and customizable requirements. Experiences of providing Web 2.0 services for identifying communities of practice through peer-to-peer search are also reported."
3711,"Web service composition is a distributed model to construct new web service on top of existing primitive or other composite web services. However, current service technologies, including proposed composition languages, do not address the reliability of web service composition. Thus it is hard to predict the system reliability. In this paper, we propose a method to compute system reliability based on Service Component Architecture(SCA), a standard that provides a language-independent way to define and compose service components in the system. We first present a formal service component signature model with respect to the specification of the SCA assembly model, and then propose a language-independent dynamic behavior model for specifying the interface behavior of the service component by port activities. Then the failure behaviors of ports are defined through the Enhanced Non-Homogeneous Poisson Process(ENHPP). Based on the semantics of ports, several rules have been generated to compute reliabilities of port expressions, thus the overall system reliability can be automatically computed. Finally, a testing bed is given to calculate port reliability."
3712,"Analyzing the functionality of Web services is the basis of using Web services effectively and efficiently. The first step in such analysis of Web services is to categorize different services, which may be offered by different service providers, based on their functionalities. In this paper, we present a clustering-based approach to Web service categorization in order to form a hierarchy of service taxonomy. Our novel clustering scheme takes into consideration not only individual factors such as input or output of service operations, but also the latent inter-relationships among the individual factors. Given a set of services that may or may not have been categorized, we adopt individual methods to handle the issue and mark out their classification labels in terms of a common (given) taxonomy, such as UNSPSC. When a new service description is published, the unclassified service is compared with the classified ones and measures of the likelihood that the new service description is belonging to each cluster are calculated. Based on this calculation, the service will be assigned to a suitable category."
3713,"Within the enterprise, domain-specific knowledge is often recorded by IT processes or employees in the form of noisy, unstructured data. The question then arises on how to gain actionable insight from the volumes of un-structured data in order to improve the bottom line in an effective and timely manner. In this paper we propose a method on how to approach the issue within the realm of software license management (SLM). After providing some background materials in the early sections, we will describe the processes, business logic, and data model-ing components of our Ontology based solution. The first technical section (""CMDB as Semantic model and Se-mantic Reconciliation Framework"") defines the CMDB information hierarchy needed to support various domain relationships that ultimately reconcile the semantic in-formation models. The following section describes how to identify and extract valid software product licenses and conditions of use from the noisy, unstructured pur-chase order data housed in legacy procurement sources. The subsequent section annotates and reconciles the unstructured data using semantic models with temporal contexts and a robust semantic reconciliation mediator. Following that we describe the application and use cas-es. Finally, we will present our observations and make recommendations gleaned from our experience and fu-ture development efforts."
3714,"Service-oriented Architectures (SOA) facilitate the provision and orchestration of business services to enable a faster adoption to changing business demands. Web Services provide a technical foundation to realize this paradigm and support a variety of different security mechanisms and approaches. Security requirements are codified in Web Service policies that control the service's behavior in terms of secure interactions with other participants in an SOA. To facilitate and simplify the generation of enforceable security policies, we foster a model-driven approach based on the modelling of security requirements in system design models. This paper introduces our security design language SecureSOA that enables the definition of these security requirements. We present the abstract syntax and notion of SecureSOA and describe a schema to integrate SecureSOA in any system design language for service-based systems. Moreover, we will demonstrate the integration of SecureSOA in Fundamental Modelling Concept (FMC) Block Diagrams."
3715,"Summary form only given. With increasing demands of effective and efficient transportation from our society, the transportation issue has become a noticeable obstacle to the economic development for all countries and regions to a great extend. ITS (intelligent transportation systems) technology is brought forward and considered as an effective approach that promises to alleviate many transportation problems such as traffic congestion, high accident rate, air pollution, and improve safety and reliability on existing roadway system. However, ITS architectures do not give proper answers to some important problems in ITS such as how to integrate heterogeneous data on the semantic level, how to manage dynamic business process, how to cooperate ITS subsystems among different domains, how to communicate among traffic control center and road sensor network, etc. Hence, a fundamental shift in approach is needed to effectively resolve these complex problems. Fortunately, grid technology, semantic Web technology, Web service technology, messaging oriented middleware (MOM) technology bring us great chance to build a integrated platform for ITS to solve the before-mentioned issues. Grid computing is an ideal technology to realize resource sharing in distributed heterogeneity environments. Semantic Web offers a great opportunity to support data semantization based on a domain-specific ontology database. Process coordination based on Web service gives a unified way to cooperate ITS subsystems among different organizations. Messaging oriented middleware enables distributed communication that is loosely coupled, reliable, and asynchronous in complex network environment. In this paper, we explore four technologies to build a novel integrated intelligent transportation information and service platform to realize traffic data semantization, traffic resource sharing, cooperation traffic process management, and traffic sensor network communication. As a result, a key project for ITISP, I..."
3716,"Grids are often used through simplified job submission interfaces that abstract the underlying grid infrastructure as a black box. These interfaces suit well to developers of simple grid applications requiring the coordination of grid jobs, for which monitoring and error handling are done more or less manually. However, developers of advanced grid applications such as grid portals need more flexible tools to develop efficient and robust applications with advanced capabilities such as monitoring, error handling, failure recovery, resource release, etc. In this paper, we present through a typical application example a XML-based language designed to describe applications orchestrating grid jobs. We also present the execution environment supporting the proposed language and we show how even simple grid applications can be expressed in this language and benefit of the advanced capabilities of the execution environment."
3717,"Cloud computing provides on-demand access to affordable hardware (e.g., multi-core CPUs, GPUs, disks, and networking equipment) and software (e.g., databases, application servers, data processing frameworks, etc.) platforms. Application services hosted on single/multiple cloud provider platforms have diverse characteristics that require extensive monitoring mechanisms to aid in controlling run-time quality of service (e.g., access latency and number of requests being served per second, etc.). To provide essential real-time information for effective and efficient cloud application quality of service (QoS) monitoring, in this paper we propose, develop and validate CLAMS-Cross-Layer Multi-Cloud Application Monitoring-as-a-Service Framework. The proposed framework is capable of: (a) performing QoS monitoring of application components (e.g., database, web server, application server, etc.) that may be deployed across multiple cloud platforms (e.g., Amazon and Azure), and (b) giving visibility into the QoS of individual application component, which is something not supported by current monitoring services and techniques. We conduct experiments on real-world multi-cloud platforms such as Amazon and Azure to empirically evaluate our framework and the results validate that CLAMS efficiently monitors applications running across multiple clouds."
3718,"Elasticity and economic considerations make Infrastructure-as-a-Service (IaaS) clouds attractive propositions for hosting enterprise IT applications. However, for prospective cloud customers, that potential is tempered by concerns, chief among them being security. We consider the problem of resource allocation in IaaS clouds while factoring in reachability and access control requirements of the cloud virtual machines (VMs). We describe a security-aware resource allocation framework that allows for effective enforcement of defense-in-depth for cloud VMs by determining (1) the grouping of VMs into security groups based on the similarity of their reachability requirements, and (2) the placement of virtual machines in a manner that reduces residual risks for individual VMs as well as security groups. We formalize security-aware resource allocation as a Constraint Satisfaction Problem (CSP), which can be solved using widely available Satisfiability Modulo Theories (SMT) solvers. Our experimental evaluation shows the effectiveness of our approach in reducing risk and improving manageability of security configurations for the cloud VMs."
3719,"The increasing ubiquity of wireless mobile devices is promoting unprecedented levels of electronic collaboration among devices interoperating to achieve a common goal. Issues related to host interoperability are addressed partially by the service-oriented computing paradigm. However, certain technical concerns relating to reliable interactions among hosts in ad hoc networks have not yet received much attention. We introduce follow-me sessions, where interactions occur between a client and a service, rather than a specific provider or server. We allow the client to switch service providers, if needed. We exploit strategies involving the use of contextual information, strong process migration, context-sensitive binding, and location-agnostic communication protocols. We show how follow-me sessions mitigate issues related to proxy-based service-oriented architectures in ad hoc networks."
3720,"With the development of service-oriented computing environments, QoS-aware service selection has been a more and more important research issue. In service composition environments, QoS attributes of atomic services are always aggregated for computing the QoS of the composite services, which has been reported in many previous studies. However, there are situations that some QoS attributes cannot be aggregated for composite services. For example, it is difficult to compute the translation quality of a composite translation service by simply aggregating its component atomic services (machine translation service, morphological analysis service, dictionary service). Moreover, when multiple QoS attributes are used for evaluating services, it is always difficult to maximize all the QoS attributes because there might be anti-correlated relations between them. To address above problems, this paper proposes an approach for selecting services based on context-aware factors of QoS attributes. In our proposed approach, context-aware factors that affect QoS attributes are first extracted from analyzing their correlation with QoS attributes. Then, QoS data is generated based on the extracted factors for QoS prediction and evaluation. Further, dynamic service selection is realized based on QoS prediction and evaluation considering user requirements. We use a case study in the domain of language service with some experiments to show the effectiveness of our approach."
3721,"The introduction of Network Functions Virtualization (NFV) enables service providers to offer software-defined network functions with elasticity and flexibility. Its core technique, dynamic allocation procedure of NFV components onto cloud resources requires rapid response to changes on-demand to remain cost and QoS effective. In this paper, Markov Decision Process (MDP) is applied to the NP-hard problem to dynamically allocate cloud resources for NFV components. In addition, Bayesian learning method is applied to monitor the historical resource usage in order to predict future resource reliability. Experimental results show that our proposed strategy outperforms related approaches."
3722,"In most commercial electronic contract management applications available today, different customized code base has to be developed, deployed and operated to support each tenant. Few advanced commercial electronic contract management applications use a single code base with configuration options to support multi-tenants. However, a separate instance of the code base still has to be deployed and operated for each tenant even in these applications. The business model of having to support a single application instance for each tenant makes an electronic contract management application and other critical business applications out of reach for most small and medium businesses (SMBs), in particular, the very small businesses (SVBs) because of its high development and maintenance cost. Recently, a new business model of a single application instance supporting multi-tenancy based on software as a service (SaaS) has emerged making expensive business applications more affordable for SMBs and SVBs for multi-tenancy [1]. In this paper, we present the first of a kind multi-tenancy SaaS electronic contract management application. We also describe several novel methods used in the metadata, security and shared services, as well as customization and tenant extensions modules to support multi-tenancy SaaS in this application. This multi-tenancy SaaS application has shown to benefit both the application service providers as well as their tenants. This new multi-tenancy SaaS model can reduce the application hosting cost and make the application more affordable to the tenants because of its capabilities in customization and scalability while continuing to support an increasing number of tenants. It furthers benefits tenants by saving their money and time with immediate access to the latest IT innovations and infrastructure improvements on a single application code base. Most end users of tenants have found their productivities increased, the contract transaction time accelerated, contra..."
3723,"Enterprises today find themselves increasingly part of networks of collaborations with their peers in order to fulfil customer requirements. Such collaborations, also known as virtual enterprises (VE), are governed by pre-defined contracts that restrict the behaviour of each participating enterprise. However, since each enterprise is autonomous, conflicts could arise during the collaboration. In this paper, we extend our earlier work on virtual enterprise modelling and present an approach for conflict modelling and management in VEs. Our approach first detects a conflict by analysing the exception that is reported during execution. It then uncovers the type of conflict from the exception and, depending on the nature of the exception, proposes the appropriate conflict management strategy. Throughout this paper, we illustrate our ideas on a running example and also present an evaluation based on a case study from the automobile production domain."
3724,"Web service composition (WSC) problems involve using domain knowledge about the underlying problem domains during the composition process. Existing research on Web service composition procedures has generally assumed that this domain knowledge is encoded as a single ontology that must be provided as an input to a composition procedure. In composition problems that span over knowledge across multiple ontologies that are connected via concept inheritence/extension, the users must examine the available ontologies and services in order to decide for the extent of the domain knowledge and the set of services relevant to their composition problems in hand. In this paper, we describe an automated way to generate solutions for service composition problems that span over multiple ontologies that have similar but not exactly the same structure. Our approach is based on a Hierarchical Task Network (HTN) planning model and extends the HTN-DL framework. This HTN planning model allows ontological problem decomposition in order to evaluate structural properties that span across multiple ontologies that are relevant to an input composition problem. We present a demonstration of the advantages of this approach."
3725,"To integrate heterogeneous distributed systems, there exist various researches and developments for the purpose of its adoption into enterprise environment. However, when service-oriented technologies are applied, it is difficult to adopt directly existing software system development methodologies, because of the primary elements of service-oriented architecture such as service provider, service requestor and service repository. In this paper, we define a conceptual model for developing systems in service-oriented computing. The model involves procedures and guidelines based on the use case model that is elicited from a business domain in requirement analysis. We focus on how to identify and how to determine key realization decisions for each service. The benefit of the identification method based on the use case model is that the service has reflected the business of the domain well and is in its proper granularity. The assessment guideline can help the settlement of a problem that is to determine the implementation strategy"
3726,"This paper proposes an ontology-based framework and tool to guide the requirements engineering process in RAD projects for financial solution application development. The framework is aimed at empowering IT, business and solutions analysts with a knowledge repository that helps to address terminology gaps between different project stakeholders and quickly manage the requirements engineering process in response to rapidly changing business demands. We also illustrate how the framework and tool are being used to support RAD projects in the development of financial trading systems for a top 5 global investment bank. The framework and tool help to bridge the knowledge and communication gap alleviating the business-IT alignment problem in financial solution application development."
3727,"Although significant advances have been seen in service oriented architecture (SOA) experiences. In practice the adoption of SOA at enterprise level is a long-term and complex transformation process. The research on how to extend SOA into an organizational context is still rare. In this paper, model-driven development methodology and platform is presented to handle the complexity of transforming an enterprise into a Service-Oriented Enterprise (SOE). The reference architecture and implementation guide for SOE is described in the methodology. The model-driven platform provides the integrated development and operational environment for SOE. Based on the platform, a total solution for SOE is given which can implement on-demand business."
3728,"Retrieving matchings between process models becomes a significant challenge in many applications. Recent attempts have been done to measure similarity of process models based on graph-edit distance. This problem is known to be difficult and computational complexity of exact algorithms for graph matching is exponential. Thus, heuristics must be proposed to obtain approximations. In this paper, we propose an approach to find relevant process models based on their decomposition into paths of possible execution sequences. Then, we propose a schema to compute the similarity between two process models using the proposed decomposition. Moreover, we give particular attention to the problem of ranking a collection of process models according to a particular query."
3729,"Nowadays, virtualization is present in almost all cloud infrastructures. In virtualized cloud, virtual machines (VMs) are the basis for allocating resources. A VM is launched with a fixed allocated computing capacity that should be strictly provided by the hosting system scheduler. Unfortunately, this allocated capacity is not always respected, due to mechanisms provided by the virtual machine monitoring system (also known as hypervisor). For instance, we observe that a significant amount of CPU is consumed by the underlying system components. This consumed CPU time is not only difficult to monitor, but also is not charged to VM capacities. Consequently, we have VMs using more computing capacities than the allocated values. Such a situation can lead to performance unpredictability for cloud clients, and resources waste for the cloud provider. In this paper, we present the design and evaluation of a mechanism which solves this issue. The proposed mechanism consists of estimating the CPU time consumed by the system component on behalf of individual VM. Subsequently, this estimated CPU time is charged to VM. We have implemented a prototype of the mechanism in Xen system. The prototype has been validated with extensive evaluations using reference benchmarks."
3730,"With the rapid growth of Web services, recommending suitable services to users has become a big challenge. The existing service recommendation works by Quality of Service (QoS) prediction fail to fully consider the influence of situation information, such as time, location, and user relations thoroughly. Two issues must be resolved to consider situation information: issue one, rating scarcity, is that there are less data to learn when importing more situations; issue two is that an effective approach is needed to adapt many situational factors. Our solution is a two-phase method: first, to overcome rating scarcity, data is augmented with estimations of unknown QoS values by learning from observable factors. The augmented data is then used to learn the important latent factors associated with the situational factors for QoS prediction. Experiments on data of real service invocations in different situations show improvement of our method in terms of QoS prediction accuracy over several existing methods, especially in the severe rating scarcity condition. In addition, analysis on parameter selection of proposed method can further assist in obtaining better QoS prediction in practical use."
3731,"Service oriented applications integrate heterogenous Web services, which are deployed and maintained by different providers, and can evolve dynamically and autonomously. Autonomous changes of web services manifest only at run time, and may lead to unexpected failures. This paper proposes an approach to designing self-adaptive applications, which can react to changes in the implementation of services, thus avoiding unexpected failures. The architecture proposed in this paper automatically detects possible integration mismatches, and dynamically executes suitable adaptation strategies. The paper presents an application, the personal mobility manager, which integrates heterogeneous Web services, identifies failures that derive from dynamic changes in the integrated services, and illustrates the self-adaptive design solution which can prevent run time failures"
3732,"The service metaphor naturally fosters integration and interoperability among distributed applications. In an open system environment, however, real integration among heterogeneous software components is a challenging issue. This paper describes an approach to interoperability between Jini services and Web services. The approach relies on the use of automatically-generated services named bridges which transparently smooth out the technological gap existing between Jini services and Web Services. Generation and management of bridges is concretely achieved by exploiting functionalities of a service architecture named GOAL. GOAL (General brOkering Architecture Layer) aims at simplifying the development and the management of highly flexible and scalable applications based on Jini services. It offers a design pattern which decouples the design of service functionalities from distribution concerns like security, remote communications, management of partial failures and so forth. The proposed approach is demonstrated through the realization of a system for on-line auctions over the Internet."
3733,"Many Web-based commercial services deliver their content using Web applications that generate pages dynamically based on user profiles, request parameters etc. The workload of these applications are often characterized by a large number of unique requests and a significant fraction of data updates. Hosting these applications drives the need for systems that replicates both the application code and its underlying data. We propose the design of such a system that is based on on-demand replication, where data units are replicated only to servers that access them often. This reduces the consistency overhead as updates are sent to a reduced number of servers. The proposed system allows complete replication transparency to the application, thereby allowing developers to build applications unaware of the underlying data replication. We show that the proposed techniques can reduce the client response time by a factor of 5 in comparison to existing techniques for a real-world e-commerce application used in the TPC-W benchmark. Furthermore, we evaluate our strategies for a wide range of workloads and show that on-demand replication performs better than centralized and fully replicated systems by reducing the average latency of read/write data accesses as well as the amount of bandwidth utilized to maintain data consistency."
3734,"Changes are continuously happening in enterprises and they impact the IT landscape. The most drastic among them are mergers and acquisitions, but recent trends like globalization and recession also lead companies to adopt new business consolidation strategies. Today, very limited support is provided to companies undergoing consolidation: the business to IT alignment is implicit, the task of identifying major IT changes compared to minor ones is pain-staking, and the ability to provide insights of the IT system as result of one strategy over another is impossible. In this paper, we are interested in precise methods that can characterize the impact of business consolidations on the companies' SOA based IT implementation, wherein the service interfaces are explicitly exposed. Our solution is to model the business to SOA dependencies and use the model to analyze IT consolidation choices when triggered by business consolidation decisions."
3735,"We first discuss the semantic heterogeneity problem of interaction in large scale MAS. Based on agent society theory, we introduce a grouping method to decompose MAS by different domain or concept, and propose an ontology mapping mechanism to resolve the heterogeneity problem. In the agent society environment, we discuss both inter-society and intra-society ontology mapping method, and finally introduce the architecture of prototype system for ontology mapping in agent society. We also use user participation to improve our mapping quality. Our research is based on agent society environment, mainly focusing on interoperability between societies and interaction in society. Based on our research and analysis, we intend to do some works for future research activities."
3736,"The Internet of Things (IoT) involves numerous connected smart things with different technologies and communication standards. While IoT opens new opportunities in various fields, it introduces new challenges in the field of digital forensics investigations. The existing tools and procedures of digital forensics cannot meet the highly distributed and heterogeneous infrastructure of the IoT. Forensics investigators will face challenges while identifying necessary pieces of evidence from the IoT environment, and collecting and analyzing those evidence. In this article, we propose the first working definition of IoT forensics and systematically analyze the IoT forensics domain to explore the challenges and issues in this special branch of digital forensics. We propose a Forensics-aware IoT (FAIoT) model for supporting reliable forensics investigations in the IoT environment."
3737,"Next generation networks (NGNs) are faced with a perennial problem of adding newer services at a very short time period, and with very high levels of quality, and this has really made them think of innovative solutions to quickly adapt to the situation. It is imperative that the NGN service providers consider and use service oriented architecture (SOA) and its principles to achieve business goals and optimize long-term capital investment without compromising customer experience. We have studied the problems faced by NGN service providers, and arrived at an innovative solution based on mathematical model to assist the next generation network (NGN) service providers to use quantitative metrics to reach higher levels of service orientation. In addition, this methodology can be used to measure and optimize every investment made towards SOA alignment. Also, this methodology can be used to perform ""what-if"" analysis, in order to assess the impact of the business decisions without actually implementing those decisions"
3738,"The growth of Web service formats over the Internet is increasing. These services should often be composed to achieve a user request. We propose in this paper a framework to cope with web service composition, from discovery to optimization. This framework, called SPOC, is composed of four successive phases: discovery, planning, quote execution and optimization. Only RFQ (Request for Quote) web services are treated in this first version of SPOC. The Discovery phase aims to search for web services in a repository. The Planning phase determines which services satisfy the user request. The Quote Execution phase involves processing a Request For Quote (RFQ) by calling web services. The last phase optimizes these results giving tradeoff compositions. As a proof-of-concept example, we applied SPOC to public competition processes in Public Markets."
3739,"As the quantity of Web services grow continuously, it becomes more challenging for developers to navigate through and make use of them. Thus, a knowledge map consisting of a summary of individual services and their relations has become increasingly useful. There are two challenges in building such a knowledge graph for Web service ecosystems. First, services keep evolving in terms of function and usage pattern, while their descriptions typically remain static and obsolete. Second, service profiles usually comprise some common background terms which do not differentiate services. To address the two challenges, we developed a novel tailored topic model, named Service Representation-LDA (SR-LDA), to mine effective representations beyond service profiles to build a knowledge map. The key idea is to incorporate mashup descriptions as an indication of service evolution, and introduce a global filter to identify and filter out background terms. Extensive experiments show that the tailored model is more effective than baselines. The methodology of building knowledge maps for the real-world ProgrammableWeb service ecosystem based on the learned representations is also presented, together with the analyses of representative functionality patterns."
3740,"Smart Home solutions are currently booming and apply to a variety of areas in order to improve inhabitants' quality of life. Many Smart Home solutions are based on service-oriented pervasive platforms providing context services. These services may correspond to devices (a lamp for instance) or to any computing unit providing useful functions (luminosity in a room for example). One of the big issues today is that applications running on top of such platforms share services, which can lead to conflicts if acting in incompatible ways. In this paper, we present an approach to manage conflicts in service-oriented pervasive platforms based on locking mechanisms installed at the context level. Our approach is optimistic and addresses conflicts at runtime before their undesired effects occur. This approach is developed and integrated in the iCASA pervasive platform."
3741,"This work presents a novel peer-to-peer based application scheduling algorithm for a cycle-sharing cluster computing system for drug discovery and design. Using the idle cycles donated by the clusters scattered over the Internet, data-intensive biologic applications in the field of drug discovery can be computed in this computing system. Clusters that donate idle cycles are organized in peer-to-peer overlay networks to eliminate problems caused by traditional client-server model. A novel application-based scheduling algorithm is designed to assign idle cycles to tasks efficiently. This algorithm also ensures the routing path length on the overlay network is close to the real network routing distance. Solid experimental results show the good performance of the proposal system."
3742,"The variability level of average-size business information systems (BIS) is highly enough for making the design of this kind of systems a complex task. There is an approach called process family engineering (PFE) that tries to ease the design of BIS using ideas from the software product lines (SPL) field. Roughly speaking, they propose to, first, study the variability of the system without entering into details by means of building a variability model (called feature model), that is used later for building the business process. However, in PFE the process of deriving the business process from the feature model is performed manually. Authors use feature models with a different meaning that is commonly accepted in SPL. In this paper, we provide a rigorous description for the new meaning of feature models, and a mapping relationship that defines how to use the information in the FM for obtaining the basic structure of the business process. In addition, as a proof of concepts, we have implemented an MDD transformation that provides the expected results."
3743,"A Web service specification can be quite complex, including multiple operations and various kinds of message exchange patterns. In this work, we focus on the problem of restructuring a Web service, which contains a process model inside, preserving at the same time its behavior that is shown to the service user. We provide a behavioral signature model for service restructuring. It is considered for Web services to be choreography equivalence if they conform to the same behavioral signature model. Based on behavioral signature models and graph searching technologies, we provide an algorithm for restructuring the process model from within the service while assuring service choreography equivalence. A case study runs throughout the paper to illustrate the feasibility of our work. We have implemented a prototype to demonstrate and evaluate our approach."
3744,"This paper describes Navdriver, a novel Software-as-a-Service technology capable of automatically inserting hyperlinks on subscribers' Web pages. Its functionality leverages hotlink assignment algorithms which have been a subject of study in the last decade. The focus of this paper is on describing the underpinnings of Navdriver from the point of view of its SaaS architecture and its interaction with subscribers' Web sites and their pages' visitors. The paper also presents lessons learned over the past five years since Navdriver was officially launched and deployed in live production websites. Navdriver is freely available and supports the most predominant Web browsers and Web technologies."
3745,"XML mediation for data validation and privacy anonymization of very large complicated XML messages defined in some industry-specific specifications such as HL7 is becoming increasingly important in SOA because a lot of applications depend on various kinds of hard-coded data validation and privacy anonymization functions, which makes it difficult to keep consistency of the functions among the applications in SOA when the schema of the XML messages in the specifications is updated. This paper proposes a uniform rule-based approach to realize the functions as an XML mediation separated from the applications, which makes it easy to maintain consistency of the functions even when they are updated in accord with the changes to the specifications. Therefore, application developers can readily utilize the mediation with many applications in SOA without additional modification to the applications. Our approach allows the developers to define a set of rules that consist of two components: 1) constraint conditions in a conceptual data notation in the XML message, and 2) actions performed only when the conditions are satisfied. In order to make the rules independent from both the implementation-specific data representation and the industry-specific knowledge, we automatically transform the rules into the implementation-specific data representation using two more factors; one is the data mappings from the data notation in the rules to the concrete data representation in the implementation, and the other is the implied data relationships hidden in the rules. It is very important to take into consideration a general way to import the implied knowledge because it often depends on the industry-specific data structure and it is usually given outside of the mediation system."
3746,"An interface adapter is used to allow a client application to transparently work with a service available in the local environment by converting the interface of an originally requested service into that of the available service. To generate adapters for users, interface matching is the first job to find out all the possible method matching pairs between a source interface and a target interface. Existing method matching schemes compare the schemas of source and target interfaces but they have low recall and precision because they only consider signatures and names of interfaces. For solving these problems, we propose a cluster-based schema matching scheme. By clustering methods in terms of parameter similarity, the proposed scheme can match methods more precisely than existing schema matching schemes. Then, we detect incorrect matches based on the analysis of incorrect method matching pairs."
3747,"In many applications involving composite Web services, one or more component services may become unavailable. This presents us with the problem of identifying other components that can take their place, while maintaining the overall functionality of the composite service. Given a choice of candidate substitutions that offer the desired functionality, it is often necessary to select the most preferred substitution based on non-functional attributes of the service, e.g., security, reliability, etc. We propose an approach to this problem using preference networks for representing and reasoning about preferences over non-functional properties. We present algorithms for solving several variants of this problem: a) when the choice of the preferred substitution is independent of the other constituents of the composite service; b) when the choice of the preferred substitution depends on the other constituents of the composite service; and c) when multiple constituents of a composite service need to be replaced simultaneously. The proposed solutions to the service substitution problem based on preferences over non-functional properties are independent of the specific formalism used to represent functional requirements of a composite service as well as the specific algorithm used to assemble the composite service."
3748,"With the development of Internet and Service/Cloud computing, more and more underlying web services are horizontally distributed on geographically distance servers connected with Internet. Inspired by MapReduce, we design a system architecture and Preprune-Refine framework in this paper to handle these distributed web services. Extensive experiments show that our Preprune-Refine framework performs efficiently and effectively and scales well."
3749,"Provenance, a record of the derivation history of scientific results, is critical for scientific workflows to support reproducibility, result interpretation, and problem diagnosis. Both prospective provenance, which captures an abstract workflow specification as a recipe for future data derivation, and retrospective provenance, which captures past workflow execution and data derivation information, provide important contextual information for the comprehensive analysis of scientific results. In this paper, we explore and design: i) a provenance model that models both prospective and retrospective provenance as an extension to the Open Provenance Model (OPM), which only models retrospective provenance; ii) a provenance collection framework to collect both prospective and retrospective provenance according to our model; iii) a relational provenance store to store, reason, and query prospective and retrospective provenance, which is captured via the proposed provenance collection framework. An experimental study is performed to show the performance of our provenance store using provenance queries for the Third Provenance Challenge. While most existing systems use an internal proprietary provenance model and develop an import/export facility to convert between the proprietary model and OPM, our provenance collection framework and provenance store feature the native support of OPM."
3750,"Latent Factor Model (LFM) is extensively used in dealing with user-item bipartite networks in service recommendation systems. To alleviate the limitations of LFM, this papers presents a novel unsupervised learning model, Latent Interest and Topic Mining model (LITM), to automatically mine the latent user interests and item topics from user-item bipartite networks. In particular, we introduce the motivation and objectives of this bipartite network based approach, and detail the model development and optimization process of the proposed LITM. This work not only provides an efficient method for latent user interest and item topic mining, but also highlights a new way to improve the accuracy of service recommendation. Experimental studies are performed and the results validate the LITM's efficiency in model training, and its ability to provide better service recommendation performance based on user-item bipartite networks are demonstrated."
3751,"Providing an easily accessible data service with high quality data is important for building big data applications. In this paper, we introduce a big data service for managing and accessing massive-scale biomedical image data. The service includes three major components: a NoSQL database for storing images and data analytics results, a client consisting of a group of query scripts for data access and management, and a data quality enhancement component for improving the performance of data analytics. Low-quality data can result in incorrect analytics results and may lead to no value even harmful conclusions. Therefore, it is important to provide an effective mechanism for ensuring data quality improvement in a big data service. We describe the implement ion of a deep learning classifier to automatically filter low quality data in datasets. To improve the effectiveness of data separation, the classifier is rigorously validated with synthetic data generated by a collection of scientific tools. Design of big data services with data quality improvement as an integral component, along with the best practices collected from this experimental study, will help researchers and practitioners to develop strategies for improving the quality of big data services, building big data applications, and designing machine learning classifiers."
3752,"Diverse knowledge and techniques are required in retail store network planning (RSNP), which involves indispensable and interdependent strategic decision processes such as performance evaluation, trade area analysis, market potential forecasting and retail network optimization. On the one hand, these processes are often tightly related and need to be integrated to solve the complex business optimization problems. On the other hand, the required information, e.g. external geographic and demographic information and internal store information, and decision units are usually geographically dispersed. Decentralization is required at both data administration level and decision process level. To offer a feasible loose-coupled and easy-to-implemented solution, this paper proposes a service-oriented framework for RSNP. Detailed specification of RSNP services, process model and corresponding messaging mechanism are also detailed. The RSNP implementation case for retail chain store operators is finally presented to illustrate how the proposed solution improves the flexibility and interoperability of existing RSNP practices."
3754,"Service orchestration logics can be difficult to be totally pre-fabricated when facing uncertain requirements. It is sometimes indispensable to get end users involved. However, there are some requirements: 1) Allow users to express their desired goals incrementally. 2) Allow users to specify key trajectory constraints. 3) Provide intelligent assistance to users in the composition. Intuitively we propose a user-steering explorative service composition approach that can support extended goals. It gives users more control and facilitates incremental development by suggesting candidate paths. During the generation of candidate paths, we use heuristics to find better plan paths, then guide users to gradually progress towards the goal. The proposed approach is verified in a typical bioinformatics scenario. The preliminary experiment results show that the approach can improve the efficiency and timelyness of service composition."
3755,"Dynamic service composition is suitable for on-demand business requests. For autonomic computing, service composition needs to deal with runtime environment faults, but also with business constraint violations which result from business requirements. We propose an approach for integrated handling of business constraint violations and runtime environment faults for dynamic service composition.We introduce a loosely coupled implementation architecture to maintain the platform independent nature."
3756,"As opposed to centralized workflow management systems, the distributed execution of workflows can not rely on a trusted centralized point of coordination. As a result, this flexible decentralized setting raises specific security requirements, such as the compliance of the overall sequence of operations with the pre-defined workflow execution plan, that are not yet met by existing decentralized workflow infrastructures. In this paper, we propose new security mechanisms capitalizing on onion encryption techniques and security policy models in order to assure the integrity of the distributed execution of workflows and to prevent workflow instance forging to name a few features. These mechanisms can easily be integrated into distributed workflow management systems as our design is strongly coupled with the runtime specification of decentralized workflows."
3757,"The turbulent modern business environments require firms to be more flexible and agile. This requires ability to quickly integrate and automate intra and inter-organizational business processes. An agile IT architecture is key to achieving success in this environment. Service oriented architecture and Web services provide means for achieving the business agility; however, significant impediments to wide scale adoption and use of these technologies remain. We will focus on significant challenges in the areas of service identification and design, cataloging and searching for appropriate services, domain specific standardization of service interfaces, matching service functionality with requirements, and service selection based on non functional characteristics. We describe an integrated framework based on standard modeling, design and code generation tools to support development of highly agile application systems. This framework is a result of multi year research effort aimed at developing methodologies and tools for defining business components and Web Services, develop schemes for describing components and services in business terms to allow efficient search of appropriate component/services and an environment for assembling application from reusable components and web services. Specific approaches used to address some of the challenges described above will be discussed."
3758,"Business Process as a Service (BPaaS) has emerged in recent years as a new paradigm for hosting and offering complex business operations using a cloud service delivery model. BPaaS providers can increase the diversity of their market by enabling their services to be configured to meet individual client needs. However, in order for the clients to trust external services for sensitive business processes, they must be assured that the transactional integrity of the service complies with their own business policies. To provide this trust, we propose a BPaaS configuration method that allows clients to ensure their transactional requirements will not be violated, while configuring the activities, resources, and data objects used in the service. Clients formalize transactional requirements using an expressive and easy-to-use template set. The service is then configured and verified in a three-step process that applies binary decision diagram analysis and model checking. An experimental study using a configurable Web store checkout BPaaS demonstrates the feasibility of our approach. Our state space reduction methods allow verification of models with hundreds of configurable features and requirements."
3759,"The prevalent approach to developing business solutions is driven by requirements expressed as process models. This is essentially a top-down model-driven development approach for building solutions from a given process model specification, from scratch. However, in order to enhance the reusability of such solutions, the emphasis has now shifted to defining business solutions via service-oriented architectures (SOAs) that compose software services to support business processes. This enables creating and reusing services and processes as software “assets” which is more scalable and profitable. In this context, the key to reusing assets is to support the right mechanisms to incrementally refine existing software services as well as business processes. Our earlier works, viz., Variation Oriented Engineering (VOE) and Variation Oriented Service Design (VOSD), proposed some mechanisms in this direction. But our experiences in developing and inducting VOE and VOSD into IBM's business units motivated the need for a rigorous formal foundation for variability in SOA-based solutions. To that end, this paper proposes a Variability Model for SOA-based solutions, through which variations in a service can be modeled and maintained. Our Variability Model also clarifies the extent to which an existing service can be modified to suit a business process requirement, while ensuring that the overall integrity of the service is maintained. We also present an algorithm for determining whether a variant of a service is a legal variant, and we demonstrate our Variability Model on a realistic running example in the insurance domain, via a prototype built on IBM's Rational Software Architect modeling tool."
3760,"The rapid proliferation of enormous sources of digital data and the development of cloud computing have led to greater dependence on data-intensive services. Each service may actually request or create a large amount of data sets. To compose these services will be more challenging. Issues of autonomy, scalability, adaptability, and robustness, become difficult to resolve. In order to automate the process of reaching an agreement in data-intensive service provision, the ant-inspired negotiation mechanism is considered in this paper. There are two-stage negotiation procedures in our model, which will provide effective and efficient service selection for service composers. We also present a multi-phase, multi-party negotiation protocol, where the ant colony system is applied for selecting the services. The experimental results show that our ant-inspired negotiation approach can facilitate the data-intensive service provision."
3761,"The workflow technology is the de facto standard for managing business processes. Today, workflows are even used for automating interactions and collaborations between business partners, e.g., for enabling just-in-time production. Every workflow that is part of such a collaboration needs to be highly available. Otherwise, the business operations, e.g., the production, might be hindered or even stopped. Since today's business partners are scattered across the globe, the workflows are executed in a highly distributed and heterogeneous environment. Those environments are, however, failure-prone and, thus, providing availability is not trivial. In this work, we improve availability by replicating workflow executions, while ensuring that the outcome is the same as in a non-replicated execution. For making workflow replication easily usable with current workflow technology, we derive the requirements for modeling a workflow replication system. Then, we propose the HAWKS system, which adheres to the previously specified requirements and is compatible with current technology. We implement a proof-of-concept in the open-source workflow execution engine Apache ODE for demonstrating this compatibility. Finally, we extensively evaluate the impact of using HAWKS in terms of performance and availability in the presence of failures."
3762,"Workflows tend to fail in real-world scenarios due to the uncertain/unreliable sensory information which sometimes needs to be updated during the execution of workflows. In a logic based framework, these dynamic predicates that can be updated are called non-monotonic predicates (NMPs). In this paper, we focus on reducing the risk of a given workflow due to the NMPs in that workflow. The main idea is to synthesize a backup workflow by augmenting the main workflow without introducing new NMPs. The backup workflow is generated by using expected values of NMPs if necessary instead of given values. The expected values are calculated from the execution history or provided by a domain expert. It is argued that total risk reduces to the square root of the main workflow itself."
3763,"In cloud service provisioning scenarios with a changing demand from consumers, it is appealing for cloud providers to leverage only a limited amount of the virtualized resources required to provide the service. However, it is not easy to determine how much resources are required to satisfy consumers expectations in terms of Quality of Service (QoS). Some existing frameworks provide mechanisms to adapt the required cloud resources in the service delivery, also called an elastic service, but only for consumers with the same QoS expectations. The problem arises when the service provider must deal with several consumers, each demanding a different QoS for the service. In such an scenario, cloud resources provisioning must deal with trade-offs between different QoS, while fulfilling these QoS, within the same service deployment. In this paper we propose an elasticity-aware governance platform for cloud service delivery that reacts to the dynamic service load introduced by consumers demand. Such a reaction consists of provisioning the required amount of cloud resources to satisfy the different QoS that is offered to the consumers by means of several service level agreements. The proposed platform aims to keep under control the QoS experienced by multiple service consumers while maintaining a controlled cost."
3764,"With the advent of ubiquitous sensor-rich environments and location-based services, distributed event-based systems based on the publish/subscribe communication paradigm are becoming very important and crucial. In many large-scale distributed mission critical areas, publish/subscribe systems must support a large number of geographically distributed publishers and subscribers and ensure real-time event data to be delivered timely. The concern is that a large number of low priority events may clog the channel thereby causing high priority events to get delayed. The challenge raised for the event-based middleware in large-scale distributed system is that event priority determination engine must be efficient and scalable in term of priority rule size and event throughputs. Checking every rule for priority on-the-fly is not friendly to the cache. This paper proposed an innovative approach based on Bloom filter technique. This approach is cache friendly and the online computation time of event priority determination is independent of the rule size. This approach is based on two ideas: make the online queries as simple as possible and exploit the power of cache on the broker. The rule instantiation engine (RIE) is made offline while online part is the priority determination engine (PDE). A Bloom filter data structure is used by RIE to store the rule instances and their priorities. The complex rule evaluation is reduced to set membership testing as queries on Bloom filters. The PDE makes the querying simplified by event discretization. Results are then cached in the broker caches. Finally, we have an analysis on the system complexity and some open issues. The further evaluation with prototype experiments are under development and the results may be found in our later publication."
3765,"Organizations sharing common goals are required to integrate their software systems in order set up collaborative business processes (BPs). An integration platform with a Business Process Management System (BPMS) can offer the needed technological support to realize these processes. Also, including widely used social platforms (e.g. Facebook, Twitter) provides support for realizing human tasks within the BPMS. In this paper, we propose a process-aware inter-organizational service integration platform to allow the interaction between collaborating organizations. The platform extends a defined integration platform by adding support for the execution of collaborative BPs and can be used within different domains."
3766,"Decentralized collaborative architectures are gaining popularity in all application areas, varying from peer-to-peer communication and content management to cloud and ubiquitous services. However, the public identity of the user is still a major concern, in terms of privacy, trace ability, verifiability, masquerading, and other attacks in such environments. We demonstrate two new attacks, identity shadowing and the Man-in-the-Loop (MITL) attacks, which are applicable in particular to multiparty collaborative environments. In this paper, we propose MIDEP, a Multiparty Identity Establishment Protocol for collaborative environments. The proposed protocol allows a client to establish a secure, multiparty, probabilistic, temporal, verifiable, and non-traceable public identity with the collaborating peers in a decentralized architecture. MIDEP allows a client to avoid identity shadowing and protects the service from the resulting threats as well as from colluded information sharing among the collaborating peers. We illustrate how existing collaborative service frameworks can utilize MIDEP to securely establish the public identity prior to beginning the service session. A prototype implementation is utilized to perform extensive experimental analysis. Our results show that MIDEP is highly suitable in terms of overhead to ensure secure identity establishment for underlying decentralized collaborative services."
3767,"As the number of Web services grows and the diversity increases, it is imperative to bring semantics to Web services to provide richer descriptions. However, many solutions developed in the field of semantic annotation suffer from lack of relevant ontologies that are comprehensive enough to contain as many concepts as possible. Based on public open ontology, this paper presents an approach for augmenting semantics of Web services. For Web services in WSDL format, the semantics is enhanced by associating inputs and outputs with concepts from public open ontology. Then two methodologies based on match degree and service discovery respectively are described to evaluate the approach. With the support of OpenCyc, one experiment is carried out on OWLS-TC4 to test the effectiveness of the approach, the other experiment is conducted on 11836 WSDL files crawled from the Internet to expand the approach to a large number of Web services. The experimental results show that the approach performs well even when the number of Web services is large, moreover, it performs better when dealing with well-described Web services."
3768,"Community data sharing is an emerging paradigm of data sharing. Web Services can be used in distributed community data sharing system to share data between communities resized in different nodes. We implement a community data sharing system based on Web services, Honeycomb. In Honeycomb, Web services invocation is the main performance bottleneck of query execution and cache technique is utilized to improve query performance. This paper describes the query evaluation, cache design and implementation in Honeycomb."
3769,"In this paper, a QoS-aware traffic classification framework for software defined networks is proposed. Instead of identifying specific applications in most of the previous work of traffic classification, our approach classifies the network traffic into different classes according to the QoS requirements, which provide the crucial information to enable the fine-grained and QoS-aware traffic engineering. The proposed framework is fully located in the network controller so that the real-time, adaptive, and accurate traffic classification can be realized by exploiting the superior computation capacity, the global visibility, and the inherent programmability of the network controller. More specifically, the proposed framework jointly exploits deep packet inspection (DPI) and semi-supervised machine learning so that accurate traffic classification can be realized, while requiring minimal communications between the network controller and the SDN switches. Based on the real Internet data set, the simulation results show the proposed classification framework can provide good performance in terms of classification accuracy and communication costs."
3770,"This paper is faced with group sensing problem, where HD map producers motivate private cars to collect data from real world. Group sensing needs vehicles to communicate physically, and drivers to collaborate strategically. First we consider communication module, three VANET-based methods are proposed to achieve inter-vehicle message relaying. Secondly, we consider collaboration module which motivates drivers to be participants, three motivating methods are derived by integrating relaying methods with collaborating strategies. Some combinations of two modules are discussed and classified from centralized or distributed perspective. Finally, we simulate and analyze two modules. The results shows that centralized method could motivate collaboration at a low price, but brings about heavy communication overhead. In contrast, distributed method requires more incentives and less communication overhead than centralized method. Map producers need to make a balance between communication module and collaboration module if they want to improve effectiveness of group sensing."
3771,"Organizations providing large scale software and hardware maintenance support services typically capture detailed metrics of each service request (SR) for a customer. Examples of such metrics include the time taken to resolve the problem, success of the resolution, escalations across levels of support, field engineer site visit statistics, and parts replacement data -- the latter two for hardware maintenance only. For some SRs, targeted customer surveys may be conducted to elicit feedback about how effectively the end-to-end problem resolution process was performed. Application of analytics to such data to enable continuous improvement of the operational efficiencies of providing maintenance services is an open area of research. This paper describes the authors' experience with several analytics projects in this domain. Improvement of maintenance support services can lead to faster and better problem resolution, leading to reduced down time and an increase in the overall resiliency of a computing environment."
3772,"This paper explores whether (n-2)-resilient services and reliable registers can wait-free solve (n, n-1)-consensus. It has already been shown to be feasible if arbitrary connection is allowed, while we prove that it's impossible otherwise."
3773,"Distributed autonomous multi-agent reasoning and classification systems have been thought of to be the basis of intelligence and have wide applications in the space of operational intelligence in closing the loop between sensing, analytics, and actions. This paper targets multi-agent systems that employ rulebased logics (i.e., rules that determine the output/response of an agent depending on the range of the input values) with pre-defined rules to accurately perceive the environment, and provide associated reactions. Such rule-based systems do not perform well in scenarios, where human generated rules cannot adapt to dynamic variations in the data distribution arising due to dynamic changes in the environment, especially if data dimensionality is very high. Examples of such scenarios exist wherever the sensed data arrives from the physical world - such as weather data, physical sensor data, human behaviour controlled data, etc. Clearly, to meet the adaptivity requirements of such scenarios we require the agents to possess adaptive reasoning capability such that they can adapt the underlying rules with respect to the changing environment. Developing such adaptive agents requires the developer to additionally possess considerable expertise of state-of-the-art machine learning techniques, apart from possessing knowledge of the agent's target domain. To address the above issues, we automate the process of development and deployment of adaptive agents. We present the fundamental design concepts behind the development of SynAdapt: a new adaptive meta-learning based multi-agent synthesis framework, that automates the synthesis of adaptive multi-agent systems from high-level user specifications. SynAdapt provides the following key features: a) Automated synthesis and deployment of adaptive agents from high-level user specification, b) Agents synthesised by SynAdapt can select a learning strategy that is particularly suited for given user specifications and input dataset, an..."
3774,"Web services technologies are being developed as the foundation of a new generation of B2B applications. One of the direct impacts is its replacement of the traditional FTP-based EDI systems in the retail industry. This paper illustrates the design of systems using these two approaches and discusses the differences and important issues related to order information storage, data transmission, system extensibility, and security."
3775,"The problem of service composition with end-to-end QoS constraints has been proven to be an NP-hard problem and various evolutionary algorithms have been successfully applied to look for approximately optimal solutions within limited computation time. Favorable heuristic rules are considered as the key of such algorithms, and historical service usage data are widely utilized to help identify the distinct features of problem domains, used as heuristic that would greatly improve the optimality. However, our experiments show that the historical usage data is not always valid on the performance improvement, and there exist underlying dependencies between domain features and optimality of service composition algorithms, and different domain feature values require the composition algorithm to have different parameter settings to ensure the higher optimality. In this paper, we consider two domain features called Priori and Similarity along with some metrics measuring their richness and confidence level. Taking the service domain-oriented artificial bee colony algorithm (S-ABCSC) as an example, we try to discover the underlying dependencies between the domain features, the algorithm parameter settings, and the optimality of the algorithm to help algorithm designers judge whether the given historical usage data delineates valuable domain features that contribute to the optimality improvement, and setting up the best values of S-ABCSC parameters. Several experiments are conducted on different historical service usage data sets, and the results have been partially shown the effectiveness of our approach."
3776,"Summary form only given. This tutorial seeks to discuss the key concepts in collaborative business transaction management. Its intent is to explain the concept in business transaction and how it is different from traditional database transaction and workflow transaction management, to evaluate existing approaches, and to present existing techniques from other areas that can be adopted for business transactions and their limitations, and lastly to discuss a framework that addresses the challenges that are unique to business transaction management in the service oriented environment."
3777,"Users of cloud platforms often must expend significant manual efforts in the deployment and orchestration of their services on cloud platforms due primarily to having to deal with the high variabilities in the configuration options for virtualized environment setup and meeting the software dependencies for each service. Despite the emergence of many DevOps cloud automation and orchestration tools, users must still rely on specifying low-level scripting details for service deployment and management. Using these tools required domain expertise along with a steep learning curve. To address these challenges in a tool-and-technology agnostic manner, which helps promote interoperability and portability of services hosted across cloud platforms, we present initial ideas on a GUI based cloud automation and orchestration framework called CloudCAMP. CloudCAMP uses model-driven engineering techniques to provide users with intuitive and higher-level modeling abstractions that preclude the need to specify all the low-level details. CloudCAMP's generative capabilities leverage a built-in knowledge-base to automate the synthesis of Infrastructure-as-Code (IAC) solution that subsequently can be used to deploy and orchestrate services in the cloud. Preliminary results from a small user study are presented in the paper."
3778,"The wireless LANs (WLANs) have become more prevalent and are widely deployed in many places such as corporate office conference rooms, industrial warehouses, campus, residences, cafe etc. The IEEE 802.11-based WLAN presents new challenges for network administrators and information security administrators alike. The security of a WLAN is very important, especially for applications hosting valuable information. Unlike the relative simplicity of wired Ethernet deployments, 802.11-based WLANs broadcast radio-frequency (RF) data for the client stations to receive. This presents new and complex security issues that involve augmenting the 802.11 standard. This paper critically reviews main security flaws of WEP, including short IV, key reuse, poor key management, and inappropriate RC4 and CRC-32 algorithms. It also describes a useful VPN security measure to enhance the level of security for the WLAN employing WEP."
3779,"In the last few years different solutions have been proposed for the composition of Web APIs. In this paper we focus on the scalability problems appearing when the software platform in charge of executing Service Compositions (which are defined as Directed Acyclic Graphs, DAGs) is supposed to support huge numbers of concurrent executions. This is the case of viral applications as well as of Cloud Computing scenarios where Service Compositions are deployed and executed in multi-tentant platforms implementing different paradigms such as Business Process as a Service, Mashup as a Service, Service Composition as a Service. The proposed solution exploits the Asynchronous I/O paradigm for the efficient utilization of system resources such as threads and memory."
3780,"With the increasing number of web services, there appear more and more services with similar functionality but different Quality of Services (QoS) or trust properties. With the emergence user behavior, the requests provided by users may not only contain non-functional properties (NFPs) as their preferences. Moreover, they may also pay attention to trust. To the best of our knowledge, there appears non previous work together consider the above two factors during service selection. Therefore, integrating trust with preference arises as a new challenge for Web service selection. In our previous work [1], [2], we perform web service selection based on user preferences, including qualitative preferences and quantitative preferences. In this paper, we further extend our work and propose a novel multi-objective optimization approach that integrate trust with user preferences to rank the relevancy of web service. Experimental results conducted on open data-sets demonstrate our high-quality service selection results."
3781,"Software Architecture (SA) plays a critical role in developing and evolving cloud-based applications. We present a Reference Architecture (RA) for designing Cloud-based Tools as a service work SPACE (TSPACE) - a platform for provisioning chain of tools following the Software as a Service (SaaS) model. The TSPACE RA has been designed by leveraging well-known design principles and patterns and has been documented using a view-based approach. The RA has been presented in terms of its context, goals and design elements by describing the requirements, design tactics, and components of the RA. We evaluate the RA in terms of completeness and feasibility. Our proposed RA can provide valuable guidance and insights for designing and implementing concrete software architectures of TSPACE."
3782,"One of the challenges for the P2P-based service composition process is how to effectively discover and select the most appropriate peers to execute the service applications when considering multiple properties of the requested services. Different ontology-based e-service profiles have been proposed to facilitate handling multiple properties and to enhance the service oriented process in order to achieve the total or partial automation of service discovery, selection and composition. This paper investigates how the ACO (Ant Colony Optimisation) algorithm and the GA (Genetic Algorithm) may facilitate P2P-based (Peer-to-Peer) service selection with multiple service properties. The performance of both algorithms is evaluated and compared statistically using a pooled t-test for 30 randomly generated composition scenarios. Our experimental results show that both algorithms can improve the quality of service composition, while showing that the ACO approach is the more effective."
3783,"In this paper we propose an extension of the service oriented architecture that supports discovery of Web services across organization boundaries. We provide a detailed discussion of both the architectural and implementation considerations, and we provide an empirical evaluation that shows that indeed our prototype implementation scales with both the number of Web services and the number of organizations involved."
3784,"Cloud infrastructure services offer elastic computing resources that particularly match the requirements of web transactional applications. Such applications, e.g., e-business applications, have high business value and variable workload patterns and therefore utilizing such elastic cloud resources in a cost-effective way is crucial need for cloud consumers. Defining economic elasticity rules, however, is not trivial as it requires (i) running considerable number of experiments with different parameters/metrics and workloads (ii) collecting appropriate cloud and application cost/performance measurements and (iii) performing cost/performance trade-off analysis. In this paper we introduce our on-going work on elasticity economics platform that supports cloud consumers to achieve the above three requirements. We also discuss the key elements of the elasticity economics platform and our early prototyping experience."
3785,"This paper proposes an Extensible Monitoring and Discovery Service for grid environment MASSIVE (MEMDS). The MASSIVE project focuses on developing a multidisciplinary applications-oriented simulation and visualization environment. In the MEMDS, appropriate mechanisms make custom information services established and deployed easily, and the developed APIs simplify the access to the MDS and provide a uniform interface for programmers. Corresponding practices show that the MEMDS improves the extensibility and flexibility of the grid information service."
3786,"Service Oriented Computing Systems can be considered as a type of complex systems consisting of a number of loosely coupled autonomous and adaptive components (i.e., service components). Service quality depends on the performance of the ""service group"", and many dynamic factors including the expectation of service consumers, the availability of resources, etc. Trust is an important factor for determining the interrelationships among service components. In this paper, we consider the dynamic factors in service composition, and propose a trust management approach, which adopts related methods in information theory, to enable more reliable service composition in dynamic environments. From the experimental results, we claim that the proposed approach can effectively handle dynamic factors in open environments, and obtain better service composition results."
3787,"Grid workflow description language is the basis of the establishment and execution of grid workflow. We propose a grid workflow description language called GPEL based on BPEL4WS. GPEL can be easily used in grid environment and is more concise and effective than BPEL4WS. In this paper, we analyze the structural characteristics and the expressive power of GPEL."
3788,"Traveling companions are object groups that move together in a period of time. In this paper, we introduce a special kind of traffic data stream, which is called Automatic Number Plate Recognition (ANPR) data. In order to quickly identify traveling companions on ANPR data stream, this paper proposes an analysis algorithm named COINCIDENT. Owing to the privacy and security of ANPR data stream, the algorithm is encapsulated as a data stream service. The main contributions include: 1) we consummate our previous stream data service model and design a traveling companion discovery service on the model. 2) the proposed COINCIDENT algorithm can instantly and continuously discover traveling companions on the live ANPR data stream. Final experiments show the effectiveness and efficiency of our developed service."
3789,"In a services marketplace where a service is provided by multiple service providers, service offerings have to be differentiated against competitor services. Differentiation helps to sustain as well as grow market share. Strategies to differentiate service offerings have to be unintrusive - without requiring major changes to the existing service realization mechanisms. In this paper, we present a strategy for service providers to differentiate their commoditized services. We show how to identify and document differentiating aspects of a service. By unintrusively manipulating these differentiation aspects we can differentiate services from that of competition. We specify these differentiating aspects as service policies using the WS-Policy framework."
3790,"Software applications that are provided through services following the Software as a service (SaaS) paradigm generally need to be compliant to some standards. In order to select the software application that provides the most conformant service interface, we develop a formal framework that tests the compliance and reports quantitative results that help experts to take the right decision. Unlike existing work that focuses on a single dimension when checking the compliance of a software application with the corresponding standard (for example the functional dimension exclusively, or the syntactic dimension exclusively), in this work we consider multiple dimensions at the same time. This provides more comprehensive results. We implemented a prototype and we tested our framework on a case study regarding the selection of software applications for our collaborative platform."
3791,"Cloud computing brings a rapid way for enterprises to launch a new software, which needs lower cost compared with self-purchased infrastructure. More and more enterprises compact their business software as services into cloud platform to the users. In service auction, the service provider aims to make more profits from the service it provides, but the service consumer hopes to select a good quality of service at a low price, which make the relationship of them become one kind of game. In this paper, the game between service providers and service consumers is analyzed. The utility functions of consumer and service provider are respectively established. Then a QoS optimization algorithm based on utility game is designed to solve an optimal QoS strategies to achieve a tradeoff between consumer satisfaction and provider profit. Finally the simulation experiments are implemented by using Matlab platform. The experimental results show that the algorithm can effectively optimize the QoS from two aspects of consumer preference and service cost, which can bring the largest improvement of user satisfaction for service providers with the lowest cost. So it helps to realize a win-win situation between service providers and consumers."
3792,"This paper presents a service-oriented framework for the development of wrapper code generators, including the methodology of designing an effective wrapper program construction facility and a concrete implementation, called XWRAPComposer. Three unique features distinguish XWRAPComposer from existing wrapper development approaches. First, XWRAPComposer is designed to enable multi-stage and multi-page data extraction. Second, XWRAPComposer is the only wrapper generation system that promotes the distinction of information extraction logic from query-answer control logic, allowing higher level of robustness against changes in the service provider's Web site design or infrastructure. Third, XWRAPComposer provides a user-friendly plug-and-play interface, allowing seamless incorporation of external services and continuous changing service interfaces and data format."
3793,"Business process management in service-oriented computing (SOC) environments poses special challenges. In particular, SOC environments are dynamic, thereby requiring frequent changes in business processes. Current business process modeling approaches handle such changes in an ad hoc manner, and lack a principled means of determining what needs to be changed and where. This paper addresses process adaptability through a novel application of business protocols, especially of protocol composition, introduced in our previous work. Through a real business scenario of auto-insurance claim processing, this paper demonstrates how a wide range of adaptations can be handled naturally and systematically via protocol composition. The illustrated adaptations have been evaluated via a prototype"
3794,"In the last few years, the Internet of Things (IoT) has obtained rapid development. The amount of devices and sensors that are connected to the Internet has increased significantly. On the other hand, the environmental monitoring and control becomes more and more important in several fields. Since devices and sensors may be produced by different manufacturers, it is necessary to find a solution for shielding the heterogeneity. What's more, vast amount of data will bring the performance issue. In this paper, we design and implement a middleware for environmental monitoring and control. The middleware system is hierarchically distributed. Data from heterogeneous devices are converted into unified virtual entities to shield the heterogeneity. The middleware provides service invocation for the upper application and supports dynamic perception and discovery of devices. Through the registration module, the middleware can be easily upgraded, and will not affect the underlying devices and the upper application. We design several mechanisms to process a mass of data. The paper presents experimental results to illustrate the good performance of the middleware system. Generally speaking, every detail of our design is for the actual situation of environmental monitoring and control."
3795,"Semantic Web and heterogeneous data integration are critical factors for the future Web intelligence. A fundamental aspect of this challenge is the provision of a wide and disseminated platform of abstract resources that permit us to get the right information at the right time. We are researching a semantic Web integrated, virtual and dynamic architecture. This paper proposes a Web service architecture as the preliminary implementation guideline for such a semantically integrated one."
3796,"With the rapid growth in the number of available services, recommending suitable services to users becomes increasingly important. A number of collaborative service recommendation methods based on user experiences have been proposed for this purpose. Most of them adopt the similarity-based Collaborative Filtering (CF) technique, which tends to identify similar users for a target user and recommends to the target user the services preferred by the similar users. However, a user similar to the target user is unnecessarily trustworthy to him/her. Therefore, the results recommended by similarity-based CF are probably unreliable. Moreover, existing service recommendation methods seldom incorporate social trust relationships among service users into service recommendation. In this paper, we propose a collaborative, trust-aware service recommendation method for service-oriented environments with social networks. The method is based on an integration of the user-service relation and the user-user social relation. Experimental results demonstrate that our service recommendation method significantly outperforms conventional similarity-based recommendation and trust-based service recommendation methods."
3797,"Infrastructure-as-a-Service (IaaS) cloud environments expose to users the infrastructure of a data center while relieving them from the burden and costs associated with its management and maintenance. IaaS clouds provide an interface by means of which users can create, configure, and control a set of virtual machines that will typically host a composite software service. Given the increasing popularity of this computing paradigm, previous work has focused on modeling composite software services to automate their deployment in IaaS clouds. This work is concerned with the runtime state of composite services during and after deployment. We propose AESON, a deployment runtime that automatically detects node (virtual machine) failures and eventually brings the composite service to the desired deployment state by using information describing relationships between the service components. We have designed AESON as a decentralized peer-to-peer publish/subscribe system leveraging IBM's Bulletin Board (BB), a topic-based distributed shared memory service built on top of an overlay network."
3798,"Peer-to-peer (P2P) computing is a distributed computing paradigm that uses large numbers of autonomous hosts as a platform for the execution of applications. This paper focuses on P2P computing in an enterprise environment. Unlike other P2P computing scenarios more access and data about the peers, network and users is available thus enabling the deployment of more complex applications that consists of components that are hosted on different peers. Unfortunately the distribution of components in a P2P network leads to serious coordination issues. This paper has two aims. First of all it is an attempt to draw more attention to the issues surrounding the use of P2P computing in an enterprise environment. We believe that there is a growing demand for using the distributed resources of an organization in a P2P manner that allows users to keep their autonomy and control over ""their"" machines. Secondly, the paper presents a framework/middleware called P2P-Manifold designed to support the seamless execution of distributed applications in a P2P network."
3799,"XML syntax and semantic validations are critical to the correct service transaction specification and service integration based on existing distributed and heterogeneous computing services. This research proves that the current industry practice of XSLT-based Schematron validation may produce invalid results, and contributes a reusable XML validator component that supports sound integrated syntax/semantic validations and event-driven integration with its environment through public APIs."
3800,"Gathering functionally similar agent-based Web services into communities has been proposed and promoted on many occasions. In this paper, we compare the performance of these communities with self-managed, single agent-based Web services from trust perspective. To this end, we deploy a reputation model that ranks communities and Web services with respect to different reputation parameters. By relating the parameters, we extend our discussion to analyze the beneficial cases and incentives for a single Web service to join a community even if this joining could negatively impact other parameters. Besides theoretical discussions of this analysis, we discuss the system implementation along with simulations that depict diverse parameters and system performance."
3801,"TSF is an Italian IT service company, focused on logistics and transportation IT market segments.. It is able to support customer's business operations through a complex IT service chain, appraised by a Service Level Management (SLM) system. Due to the long term contractual relationship (10 years) with Italian Railways, TSF developed a symbiotic relationship with its main customer, becoming co-accountable for its business performance. This aspect has highlighted the necessity of the definition of an extended SLM model able to correlate the customer business performances with the delivered ICT service levels, in the perspective of an end-to-end service delivery chain. The paper illustrates the conceptual approach and the method adopted to overcome this problem in a pilot project."
3802,"Nowadays, process-aware information systems (PAISs) are widely used for the management of “administrative” processes characterized by clear and well-defined structures. Besides those scenarios, PAISs can be used also in mobile and pervasive scenarios, where process participants can be only equipped with smart devices, such as PDAs. This paper illustrates ROME4EU, a fully-fledged PAIS that can be entirely developed on Windows Mobile PDAs. To our knowledge, all existing PAISs are equipped with an engine meant to run only on laptop/desktop. And this prevents them from being used in mobile scenarios, such as emergency management. ROME4EU is based on a mobile Web service middleware and a WS-BPEL orchestrator engine. The feasibility for mobile settings introduces new challenging issues to face with, such as reduced computational power, small screen size, battery consumption, automatic adaptability to anomalous events. The paper details also an evaluation of the ROME4EU's performances and illustrates its use by civil protection operators to manage the aftermath of a (simulated) emergency."
3803,"Discovering and assembling individual Web services into more complex yet new and more useful Web processes receive significant attention from the academia recently. In this paper, we explore using pre and post-conditions of Web services to enable their automatic composition. Also, we present a novel technique for discovering semantic relations between pre and post-conditions of different services using their ontological descriptions. This enables determining services with complimentary functions and generating a semantic Web of services. Our technique takes semantic similarity of pre and post-conditions into account and builds on our earlier work on discovering semantic relationships between interfaces (input and output) of Web services."
3804,"This paper presents an approach for observing and reacting on the execution of services' coordinations in order to ensure NFP policies specified by the coordination designer. Thanks to policies associated to a services based application running in a Web dynamic environment, it is possible to associate a personalized behaviour: atomic integration of information retrieved from different social network services, automatic generation of an integrated view of the operations executed in different social networks."
3805,"One of the prerequisites for the success of a QoS-based web service selection process is an accurately formulated QoS query. It is usually not an easy task for users to formulate an accurate query considering the complexity of many current QoS languages and users' lack of knowledge on realistic QoS values. It would be very helpful if the system can provide some assistance to users during the whole process. Nonetheless, not many research works put user support to the center of their system design. In this paper we want to tackle this issue by proposing a QoS query language which is expressive while not so complicated, together with a comprehensive user support mechanism to guide users through the query formulation process. A few unique features of the language include its time dimension, user-defined relaxation order which could be different from the preference order, and the support for the mixed fuzzy and range requirement. How to handle these new features is also discussed as case studies in the paper."
3806,"Dynamic and flexible service composition and interactionare a must in service oriented computing (SOC) scenarios. In this paper the authors present an extension to a previous work, the DENEB platform for the development and execution of Web processes, allowing Web processes to acquire and execute new interaction protocols at runtime. This makes DENEB a well adapted and flexible platform for service integration, useful to work in complex and evolving inter-organisational environments. The proposed solution is exemplified by means of the development of a real use case."
3807,"With the development of the location-based social networks (LBSNs) and the popular of mobile devices, a plenty of user's check-in data accumulated enough to enable personalized Point-of-Interest recommendations services. In this paper, we propose a scheme of modeling user's preferences on spatiotemporal topics (UPOST scheme) for accurate individualized POI recommendation. In the UPOST scheme, we discover temporal topics from semantic locations (i.e., people's description words for a location) to learn users' preferences. UPOST infers user's preference for different types of places during different periods by learning the spatiotemporal topics from the historical semantic locations of users. We have developed two algorithms under the UPOST scheme: the time division LDA algorithm (TDLDA) and the time adaptive topic discovery algorithm (TATD). In TDLDA, we divide the check-in dataset into different time segments and use one LDA for one segment. Then we improve TDLDA further by developing a new TATD algorithm to discover spatiotemporal topics. The experimental results demonstrate the effectiveness of our UPOST scheme, both TDLDA and TATD outperform the counterpart method that do not consider semantic locations."
3808,"UN/CEFACT's modeling methodology (UMM) is a UML profile for modeling global B2B choreographies. The basic building blocks of UMM are business transactions, which describe the exchange of a business document and an optional response. In addition to these business document exchanges, UMM business transactions mandate business signals that acknowledge the correctness of business documents. It is expected that a business service interface (BSI) on each business partner's side reacts on incoming messages and on messages expected but not received. However the internal orchestration of the BSI is open to interpretations. In this paper we demonstrate an unambiguous mapping from global choreographies described by UMM transactions to a BPEL-based orchestration of the business service interface. It becomes obvious that rather simple looking UMM transactions lead to a more complex message exchange mechanism when implemented on top of Web services."
3809,"Recent attempts have been done to measure similarity of process models based on graph matching. This problem is known to be difficult and its computational complexity is exponential. Thus, heuristics should be proposed to obtain approximations. Spectral graph matching methods, in particular eigenvalue-based projections, are know to be fast but they lost some quality in the obtained matchmaking. In this paper, we propose a graph approach for the problem of inexact matching of process models. Our approach combines a spectral graph matching method and a string comparator based algorithm in order to improve the quality of process models matchmaking. The proposed method performs the matchmaking at both structural and semantic levels. Experimentation is provided to show the performance of our method to rank a collection of process models according to a particular user query, compared to previous work."
3810,"Every day, the services or composite services satisfy billions of customers' requirements. However, these requirements are not explicit to the service providers, because people always make their decisions randomly or with specific reasons that are not announced directly online. Therefore, digging user interest (portrait) became a trendy measure for Web personalized recommendation. However, user requirement is not only a collection of user interests. It will be more comprehensive, if it contains the mode of user demand occurrence and the trend of user preference evolution. This paper proposes a novel approach based on Artificial Neural Network (ANN) to elicit the service requirement patterns and detect the individual preference evolution under these patterns. A case study is performed in pharmaceutical retail service market to verify this approach."
3811,"One of the major difficulties for service-based application designers and developers is ensuring that the services participating in a distributed application are compatible when composed together under service oriented architectures. This paper examines this problem, and proposes a set of solutions by first defining ways of specifying compatibility conditions on contracts using standards such as WS-business activity and then presenting a tool that enables application developers check those conditions at design time"
3812,"Good IT decision making is a highly desirable property that can be furthered by the use of enterprise architecture, an approach to IT management using diagrammatic models. In order to support decision making, the models must be amenable to various kinds of analysis. It is desirable that the models support the sought after analysis effectively since creation of enterprise architecture models often is a demanding task. This paper suggests a framework for enterprise service interoperability analysis and a metamodel containing the information needed to perform the analysis. The paper also illustrates the use of the framework and metamodel in a fictional example."
3814,"Available frameworks and middleware implementing run-time adaptivity mechanisms exploit qualities of services (QoSs) and/or other properties of the systems' resources when choosing the most appropriate one in order to provide the services requested by the users. Usually, the case studies built upon these solutions claim for an immediate execution of services. In this paper, we describe an adaptive resource management (ARM) approach which considers both the immediate execution of services and their bookings."
3815,"Web services (WSs) are emerging as the building block of Internet scale database management systems (IDBMSs). These systems must intelligently execute plans that reference autonomous WSs. This requires policies and mechanisms for both scheduling and allocating WSs that constitute a plan. In this study, we analyze two scheduling strategies and four allocation policies. Obtained results show that dynamic scheduling with least response time (LRT) allocation policy is superior to other alternatives when the service time of a WS can be estimated accurately. The traditional least recently used (LRU) allocation policy is inferior to all policies including random. These observations are important because they impact the scalability of a system. Only with a smart allocation policy, one should expect improved system performance by increasing the number of nodes that constitute an IDBMS to support a larger number of WS replicas."
3816,"Traditionally, system and network management software is tightly tied with the managed IT resources through their specific manageability interfaces, such as SNMP, CIM, WMI, JMX. Because Web service distributed management specification (WSDM), as an initiative of OASIS organization, helps to build the SOA-support manageability interfaces of IT resources, we proposed SOAMS-a novel SOA-based system and network management scheme and WSDM-based management middle mode for SOAMS. As part of experiment work, we discuss MUSE-JMX based management scheme implementation- SOAMS-platform, including manager layer, WSDM-gateway layer, and agent layer, and then analyze the prospective research direction."
3817,"This paper presents a formal framework to perform passive testing of service-oriented systems. Our approach uses the historical interaction files between web services to check the absence of faults. It uses a set of properties, that we call invariants, to represent the most relevant expected behaviour of the web services under test. Intuitively, an invariant expresses the fact that each time the system under test performs a given sequence of actions, it must exhibit a behavior reflected in the invariant. Invariants can be defined from a local point of view, that is, to check properties of isolated web services, and from a global point of view, that is, to check web service interaction properties. In order to increase applicability and adaption to a real environment, we assume that we do not have a global log. We show how to use local logs (recorded in each web service) in order to check local properties and how to combine them in order to check global properties."
3818,"Cloud/Grid environments are characterized by a diverse set of technologies used for communication, execution and management. Service Providers, in this context, need to be equipped with an automated process in order to optimize service provisioning through advanced performance prediction methods. Furthermore, existing software solutions such as GNU Octave offer a wide range of possibilities for implementing these methods. However, their automated use as services in the distributed computing paradigm includes a number of challenges from a design and implementation point of view. In this paper, a loosely coupled service-oriented implementation is presented, for taking advantage of software like Octave in the process of creating and using prediction models during the service lifecycle of a SOI. In this framework, every method is applied as an Octave script in a plug-in fashion. The design and implementation of the approach is validated through a case study application which involves the transcoding of raw video to MPEG4."
3820,"With the increasing growth in popularity of Web services, matchmaking of relevant Web services becomes a significant challenge. Commonly, Web service is described by WSDL and published on UDDI registers. UDDI provides limited search facilities allowing only a keyword-based search of businesses, services, and the so called tModels based on names and identifiers. This category-based keyword-browsing method is clearly insufficient. Semantic Web service uses DAML-S instead of WSDL to represent capabilities of Web services. This improvement enables software agents or search engines to automatically find appropriate Web services via ontologies and reasoning algorithm enriched methods. However, the high cost of formally defining to the heavy and complicated services makes this improvement widespread adoption unlikely. To cope with these limitations, we have developed a suite of methods which assesses the similarity of Web services to achieve matchmaking. In particular, we present a conceptual model which classifies properties of Web services into four categories. For each category, a similarity assessment method has been given. In Web service matchmaking process, these similarity assessment methods can be used together or individually. Experiments highlight complementary contributions that our work makes to facilitate Web service matchmaking."
3821,"Experiments submitted to equipment grid have quality of service (QoS) requirements, and advance reservation is used to satisfy such requirements. Due to the dynamic behaviors and fluctuations of resources in equipment grid, some previously accepted advance reservations are unable to be fulfilled. In this paper, we present a predictive admission control algorithm to decide whether new advance reservation requests can be accepted according to their QoS requirements and prediction of future resource utilization. Historical data are used in this algorithm to predict future status of resources. Experiments demonstrate that our algorithm can reduce the number of accepted advance reservations that fail to be fulfilled and keep the resource utilization ratio at an acceptable level."
3822,"Web services are among the applications involving closely the customers' private informations. In order to take into account the privacy concerns of the individuals, organizations (e.g Web services) provide privacy policies as promises describing how they will handle personal data of the individual. However, privacy policies do not convince potential individuals to disclose their personal data, do not guarantee the protection of personal information, and do not provide how to handle a possible evolution of the policies. In this paper, we propose a framework based on an agreement as a solution to these problems. It contains a privacy model defined in the policy level of the agreement. The framework supports in the negotiation level of the agreement a lifecycle management which is an important deal of a dynamic environment that characterizes Web services. A negotiation protocol is proposed that enable ongoing privacy negotiation to be translated into a new privacy agreement."
3823,"Owing to the dynamicity of business environments in which organizations must quickly adapt to changes, the information systems have recently had to adapt to new situations so that they can keep adding value efficiently and effectively. In the light of this scenario, a new discipline called Service-oriented Systems Engineering has emerged in the academic scene and this has highlighted the disciplined, systematic and quantified development of Service-oriented Computing systems. This discipline is divided into other sub-disciplines, one of these sub-disciplines is Service-oriented Requirements Engineering (SORE) and it is concerned with defining processes and methodologies to address the question of services requirements from two different perspectives: service consumer and service provider. In the SORE context, this paper proposes the WS&amp;i*-RGPS approach that explores some alternatives to the descriptions proposed by the Role, Goal, Process and Service (RGPS) metamodels - an approach in SORE. This involves a new definition that seeks to incorporate the benefits of other methodologies - established in the literature - with RGPS. Accordingly, this new approach outlines the use of the i Framework to describe the Role and Goal layers and shows the use of WS-BPEL/WSDL to describe the process and service layers. The use of the i Framework sets out the goals for managing different aspects of the systems specification. Moreover, this paper presents a comparison among WS&amp;i*-RGPS and other SORE approaches based in three parameters presented in SORE literature."
3824,"Infrastructure and Application Service Delivery typically comprises of complex workflows such as software installation/configuration, OS configuration, file system management etc. To manage costs as well as SLAs, enterprises constantly look towards automation of these workflows. While declarative approaches for automated workflow planning have been described in the literature, they do not address the risk exposure of the system. We present a novel two-phase risk-aware approach to planning which allows to ""look-ahead"" beyond the desired end state and compute a vulnerability score that measures how vulnerable the system could be in future, if the plan were to be executed now. We demonstrate our approach on a web service migration use-case using SGPlan as the preference-based planner. We also implement an end-to-end framework based on Puppet and validate it on a test bed of virtual machines. The implementation validates the generality of our approach in finding a less-vulnerable plan in scenarios where there are multiple plans."
3825,"Using Blockchain seems a promising approach for Business Process Reengineering (BPR) to alleviate trust issues among stakeholders, by providing decentralization, transparency, traceability, and immutability of information along with its business logic. However, little work seems to be available on utilizing Blockchain for supporting BPR in a systematic and rational way, potentially leading to disappointments and even doubts on the utility of Blockchain. In this paper, as ongoing research, we outline Fides - a framework for exploiting Blockchain towards enhancing the trustworthiness for BPR. Fides supports diagnosing trust issues with AS-IS business processes, exploring TO-BE business process alternatives using Blockchain, and selecting among the alternatives. A business process of a retail chain for a food supply chain is used throughout the paper to illustrate Fides concepts."
3826,"For adapting to the dynamic characteristics of Internet of Things, a dynamic integration mechanism of business process has been proposed. The mechanism realizes redirecting and restructuring of process logic using rules that can be edited at runtime. A dynamic integration algorithm is designed to implement this mechanism. First, the algorithm creates a state transition digraph based on the current rules. Then, it finds a path to meet business objectives according to the current states. Finally, it performs services sequence obtained from the path to complete the business objectives. Our proposed method has a high flexibility, and process logic can easily be redirected according to different requirements."
3827,"This paper explores how cloud provider competition influences instance pricing in an IaaS (Infrastructure-as-a-Service) market. When reserved instance pricing includes an on-demand price component in addition to a reservation fee (two-part tariffs), different providers might offer different price combinations, where the client's choice depends on its load profile. We investigate a duopoly of providers and analyze stable market prices in two-part tariffs. Further, we study offers that allow a specified amount of included usage (three-part tariffs). Neither two-part nor three-part tariffs produce an equilibrium market outcome other than a service pricing that equals production cost, i.e., complex price structures do not significantly affect the results from ordinary Bertrand competition."
3828,"This paper provides a multi-agent negotiation based service composition approach to deal with end user on-demand service requirements. We introduce a system framework for service composition. In this framework, a workflow defines a generic and service-independent process model within which each service agent is able to select appropriate task node that fits for the end user requirement in run time. Specifically, agents will negotiate with one other to guarantee the satisfaction of the service consumers' requirements. This paper introduces a methodology of how to model a service composition as a constraint satisfaction problem and solve the problem by a multi-agent negotiation algorithm."
3829,An identity-based cryptosystem is a novel type of public cryptographic scheme in which the public keys of the users are their identities or strings derived from their identities. A signcryption is a primitive that provides private and authenticated delivery of messages between two parties. Proxy signature schemes are variations of ordinary digital signature schemes and have been shown to be useful in many applications. We proposed an identity-based proxy-signcryption scheme from pairings. Also we analyze the proposed scheme from efficiency and security points of view. Heuristic arguments have been given for those security properties. We have shown that the proxy-signcryption scheme is as efficient as ordinary identity-based signcryption schemes under certain circumstances.
3830,"Lenders require accurate and interpretable credit scoring models palatable to regulators, financial services staff and consumers. Expert-designed segmented scorecards fill this need. Building such models is a laborious data-guided task for experienced modelers. It can take weeks to hone a model for deployment. Lenders would like to design, update and test models, predictors and segmentation schemes more frequently, objectively and cost-effectively, as environments change fast and as new data emerge. We propose scorecard design as an automated analytic computing service used by domain experts, comprising data-driven machine learning with expert-imposed palatability restrictions and model visualization, in two stages: Stage I fits a tree ensemble model to render a best-fit score and a list of segmentation candidates. Stage II uses this information to generate optimal palatable segmented scorecards subject to restrictions provided by the experts. When implemented on a computer cluster, our process yields close to deployment-ready scorecards within minutes to hours, which can be rapidly honed and upon approval deployed into a separate scoring service. While motivated by transparency needs of credit scoring, such a service can be valuable for any application requiring highly predictive yet palatable scoring algorithms."
3831,"There is a trend that more and more enterprises utilize SDN (Software Defined Network) technology to manage the network of their cloud platform. For example, cloud computing datacenters define their own virtual networks or virtual firewalls using SDN controller. Tenants need the network components of cloud platform to call northbound interfaces of SDN controller if they want to manage the network of cloud platform. If there are a large number of tenants, the interaction between the cloud platform and the SDN controller is very frequent. In order to simplify this process, we propose a policy-driven based batch processing network operation SDN controller scheme in which the cloud datacenter manager sends the SDN-related policies to the SDN controller, and the SDN controller processes the policies received according to user's permissions and operations priority. The management of network resources can integrate with other resources management in the cloud datacenter environment effectively. We then build a prototype, a policy-driven SDN controller (PDSDN), to demonstrate the efficiency and performance of our design."
3832,"Currently, grid technologies are widely used in large-scale scientific applications. Grids support stateful interactions with explicit exposure of state information across the boundaries of a service. In this paper, we present a stateful Web service architecture that provides efficient sharing of a service instance between heterogeneous service requesters with monitoring of the interactions. We describe how we compose the state information based on the formalized sequence of the interactions. We also describe how the shared service instance is managed, and interacted with, through standard Web services interfaces. We analyze the performance of our approach in a large-scale scientific grid application."
3833,"How to find a suitable service for a user is a challenging task. This paper introduces a method of concept categorization with the use of personal ontology to select more suitable service according to the preferences of users. In addition, a new attribute called 'reformation' is added to evaluate service quality. Our method is to aide the evaluation process with user preferences to reflect the fact that the user difference will affect the selection of different service providers."
3834,"Context-aware Web service is an interactive model between the context of service requesters and the services in Web-enabled environments. We envision that providing context-aware services is the first step toward ubiquitous Web services to enhance current Web-based e-business by finding right business partners, right business information, and right business services in the right place at the right time. The major contributions of this paper are the development of our context model and context aware service oriented architecture (CA-SOA). We have developed a context model to formally describe and acquire contextual information pertaining to the service requesters and services. Based on the model, we have constructed CA-SOA for ubiquitous Web service discovery and access based on service and requesters 'surrounding context"
3835,"Stream processing software frameworks enable real-time processing of continuous unbounded streams of data at a high speed. Leveraging the elasticity of cloud computing infrastructure, stream processing frameworks can become Software as a Service for many domain applications that provide simplified development and run-time management. An issue of making such a SaaS scalable is to allocate data processing operators on nodes of clusters and balance the workload dynamically. Since the data volume and rate can be unpredictable, static mapping between operators and cluster resources often results in unbalanced operator load distribution. This paper proposes an optimization method that combines correlation of resource utilization of nodes and capacity of clusters. The associated software components form a layer between a stream processing software framework and cloud clusters and nodes. This software layer allows dynamic transferring of an operator to different cluster nodes at runtime and keeps transparent to developers. We present a prototype evaluation on Yahoo's S4 and clusters on Emulab.org. Our implementation is evaluated by a top-N topic list application on Twitter streams. The results demonstrate improved stream processing throughputs and cluster resource utilization."
3836,"Cloud services are Internet-based XaaS (X as a Service) services, where X can be hardware, software or applications. As Cloud consumers value QoS (Quality of Service), Cloud providers should make certain service level commitments in order to achieve business success. This paper argues for Cloud service negotiation. It outlines a research roadmap, reviews the state of the art, and reports our work on Cloud service negotiation. Three research problems that we formulate are QoS measurement, QoS negotiation, and QoS enforcement. To address QoS measurement, we pioneer a quality model named CLOUDQUAL for Cloud services. To address QoS negotiation, we propose a tradeoff negotiation approach for Cloud services, which can achieve a higher utility. We also give some ideas to solve QoS enforcement, and balance utility and success rate for QoS negotiation."
3837,Business flexibility requires IT flexibility. IT flexibility can not occur if the old ways continue to be followed. Transformation to SOA is the way forward. SIMM has proven to be an effective means to achieve that transformation to SOA through a successive set of adoption of states of maturity that is relevant to the enterprise
3838,"We address the problem of optimal resource action planning, which is faced by firms in the services business in the context of planning delivery of services with an expected demand outlook for various skills. Firms have the option of cross-training a chosen number of resource-units from a primary skill type into secondary skills, tertiary skills, and so on. They have the option of further hiring and increasing the system availability of any chosen skill type. They also have the option of contracting out a required amount of any chosen skill type. We address the problem of identifying the optimal set of planned resource control actions, in terms of both the timing and extent of hiring, cross-training, and contracting combination, so that they are well positioned to meet the actual demand. We present a mathematical model that can be described in terms of a constrained network flow and map it to a mixed integer linear programming formulation. The model considers practical constraints such as minimum residence time that a resource unit needs to spend in a skill-type into which it gets cross-trained or hired, minimum acceptable contracting duration for a contracted skill type, first order and higher orders of cross-training for a skill type, with lead-times, logical business rules associated with cross-training transfer sequences, as well as firm-specific targets on service levels and resource utilization, in determining the optimal resource action plan."
3839,"Web services define a new paradigm in collaborative applications development in enterprises. They span systems and organizations. The use of CAD systems calls for more productive collaborative work on the Internet. An efficient framework will bring a great deal of convenience to meet this demand. Considering the inherent advantages of Web services, the paper presented a collaborative working framework of CAD systems based on Web services and described the architecture of the framework in detail. Furthermore, several key technologies used in this framework are discussed, including the services division method, which guarantees highly feasible maintenance, and the deploying policy of distribution services for balancing Web server burden. Finally, an example using this framework is presented."
3840,"A single-instance multi-tenant (SIMT) SaaS application enables a SaaS provider to achieve economies of scale through runtime sharing. However, runtime sharing can make tenant-specific variations difficult to achieve in such an application. In this paper, we propose an approach to realizing SIMT SaaS applications, which is based on Dynamic Software Product Lines (DSPL) and supports runtime sharing and variation. With the collaboration among a subset of services as the unit of composition, the commonality among the tenants' requirements is realized in the DSPL architecture by sharing collaboration units, and their variability is realized by composing different collaboration units, all at runtime. In addition, we adopt a feature-based high-level representation of the commonality and variability between the tenants' requirements to facilitate the runtime creation and reconfiguration of their application variants. We compare our approach with two alternative approaches in terms of development effort and degree of sharing. We further quantify the runtime overhead incurred by our multi-tenancy support."
3841,"The emerging paradigm of Web services has been gaining significant momentum in the recent years since it offers a promising way to facilitate business-to-business (B2B) collaboration. However, it is not clear that this new model of Web services provides any measurable increase in computing trustworthiness. We propose a generic framework to control the trustworthiness of computing in the domain of Web services. A layered model is established to highlight four key elements: resources, policies, validation processes, and management. The robustness of this model exhibits its flexibility and extensibility. Examples utilizing our framework are reported."
3842,"As an important technique in modern sociology, social network analysis has gained a lot of attention from many disciplines, and been used as important complements to traditional statistics and data analysis. In order to make it affordable for analysts with massive and fast growing networks, we present X-RIME, a cloud-based library for large scale social network analysis. We propose an implementation-oriented classification of social network analysis structures and structure variables to guide the implementation of them over MapReduce parallel programming model. The layered architecture of the library is described along with design consideration of each layer. By sharing the same infrastructure and integrating with existing cloud-based data warehouse and data mining libraries and tools, more comprehensive and cost effective social network aware business intelligence solutions could be built. We present several case studies on an online community to illustrate the usage of X-RIME library. The performance of the library is evaluated with experiments, which demonstrates good scalability."
3843,"The prevalence of multi-core processors has raised the question of whether applications can use the increasing number of cores efficiently in order to provide predictable quality of service (QoS). In this paper, we study the horizontal scalability of n-tier application performance within a multicore processor (MCP). Through extensive measurements of the RUBBoS benchmark, we found one major source of performance variations within MCP: the mapping of cores to virtual CPUs can significantly lower on-chip cache hit ratio, causing performance drops of up to 22% without obvious changes in resource utilization. After we eliminated these variations by fixing the MCP core mapping, we measured the impact of three mainstream hypervisors (the dominant Commercial Hypervisor, Xen, and KVM) on intra-MCP horizontal scalability. On a quad-core dual-processor (total 8 cores), we found some interesting similarities and dissimilarities among the hypervisors. An example of similarities is a non-monotonic scalability trend (throughput increasing up to 4 cores and then decreasing for more than 4 cores) when running a browse-only CPU-intensive workload. This problem can be traced to the management of last level cache of CPU packages. An example of dissimilarities among hypervisors is their handling of write operations in mixed read/write, I/O-intensive workloads. Specifically, the Commercial Hypervisor is able to provide more than twice the throughput compared to KVM. Our measurements show that both MCP cache architecture and the choice of hypervisors indeed have an impact on the efficiency and horizontal scalability achievable by applications. However, despite their differences, all three mainstream hypervisors have difficulties with the intra-MCP horizontal scalability beyond 4 cores for n-tier applications."
3844,"In this paper, we study the service request (ticket) allocation problem which arises in every IT service delivery organization. We refer to this problem as Ticket Allocation Problem (TAP). We first show that TAP is an instance of the online scheduling problem on unrelated machines, which is known to be a hard problem. Next, we describe a baseline model, namely push model, that deals with the TAP. The push model is an industry wide standard and can be used with any known online scheduling algorithm for unrelated machines. To elaborate this further, we discuss a well known Generalized List Scheduling algorithm which can be used by the push model. We prove a bound for this algorithm's competitive ratio which beats all the known bounds. We show that push model suffers from an inherent inefficiency due to scheduler having incomplete and imprecise information regarding agents' proficiency. Finally, we show that if the scheduling algorithm used by the push model can be converted into a truthful auction mechanism then all the inefficiencies of the push model can be overcome. We refer to the resulting model as the pull model. To illustrate the idea, we map the Generalized List Scheduling algorithm into a truthful auction mechanism. Through simulation experiments, we show that the auction based pull model results in higher efficiency than the push model."
3845,"Rapid evaluation and treatment of time-dependent diseases such as acute stroke need fast and consistent communication between medical service providers. Our framework accepts data from hospital systems and medical evaluations, and composes it into new forms to suit different recipients and mobile devices. We have implemented a prototype application for stroke therapy which reduces the time from patient arrival to CT scan by 43%, and the time from arrival to treatment by 48% respectively. Our system can perform context-based CT image retrieval and allows images to be reviewed on mobile devices. Context-based QoS adaptation has been shown to reduce the transmission time required to transmit 619 brain images to mobile devices from 1336 seconds to 106 seconds. The system utilizes medical information standards, including DICOM and HL7."
3846,"The easiest way for a user to express his needs regarding a desired service is to use natural language. The main issues come from the fact that the natural language is incomplete and ambiguous, while the service composition process should lead to valid services. In this paper we propose a natural language service assemblage method based on composition templates (patterns). The use of templates assures that the composition result is always valid. The proposed system, called NLSC (Natural Language Service Composer), was implemented on the top of a service-oriented middleware called WComp and tested in an intelligent home environment."
3847,"The realization of business process with services has gain more importance in the last decade. Several organizations are focusing nowadays in the modeling and execution of their business processes, seizing the advantages provided by both the BPMN 2.0 notation which allows these models to be executed, and the emergence of BPMS platforms which are able to execute business processes invoking internal and external services from partners and/or the cloud when needed. Although many lifecycle proposals exist to guide the definition and management of both business process and services, there is no clear relationship defined between them, i.e. how services should be defined and managed to support business processes. This is a key element that should be taken into account when implementing services for this kind of systems, in order to systematize the work and obtain better results. In this paper we present a service lifecycle to support business processes, which helps developing services for business process systems."
3848,"In this paper, we study legacy asset reuse for SOA design. Typically, the cost of reusing legacy assets is much lower than the cost of creating new services from scratch. As an extension of our SOA service modeling, we use top-down approaches to identify business services, and use asset modeling for legacy asset identification to find potential reusable assets for those services. Using service capacity analysis, we evaluate the maturity of a legacy asset for transformation to a service. Finally, we provide a model transformation to convert a service model with legacy assets into an SOA reference architecture model. Our effort integrates asset reuse as a part of the overall SOA design cycle and increases the possibility of asset reuse. To help architects use our method in engineering practice, we have implemented our method in the SOMA-ME tooling environment."
3849,"Memes are topics that spread among people in a community. They could be ideas, behaviors, or styles, etc. Recent research found that the spreading of memes on microblogs displays epidemic patterns, while their popularity is predictable. Aiming for high accuracy, complex models that consider the structural details of community graphs are common. On the contrary, some models simply assume a community is evenly connected to sacrifice accuracy for efficiency. In this paper, we propose a prediction service that delivers the benefits of both worlds. Our service considers the properties of community graphs yet without the burden of their structural details to reduce complexity for online consumption. By comparing the viralities of memes against a community threshold, memes that would lead to population-wide coverage are identified. We also carefully examine sampling biases. Hence, reliable thresholds and accurate predictions are made as shown in our experiments."
3850,"This paper is based on the view that the adoption of service oriented architectures (SOA) by large information system (IS) vendors might push software development towards an industrialized mode of operation. The emergence of such a software ecosystem is analyzed through an innovation perspective. The special nature of software as an intellectual technology, thus being an intangible product as opposed to physical products, is given special emphasis. Since digital products are expensive to produce, but cheap to reproduce, it is argued on the basis of a resource-based view that this ecosystem should not solely be built on the trading of software, but also on the exchange of business knowledge"
3851,"We present SPIRIT, a Service for Providing Infrastructure Recommendations for Information Technology. SPIRIT allows maintenance support providers for Small-to-Medium Businesses (SMBs) to recommend solutions which are standardized (SMBs usually cannot afford customized IT solutions), flexible (accommodating as much as possible the customer's existing IT environment), and cost-effective (minimizing the cost of upgrading the customer's environment). SPIRIT works by first aligning the customer's IT infrastructure with a ""template"" describing the best practices recommended by the maintenance support provider. This step is done using an efficient graph algorithm that finds the most cost-effective transformations of the customer's environment that are consistent with the template. Then the aligned environment can be upgraded by choosing from a standard set of well understood, highly automated (and therefore economical) options. We show that the algorithm performs well on both real and synthetic data."
3852,"Composing and reusing services is the main advantage of Service Oriented Software Engineering (SOSE). Faced with the large amount of Web services which are available on World Wide Web, how to select and recommend suitable Web services becomes a key issue in service computing. The most popular service recommendation technique is QoS-based Collaborate Filtering (CF) with user-service QoS matrix. However, it cannot well capture the new interests of users and handle the cold start problem. In this paper we propose an interest-driven recommendation approach which leverage User Interest Profile (UIP) to represent users' interests. UIP is generated in accordance with an ISO standard named as MFI-7. The similarity between UIPs is used to produce users' nearest neighbors. The Top-K recommendations will be generated from these neighbors' mostly-used services. To show the effectiveness of our approach, a developed Web service registry and repository platform is used as a testbed to produce preliminary evaluation results."
3853,"In the Image Processing domain, automated generation of complex Image Processing functionality is highly desirable, e.g., for rapid prototyping. Service composition techniques, in turn, facilitate automated generation of complex functionality based on building blocks in terms of services. For that reason, we aim for transferring the Service Composition paradigm into the Image Processing domain. In this paper, we present our symbolic composition approach that enables us to automatically generate Image Processing applications. Functionality of Image Processing services is described by means of a variant of first-order logic, which grounds on domain knowledge operationalized in terms of ontologies. A Petri-net formalism serves as basis for modeling data-flow of services and composed services. A planning-based composition algorithm automatically composes complex data-flow for a required functionality. A brief evaluation serves as proof of concept."
3854,"Software as a service (SaaS) is a new delivery model for software. Software in a SaaS model is no longer run exclusively for one customer at a customer's premise but run at a service provider and accessed via the Internet. A provider of software as a service exploits economies of scale by hosting and providing the same application for several different customers. However, each individual customer has different requirements for the same basic application. In order to allow each customer to customize the process layer and related artifacts of a SaaS application to their specific needs the application needs to provide a set of variability points that can be modified by customers. In this paper we describe the notion of a variability descriptor that defines variability points for the process layer and related artifacts of process-based, service-oriented SaaS applications. Furthermore we describe how these variability descriptors can be transformed into a WS-BPEL process model that can then be used to guide a customer through the customization of the SaaS application."
3855,"Service-oriented architectures enable an environment where businesses can expose services for use by their collaborators and their peer organizations. In such an environment, organizations may have long-standing cooperative agreements representing the existing services that they share. Furthermore, they may have service level agreements (SLAs) that assure the quality of service standards of the existing services. In an ad-hoc workflow scenario, a business may need to perform real-time composition of the existing services in response to consumer requests. In this work, we suggest that, in parallel to traditional Web service composition, the business must also compose the corresponding SLAs of the chosen services to generate a guaranteed service level to the consumer. In this paper, we introduce a process for composing SLAs associated with a workflow of Web services. This process results in the discovery of the best composite (i.e. workflow) capability while optimizing the underlying service-level attributes."
3856,"Service-oriented enterprise platforms are increasingly called to support sense-and-respond capabilities in several application domains. In this context, Complex Event Processing is considered as a promising asset, as it enables to effectively extract meaningful events from raw data streams originated by sensing infrastructures, for enterprise processes and applications consumption. This paper proposes a novel CEP engine conceived with extensibility, interoperability, modularity and scalability requirements in mind. More specifically, we propose a Lightweight Stage-based Event Processor (LiSEP), based on a layered architectural design. Thanks to the adoption of Stage-Event Driven Architecture principles, core event processing logic is decoupled from low-level thread management issues. This results in an easy-to-understand and extensible implementation while testing results show performance scalability. We also report on the development of an ongoing case study on dangerous goods monitoring."
3857,"Dependability is an important consideration during the design and development of IT systems and services. But in service computing systems, the traditional definition and evaluation methods from the systems' and components' point of view meet challenges. In this paper, we change the angle of view, and study the dependability and their attributes from the service-oriented perspective. A stochastic model using semi-Markov process is put forward, and the quantitative analysis of the dependability attributes is carried out. By extending and transforming this model, the mean time to dependability attributes failure is computed. Based on the analysis and calculations, some theorems are proposed and proved, to show the interrelationships and comparisons of the different dependability attributes."
3858,"Resource usage is important input for a number of IT processes including those related to: financial management, capacity planning; service level management; and configuration management. Accounting systems in traditional UNIX/Linux environments generate a usage record at the end of each process. Administrators periodically use standard accounting system utilities to summarize resource usage using the user and application data recorded with each of these records. Recent advances in dynamic LPAR technologies and service oriented architectures have placed new requirements on the metering of resource usage. In a Dynamic LPAR environment, allocations must be measured and processed to obtain accurate information on resource usage. Transactions performed using service oriented architectures deployed on heterogeneous, distributed systems introduce new challenges for metering resource usage based on transaction classes. In this paper we present the design and implementation of a J2EE-based accounting service which exposes manageability and reporting interfaces for the AIX advanced accounting system. The reporting interface can be used to generate process-based, LPAR-based and transaction-based usage reports."
3859,"Web services, as loosely-coupled software systems, are increasingly being published to the web and there are a large number of services with similar functions. Therefore, service users compare the non-functional properties of services, e.g., Quality of Service (QoS), when they make service selection. This paper aims at generating a more comprehensive web service recommendation to users with a novel approach to fulfill more accurate prediction of unknown services' QoS values. We accomplish the QoS prediction by using fuzzy clustering method with calculating the users' similarity. Our approach improves the prediction accuracy and this is confirmed by comparing experiments with other methods. In addition, the quality of web services is considered as a multi-dimensional object, and each dimension is one aspect of the web service's non-functional properties. We also provide an application example to demonstrate how to utilize our approach to rank services by a score function and map multi-dimensional QoS properties into a single dimensional value."
3860,"Quality of service (QoS) plays a key role in Web services. In an open and volatile environment, a provider may not deliver the QoS it declared. Hence, it's necessary to provide a QoS assessment model to determine the likely behavior of a provider. Although many researches have been done to develop models and techniques to assist users in QoS assessment, most of them ignore various QoS requirements of users, which are great important to evaluate a provider adopting the policy based on service differentiation. In this paper, we propose an approach, called Bayesian network based QoS assessment model, to QoS assessment. Through online learning, it supports to update the corresponding Bayesian network dynamically. The salient feature of this model is that it can correctly predict the provider's capability in various combinations of users' QoS requirements, especially to the provider with different service levels. Experimental results show that the proposed QoS assessment model is effective."
3861,"With dispersing of information on social networks - both personally identifiable and general - comes the risk of these information falling into wrong hands. Users are burdened with setting privacy of multiple social networks, each with growing number of privacy settings. Exponential growth of applications (App) running on social networks have made privacy control increasingly difficult. This necessitates Privacy as a service model, especially for social networks, to handle privacy across multiple applications and platforms. Privacy aware information dispersal involves knowing who is receiving what information of ours. Our proposed service employs a supervised learning model to assist user in spotting unintended audience for a post. Different from previous work, we combine both Tie-strength and Context of the information as features in learning. Our evaluation using several classification techniques shows that the proposed method is effective and better than methods using either only Tie-strength or only Context of the information for classification."
3862,"As Cloud Computing has become more and more popular, various Cloud Computing architectures or infrastructure have been defined, given their specific circumstances for the applications. However, to effectively achieve the potential of cloud computing, there is need for the definition of system architecture of the software systems involved in the delivery of cloud computing, so that it can be used as a reference for the architects or software engineering. In this paper, reference architecture of Cloud Computing is proposed. Its objective and principles are illustrated. And case studies of a SaaS, PaaS platform architecture instantiated from CCRA are given."
3863,"Services are self-contained and platform independent software components that aim at maximizing software reuse. The automated composition of services to a target software artifact has been tackled with many AI techniques, but existing approaches make unreasonably strong assumptions such as a predefined data flow, are limited to tiny problem sizes, ignore non-functional properties, or assume offline service repositories. This paper presents an algorithm that automatically composes services without making such assumptions. We employ a backward search algorithm that starts from an empty composition and prep ends service calls to already discovered candidates until a solution is found. Available services are determined during the search process. We implemented our algorithm, performed an experimental evaluation, and compared it to other approaches."
3864,"In the near future, Industrial Internet of Things can provide various useful spatial information in urban areas. Based on real time resource discovery and data retrieval technologies, mobile devices can continuously interact with surrounding things and provide real time content mash up services to their users. One challenge involved in such a scenario falls in the resource management. Continuous resource discovery and content mash up processes can be resource intensive for common handheld mobile devices. In order to reduce the resource usage, certain tasks of mobile application can be offloaded to Utility Cloud services. However, the task offloading process needs to be context-aware. In certain cases, performing tasks in mobile devices is more cost-efficient. This paper proposes a service-oriented workflow based mobile Cloud middleware framework for balancing the task allocation between the mobile terminal and utility Cloud service. The proposed cost-performance index scheme assists workflow configuration decision-making based on fuzzy set and weight of context schemes. The prototype has been implemented in real mobile devices and the evaluation has shown that the workflow system can automatically configure the task allocation based on resource availabilities."
3865,"Services can be characterized as activities in which providers and customers co-create value. The need for a tight collaboration between providers and customers is thus an important distinguishing aspect of services compared to products. Traditional software engineering methodologies provide limited support for collaborative processes at runtime and are therefore not directly applicable to service engineering. In particular, the design of coordination mechanisms between service provider and customers as well as the design of a common vocabulary in an interorganizational setting is not adequately addressed. To face these shortcomings, we present an interdisciplinary approach integrating just recently proposed Web service engineering methodologies with market and ontology engineering to one coherent service engineering methodology."
3866,"Service Oriented Architecture (SOA) is a recently emerged enterprise architecture promising to perform business processes efficiently and effectively. Service Providers and their customers negotiate utility based Service Level Agreements (SLA) to determine costs and penalties based on the achieved performance levels. In this work, we propose to enhance the flexibility of the WS-Agreement specification, a Web Service protocol to establish agreements on the QoS level to be guaranteed in the provision of a service. This protocol is generally aimed to be a ""one-shot"" interaction, i.e., the WS-Agreement is created based on a creation offer; the created agreement can not be modified and is effective until all activities pertaining to the agreement are finished or until one of the signing party decides to terminate it. Our main contribution provides for the integration of new functionality to the protocol that enable the parties of a WS-Agreement to re-negotiate and modify its terms during the service provision."
3867,"The paper presents an extensible resource encapsulation framework to encapsulate manufacturing resources into services following web service resource framework (WSRF) specification, which can make them offer their services and functionality in a standard web service environment. The encapsulation framework has many advantages such as extensible, plug-and-play deployment, automatic encapsulating and manageable. The design principle of the resource encapsulation framework is introduced in the paper. We developed the prototype of resource container for legacy binary codes, which can cast legacy binary codes into web services."
3868,"The MIDAS system that we have developed is an automated supply chain management system based on the service oriented architecture and Web services. MIDAS provides a loosely-coupled distributed environment that allows customers, manufacturers, and suppliers to cooperate over the Internet and World Wide Web. It aims to reduce inventory carrying costs and logistics administration costs, yielding a more efficient supply chain, by supporting just-in-time manufacturing. It eliminates, or substantially reduces, human intervention on both the customer/manufacturer side and the manufacturer/supplier side. It allows a manufacturer to select, dynamically from the MIDAS registry, suppliers of components, based on the price, availability, and delivery time of those components. A manufacturer can use one of several strategies to aggregate customers' orders before it processes them and to accumulate suppliers' quotes before it decides on a particular supplier. The use of a service oriented architecture, such as MIDAS, can substantially improve the efficiency of a supply chain"
3869,"Recently, there has been an increasing need in scientific workflows to solve the shimming problem, the use of a special kind of adaptors, called shims, to link related but incompatible workflow tasks. However, existing techniques produce scientific workflows that are cluttered with many visible shims, which distract a scientist's focus on functional components. Moreover, these techniques do not address a new type of shimming problem that occurs due to the incompatibility between the ports of a task and the inputs/outputs of its internal task component. To address these issues, (1) we propose a task template model which encapsulates the composition and mapping of shims and functional task component within a task interface; (2) we design an XML based task specification language, called TSL, to realize the proposed task template model; (3) we propose a service oriented architecture for task management to enable the distributed execution of shims and functional components; and (4) we implement the proposed model, language and architecture and present a case study to validate them. Our technique uniquely addresses both types of shimming problems. To our best knowledge, this is the first shimming technique that makes shims invisible at the workflow level, resulting in scientific workflows that are more elegant and readable."
3870,"Service-oriented architecture (SOA) provides a considerably new of invoking functionality. Service providers are to develop and deploy reusable services, and service consumers are to discover right services. A main difficulty in developing generic services is to design services generic enough so that the services can be effectively adapted to diverse service consumers. Moreover, this adaptation takes place at runtime, and hence dynamic adaptation and compositions are essential in service-oriented computing (SOC). For this, service variability among different consumers and contexts should be analyzed and designed into service components. In this dissertation, a service-oriented analysis and design (SOAD) process is presented, which focuses on modeling service variability and mismatch, engineering adaptability into service components and adapters, and enabling dynamic composition of services."
3871,"The article tackles the problem of conformance checking of communicating resource systems, such as hierarchical distributed systems, Restful Web services, ROA systems, etc. We present a framework, consisting of methods and algorithms, which allows to check whether a system's behavior, as derived from logs, conforms to its ideal model (derived from APIs and specifications). We define several system properties and present how they can be verified using our approach. To express the model formally, as well as minimize representational bias, we introduce RAs process calculus, a formal language specifically designed to model communicating resource systems."
3872,"BSPL, the Blindingly Simple Protocol Language, is a recent approach for declaratively expressing service communication protocols that involves only two main constructs: a way to specify a message as an atomic protocol and a way to compose protocols. BSPL supports the Local State Transfer architectural style for decentralized service enactment. BSPL offers significant gains in expressing protocols (i.e., specifications) that decouple participants in service engagements (i.e., agents) as much as possible given the causal constraints induced from the information exchanged by them. Importantly, BSPL relies exclusively on how appropriate information flows are induced from the specification. This paper proposes Bliss, a conceptual model for interaction that is based on information flow. The idea behind Bliss is to incrementally develop the information needed to complete the social object that a protocol computes. Bliss yields simple steps to help ensure that the resulting protocol adequately captures the given requirements with respect to the social object."
3873,"Automated service composition has been investigated thoroughly during the last years. Although it promises to alleviate the difficulties of manual service composition, it will only work if complete and correct service specifications are available. In this paper, we present a third approach - semi-automated composition - based on three mixed initiative features which we have derived from an industry case study with SAP. These features, filter inappropriate services, check validity, and suggest partial plans, are definded on the basis of a common formal model. Related approaches, in contrast, are limited to supporting individual mixed initiative features. To show the applicability of our approach, we have developed a prototypical implementation. Our results show that our mixed initiative approach significantly eases the modeling of service compositions."
3874,"CCLRC is involved in the development of grid and data management tools for a number of e-Science projects in the UK including the Natural Environment Research Council (NERC) funded ""Environment from the Molecular Level"". CCLRC's Web services based multidisciplinary data portal, uses an XML metadata model of scientific data to explore and access the content of data resources within CCLRC""s main laboratories in the UK and other facilities in Europe. Last year CCLRC adapted the data portal for the project so that earth scientists could store and access their own metadata and datasets, and simultaneously access related metadata and datasets from other facilities around the world. To achieve this the data portal was redeveloped using Web Service technology so that the internal services could be accessed via any user interface or system specific to the e-science project community. Previously, access was provided only via a standard Web browser."
3875,"Online support centers are emerging as a cost-effective and innovative solution designed to enable end-users to resolve technical problems more effectively without relying on live support from contact center agents. However, the capacity limitation of corporate knowledge bases prevents online support centers from effectively resolving user problems. In addition, traditional textual search techniques employed by most online support centers fall short from accurately interpreting user queries due to the ambiguity of user requests and the heterogeneity of technical problems. In this paper, we present SolutionFinder, an autonomous framework, which dynamically integrates online resources to enrich the knowledge base for IT support systems. SolutionFinder provides context-aware search support to remove the textual ambiguity embedded in user queries. Furthermore, SolutionFinder transforms solution documents into solution paths to analyze their similarity to provide high-quality solution recommendations. Evaluation results suggest by leveraging our proposed algorithms, a support service can accurately locate Web solution resources and provide high-quality services."
3876,"Event-driven Process Chains (EPC) has been widely adopted as a business process modeling notation. The common practice is to use a software tool to build a directed graph consisting of EPC constructs to model a business process. If a change is required, the constructed graph is partially dismantled and reconstructed. While the graph-based representation is beneficial in providing visualization, we investigate in this paper the advantages of having a textual representation alongside the graph representation. We introduce a textual language, called EPClets, to represent an EPC graph as a set of declarative event-action rules. The EPC graph can be constructed incrementally and automatically from the textual representation, separating the business-process specification and graph (re)construction concerns. The advantages of our approach have been evaluated through a controlled experiment. The experimental results suggest that having a textual representation alongside the graph representation increases the efficiency compared to an entirely graph-based approach."
3877,"Real-world networks often contain communities with pervasive overlaps such that nodes simultaneously belong to several groups. Community extraction, emerging in recent years, is considered to be a promising solution for finding meaningful communities from social networks. In this paper, we explore overlapping community extraction from a link partitioning perspective. First, we define the local link structure composed of a set of closely interrelated links, by extending the similarity of link-pairs to that of a group of links. Second, based upon our prior work, we transform the problem of mining local link structures into a pattern mining problem, and thus present an efficient mining algorithm. Third, we propose to use the hypergraph to assemble all local link structures, and employ hMETIS for hypergraph partitioning. Finally, based on extracted link communities, we restore the membership of nodes in the original graph owing to its links. Experimental results on various real-life social networks validate the effectiveness of the proposed method."
3878,"Workflow is one of vital facilities in virtual laboratory for complex scientific computation. However, where to deploy a workflow service in a virtual laboratory is a challenging software architectural decision to make with tradeoff of requirements and resources. This paper presents an architectural evaluation for deploying a scientific workflow service in a collaborative environment. We evaluate three deployment options by comparing them against common scientific virtual laboratory requirements. While this case study is for a particular collaboration platform and application domain, the evaluation method, process and conclusion can be useful and reused in other service deployment decision making."
3879,"Heterogeneity and dynamicity of pervasive, service-based environments require the construction of flexible multimodal interfaces at run time. In this paper, we present how we use an autonomic approach to build and maintain adaptable input multimodal interfaces in smart building environments. We have developed an autonomic solution relying on interaction models specified by interaction designers and mediation components implemented by software developers. An interaction model is built around the notions of abstract device and abstract applications. The role of the autonomic manager is to build complete interaction techniques based on runtime conditions and in conformance with the predicted models."
3880,"Location-aware devices and cloud computing services have triggered the widespread development of location-based applications. However, such applications require users to disclose their actual locations, which raises serious privacy concerns. As a kind of spatial transformation method, the standard Hilbert curve (SHC) is widely used in privacy preserving for spatial data. But there are still many limitations for SHC, such as high CPU cost, using the unified partition granularity and inability to support partial region authorization, etc. In this paper, we present a density-based space filling curve (DSC) for spatial data transformation, which partition the space based on the density of points of interest (POI) and supports partial authorization of the space, and need relatively low CPU cost. Our experiments show that the security risk of DSC is lower than that of SHC, and DSC provides better index building and range query processing performance than SHC."
3881,"We propose a novel market-based approach for dynamic composite service selection based on combinatorial auctions. The combinatorial auction model that we developed allows us to incorporate service providers' and requesters' preferences in the service selection process. From the providers' perspective, the combinatorial formulation allows them to express their preferences for offering combinations of services, or bundles. Moreover, the combinatorial model has the potential to lower the overall cost to the service requester as a result of providers offering discounts for service bundles. The proposed model also enables the service requester to express their preferences for the types of bundles by defining constraints over the configuration of the composite service provisioning, and data-cohesion of the bundles. We have mapped the problem to an Integer Linear Programming formulation and performed a number of experiments to evaluate the proposed model. In addition to demonstrating the relevance and applicability of combinatorial auction models for service selection, our experiments show that the cost of the composite service provisioning decreases with having more bidders in the auction, offering more crowded bundles is more profitable for service providers, and achieving high cohesion is more expensive than low cohesion for service requesters."
3882,"The development of Cloud Computing drives the change of value creation towards services and creates a new networked economic structure for challenging organizations to adapt their existing business models. Despite agreement on that the transformation of business model is importance to an organization's success, the evolution of business model enabled by Cloud is still fuzzy and vague within many research fields including both Management Strategy Theory and Information Systems. This paper, following a review of existing works and a series of case studies from several typical industries, employs a deductive reasoning method to investigate the influence of cloud computing on business model evolution and presents a specific service-centric business model. First, the evolution of business model in cloud environment is developed. Second, by analyzing business model innovations on business model nine building blocks in cloud environment, the element of service-centric business model is defined. Then, an ontological framework which includes value, service, and business model dimension for service-centric innovative business model is proposed. Finally, several industrial cases validate that this paper provides a comprehensive framework of organization's business model which can properly reveals an organization's complex nature of business logic and operation management in cloud environment."
3883,"Service aggregation is becoming a cost-effective and time-efficient way for a business to develop new applications and services. While it creates tremendous opportunities in various industry sectors, its cross-organization nature raises serious challenges in the security domains for authentication. In this paper we formulate a formal definition of authentication in service aggregation and a security model for it, and propose two authentication protocols. One is a one-way protocol and another is an interactive one. In particular, the constructed authentication tokens are anonymous to verifiers. We prove their security, show how to choose optimal system parameters, and analyse the efficiency."
3884,"As the world of Web services is dynamic and heterogeneous, a requester often needs to invoke an unfamiliar Web service at run time. However, current Web services technology pays little attention to this issue. In this paper, we propose a requester-based mediation framework for a requester to dynamically invoke Web services. The framework can increase the flexibility and reliability of Web services invocation in a truly dynamic, heterogeneous environment"
3885,"This paper presents an empirical approach for microservice automated testing. With the rise of the agile methodology, automated testing has gained momentum in software development, including using microservices as an architectural style. However, the tests are not always related to the core specifications of the system being developed. In this paper, we discuss an approach to derive the tests, especially the acceptance tests, from the specifications of the systems. To avoid any ambiguity in the specifications, we focus on the formal specifications of the system. To this end, we introduce intelligent agents as a conceptual unit to encapsulate the formal specifications of services. Indeed, a comparison of microservice tenets and the general characterization of agents reveals that both can be thought of as autonomous software entities, driven by goals and evolving within a distributed environment and communicating with one another. Using a real-world application we show how agent formal specifications can be linked to microservice automated testing."
3886,"With the increased awareness of security and safety of services in on-demand distributed service provisioning (such as the recent adoption of Cloud infrastructures), certification and compliance checking of services is becoming a key element for service engineering. Existing certification techniques tend to support mainly design-time checking of service properties and tend not to support the run-time monitoring and progressive certification in the service execution environment. In this paper we discuss an approach which provides both design-time and runtime behavioural compliance checking for a services architecture, through enabling a progressive event-driven model-checking technique. Providing an integrated approach to certification and compliance is a challenge however using analysis and monitoring techniques we present such an approach for on-going compliance checking."
3887,"Grid computing has been touted as an emerging revolution that transforms the enterprise information technology landscape by enabling organizations to share access to computing systems and data across organizational boundaries. The promised benefits include increased utilization of otherwise idle computing systems, and sharing data between organizations in a seamless but secure fashion. If these promises are fulfilled, grids can enable business partners to create virtual organizations with unique competitive advantages that are difficult to imitate. However, there are a number of technical and business challenges that must be overcome before the benefits of grid computing can be realized. In particular, this paper analyzes how contractual issues and government regulations can impact grid computing projects."
3888,"This paper describes a rule-based framework for business activity management called RuleBAM. It is a novel framework whose objective is to support the requirements of dynamic monitoring and control of business applications using policies and rules. RuleBAM is composed of a customized assemblage of built-time and run-time technologies that include business rules and application modeling tools, code generators, transformation services, rules engines, business integration adaptors and Web services. Business activity management (BAM) polices are used to define the requirements of RuleBAM systems and business rules are exploited as execution platform for BAM policies. Taken as a whole, these form a new method of using business rules to enable business activity management. A case study is also presented in this paper."
3889,"Discovering relations among web services has been a fruitful research topic in services computing. Such relations are useful for service discovery, selection, composition, etc. The much information about web services is available, the better/more relations can be discovered. Based on the technical information provided in WSDL files, simple relations can be discovered. Finding more comprehensive relations requires additional information, such as semantic descriptions, information about service consumers, or service compositions. Rich semantic service descriptions are not so common in practice. Experiments have shown that similar users do not necessarily use the same web services. Using service compositions gives good results, however, researchers assume that they have access to such compositions, which is not the typical case in practice. Additionally, using service compositions is not sufficient to determine types and strengths of such relations. In this work, we propose a novel approach to discover relations among web services in the form of linkage patterns based on the configurations of business processes that use them. We specify types and weights of the discovered linkage patterns based on control flow patterns in business processes. We have implemented this approach using Oryx, a process modeling tool and repository, and Depot, a service registry, and validated it through real-world examples, as we show in this paper."
3890,"The ability to react quickly to unpredictable changes in the environment is a key requirement in pervasive computing. This paper presents fANFARE, a framework for the autonomic management of service-oriented applications in pervasive environments. Specifically, it focuses on the configuration and optimization of pervasive applications deployed on OSGi platforms. We propose to handle runtime administration through a hierarchy of autonomic managers, that is a platform manager and a number of application managers and dependency managers. Our approach has been implemented and validated on pervasive use cases within the MEDICAL project funded by the French Ministry of Industry."
3891,"Web services are becoming widely deployed to implement the automation of business processes such as supply chain management, inventory tracking, and healthcare management, just to name a few. A Web service is a new breed of web application that supports interoperable application-to-application interaction over a network based on a set of XML standards. This new architecture and new set of protocols brings a new set of security challenges such as confidentiality, integrity, anonymity, authentication, authorization and availability. As security has become an essential component for all information systems, several security solutions for Web services data have been proposed such as WS-Security, SAML and XACML. To enable privacy protection for Web service consumers across multiple domains and services, the World Wide Web Consortium (W3C) published a document called ""Web Services Architecture (WSA) Requirements"" that defines some specific privacy requirements for Web services as a future research topic."
3892,"In mobile environments, packet transmission services suffer from packet losses due to e.g. inadequate received signal quality, or forced by protocols in the signaling domain of the infrastructure of a mobile communications network. To reduce the occurrence of packet losses and, hence, to improve the quality of packet transmission services, such as e.g. the short message service (SMS) deployed in GSM (Global System for Mobile Communications) based mobile communications networks, a quantitative analysis of the quality of service (QoS) in the signaling domain is mandatory. For this reason, appropriate QoS parameters are needed. In this communication, the authors propose such QoS parameters and apply them to the SMS in GSM networks. Furthermore, a system framework for QoS monitoring, alerting and reconfiguring an SMS Center is presented. It operates near real-time and, therefore, helps to maintain a high QoS level. Selected monitoring results gathered during real world network operation are presented."
3893,"Dynamic composition is a key feature of service-oriented computing (SOC) where services are discovered and composed at runtime. Current standards and methods for service compositions require statically binding published WSDL interfaces into compositions in BPEL. In this paper, we present our early result of a framework for dynamic composition on enterprise service bus (ESB). We present the design and implementation of Dynamic Composition Handler."
3894,"Typical approaches to service composition seek to realize a goal service specification, described using a labeled transition system (LTS) provided by a service developer, by constructing a structurally equivalent LTS using a set of available component services (also described using LTSs) that match the input and output requirements ofthe transitions. As such, existing composition approaches fail to realize the goal LTS whenever available component service LTSs cannot be used to ""mimic"" the structure of the goal LTS. This failure requires that the service developer formulates an alternate goal LTS and re-iterates the composition step. However, the process of manual reformulation of the goal LTS is both laborious and error prone. In this setting, we describe an efficient data structure and algorithms for analyzing data and control flow dependencies implicit in a user-supplied goal LTS specification to automatically generate alternate LTS specifications that capture the same overall functionality with respect to the data and control dependencies, and determine whether any of the alternatives can lead to a feasible composition. The result is a significant reduction in the need for the tedious manual intervention in reformulating LTS specifications of the goal service."
3895,"The paper describes a service oriented knowledge-based architecture to support supply chain application. The proposed architecture has the capability to meet the on- demand requirements of dynamic supply chains. Business requirements and potential benefits associated with our solution have been discussed. The research has led to an intelligent SOA application that is responsive to e- business requirements specifically in the supply chain sector. The design and implementation details of the SOA solution are given in details, which is supported through a software prototype."
3896,"Mobile handheld devices such as PDAs and smartphones are increasingly being used by service providers to deliver application functionality similar to that found in traditional desktop computing environments. However, these handheld applications can be quite slow and often lack important functionality compared to their desktop counterparts. We have developed PASSPORT, (PDA application streaming service portal) a thin-client solution that leverages more powerful servers to run full-function desktop applications and then simply stream screen updates to the PDA for display. PASSPORT uses server-side screen scaling to provide high-fidelity display and seamless mobility across a broad range of different clients and screen sizes, including both portrait and landscape viewing modes. PASSPORT also leverages existing PDA control buttons to improve system usability and maximize available screen resolution for application display. We have implemented PASSPORT on Windows PDAs and demonstrate that it can provide superior application performance and functionality compared to the traditional approach of running applications directly on handheld devices"
3897,"In workflow systems or other applications, users need more semantic information. Ontologies and semantic Web are important technologies for managing semantic information. Our work is focused on semantic spatial information in distributed workflow. The spatial relations was described by Description Logic, the ontologies was then constructed using the RDF Schemas and OWL Capabilities. These theories and methods are enclosed in a middleware, and the middleware was embedded in workflow system of the government in Jilin Province."
3898,"In spite of many advances in business process modeling and simulation technologies, their adoption by the business analyst community has been primarily limited to specialists. We propose a business process transformation wizard as a tool to bridge this gap. This enables users to easily analyze their performance using quantitative technologies by using a business process transformation patterns library to explore different transformation options and by showing the minimum and most commonly required information that can drive transformation initiatives."
3899,"Local transportation services are an essential component of all modern urban infrastructures. The constantly growing requirements and expectations from several stakeholders, ranging from public authorities, service providers and customers, demand a deep rethinking of the entire system. Such services must be oriented towards sustainability and everyday living, thus needing significant Quality of Service (QoS) improvement in this sector. This paper aims at highlighting the main challenges for modeling and implementing the relationships between the stakeholders in the public transportation sector and proposes a systemic modeling approach that would serve to design, implement, and monitor a QoS-regulatory document. A taxonomy of quality indicators and service levels are proposed along with metrics on the basis of athe European and Italian regulatory background as well as of the public transportation service lifecycle model. The proposed approach has been validated by developing a service analytics system deployed at the regional transportation agency in the Apulia administrative region of Southern Italy."
3901,"Internet of Things (IoT) services are improving our life, supporting people in a variety of situations. However, due to the high volume of managed personal data, they can be a serious threat for individuals privacy. Users data are commonly gathered by devices scattered in the IoT, each of which sees a portion of them. The combination of different data may lead to infer users sensitive information. The distributed nature and the complexity of the IoT scenario cause users to lose the control on how their data are handled. In this paper, we start addressing this issue with a framework that empowers users to better control data management within IoT ecosystems. A novel privacy reference model allows users to state how their data can be processed and what cannot be inferred from them, and a dedicated mechanism allows enforcing the stated references. Experimental results show the efficiency of the enforcement."
3902,"The evolution of wireless network technology has led to an increasing number of applications for mobile use. In addition, the paradigm of service-oriented architectures allows a composition of applications based on distributed web services. By using registries it becomes possible to choose the appropriate services at runtime. It needs to be considered that services for mobile use may have different scopes. For instance, routing or weather services may be dedicated to a specific region so that they may be useless if a user's area of interest does not match the service's scope. It is hence necessary to identify and use those services with a scope that corresponds to the relevant context such as a user's location. Moreover, if the context changes, it is desirable to automatically switch between different equivalent service instances to provide a user with a continuous connection to a desired service. We denote this mechanism as service roaming. This paper presents the major concepts needed for service roaming as well as a roaming model based on n-dimensional context spaces."
3903,"With the increasing adoption of Cloud Computing, Business Process as a Service (BPaaS) has recently emerged as the delivery of business process outsourcing services that are sourced from the cloud and constructed for multitenancy. BPaaS providers may look for available business processes from other cloud providers to improve their processes. However, today's BPaaS solutions lack an explicit and formal semantics which prevent an easy and dynamic interoperability between different cloud providers. In this paper, we propose firstly to semantically populate a shared knowledge base of process models deployed by BPaaS providers. Therefore, using this knowledge base we aim at finding for selected positions in a business process suitable process fragments for recommendation to assist process variant modeling. To do so, we define a process fragment as a neighborhood context graph, which captures order constraints between tasks and their neighbors. Thus, we compute similarity between fragments to select the most similar ones for recommendation. As a proof of concept, we provide a tool that allows process designers to retrieve similar process fragments that can be used to design new process variants. We also performed experiments on large public datasets and experimental results show that our approach is feasible and efficient."
3904,"The wide use of Web services and scientific workflows has enabled bioinformaticians to reuse experimental resources and streamline data processing. This paper presents a follow-up work of our network analysis on my Experiment, an online scientific workflow repository. The motivation comes from two common questions raised by bio-scientists: 1) Given the services that I plan to use, what are other services usually used together with them? and 2) Given two or more services I plan to use together, can I find an operation chain to connect them based on others' past usage? Aiming to provide a system-level GPS-like support to answer the two questions, we present Service Map, a network model established to study the best practice of service use. Two approaches are proposed over the Service Map: association rule mining and relation-aware, cross workflow searching. Both approaches were validated using the real-life data obtained from the my Experiment repository."
3905,"Quality attributes play a very relevant role in the service-oriented computing world, as they allow distinguishing between functionally equivalent services. In fact, these attributes impact various activities related to the life-cycle of service-based applications (SBAs), starting from service discovery and permeating other activities such as service level agreement establishment and monitoring. Considering their relevance, it is essential that these attributes are precisely defined. Moreover, as quality attributes are inherently dynamic, they must be continuously monitored. In this context, this paper proposes a meta-model that formally defines and connects service, quality and event domains. From one perspective, the connection between service and quality domains enables the design of models that formally specify applications' functional and non-functional requirements. From another perspective, connections between quality and event domains enable quality engineers to define how relevant quality attributes and metrics can be computed based on a set of runtime events. Conformant models can be interpreted at runtime by compatible platforms allowing them to dynamically (re)configure their monitoring mechanisms."
3906,"XML message filtering and routing have been recognized as a standard for data exchange for XML dissemination services. These services are often realized using the Publish/Subscribe (pub/sub) model. The Pub/sub model is commonly used by various web-based systems, such as Web content syndication, RSS feeds, location-based services. Conventional XML filtering and forwarding is an application-layer multicast approach that relies on XML-capable brokers. Those XML-capable brokers are built above the network layer using an overlay model for message dissemination. Such an overlay model introduces a great deal of overhead in terms of initial deployment and subsequent operational cost for those XML-capable brokers that typically are supported by Internet Service Providers (ISPs). In other words, those XML-capable brokers need to be set up across geographically distributed areas and may need to be deployed and maintained even by multiple ISPs or special providers. This paper presents a XML message dissemination system using both the conventional XML multicast model and the peer model executing on a cloud, that can be deployed rapidly without the need of any ISP or special arrangements of physical brokers across the networks. In addition, changes to software deployed in the cloud can be made directly and easily. The paper demonstrates experiments over the Amazon EC2 clouds spanning different geographical locations. In addition, the paper presents a performance comparison between the conventional XML multicast model and the peer model deployed on a cloud."
3907,"SlapOS is an open source grid operating system for distributed cloud computing based on the moto everything is a process. SlapOS combines grid computing and Enterprise Resource Modeling (ERP) to provide Infrastructure as a Service (IaaS), Platform as a Service (PaaS) and Software as a Service (SaaS) through a simple, unified API which one can learn in a matter of minutes. Thanks to its unified approach and modular architecture, SlapOS has been used as a research test bed to benchmark NoSQL databases and optimize process allocation over intercontinental Cloud. SlapOS opens new perspectives for research in the area of resilience and security on the Cloud."
3908,"We present a solution for test-based security certification of services that models the service under certification using a Symbolic Transition System (STS). The STS-based model is readily derivable from the Web Service Description Language (WSDL) and Web Service Conversation Language (WSCL) of the service, and can be enriched with details about test-based conditions on inputs and outputs, implementation details, and security specifications. In addition, we show how such fine-grained modeling can be included in a test-based security certification process. Finally, we discuss how this process can be integrated within the Web service life-cycle and used for matching users' preferences and comparing certificates of different services."
3909,"Inspired by service computing principles, in cloud manufacturing, manufacturers encapsulate their resources into consumable services that can be looked up and accessed over the Internet. Manufacturing ontologies are used to store the service information. Manufacturers use service rules to control how their resources can be accessed. The rules are normally written in natural language. Thus, they need to be converted to semantic rules that can be understood by the search engine of the manufacturing ontologies. Manually converting service rules to semantic rules is time-consuming and error-prone. This paper proposed an approach that automatically converts service rules to semantic rules. The proposed scheme classifies the semantics of typical service rules into several semantic categories. Natural language processing techniques are used to process the service rules to map the semantic meanings of the rules to the relevant semantic categories. Then, the identified semantic categories are converted to semantic rules. The evaluation of the scheme shows that the scheme achieves good conversion accuracy."
3910,"In this paper, we present a comprehensive study on how to achieve Byzantine fault tolerance for services with commutative operations. Recent research suggests that services may be implemented using Conflict-free Replicated Data Types (CRDTs) for highly efficient optimistic replication with the crash-fault model. We extend such studies by adopting the Byzantine fault model, which encompasses crash faults as well as malicious faults. We carefully analyze the threats towards the operations in a system constructed with CRDTs, and propose a lightweight solution to achieve Byzantine fault tolerance with low runtime overhead. We define a set of correctness properties for such systems and prove that the proposed Byzantine fault tolerance mechanisms guarantee these properties. Furthermore, we show that our mechanisms exhibit excellent performance with a proof-of-concept replicated shopping cart service constructed using CRDTs."
3911,"Engineering approach for transforming business process and workforce management is an emerging area that is gaining increasing interest for its many promising applications in service science. Yet, three major challenges stand out in engineering any business transformation process. The first challenge is accurate modeling of the transformation knowledge that may be scattered across multiple application domains. The second challenge is efficient and precise real-time evaluation based on the knowledge model. The third challenge will be to generate intelligent recommendations about business transformation that can significantly improve the business process and drive down opportunity costs. This paper proposes an integrated knowledge model-driven recommender system that effectively addresses all of the three abovementioned challenges of modeling, evaluating, and recommending. Using a real-world case study on warranty processing at a major automotive manufacturer, this paper presents a novel business transformation framework that consists of a knowledge model on business value drivers and metrics, an evaluation engine for processing real-time business events, and a recommendation engine that utilizes information obtained from the evaluation engine to suggest new processes and workforce allocation strategy, which can be subjected to a new cycle of modeling and evaluation to complete a feedback loop. Our experimental study using the real-world data results in a ""25/75"" rule in predictive warranty data processing: 25% of information contains 75% of business information entropy, thereby demonstrating the effectiveness of the system"
3912,"The competitiveness of the market place and the advent of on demand services computing are encouraging many organizations to improve their business efficiency and agility via business process management technologies. A lot of work has been done in process codification, tracking, and automation. However, a significant gap still remains between the way an organization's codified processes execute and the organization's business objectives such as maximizing profit with high-degree of customer satisfaction. This paper addresses this gap by proposing a process execution management (PEM) framework which enables continual optimization of workflow process executions based upon business value metrics such as SLA breach penalty, revenue, and customer satisfaction index. We have implemented the PEM framework based upon leading commercial products. We have also used the framework to develop two representative business performance management solutions for service quality management processes and application execution workflows. Our experimental results show that, when compared with a state-of-the-art commercial workflow product, our PEM system can reduce the loss of business value of a set of process execution requests by 67% on average."
3913,"Service-oriented architecture (SOA) is emerging as a very significant approach to enterprise computing. SOA is seen as a major impetus to providing agility and consumer centric approach to enterprise computing. Integral to the service oriented architecture approach is the concept of a ""service"", a service provider and a service consumer. The services in SOA context are realized by software or software in conjunction with business processes. It is believed that significant value can be derived by designing intelligent services. Benefit includes overall performance and flexibility. Intelligent services are those that are able to compose themselves based on certain rules and configurations, able to come into existence only when required and only for a required duration, and are prioritized. It is found out that these attributes make the services extremely useful, and enhance the overall performance of the SOA deployment"
3914,"Radio Frequency Identification (RFID), specifically the Electronic Product Code (EPC, the next generation of barcode), have improved the way the world works and lives. Basing EPC as an identification, GS1 developed standard framework for common visibility of supply chain data called EPCglobal. EPC information system(EPCIS) is one of the standards which encompasses interface for sharing and capturing data triggered by different events. Because the amount of data captured and stored in each event type is huge and the query interfaces are general, it is difficult to get traceability data easily in reasonable time. Even for some traceability queries like finding products at a location in certain time are too complex to fulfill with the current EPCIS standard implementation. In this paper, by extending EPCIS query interface we have proposed an efficient, well-organized, and scalable traceability architecture. In order to effectively collect and process events in real time using our stream processing, we first proposed an efficient pub/sub based EPCIS. Then, to retrieve information effectively we proposed an adequate column family based data modeling. Finally, we have implemented our system and developed a RESTfull service interface to query track and trace information. Experimental result shows that our traceability architecture with the column based data modeling process traceability-queries more efficiently and scales gracefully."
3915,"The emerging Web service technologies have received increasing attention in both industry and academic communities. The interoperability it promised has made it more welcome than other integration technologies. However, Web services may not be able to interact with one another directly due to interface mismatches. It happens when the service consumer and service provider have different message signatures. In this paper, we proposed an automatic interface adaptation approach that exploits ontology alignment tool. We propose a method that enables the interaction between WSDL described Web service and semantic Web services .We have also developed a method that extracts relevant semantic information from original annotation ontology to construct a compact ontology, in order to improve the efficiency and accuracy of ontology alignment results."
3916,"Due to the popularity of web services, similar services yet with different service-level characteristics are commonly available. How to consume these services intelligently has become a challenge. While network-based service selection has been proposed to manage the complexity in recent time, the degree of inter-service connectivity is assumed to be constant for efficiency boost. However, it does vary in the real world. In this paper, we propose dynamic QoS networks to cater for this property in service selection to further enhance service efficiency. Regularization levels are adjusted dynamically according to the variation of the degree of connections with minimal impact to computational performance. Based on historical QoS performances retrieved from service logs, dynamic QoS networks are constructed to reveal the relationships among services. Timely service selections are then guided by recently observed QoS of related services. The effectiveness of our proposed model is shown by experiments."
3917,"Web services have experienced great interest during the last years as they were expected to play a key role as enablers of seamless application-to-application integration both within company boundaries and on a global, cross-organizational scale. As a technical foundation for the realization of service-oriented architectures (SOAs), Web services encapsulate complexity inherent to individual applications and allow for their loose coupling. However, a truly global mesh of such services has not yet become reality due to various reasons. Novel technologies and design principles are currently about to emerge which allow human users to use, customize, combine, interconnect and finally expose Web-based content or functionality as new resources which are often referred to as mash-ups. In this article, we provide an overview of existing mash-ups as well as tools and platforms that empower users to build them in a highly efficient and intuitive fashion. Statistical data and case studies are leveraged to examine new ways of resource provision and consumption and also the relevance of upcoming intermediaries. Finally, we investigate remaining research challenges on the path to a truly global SOA."
3918,"This paper proposes Event-driven SOA as an extension of SOA for business process collaboration in Internet of Things (IoT). Compared with traditional SOA based service collaboration method, this approach will facilitate real-time event processing as well as decentralized and autonomous service coordination. We illustrate the collaboration process under the proposed architecture by applying it to a smart surgical management scenario."
3919,"Businesses need to adhere to certain regulations to remain compliant. They want to expand or move to a new geography and find itself subject to a slightly different set of regulations. Also, regulations themselves change over time and force the business to change its internal working to remain compliant. When a compliance officer is presented with a new regulatory document, he has to manually compare corresponding sentences between previous and the new version. While most studies in text mining have focused on measuring textual similarity, textual entailment detection and paraphrase identification etc., there has been very little focus on the problem of change tracking (CT). Change tracking can be defined as the task of identifying the phrase pair(s) that captures the semantic difference between two given sentences, and plays an important role in domains such as financial regulatory compliance where core changes introduced by regulators to existing regulations need to identified quickly. Naturally, the change tracking has to satisfy the minimality and comprehen-siveness requirements even in presence of complex language structure, context dependence and paraphrasing between com-pared sentences. In this paper, we address these challenges and devise a graph-based approach called DeepAntara1 and show its performance for change tracking task over multiple sentence pairs extracted from different versions of publicly available financial CRS treaties."
3920,"AutoSLAM (Automated SLA Management) is a policy-based framework for the automated establishment of Service Level Agreements (SLAs) in open, diverse and dynamic Service Oriented Architecture (SOA) environments. The novelty of our framework lies in the support for multiple SLA interaction models, giving service consumers and providers the flexibility to choose the one that is most appropriate in a given context, while simultaneously participating in multiple concurrent SLA interactions using different interaction models. As part of the framework, we present an overview of the reference architecture for the AutoSLAM middleware. We also present WS-SLAM, a domain-independent policy representation language that we have developed by extending the WS-Policy specification language. We validate our framework through a proof-of-concept prototype implementation for purchasing computing resources on Amazon EC2 under different contexts."
3921,In this paper we discuss a case study for the UK Police IT Organisation (PITO) on using a model-based approach to verifying Web service composition interactions for a coordinated service-oriented architecture. The move towards implementing Web service compositions by multiple interested parties as a form of distributed system architecture promotes the ability to support 1) early verification of service implementations against design specifications and 2) that compositions are built with compatible interfaces for differing scenarios in such a collaborative environment. The approach uses finite state machine representations of Web service orchestrations and distributed process interactions. The described approach is supported by an integrated tool environment for providing verification and validation results from checking designated properties of service models.
3922,"XML vocabularies like RSS (really simple syndication) or domain specific syntactic syndication frameworks allow for creation of raw data that can be understood by a wide range of consuming portals and Web sites. Often individual aggregators create content from such feeds however, every single aggregator needs to render UI content separately. Avoiding this via reuse requires creation of presentation-oriented services for dynamic generation of content from same base XML data. Web services for remote portlets (WSRP) provides standards based interfaces for creation of presentation oriented services using Web services technology.However, any WSRP based remote portlet needs to be refreshed every time when even a single data element changes in the portlet. Complementarity, technologies like Asynchronous JavaScript and XML (AJAX) also allow for content aggregation and asynchronous data retrieval. Unlike WSRP, in AJAX only the required control needs to be refreshed, whenever data changes. In AJAX too, every aggregator is required to include AJAX control accessing the same back end service leading to considerable replication of effort. These two technologies complement each other in terms of ease of sharing of aggregated content and better usability of such content. Overall, often there is a requirement to dynamically generate UI content from XML data, customize and share aggregated content with remote consumers and finally allow for partial updates of remote aggregated content for better usability. We take a holistic view of the problem statement and propose an end to end architectural approach that combines usage of WSRP, AJAX along with XML syndication feeds like RSS for creation of standards based, customizable, and dynamically generated reusable UI that has better interactivity, speed and usability."
3923,"With the development of Service-Oriented Technologies, the amount of Web services grows rapidly. Situation awareness, as a very important computing paradigm which can provide more reasonable and complete representation of the user's environment, can be benefit to the discovering of the service user's requirement. Therefore in this paper, we explore to use the situational information to recommend services which satisfy the requirement of users better and meanwhile improve the QoS prediction accuracy. Both the convergences between users' service selections in different situations and their QoS experiences have been taken into account. Moreover, we investigate the potential relationships between the situations structure and the services selection. Experiment results indicate that our method achieves ideal performance."
3924,"Choice is a pervasive feature of social life that profoundly affects us. Ranking results can be used as a reference to help people make a correct choice. But there are two problems. One problem is that fixed ranking results instead of the ranking methods are provided to people by service providers as a reference when making choice at most time. For example, TIMES World University Rankings can be used as a reference when choosing a college. However, in the numerous factors that affect objects ranking, people have their own understanding on the effect of each factor on objects ranking. Using mobile phone-selection as a practical case, some people think performance of a mobile phone is more important, while others hold the view that appearance of a mobile phone is more attractive. What's more, there are many ranking methods proposed, such as The Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) and expert marking. Using only one kind of ranking methods for object ranking may lead to over objective or subjective ranking results. Although various ranking algorithm are studied, very little is known about the detailed development and deployment of the ranking services. This paper proposes a comprehensive solution of Ranking as a Service (RaaS), with the manifold contributions: Firstly, we use combination weighting method in RaaS and it can overcome the defects of subjective and objective weighting methods. Secondly, we develop ranking service APIs that bring convenience to people when making choices. Thirdly, ranking service provides ranking results for people according to their own understanding on the effect of each factor on objects ranking. Fourthly, this paper is arguably the first one that proposes using Ranking as a Service. Finally, we evaluate and analyze the proposed strategies and technologies in accordance to the experimental results."
3925,"In this paper, we propose a novel approach for graph based web service matching. Our approach is inspired by spectral graph matching methods, in particular, by eigen-based projections. We introduce new mechanisms to perform the matchmaking at both structural and semantic levels. These mechanisms are based on algebraic graph techniques that make them run fast and thus suitable for large scale web service matching problems. Experimentation is provided to show the performance of the proposed approach."
3926,"Workflows coordinate the execution of multiple tasks or services. This paper presents a novel approach to model workflows based on partial synchronization. We develop new notions of weak correctness called semantic correctness and redundancy soundness that permit semantically correct execution instances with some partially finished paths, and show how to deal with them by the notion of transactional ""undo"". The properties of partial synchronization using strong and weak corresponding pairs are analyzed, and illustrated with realistic examples. An algorithm called Weak Verify is developed to check a workflow for weak correctness properties. This algorithm makes use of an existing algorithm for strict verification. Our design approach mimics how end users think about workflows in practice, and we contrast it with other notions of strict and weak correctness. We compare our approach with various other approaches."
3927,Service-oriented software systems (SOSS) are becoming the leading edge of software engineering. SOSS are virtual peer-to-peer networks of autonomous software components behaving like the services in real world mass service systems. As such service-oriented systems have no inherent tool to support business processes. The paper proposes an implementation of business processes in SOSS. The implementation can be improved if data stores can be integrated into SOSS and used together with the classical message passing. It indicates that service orientation is based on a paradigm different from the object-oriented one.
3928,"Globalization helps international companies create distinct competitive and operational advantages over centrally managed counterparts. Such advantages come from increased business opportunity and high profitability through outsourcing. To achieve the benefits of outsourcing, in this paper we describe a general model that characterizes globalized project productivity. We examine several factors related to global team performance including: 1) time zone differences, 2) language difference, 3) working time alignment (such as vacations and holidays), and 4) turnover rates among skilled workers in expanding economies. We then show how the models can be used to analyze project profitability through team performance, and how to address the combined impact of multiple globalization factors. Beyond this, we also provide suggestions on how to overcome those challenges. The key contribution of this paper is the development of models that quantify the impact of several global factors. In two cases, we show that the right organizational structure is required to achieve benefits of lower labor costs. In another case, due to the smaller difference in labor costs and the high complexity of the project, we show that the combined effects of the factors could lead to reduced profitability. It is these cases that reveal there is an opportunity to improve global delivery. Our objective is to show how to maximize the benefits of globalization and how an effective management approach can avoid some of the potential risks by deeply understanding the effects of global factors."
3929,"Although past research in finance showed that future investment returns are hardly predictable, predictions in volatility are found possible. Financial news is known to be making persuasive impacts on investor behavior, and hence injecting disturbances to markets. While there exists many prediction services, most of them do not factor in financial news directly. Previously proposed supervised topic model provides an avenue to associate volatility with news, yet it displays poor resolutions at extreme regions. To address this problem, we propose a novel extreme topic model to provide a better service for market alerts. By mapping extreme events into Poisson point processes, volatile regions are magnified to reveal their hidden volatility-topic relationships. Derivative approach is used to quantify the volatility of financial instruments. It captures immediate, collective responses from investors on market events, e.g., Brexit. By acquiring domain knowledge on how financial news influence investor behavior, accurate volatility predictions are made under extreme conditions as shown by the improved prediction accuracy in our experiments."
3930,"Service level agreements (SLAs) are commonly prepared and signed documents that form the contracts between a service provider and its customers, defining the obligations and liabilities of the parties. SLAs reflect the business needs of both customer and supplier as far as possible. SLAs are usually formed through either the adoption of a boiler plate agreement from the provider, or by negotiation between the parties. With the increasing adoption of software supply being implemented as a network service - software as a service (SaaS) - these methods are rigid or slow and costly. We propose a system that the parties can use to facilitate both fast and flexible agreements through the automation of SLA creation from a combination of service level objectives (SLOs) and business rules. We look at a means for generating these with a SLA-NM (SLA negotiation manager), complying with e-negotiation rules and creates agreements from existing business objectives."
3932,"Decentralized approaches for Web services discovery, such as peer-to-peer, become more and more attention - getting in the scientific community. In this paper, we propose a peer-to-peer framework, which adopts an enhanced skip graph named Servicelndex as the overlay network for service discovery. To guarantee discovery efficiency, Servicelndex schemed WSDL-S (Web services semantics) as semantic Web services description language and extracted its semantic attributes as indexing keys in skip graph. Also a multi-layer P2P overlay network was constructed to aggregate similar keys and keep load balancing on peer nodes. The evaluation showed that the Servicelndex system performs considerable service discovery efficiency."
3933,"Semantic Web technology is a promising first step for automated service discovery. Most current approaches for web service discovery cater to semantic web services, i.e., web services that have semantic tagged descriptions. It is difficult, however, to expect all new services to have semantic descriptions associated. Furthermore, the descriptions of the vast majority of already existing services do not have explicitly associated semantics. There are also severe restrictions on the prospective conversion of existing, non-semantic, descriptions, e.g., Web service description language (WSDL), to corresponding semantic descriptions, e.g., OWL-based Web service ontology (OWL-S). In this paper we present a novel approach for web service discovery that combines ontology linking with latent semantic indexing (LSI). The basic idea is to build the service request vector according to the domain ontology, have the training set of the LSI classifier based on features extracted from selected WSDL files, and finally project the description vectors and the request vector and utilize the cosine measure to determine similarities and to retrieve relevant WSDL service descriptions."
3934,"In most Service-Oriented frameworks service selection is based on provider-centric information despite well-known shortcomings (trustworthiness, incompleteness, subjectivity). We propose to allow consumers and third-party modules to extend information about providers and their services directly into the service registry. The knowledge is organized and shared via access controlled groups of interest. Services can be discovered using this knowledge. Our solution is integrated to OSGi and uses the decorator-pattern to make usage transparent."
3935,"One of the key activities that are needed to construct a quality service-oriented solution is the identification of its architectural elements with the right granularity. The selection of an appropriate method for identification of services from business models of an enterprise is thus quite crucial to the success of any service-oriented solution for that enterprise. Existing methods for service identification ignore required performance metrics and semantics integrity of business elements; more importantly, they focus on entity-based services while ignoring process based ones. This paper proposes a new process for identification and specification of enterprise software services and their architectural elements. A novel clustering technique, named elementary business process and business entity affinity analysis technique (EEAT), is introduced for identifying candidate architectural elements. This technique identifies each candidate service with the right granularity, while satisfying low coupling, high cohesion, and low reuse cost principles for reusable software services."
3936,"UML view integration has been extensively studied in the area of model transformation in Model Driven Development. Empirical processing rules are among the most widely employed approaches for processing view abstraction, which can support model simplification, consistency checking, and management complexity reduction. However, empirical rules face some challenges such as completeness validation, consistency among rules, and composition priority arrangement. The challenge of rule composition is amplified in the environment of distributed model driven development for web service-based systems where redundant information/data is emphasized. The same redundant information can be expressed in different forms that comprise various topological structures for entity relationship networks representing the same part of the system. Such variation will result in choosing different compositions of rules executed in different orders, which will increase the severity of non-determinism from the empirical probability of some rules. In this paper, we propose a formal solution for addressing this challenge through constructing finite-state automaton for unifying empirical abstraction rules while relieving the side effects caused by redundancy. We investigate the effect of redundancy on rules application through designing a simulated distributed storage for an example diagram model. We also show the results obtained from a prototype implementation."
3937,"The 
<em>Business Process Execution Language for Web Services WS-BPEL</em>
 provides an technology to aggregate encapsulated functionalities for defining high-value Web services. For a distributed application in a B2B interaction, the partners simply need to expose their provided functionality as BPEL processes and compose them. Verifying such distributed web service based systems has been a huge topic in the research community lately–cf. [4]for a good overview. However, in most of the work on analyzing properties of interacting Web Services, especially when backed by stateful implementations like WS-BPEL, the data flow present in the implementation is widely neglected, and the analysis focusses on control flow only. This might lead to false-positive analysis results when searching for design weaknesses and errors, e. g. analyzing the controllability [14] of a given BPEL process. In this paper, we present a method to extract dataflow information by constructing a CSSA representation and detecting data dependencies that effect communication behavior. Those discovered dependencies are used to construct a more precise formal model of the given BPEL process and hence to improve the quality of analysis results."
3938,"For a service to be used in a commercial setting, certain service delivery functions such as monitoring, billing, authentication, etc need to be packaged with the service. Because of the growing number of available services and the diversity in their offerings, predefined service delivery functions like that offered by existing service brokers (e.g. StrikeIron or Salesforce) cannot be easily adapted and plugged in with the core service from a third party provider. The need is to have a lightweight service packaging platform that offers service providers the ability to create reusable service packages by assembling existing service delivery functions or creating new ones. In this paper we introduce the notion of service packages and a pattern-based approach to service packaging. We highlight what is required from a platform to have such service packaging capabilities, and detail a prototype platform that we have developed to meet these requirements."
3939,"Application integrations employ interacting software components and services. Components - distributed, black-box, processing elements - may have distinct communication styles. Services can be used to augment the interaction of components to deliver on a common task. Unfortunately, there exist scenarios where components and services do not interact congenially. At issue is the service design that, even with standards in place, may ignore important aspects of component communication expectations. Thus, a complete integration is not achieved without implementing ad hoc glue code external to the design strategy. Introducing glue code, without design traceability, limits control, dynamism, and upgrades to components and services. We specify templates for framing glue code as integration completion functions (ICFs). Uniform designs are given using multiple modeling techniques to foster compilation of an ICF repository, where their instantiation can be plug-ins to application integrations."
3940,"Availability of high quality technical documentation is a critical enabler for users to fully take advantage of service capabilities, such as Cloud-based offerings where clients interact with the service via a portal and have no direct access to a service support. Technical writers are relying on the input from subject matter experts (SMEs) to create usable portal content. This disrupts service operation and reduces the productivity of the entire service offering team. While technical documentation may include contributions from a large distributed team, it often results in disconnected content with different writing styles. In this paper, we describe Scribe Crowd, a service for managing the process of composition of technical documents. Based on the principles of crowdsourcing, where a task may be outsources to many qualified agents, Scribe Crowd manages the distributed process of documentation and assigns incomplete pieces of write-up to appropriate experts. We present how Scribe Crowd has been used to engage over 120 business consultants to efficiently create content for 450 documents as part of an enterprise learning portal that serves information about banking, insurance and financial markets."
3941,"Automated service composition is critical for successfully implementing Building Automation Systems which facilitate Web services over physical devices. In this paper, we propose a new service composition model using a policy ontology represented by Semantic Web languages. Our proposed model can be a powerful tool to automatically compose services with associating meaningful policies."
3942,"Finding relevant files in a personal file system continues to be a challenge. It is still easier to find stuff on the Web with its exponential growth than in one's personal file system. Yet, the exponential growth of personal data renders the current services of personal file systems increasingly inadequate. A reason for this failure is the ldquocold-startrdquo problem: algorithms that dramatically improve a user's ability to find documents on the Web become ineffective in personal file systems because there is not enough information about these documents. We propose JabberWocky, a service that allows users to manage the content of their personal file system by leveraging semantic relationships available on the Web. More specifically, JabberWocky is using keyword/resource associations of social bookmarking web sites as a basis for recommending keywords for files. We chose social bookmarking web sites because of their popularity and because the assignment of keywords (a process also referred to as ldquotaggingrdquo) is an established and popular way to manage photos, music, movies, and audio resources on the Web - very much the kind of resources that need to be managed in personal file systems. The goal of JabberWocky is to overcome the ldquocold-startrdquo problem of personal file systems and to provide recommendations in a scalable way while maintaining the user's privacy. In this work-in-progress report we describe the motivation and challenges of designing a system like JabberWocky, present the initial design of an on-going user study, and briefly discuss what we have learned so far."
3943,"Process model improvement is one of the most important challenges to deal with in the field of evolving process-aware information systems. In this paper, we present, PRV, an approach to process model refactoring supporting automatic process model improvement, which enables PAIS to adapt to the varying business requirements. First, a set of change operations preserving soundness is defined to avoid complex verification. Second, an algorithm for process model refactoring based on variants is designed to restructure optimal models automatically. Finally, an extensive set of simulations are performed to show the feasibility and effectiveness of the proposed PRV algorithm."
3944,"Advent of 4G networks, IPV6 and increased number of subscribers to these, has triggered many free applications that are easy to install on smart mobile devices, a primary computing device for many. The free application markets are sustainable as revenue model for most of these service providers is through profiling of users and pushing of the advertisements to the users. This imposes a serious threat to user's privacy. Most of the existing solutions starve the developers of their revenue by falsifying / altering the information of the users. In this paper, we attempt to bridge this gap by extending our integrated Context Cloaking Privacy Protection framework (CLOPRO) that achieves identity privacy, location privacy, and query privacy without depriving the service provider of sustainable revenue generated through the use of the Context Aware Privacy Preserving Advertising (CAPPA). The CLOPRO framework has been shown to provide privacy to the user while using location based services. In this paper we demonstrate how this framework can be extended to deliver the advertisements / coupons based on users interests, specified at the time of registration, and the current context of the user without revealing these details to the service provider. The original service requests of the registered users are modified by the CLOPRO framework using concepts of clustering and abstraction. The results are filtered to deliver the relevant information to the user. Since the advertisements received are relevant to the user, the click rate is likely to increase ensuring increased revenue for service provider. The proposed framework has O(n) complexity."
3945,"The chances of winning highly valued Information Technology (IT) service contracts are influenced by various factors. Identifying key factors driving the competition and the early prediction of the outcome (either winning or losing such sales opportunities) can have significant business benefits. Given the complexity of IT services, range of potential attributes, and scarcity of comparable data sets, the straightforward approach of developing predictive analytical models that works well in other industries, such as consumer products, tends to achieve lower accuracy in this context. In this paper, we develop an approach that uses business insights and domain knowledge in the classification of several of the attributes influencing the outcome. We show how using this approach in a naïve Bayes predictive analytics framework can vastly improve the prediction accuracy. Further, we discuss two applications of our model, early prioritization of newly validated sales opportunities and optimization of sales force allocation and planning."
3946,"Existing registries organize functionally similar services into groups without considering past service-usage from the consumers' perspective, a.k.a. pragmatics. Pragmatics can help registries to calculate service similarity more effectively and improve organization schemes. However, pragmatics are not available beforehand and their highly accumulated number over time creates time and space efficiency challenges. To be responsive to dynamically arrived pragmatics, we propose a self-adaptive service-organization approach that follows an iterative life-cycle for autonomously evolving organization schemes. To face the emerging efficiency challenges, our approach adopts a greedy online algorithm for the evolution of organization schemes that considers and stores only the top-k pragmatics of each service. We evaluate our approach on benchmark services and the results show that the effectiveness and efficiency of our approach are higher than those of a state-of-the-art service-organization approach, while a low total number of pragmatics is greedily stored by our approach."
3947,"Service oriented architecture (SOA) is an approach for building distributed systems that deliver application functionality as a set of self-contained business-aligned services with well-defined and discoverable interfaces. This paper presents a systematic and architecture-centric framework, named service oriented architecture framework (SOAF), to ease the definition, the design and the realization of SOA in order to achieve a better business and IT alignment. The proposed framework is business-process centric and comprises a set of structured activities grouped in five phases. It incorporates a range of techniques and guidelines for systematically identifying services, deciding service granularity and modeling services while integrating existing operational/legacy systems. The results from a pilot validation of SOAF for SOA enablement of a realistic securities trading application are presented. Best practices and lessons learned are also discussed"
3948,"Recently, process-aware information systems (PAIS) were introduced, which allow for dynamic process and service changes. This, in turn, has led to a large number of process model variants, which are difficult to maintain and expensive to configure. This paper deals with goals and issues related to the mining of process model variants. Our overall goal is to learn from process changes and to ""merge"" the resulting model variants into a generic process model in the best possible way. By adopting this generic process model in the PAIS, future cost of process change and need for process adaptations will decrease."
3949,"HTTP is currently being used as the communication protocol for many applications on the web, supporting business and safety-critical services throughout the world. Despite the growing importance, HTTP-based applications are quite exposed to network failures, which can bring in huge losses to the service users and providers, including financial and reputation losses. Several approaches try to achieve reliable communication by using logging and retransmission of whole HTTP messages, which is especially ill-adapted to large messages. Stream-based approaches are more efficient as, upon failure, they transparently resume data transmission from where it stopped. Despite this, designing a stream-based mechanism involves significant challenges, as it is very difficult to know how much data is lost due to failure. In this paper we propose a stream-based mechanism for reliable HTTP communication that is entirely based on HTTP messages and is compatible with existing software. The mechanism is presented as a design pattern and relieves developers from explicitly handling connection failures, providing a standard way for building more reliable applications. Results show that the mechanism is functional, compatible with legacy applications, and that the coding and runtime costs of this design pattern are quite low."
3950,"Operational efficiency is a major indicator by which the profitability of a business process outsourcing (BPO) service is evaluated. To measure such operational efficiency, BPO service providers define and monitor a set of key performance indicators (KPI) (e.g., productivity of employees, turn-around-time). While a pair of clients can be directly compared using a KPI, comparing the aggregate client operations across multiple KPIs is non-trivial. This is primarily because KPIs are disparate in nature (e.g., cost is measured in dollar while turn-around-time is measured in minutes). In this paper, we present CoCOA, a framework that compares aggregate operations of clients in BPO services so that they can be viewed in a single pane of glass. Two key modules of CoCOA are: (a) client rank aggregator and (b) KPI importance classifier. For a given time period, the rank aggregator module determines an aggregate ranking of clients using variety of inputs (e.g., individual KPI rank, priority of a KPI). When the aggregate rank of a client deteriorates over successive time periods, KPI importance classifier identifies the responsible KPIs for such deterioration. Thus, CoCOA not only helps in comparing the aggregate operation of clients, but also provides prescriptive analytics for improving organizational performance for a given client. We evaluate our approach using anonymized data set collected from a real BPO business and show how responsible KPIs can be identified when there is a deterioration in aggregate client rank."
3951,"Service oriented computing enables the development of applications as compositions of basic entities called services. These services offer business functions, which are used as primary criteria in the service selection algorithms. In distributed scenarios, a large number of services can offer similar functionalities, motivating the embracement of quality attributes as fundamental selection elements. Different strategies can be used to classify services based on these attributes and it is quite difficult to elect one that seems adequate in every scenario. In this context, we present DSOA platform, which is an environment that supports dynamic service compositions based on quality attributes. Particularly, our focus in this paper is in its Service Selection component which supports the definition of new selection strategies at runtime. In order to validate our solution, we perform an experimental evaluation that shows the utilization of different service selection strategies and their performance impacts and effectiveness in selecting services."
3952,"In this work we propose an approach for verifying privacy timed-related properties of web service protocol. While in [anal] the addressed problem in business protocols is focused on the analysis and management of functional requirements that support rich timing constraints, our approach extends the previous results to capture the timed behavior of privacy constraints. Hence, we provide a model called Timed Private Business Protocol TPBP. Next, we emphasize the timed properties related to privacy in TPBP. Finally, we present the different types of timed property verification to achieve upon the timed private business protocol."
3953,"AI planning techniques is exploited for automatic service composition by representing service composition as a planning problem. An enhanced HTN planning method is developed in which action decomposition is used as plan refinements in partial-order planning. How abstract planning constructs map to the domain of Web services is also discussed, which is based on semantic type matching. Furthermore, execution for service composition is separated from planning and an ECA-based workflow engine is used to execute the service-dependent plan."
3954,"While services are widely regarded as an important new concept in IT architecture, so far there is no consolidated concept about the exact meaning of the term ""service orientation"". While there are many problems which are simply problems of certain technical decisions, other areas are more fundamental and lead to different perspectives and eventually implementations of service oriented systems. We argue that the current emphasis of service orientation as a collection of interface descriptions misses the critical point of services, which is that they revolve around resources. With a more resource-centered approach, the investment into a service oriented architecture can be made much more promising, because the resource-centered approach is better suited for the design of loosely coupled systems than the current interface-based approach."
3955,"In a web service composition, an electronic contract (e-contract) regulates how the services participating in the composition should behave, including the restrictions that these services must fulfill, such as real-time constraints. In this work we present a visual model that allows us to specify e-contracts in a user friendly way, including conditional behavior and realtime constraints. A case study is presented to illustrate how this visual model defines e-contracts and a preliminary evaluation of the model is also done."
3956,"Compared with packaged application, custom application developments (CAD) experience the frustration of higher project overhead and less certainty. The typical time spent on building the infrastructure for a CAD project is, on average, several weeks. Project uncertainty comes from unique customer requirements and lack of standardized methods and toolsets to follow. Therefore, a CAD project is more difficult to achieve cost reduction and asset reuse. In this paper, we present a cloud platform to alleviate this problem through an integration of a) standard methods, b) standardized toolsets aligned with those methods, c) project management environments with pre-defined work breakdown structure (WBS) aligned with those methods and toolsets, and d) infrastructure support from the cloud technology. We believe that such a cloud platform will become a fundamental approach for large enterprises to develop CAD or other solutions for their clients."
3957,"Service-oriented computing (SOC) has been marked as the technology trend which caters for the interoperability among the components of a distributed system. However, the emergence of various incompatible instantiations of the SOC paradigm e.g. Web, grid and P2P services, as well as the interoperability problems encountered within each of these instantiations (e.g. Web service interoperability problems addressed by the WS-I basic profile) state clearly that interoperability is still elusive. In order to address this problem we first need to identify all problem dimensions and consequently to provide appropriate solutions. Within this paper we describe a set of interoperability dimensions that need to be considered and we present a generic service model which we view as a first step in addressing some of the identified problem dimensions"
3958,"This work presents an assessment model of service interoperability for service composition, which helps users to judge whether their composite services can be invoked properly. The model provides a clean and easy way to evaluate interoperability between autonomous services in open and large-scale distributed environments. This paper first summarizes and analyzes different levels of service interoperability, then proposes a multifactor decision model to assess interoperability, and finally discusses some patterns to use the model."
3959,"We present a general model, called the Query/IndexLink (QIL) model, for studying unstructured peer-to-peer search networks. QIL aims at providing a simple framework for visualizing and analyzing P2P search networks. QIL introduces the memorizing mechanism into the model, and improves the syntax of the former model. It is a more general and simpler model, which makes analyzing P2P search networks easier."
3960,"Various phases in the delivery of software services such as solution design, application deployment, and maintenance require analysis of the dependencies of software products that form the solution. As software systems become more complex and involve a large number of software products from multiple vendors, availability of correct and up-to-date system requirement information becomes critical to ensure proper functioning of managed and maintained software solutions. System requirement information, is mostly made available in unstructured formats from sources such as websites or product documents and are not amenable to programmatic analysis. In this paper, we motivate the benefits of capturing this information in a structured format for software service delivery, and present a dependency analysis system that collects and integrates software dependency/interoperability information from multiple unstructured sources using text mining techniques. Information hence collected, is used to support analytics useful in software service delivery. We report the results of our experiments on mining millions of web pages to collect dependency information for more than 700 software products."
3961,"As services for Big Data Analysis (BDA) become prevalent, analysis services with intelligence and autonomy using automatic service composition (ASC) show bright prospects in the BDA market. Selection is one of the most important phases of successful ASC process. Moreover, it became competitive with the rise of demand for the services and criticalness of the BDA process. It is a challenge to accomplish a successful uninterruptable composition while serving diverse custom selection requirements. In the case of failure, it results in complete loss of time and resources. Traditional approaches are not applicable to handle failures during long running transactions. Instead, compensation suggests to being an error recovery. Therefore, analytics transactions scheduled as a composition of a set of compensable transactions. However, compensable services are a higher price and consume more time. Moreover, consumers equipped with diverse requirements. It is necessary to guarantee the critical stages of workflow using compensable services rather than whole workflow. Therefore, we proposed customizable Transaction and QoS-aware service selection approach under five user custom settings based on genetic algorithm (GA) to address above concerns. QoS-awareness facilitated by multi-objective QoS criteria and GA is used for multivariate optimization. We conducted a thorough evaluation, and it shows proposed method effectively and efficiently reach the global optimal of the overall selection criteria."
3962,"The increasing complexity of IT environments dictates the usage of intelligent automation driven by cognitive technologies, aiming at providing higher quality and more complex services. Inspired by cognitive computing, an integrated framework is proposed for a problem resolution. In order to improve the efficiency of the problem resolution process, it is crucial to formalize problem records and discover relationships between elements of the records, records overall and other technical information. In the proposed framework, the domain knowledge is modeled using ontology. The key contribution of the framework is a novel domain specific approach for extracting useful phrases, that enables an automation improvement through resolution recommendation utilizing the ontology modeling technique. The effectiveness and efficiency of our framework are evaluated by an extensive empirical study of a large scale real ticket data."
3963,"In healthcare IT environment there are disparate functional systems and they need to collaborate to support healthcare enterprise workflow. While messaging standard and service-oriented approach provides general interoperability practice, they lack sufficient support for semantic interoperability. Semantic web service technology has proved useful to improve semantic interoperability by annotating service interfaces with domain ontologies in the healthcare domain. However, current approaches rely on ad-hoc manual effort to define semantic annotation and there lacks a generalized, cost-effective approach to automatically generate semantic web service descriptions from domain knowledge to ensure consistency and interoperability. In this paper, we propose a mapping-based approach to utilize healthcare domain knowledge to reduce the effort of manually creating semantic annotation. Specifically, we establish modeling ontology for Health Level 7 (HL7) messaging standard and define its semantic mapping to Web Service Modeling Ontology (WSMO). A use case illustrates generating WSMO service annotation using this mapping with HL7 standard model as input. This mapping allows separation of concern and enables automatic annotation, and thus eases introducing semantic technology into healthcare environment unfamiliar with semantics."
3964,"Constant and rapid changes in the market place have inevitably brought changes to business process including the long running process. When changes are mandatory or require retroactive compliance, it may be necessary to migrate existing running process instances to a new version model. Current dynamic instance adaptation approach requires each instance to adapt to the changes individually. To support multiple instance migration across process models, we present an approach where we leverage model comparison technique; control and data flow analyses to identify data constraints for changed activities and identify potential migration points where we could migrate the instances. We then capture the migration point information in a migration plan which can later be used in the runtime environment to orchestrate the migration. We have implemented a prototype of our approach to demonstrate its applicability."
3965,"It is common practice today for small and medium business houses to assemble and host services, than hosting everything themselves. To cater to diverse market needs, these houses often need to subscribe to different services from different information providers. The service contracts and the range of features and facilities supported and provided by the providers vary widely. A non-trivial challenge for a service assembler is in deciding the set of information providers to subscribe to, given the heterogeneity in the offerings provided, the economics of the business model, the target set of customers in the market place and most importantly, the profit margin. We present in this paper, an automated framework that addresses this challenge and aids a service assembler with a cost-feature-performance balanced recommendation of the providers that can best serve his needs. The problem gets exacerbated since there can be multiple dimensions/categories of services (e.g., hotel, flight, and local conveyance in the travel domain) and there can be multiple relevant recommendations which may be of use for the service assemblers. We examine the service subscription recommendation problem from different perspectives and present algorithms for service assembly. Experimental results on small-scale real data as well as large-scale simulation data show the efficacy of our proposal."
3966,"The recognition of unusual customers is one of the most important applications in data mining. Using an example from telecom trade, This work presents a kind of classification and regression tree algorithm to recognize unusual customers in telecommunication trade databases (TCART for short). The TCART includes the following process: maximal classification tree construction, regression pruning and optimal classification tree selection. During the procedure of classification tree growing, we propose improved classification approaches. Finally, the TCART algorithm is validated by large numbers of customer data in the Zhejiang telecom database. The experimental results indicate that the TCART algorithm has the virtues of robustness and effectiveness."
3968,"Today's business processes are not static, they need to be adapted frequently to reflect changing business requirements. Several business process languages such as WS-BPEL have emerged for specifying business processes based on Web service technologies. Activities in such business processes are typically implemented as Web services by using modern programming languages. These services encapsulate the business logic in terms of application-specific code. This approach lacks flexibility in terms of capturing and executing the business rules that define how certain activities work and how decisions are made. Changing ""hard-coded"" business rules leads to changes in the service implementations and it cannot be done efficiently without redeploying the service which may affect running business processes. Therefore, we propose VIDRE a distributed service-oriented business rule engine, which enables business processes or enterprise applications to access business rules as easily as a database by exposing them as Web services. Furthermore, VIDRE enables the definition of distributed business rules, a novel feature allowing a distributed execution of rules"
3969,"With the widely application of information technology in the coal mining industry, the number of information systems has grown significantly. However, how to take advantage of the information fully from those heterogeneous systems to provide an integrated service has become a barrier to the development of mine. Meanwhile, the scope of the information services in coal mine is so limited that only those managers can enjoy the information and decision supporting services. Because of these above reasons, most of miners can't enjoy the benefits of informatization. Based on the background, a framework of the context-aware-based intelligent service system for miners is proposed for the coal industry in the paper. In order to construct the system, three key technologies are emphasized, i.e. context information modeling, context information reasoning, and business service calling &amp; composition. In the end, a detailed system implementation is given to illustrate how to apply the framework to establish the context-aware-based service system in the coal industry. The result of application is satisfactory."
3970,"Today, the majority of solution development is completed under a fixed-price contract. Due to the inherent uncertainty of executing a project with many unpredictable factors, the solution developer bears significant risk in terms of cost, delivery time and quality. Compared to a variable-price (time-and-materials) contract, a fixed-price contract shifts most of the development risk from clients to developers. Therefore, it is important for developers to understand the level of risk inherent in a project and take the proper steps to mitigate risks. From the analysis of custom application development projects using service-oriented architecture methods, we observe that extension of the project duration utilizing a smaller development team can help to reduce the project effort (working hours) and therefore reduce the development cost. We characterize the relationship between deterioration in team performance and the increase in project team size through multivariate regression. We also provide a method to adjust the results of standard project estimation methods to take into account client-side risks. We then propose simple client and developer utility functions to quantify these trade-offs. This serves as an initial framework to identify choices that can be made during the bidding process to reduce development risks."
3971,"Current data management systems are mainly divided into two categories: Database Management System (DBMS) and Data Stream Management System (DSMS). The increasing use of streaming analysis in modern service-based cloud applications has created an arms race among DBMS vendors to offer ever more sophisticated in-database streaming support, which requires handling the volume, variety, velocity and variability of fast data collections. Unfortunately, current solutions either only provide limited streaming analysis capacity and horizontal scalability (classic RDBMS) or trade off transaction processing for other properties (NoSQL DBMS), leading to the curse of no ""one size fits all"" for DBMS. In this paper, we argue that transaction processing is a relevant concept for DSMS. As a first step toward ""One Size Fits All"" Data Management System, we present StreamDB, which integrates transaction processing in DSMS as opposed to extending DBMS to support streams. First, we describe how StreamDB processes transactions in a streaming environment, then we compare our approach with traditional in-memory DBMS on typical transactional benchmarks. Our results show that StreamDB is advantageous in terms of throughput, scalability, and latency. Finally, we argue that the ideas present here provide insight on the development of next-generation data management systems and motivate further study of the challenges inherent in unifying DBMS and DSMS."
3972,"Gaining visibility into their retail supply chain has become a top priority for the Consumer Product (CP) industry. However, taking a “do-it-yourself” approach to the problem is proving to be both expensive and complex. Cloud Computing, with its on-demand provisioning capability on shared resources, has emerged as a new paradigm to address the challenges of the CP industry. In this paper, we describe a framework for deployment of business analytic solutions on a Cloud platform. We illustrate the benefits of the approach in context of the Demand Driven Business Analytic solution that provides demand signals to CP manufacturers."
3973,"Enterprises of today face the challenge of managing large, complex IT eco-systems consisting of software applications, servers, network routers, and other type of resources. Change management, especially scheduling of changes, is known to be one of the most challenging problems in managing IT operations. In this paper, we propose an optimization model for IT change scheduling that takes into account the constraints and cost factors typically encountered in a service provider environment. In particular, we formulate the model in a way that can be solved using standard mathematical programming techniques (i.e., mixed integer programming). This not only results in strictly optimal solutions, but also provides a scalable means for scheduling a large set of change requests with complex constraints. Furthermore, having a computational efficient optimization solution facilitates the study of the scheduling sensitivity with respect to parameter inaccuracy and leads to more robust change schedules. Finally, we demonstrate the effectiveness of the proposed approach in an IT change management example which is built using insights from a large service delivery account and over two hundred thousand change instances."
3974,"Modeling and deployment of e-contracts is a challenging task because of the involvement of both technological and business aspects. There are several frameworks and systems available in the literature. Some works mainly deal with the automatic handling of paper contracts and others provide monitoring and enactment of contracts. Because contracts evolve, it is useful to have a system that models and enacts the evolution of e-contracts."
3975,"The increasing complexity and dynamics in IT infrastructure and the emerging Cloud services present challenges to timely incident/problem diagnosis and resolution. In this paper we present a problem determination platform with multi-dimensional knowledge integration (e.g. configuration data, system vital data, log data, related tickets) and enablement for efficient incident and problem management of the enterprise. Three features of the platform are discussed: automated ticket classification, the automated association of resource with tickets based on integration with configuration database, and the collection of the system vitals relevant to the ticket through integration with monitoring systems. In response to the emerging Cloud services and their highly dynamic service operation context, we identify the need for a proactive service management approach which incorporates configurations and deployment of incident management tools, policies, and templates throughout the service life cycle in order to enable effective and efficient incident management in service operation."
3976,"The Gridcast project is pioneering the use of grid and Web technologies to prototype the next generation of broadcast media infrastructure. The project has a physical network infrastructure that connects BBC Northern Ireland, BBC R&amp;D in London, the Belfast e-Science Centre and the emerging UK grid infrastructure. This physical network infrastructure is being used to test grid and Web services that manage a television broadcast infrastructure; that is, an infrastructure that manages a collection of broadcast schedules to be transmitted to viewers and that contains a mix of live and recorded video. Broadcasting is a highly demanding industry with high levels of reactivity and robustness required in its infrastructure - it is a significant test of current grid and Web technologies. The project has implemented schedule-based transport of recorded video between broadcast locations that assumes a highly reactive broadcasting environment, the remote, secure use of technical resources and the automation of broadcast and production workflows."
3977,"It can be time consuming to search Internet news, due to multiple sources reporting repetitive information. Given a query and a set of relevant text articles, query-focused multi-document summarization (QMDS) aims to generate a fluent, well-organized, and compact summary that answers the query. While QMDS helps to summarize search results, most top-performing systems for this purpose remain largely extractive. Extractive summarization extracts a group of sentences and concatenates them. In this paper, we propose a summarization service based on abstractive QMDS using multi-sentence compression (MSC). Our proposed service generates a novel summary representing the gist of the content of the source document(s). Experiments using popular summarization benchmark datasets demonstrate the effectiveness of the proposed service."
3978,"Two practical paradigms are presented, which facilitate domain concepts to be directly used to model business operations: the first paradigm is based on the business artifacts and their life cycle; the second paradigm is based on the business tasks and their sequencing. Transformation is an effective way to bridge the gap between business level analysis and IT solutions. We present algorithms that transform business process models based on these two paradigms into IT solutions of Web service platform. The specific problems we addressed in this transformation are: 1) how to generate the implementation code of an optimal size; 2) how to preserve the natural structure of business process models in the generated code."
3979,Performance analysis of Web applications are rather difficult since people can perform parts of an activity outside the application or get interrupted while performing an activity in the system. The lack of a performance model makes it hard to plan resources or get a better understanding of the way available resources are used. In this paper an approach for determining a performance model for semi-structured processes is applied to a case study.
3980,"Security concerns are frequently mentioned among the reasons why organizations hesitate to adopt cloud computing. Given the numerous choices of cloud-resource providers, clients often find it difficult to assess their relative advantages and shortcomings with respect to security, which may prevent them from making any choice. In this paper, we describe our methodology for a hierarchical security-audit method for cloud-computing services. Our method examines the overall security of the cloud offering, based on the examination of a comprehensive set of security concerns at the IaaS, PaaS, and SaaS layers. For each layer, relevant evidence regarding its security is collected and subsequently synthesized into an overall security score. We illustrate our method through a case study, examining the relative security merits of the Google Cloud and the Microsoft Azure Cloud."
3981,"In the context of genome research, the method of gene expression analysis has been used for several years. Related microarray experiments are conducted all over the world, and consequently, a vast amount of microarray data sets are produced. Having access to this variety of repositories, researchers would like to incorporate this data in their analyses to increase the statistical significance of their results. In this paper, we present our service-oriented approach for the gene expression analysis. Our approach is oriented towards scalability and efficiency."
3982,"Scientific workflows have become an important instrument for domain scientists to synergistically integrate distributed computations and data to accelerate scientific discoveries. Existing scientific workflow tools, however, only support single scientists to compose scientific workflows in a desktop application. Nowadays, many scientific research projects are becoming increasingly larger scale, requiring that multiple research partners with different expertise collaborate from distributed organizations. Therefore, there is a critical need of a collaborative scientific workflow tool that supports domain scientists to cooperatively design, compose, annotate, execute, monitor, and manage scientific workflows over the Internet in both synchronous and asynchronous modes. This research reports the design and development of our preliminary version of a collaborative scientific workflow tool based on an open-source, single-user tool Taverna. We present our study of the role-organization-based access control technique over collaborative scientific workflow composition."
3984,Rapidly growing service delivery organizations need efficient tools to manage their business and reduce the cost of growth. These delivery businesses run round the clock due to demands coming from different geographical locations for different shifts. Cost of adding new physical infrastructure and space to tap incoming business is a key inhibitor of growth. Physical space availability and its effective management is the key infrastructure enabler for effective business delivery apart from human resources. A considerable impact can be realized if space utilization can be increased by optimal allocation around different work shifts. Optimal space utilization can bring down the number of physical seats required to serve the existing demand and also allow service delivery organizations to commit to new business needs with the existing capacity. The paper addresses this business problem and proposes an effective seat utilization planning solution using a mathematical programming approach to meet various strategic and tactical business objectives in this setting.
3985,"Current frameworks to track the Quality of Services (QoS) are strictly related to the traditional IT approach and they are not suitable to ensure a proper control on Cloud Computing environments. This paper firstly discusses the differences of a cloud SLM versus traditional SLMs and summarizes emerging challenges in service discovery, selection and management process of SLM in cloud. Secondly it overviews the current state of business and academy by a survey and a literature review. Finally it introduces a third party SLM framework for Cloud Service Providers (CSPs) and Cloud Service Consumers (CSCs). This framework, which aims at an easy-to-deploy and easy-to-use approach, called SLM as a Service, was tested in a real case. Results show that a third party SLM is equal, dependable and reasonably easy to implement."
3986,"This paper addresses the problem of managing business process changes at the design level within a complex business process repository. We argue that a formal process eco-systems view can provide a particular useful solution to the problem in which their inter-process relationships can be properly described. Our intent is to build a modular infrastructure to support change management in process eco-systems by leveraging a change propagation approach that would maintain the inter-process relationships. This permits us to propagate the changes, made on a particular process, to the rest of the processes for maintaining the relationship equilibrium of such eco-system. Our experimental evaluation suggests that the propagation of change can be efficiently achieved, which suggests that the real source of complexity stems from the redesign of individual models."
3987,"Service-Oriented architecture (SOA) based applications have assumed widespread acceptance owing to their agility, maintainability and modularity. However, the safety and reliability of such loosely coupled systems entirely depend on the precision of service descriptions. Consequently any implicit assumption or unforeseen usage scenarios can lead to catastrophic fiascos. This is further exacerbated by the overlapping constructs and inconsistencies in Business Process Execution Language (BPEL), the de-facto industry standard for service composition. This paper extends the Spring framework to devise a verification framework for service composition wherein each BPEL activity is represented by a Java bean. The framework instantiates the beans corresponding to activities in a BPEL specification and injects the dependencies to yield a bean-factory. Thereafter Java Architecture for XML Binding (JAXB) 2 APIs are used to transform the bean-factory into an XML based formal-model (e.g. Coloured Petri nets (CPN)) or an interchange format (e.g. Petri Net Markup Language (PNML)) for simulation and verification. In addition to automating the verification process, the proposed framework helps to combat the ad-hoc nature of existing solutions. Results indicate that the framework has an average transformation time of. 7 sec."
3988,"We describe a framework for personalized context aware search of ontology-based tagged data. As more services are offered on the Web, it is becoming increasingly difficult for users to manage their content online. We propose a framework that performs context-aware search of tagged data using a tag ontology that includes context information in addition to tagged keywords in order to present more meaningful search results to users."
3989,"Attributes credibility, trustiness and accuracy are often considered, but not incorporated in decision making algorithms. In this paper a credibility-based risk/business context model is proposed, to profile communication service requests using sources' credibility, as well as observed intensity and customizable policy-based prioritization. The paper proves that this Credibility-based approach can cope with the complexity and uncertainty of context models, which Bayes probabilities cannot, and with degrees of discordance and uncertainty, which DST cannot. The paper provides reviews of concepts and techniques to determine credibility from its components and aggregate corroborated credibility within observations and ultimately key factors of the model. The paper tests and proves the effectiveness of the modelled credibility, intensity and policy, and shows that incorporating Credibility is a better way to make informed decisions."
3990,"The main target of IT architects being responsible for complex and heterogeneous IT architectures is to align IT to business needs at the lowest costs possible. A recent approach to this issue is service oriented architectures (SOA). In order to design and deploy a SOA methodological and technological aspects have to be taken into account. This contribution focuses on methodological aspects like how to find appropriate service domains and an appropriate granularity of the services. Therefore existing business processes, organizational structures and their linkage with the different elements of an organization's IT landscape were analyzed. The paper presents a software system for automatically analyzing enterprise architectures incorporating clustering algorithms developed for analyzing community structures in a network oriented fashion. It will be shown, how clustering and visualization of enterprise architecture networks improve the understanding of underlying structures of enterprise architectures and thus lead to a better design of SOA"
3991,"Software service organizations typically develop custom solutions from scratch in each project engagement. This is not a scalable proposition, since it depends too heavily on labor alone. Rather, creating and reusing software ""assets"" is more scalable and profitable. One prevalent approach to building software solutions is to use service-oriented architecture (SOA) to compose software services to support business processes. In this context, the key to reusing assets is to support the right mechanisms to incrementally refine existing software services as well as business processes. In this paper, we propose a set of mechanisms called Variation-Oriented Engineering (VOE) to support this incremental refinement. VOE is a comprehensive formal approach for modeling end-to-end variability in SOA-based solutions for the purpose of enhancing reusability. We illustrate our approach on a realistic example from the insurance domain, via a prototype implementation."
3992,"Driven by the emergence of new computing environments, dynamically evolving software systems makes it impossible for developers to deploy software with human-centric processes. Instead, there is an increasing need for automation tools that continuously deploy software into execution, in order to push updates or adapt existing software regarding contextual and business changes. Existing solutions fall short on providing fault-tolerant, reproducible deployments that can scale on heterogeneous environments. In this paper we present Rondo, a tool suite that enables continuous deployment for dynamic, service-oriented applications. At the center of these tools, we propose a deterministic and idem potent deployment process. We provide with Rondo a deployment manager that implements this process and capable of conducting deployments and continuously adapting applications according to the changes in the current target platform. The tool suite also includes a domain-specific language for describing deployment requests. We validate our approach in multiple projects, for provisioning the platform as well as for installing applications and continuous reconfigurations."
3993,"Load balancing is the salient problem in data stream processing systems and also in complex event processing systems. And the imbalance in operational workers becomes extremely apparent as the scale is up with more workers and skewed datasets. In this paper, we find out that the upstream skewed sources can also exacerbate the load imbalance in the downstream workers and this bottleneck cannot be handled well by existing schemes. Thus, we propose a novel stream partitioning solution called BACK PROPAGATION GROUPING (BPG), and its core components are key splitting, back propagation and calibration signal. We verify BPG in theory and test it on both real-world and synthetic data streams. The results show that the imbalance is 10-100x less with BPG than with the previous state-of-the-art. And this metric translates into an improvement of up to 34% in throughput when deployed on Apache Storm cluster. In conclusion, BPG mitigates the load imbalance dramatically on highly skewed datasets and especially when the sources are also skewed."
3994,"Due to the high proliferation of web services, selecting the best services from functional equivalent service providers have become a real challenge, where the quality of the services plays a crucial role. But quality is uncertain, therefore, several researchers have applied Fuzzy logic to address the imprecision of the quality of service (QoS) constraints. Furthermore, the service market is highly dynamic and competitive, where web services are constantly entering and exiting this market, and they are continually improving themselves due to the competition. Current fuzzy-based techniques are expert and/or consensus-based, and therefore too fragile, expensive, non-scalable and non-self-adaptive. In this paper we introduce a new methodology to support requesters in selecting Web services by automatically connecting imprecisely defined QoS constraints with overly precise service QoS offerings over the time. We address the dynamism of the market by using each time a modified fuzzy c-means module that allows providers to automatically organize themselves around the QoS levels. The advantage of our approach is that consumers can specify their QoS constraints without really knowing what are the current best quality ranges. We illustrate our approach with a case of study."
3995,"Service-oriented architecture (SOA) is an architectural paradigm that has emerged to help facing new challenges in software development and integration. SOA projects use to be complex and costly. ICT companies are by far composed of SMEs (Small &amp; Medium sized Enterprises), having many levels of limitations, hazarding their innovation capabilities and hence their long-term sustainability. This can be mitigated if they do innovation collaboratively, with other SMEs. This paper presents results of an innovation model devoted to leverage independent SMEs of SOA services providers to jointly develop a SOA product, sharing costs, risks and benefits. The model is very flexible so as to support the intrinsic uniqueness of an innovation process. The model is complemented with so-called functional guidelines, which help SMEs managers knowing what are the usually most important issues to handle along the different steps of an innovation. Results of its initial evaluation by end-users are presented. Final considerations are presented at the end."
3996,"Many existing reputation based trust management frameworks for Web services are built on collecting and aggregating the feedback ratings reported by the service consumers. Therefore, the reliability of reputation evaluation mainly depends on the integrity and the accuracy of the reported feedback ratings. In the lectures, various statistical filtering techniques have been proposed to exclude those unfair ratings (a consumer rates a service more positively or more negatively than the real experience). As a kind of unfair ratings, prejudicial feedbacks will obviously reduce the accuracy of the reputation evaluation in the situation that the feedback data are lacking and insufficient. In this paper, we presented our works on rectifying prejudicial feedbacks in a Web services management environment. We also presented an experimental study to demonstrate the effectiveness of our rectifying algorithm in increasing the accuracy of reputation evaluation especially when feedback data are not sufficient."
3997,"In recent years, IT Service Management (ITSM) has become one of the most researched areas of IT. Incident Management and Problem Management form the basis of the tooling provided by an Incident Ticket System (ITS). As more compound or interdependent services are collaboratively offered by providers, the delivery of a service therefore becomes a responsibility of more than one provider's organization. In the ITS systems of various providers seemingly unrelated tickets are created and the connection between them is not realized automatically. The introduction of automation will reduce human involvement and time required for incident resolution.In this paper we consider a collaborative service delivery model that supports both per-request services and continuous high-availability services. In the case of high availability service the information stored in the ITS of the provider often includes information on the outage of a particular service rather than on the failure of a particular request. In this paper we offer an information model that consolidates and supports inter-organizational incident management and probabilistic model for fault discovery."
3998,"Faced with the increasing services and users' personalized requirements, it remains a big challenge for users to effectively and accurately discover and reuse interested services. Goal oriented requirements modeling has attracted more and more attentions in services discovery and modeling, but little work has focused on extracting intentional goals from service descriptions. In this paper, based on the ranked domain keywords, we investigate how to extract domain-specific service goals from service descriptions, which can contribute to services discovery and recommendation. Programmable Web, a publicly accessible service repository, is selected as the testbed. Experiments show the feasibility of the proposed approach."
3999,"Mobile devices today are increasingly being looked upon as potential platforms for service provisioning rather than mere 'service consumers'. There is, however, much that needs to be done to realize this. A detailed, dynamic, and lightweight service description is an important requirement for the automatic and efficient discovery, selection, and subsequently provisioning of services over mobile devices. Traditional approaches for service provisioning are usually not directly adaptable to the mobile environments owing to the latter's dynamic and distinct nature. Hence, in this paper a lightweight and extensible approach for service description especially designed for mobile environments is proposed. The proposed approach incorporates detailed and rich descriptions covering functional, non-functional, contextual, and business aspects of services to be provisioned over mobile devices. The service descriptions have been partitioned along these lines and the various parts are distributed between service registries and the mobile service providers. This facilitates the service provider to maintain an up-to-date description without having to compromise on the overall consistency of the description. The proposed service description is good for a heterogeneous environment comprising both wired and wireless systems. A prototype of the proposed system has been implemented with the intent of validating the feasibility of the approach."
4000,"This paper presents a novel approach to model a complex evolving system, a digital business ecosystem (DBE) that takes the specific needs of small and medium-sized enterprises (SMEs) into account. It aims to provide an open-source distributed environment to support the spontaneous evolution and composition of services for SMEs. A two-tier architecture model as a simplified version of the system is presented, consisting of a business network layer and a P2P communication layer that interact with each other and evolve over time. The benefit of the model-driven approach is to allow us running discrete event simulations on it to study, e.g how the topology of the SME network affects the topology of the P2P communication network."
4001,"This paper describes an e-business solution model designed in Wachovia Corporation, one of the top largest banks in USA, serving millions of customers with a variety of financial services such as online banking, billpay and brokerage. A pragmatic process is designed to migrate conventional n-tier ecommerce systems to a service-oriented computing paradigm, which comprises service-oriented architecture (SOA), integration (SOI), process (SOP) and management (SOM). A hybrid methodology is developed to leverage the benefits of both top-down and bottom-up approaches. E-business patterns are applied to categorize various online services, which are subsequently mapped to appropriate technologies, products/tools, and infrastructure. Common business functionalities are built as shared services to be reused across channels. A multilayer model is conceived to converge the latest technologies such as portal, process orchestration, Web services, service bus, grid computing, business rules, etc. Best practices as well as lessons learned are discussed in the context."
4002,"We propose an approach that aims to provide services that are able to adapt at run-time to the quality of services (QoSs) required and expected by the users in the context of an adaptive resource management system. When requiring a service, users may specify additional information which is of two types: QoSs (i.e., the print resolution or the transmission bandwidth) and properties (i.e., the location of the resource that provides the service or the provider of the service). In the adaptivity process, both types of information are mapped on the QoSs and properties of the underlying system's resources, which are explicitly represented through architectural reflection. As a consequence, resources try to adapt their QoSs to those required by the current service"
4003,"During the execution of a QoS-aware service process, Web Services (WSs) may become faulty and cause the whole process to violate the predefined QoS constraints. Service processes need to adapt to the runtime faults so as to support reliable service-based applications. In this paper, we study the problem of runtime service process reconfiguration under end-to-end QoS constraints. An adaptive QoS-aware service process reconfiguration approach is proposed to solve the problem efficiently with low reconfiguration costs. In our approach, supplementary services are selected during service composition and then used as a backup source for efficient recovery, a region-based reconfiguration algorithm is proposed to minimize the reconfiguration cost by identifying a limited reconfiguration region that includes a small number of services. Experimental results show that, by using our approach, a high percentage of faulty service processes can be successfully reconfigured with high efficiency and significant cost savings."
4004,"Because web services are loosely-coupled business applications, they are called to cooperate in distributed computing for the sake of efficiency. In this paper we propose a model formalizing web services efficiency considering different related parameters and a game-theoretical framework analyzing the web services strategies allowing them to maximize this efficiency. Many theoretical results are proved and confirmed through extensive simulations."
4005,"Business partners willing to do business electronically with each other must reach an agreement (1) on the economic level, (2) on the inter-organizational process choreography, and (3) on the services implementing the choreography. In order to search for a potential business partner, one will first look for a partner who offers a required service on the economic level and who supports a complementary role in a choreography, before binding to its IT services. In as much, a registry for inter-organizational systems should cover all three levels and maintain the dependencies between them. In this paper we set up on well accepted approaches on the different levels, i.e. (1) the e
<sup>3</sup>
value ontology, (2) the UN/CEFACT modeling methodology (UMM), and (3) the business process execution language (BPEL). We specify a registry meta model on top of ebRIM registering the artifacts on the different levels and defining their inter-dependencies."
4006,"Large scale, multi-dimensional resource provisioning for Software as a Service (SaaS) presents a significant challenge. Analytical calculation of the quality of a configuration is necessary for effectively assigning new services to servers and reorganizing assigned services. This work describes the Provisioning Norm which meets this critical need. The Provisioning Norm (an asymmetric norm) analytically calculates the quality of a configuration, the placement of services on server nodes. The Provisioning Norm partially orders all possible configurations for a set of nodes and services from best to worst by numerically biasing over-provisioned configurations relative to under-provisioned configurations. This work proves that the parameter to the Provisioning Norm function has a value which partitions the partially ordered configurations into over-provisioned configurations and under-provisioned configurations. The application of the Provisioning Norm in a testing environment demonstrates a correlation between the analytical quality and the empirical performance."
4007,"In this highly dynamic era of technology, most of the data-intensive applications are designed to target a good combination of high operation availabilities, scalability, consistency, fault tolerance etc. To address this issue, the research community has introduced many valuable techniques based on data replication and data distribution. However, in the ongoing research there is a paradigm shift: the research community is targeting to minimize the inter-application coordination - the coordination required to ensure consistency - to achieve high operation availabilities. In this work, we present, a Component-based Highly Available Replication Strategy (CbHaRS) which exploits operation types and hybrid communication model to achieve high operation availabilities. CbHaRS is highly scalable as it utilizes data components as the building blocks for the replication strategy. Causal data consistency in CbHaRS is ensured by a so-called component administrator. The Communication type between the data component and the component administrator depends upon the state of the data component. Additionally, the state of the data component depends upon the operation type. We further extend the concept of client specific on-demand replication to general component-based replication. CbHaRS is a project in progress. To prove the effectiveness of CbHaRS, we have implemented the CbHaRS prototype and discuss the achieved results."
4008,"The crucial role of networking in Cloud computing calls for federated management of both computing and networking resources for end-to-end service provisioning. Application of the Service-Oriented Architecture (SOA) in both Cloud computing and networking enables a convergence of network and Cloud service provisioning. One of the key challenges to network -- Cloud convergence lies in QoS-aware composition of network and Cloud services. In this paper, we propose a QoS-aware service composition method to tackle this challenging issue. We first present a system model for network -- Cloud service composition and formulate the service composition problem as a variant of Multi-Constrained Optimal Path (MCOP) problem. We then develop an algorithm to solve the problem and give theoretical analysis on properties of the algorithm to show its effectiveness and efficiency for QoS-aware network-Cloud service composition. Performance of the proposed algorithm is evaluated through extensive simulation experiments and the obtained results indicate that the proposed method achieves better performance in service composition than the best currently available MCOP approach."
4009,"As service computing becomes increasingly prevalent, the number of web services grows rapidly. It becomes very important to recommend suitable, personalized web services to users. Collaborative Filtering based on Quality of Service (QoS) has been widely used for service recommendation, and variety of factors such as location, environment are taken into account to improve the accuracy of recommendation. However, temporal influences, which is one of key factors affecting the QoS, are not fully considered by the investigators. In this paper, we propose a novel temporal influences-aware collaborative filtering method which designs an enhanced temporal influences-aware similarity measurement to predict QoS values. Finally, we conduct a series of experiments to evaluate the effectiveness of our method, and results show that our method outperforms other state-of-the-art methods."
4010,"Enterprises today are keen to unlock new business values of their legacy services towards new trends (e.g., cloud and mobile). To accelerate such process, automatic feature location techniques can enable developers to rapidly locate/understand implementations of certain services (e.g., services to expose, transform or improve). Existing feature location techniques [1-3, 5-10, 32] provide a good foundation but have several key limitations: limited leverage of description sources, less considerations of internal behaviors, and ineffectiveness for the identification of service-relevant code entries. To address these limitations, we propose a behavior model based feature location approach and implement a tool named BMLocator. In the offline phase, BMLocator applies Natural Language Processing (NLP) techniques and static code analysis to extract “behavior models” of code units via considering multiple information sources. While in the online phase, given a service description, BMLocator first extracts its behavior model and then recommends service-relevant code units/entries by matching its behavior model with code units under analysis. Through evaluations with public service requests of open-source projects (e.g., Tomcat and Hadoop), we show that the approach is more effective in recommending service-relevant code entries (e.g., most of entries are prioritized as the first ones) than existing techniques (i.e., TopicXP[37], CVSSearch[6])."
4011,"This dissertation contributes to the services science discipline by examining appropriateness of Language- Action Perspective (LAP) as a theoretical framework for web services, the technology component of services science. This research consists of three inter-dependent studies. The first study (completed) investigates whether LAP constructs can describe and explain the web services architecture. Findings from this study indicate that there is a lack of mechanisms to generate conversation specifications that guide interactions among services. Conversation specifications are crucial for developing large-scaled enterprise integration solutions using web services. The second study (work-in-progress) builds on this finding and demonstrates the appropriateness of LAP constructs to access design knowledge to develop web service solutions for enterprise integration. The third study (work-in-progress) evaluates the usefulness of LAP constructs to develop effective web service solutions (artifact developed in the second study)."
4012,"The Big Data community has started noticing that there is the need to complete Big Data platforms with assurance techniques proving the correct behavior of Big Data analytics and management. In this paper, we propose a Big Data assurance solution based on Service-Level Agreements (SLAs), focusing on a platform providing Model-based Big Data Analytics-as-a-Service (MBDAaaS)."
4013,"Cloud infrastructure should accommodate changing demands for different types of transactions with heterogeneous requests workload and response time constraints. The multi-layer nature of service computing results in the difficulty of designing an optimal cloud infrastructure. The objective of the paper focuses on enabling clouds infrastructure to provide guaranteed performance in minimum cost. The global SLOs are decomposed into subsystems in different layers according to transaction profile. The performance of cloud infrastructure is described as a multi-station queuing model. The approximation mean-value analysis algorithm is used to get the approximate solution of performance variables. Experimental results show that the proposed approach is effective, capable of reducing the system cost while improving the performance."
4014,"Research shows that in many cloud data centers, physical resources are not used efficiently and thereby cost extra overhead. To improve cost-effectiveness of resources in cloud data centers, running big data applications to share residual capacity is a practical solution. However, performance loss brought by resource competition and interference from different types of applications is the main challenge for us. In this paper, we design, implement and evaluate the InSTechAH, an auto scaling scheme for a Hadoop system in a private cloud, which attempts to improve the resource utilization in cloud data centers as well as to maintain required quality of services by auto scaling and scheduling background analytics tasks. In this system, we design the multilayer node model to reduce interference from other services by automatically scaling the clusters according to the auto scale algorithm we introduced. We then build the resource scheduling model which use prediction based scheduling method to reduce the cost brought by scaling. We evaluate our scheme partly on a real data trace and partly on simulation, with Hadoop as the parallel data analytics frameworks and Open Stack as the cloud management architecture, to show the efficiency of InSTechAH system."
4015,"Asset-based approaches, involving the use of standardized reusable components (as opposed to building custom solutions), are increasingly being adopted by IT service industries to achieve higher standardization, quality and cost reduction goals. In this paper, we address issues related to the use of an asset-based approach for authoring service contracts, where standard templates are defined for each type of service offered. The success of such an approach relies on a compliance checking system. We focus on three key components of such a system. The first measures how well actual contracts comply with the standard templates. The second analyzes compliant contracts containing moderate deviations and reports on the consistent patterns of deviations observed for each template to help identify necessary modifications required in templates to keep them up-to-date with evolving business requirements and customer needs. The third analyzes noncompliant contracts and identifies groups within them such that members of each group have enough similarity to each other to warrant consideration for development of new templates for each group. We describe the architecture of the proposed system, our experience in the use of various text analysis techniques to prototype different system components, and the lessons learned."
4016,"In today's fast paced world, it is necessary to process business documents expediently, accurately, and diligently. In other words, processing has to be fast, errors must be prevented (or caught and corrected quickly), and documents cannot be lost or misplaced. The failure to meet these criteria, depending on the type and purpose of the documents, can have serious business, legal, or safety consequences. In this paper, we evaluated a B2B order placement service system that allows clients to place orders for products and services over a network. We describe the order placement service before and after deploying the Intelligent Document Gateway (IDG), a document-centric business process automation technology from IBM Research. Using service science perspective and service systems frameworks, we provide an analysis of how IDG improved the value proposition for both the service providers and service clients."
4017,"In the Grid computing environment, there are many important issues, including information service, information security, resource management, routing, fault tolerance, and so on. Job scheduling is a major problem since it is a fundamental and crucial step in achieving high performance. The job scheduling problem has been treated as a combinatorial-optimization problem. Scheduling in a Grid environment can be seen as an extension to the scheduling problem on local parallel systems. In the research, we focus on applying the technologies of RFID and Grid computing to the architecture of the EPC network. There are many applications needing computing power of the Grid to deal with the huge quantity of incoming EPC data. According to the architecture of the EPC network and the environment of Grid computing, they have several similar characteristics. Therefore, we propose a new algorithm that modifies the traditional GA and integrates SA. Since the processes of GA and SA keep no memory, some problems may be visited again. In order to overcome this drawback, we design a learning scheme to remember visited statues to reduce the probability of the re-visiting improvement the performance in the search space. It can help to find the optimal or near-optimal scheduling efficiently and avoid the resource deadlock. Furthermore, our proposed algorithm, HGASA - with learning scheme, also considers about several properties of grid computing environment and EPC network, such as heterogeneity, dynamic adaptation and the independent relationship of jobs."
4018,Information service in the grid provides the ability to discover and monitor resources which is fundamental for the grid infrastructure. A framework of a tree-based grid information service (TGIS) is proposed in this paper. This framework is based on GT's MDS. MDS uses centralized GIIS to index services. Performance study shows that GIIS can not sustain large users and GRIS. Our TGIS framework uses decentralized GIIS to solve that problem. We construct each GIIS node in a tree structure which can solve system performance and scalability issue.
4019,"This paper describes a method for automatically detecting key performance indicator (KPI) thresholds by dividing and aggregating process instances on the basis of differences in process models. The thresholds can be used as an analysis axis of data exploration to investigate process models that are discovered from huge logs. The proposed method enables users to minimize the time needed to detect KPI thresholds through trial and error. We applied the method to real-life logs and experiment results showed that thresholds were detected for two types of KPIs. Although one type did not correlate with process patterns, the other highly correlated with them. Such findings are usually obtained from the domain knowledge of business users and analysis results acquired by data analysts with technical expertise. However, with our approach the thresholds can be detected automatically and this helps to expand process analysis for end users."
4021,"The problem of fair exchange is a fundamental problem in secure electronic transactions and digital rights management. Park et al. (2003) presented an optimistic fair-exchange protocol based on RSA. Dodis and Reyain (2003) analyzed the vulnerability of Park's scheme and presented an optimistic fair-exchange protocol based on GDH. This paper point out that Dodis and Reyain's scheme is also insecure and inefficient. This work presents a multisignature scheme based on DSA, and describes a novel method of constructing very efficient fair-exchange protocols based on improved DSA signatures. The protocols in this paper are more secure and efficient."
4022,"Though there are some existing data service mashup tools, it is still challenging for novice end users to develop data service mashup to solve data query problem in the situational and ad-hoc business scenario. This paper focuses on the problem of recommending data service mashup plans under the condition that 1) the mashup plan cannot be determined in advance and 2) user simple request description. In our approach, some keywords are allowed as input and then the Top-K data service mashup plans are generated as output. This paper analyzes the problem with a motivating scenario, introduces the core definitions and proposes an approach to creating data service mashup plans. Inspired by the idea of keyword query in structured data, the ACOQT algorithm for generating Top-K data service composition sequence is proposed. Based on the data service and data service operator model defined, the method of configuration data service operator is presented. Experiment results show that the approach of data service mashup plan creation is effective."
4023,"Traditional ways of collaboration among enterprises are no longer adequate and efficient due to increasingly short cycle times and constantly changing business circumstances. Companies compete as value chains using Business-to-Business (B2B) collaborations based on infocomm technologies, in particular the Internet. Most B2B collaborations are typically long term and static but are pushing towards short term and dynamic connectivity. This paper introduces a context-aware framework to support such short and dynamic B2B collaborations. In this framework, context information in terms of the user (company), temporal and location information is then used to match of suitable services to realize the business processes formed for B2B collaborations. An initial implementation of this approach is presented together with proposed improvements."
4024,"The SOA Innovation Lab - an innovation network of industry leaders in Germany and Europe - investigates the practical use of vendor platforms in a service-oriented context. For this purpose an original service-oriented ESA-Enterprise-Software-Architecture-Reference-Model, an associated ESA-Architecture-Maturity-Framework and an ESA-Pattern-Language for supporting architecture evaluation and optimization has been researched, leveraging and extending CMMI and TOGAF, as well as other service-oriented state-of-the art frameworks and methods. Current approaches for assessing architecture quality and maturity of service-oriented enterprise software architectures are rarely validated and were intuitively developed, having sparse reference model, metamodel or pattern foundations. This is a real problem because enterprise and software architects should know how advanced architecture quality concepts can successfully be used and how a stable foundation for introducing service-oriented enterprise architectures for adaptive systems looks like. Our idea and contribution is to extend existing enterprise and software architecture reference models and maturity frameworks to accord with a sound metamodel approach. We have developed and are presenting the idea of a pattern language for assessing the architecture quality of adaptable service-oriented enterprise systems. Our approach is based on ESARC -- an Enterprise Software Architecture Reference Model we have developed."
4025,"In Internet of Things (IoT) environments, there are multiple sensors and devices monitoring different metrics and producing massive amounts of data. Monitoring of metrics can be done at different frequencies. Systems and applications that consume monitoring data typically use constrained IT resources, e.g., constrained network facilities, storage, display, processing/computing power, and energy. Given the limited quantity of resources used by these monitoring systems and applications, it is impossible to be able to collect data of all metrics in the application's context with a very high monitoring frequency. Additionally, changes in the IoT environmental context may affect the choice of metrics that should be monitored and their monitoring frequencies. To address these issues, we propose in this paper a novel approach based on optimization model to optimally determine the metrics that should be monitored and the frequencies at which these metrics are to be monitored. Our approach is an adaptive iterative approach in which the metric frequencies are re-optimized whenever an environmental event or a monitored metric value triggers the need for such re-optimization. We also present a proof-of-concept implementation of our approach that shows the efficiency of adopting it."
4026,"Services Computing has turned into the mainstream programming paradigm for building enterprise systems that are distributed in nature. However, the programming power available to the developers of service oriented systems has been slow to catch up. The object abstraction continues to be the prevalent mechanism for implementing services based software systems and has several drawbacks. A key drawback is the fact that programmers are provided the business requirements in terms of services but are expected to implement them using objects, leading to an abstraction gap that the programmer is expected to fill. In this paper, we formalize the notion of services as first class entities through a typed calculus, called Psi-CAL. Psi-CAL models major operations for service manipulation including creation, discovery, and invocation as well as establishing relationships among services. We present the syntax and semantics of Psi-CAL with a corresponding type system, towards building a programming language for services computing."
4027,"Hosting Web services on mobile and wireless devices is challenging because of their limited resources. A few efforts have been made to facilitate hosting of Web services on such devices, but the proposed solutions do not always conform to the multiple Web service standards and security frameworks that have been proposed by the Web service community. Secondly, the performance of these solutions is often limited due to the resource limitations of mobile devices. In this paper, we propose to address this issue by introducing a novel system partitioning technique to be used by a web service provider so that a few WS components can be executed on an intermediate node. The use of the proposed partitioning technique for a WS execution environment has shown a significant performance improvement. The proposed partitioning technique is analyzed using a prototype WS execution environment hosting sample Web services."
4028,"The emerging paradigm of distributed Web services has promised seamless business integration across the Internet that shares not only computing power but also applications. One major challenge is resource allocation to each end user in an economically efficient and low latency manner. A common solution is proportional sharing where each user gets his resource in proportion to the predetermined weight. However, this does not allow users to differentiate the value of their services and to dynamically adjust the weights. A resource reservation mechanism may improve the information and service sharing but usually introduces a high latency. In this paper, we propose a progressive auction based market mechanism for resource allocation that allows user differentiation of the service value and ensures the resource acquisition latency to be limited only to the communication delay in service-oriented architecture (SOA) systems. We present a service sharing prototype for selecting services for further business process composition or solution creation and show experimental results on the formation of reliable user ranking in resource sharing through the progressive auction."
4029,"In a service-oriented IT infrastructure, functional capabilities of a computing component are externalized via one or more service interfaces. Driven by the demand for business agility and return-on-investment optimization, various dynamic service discovery and composition technologies have been proposed and developed with a common goal of enabling business-aligned fulfillment of customer requests. However, from the viewpoint of capacity planning and IT optimization, much work is still needed in helping an enterprise decide the ""optimal"" IT resources necessary for deploying the atomic services in support of those composite ones. The service deployment decision must be integrated with the request fulfillment policy so that the differentiated quality-of-service (QoS) requirements of service requests can be met, for instance, with minimum hardware/software cost. In this paper, we propose an approach for QoS-aware optimization of composite-service fulfillment policy. Without loss of generality, we assume that the optimization goal is to minimize the number of machines subject to response time and throughput requirements. After presenting our approach to the optimization problem using the assumption, we show that an NP-hard throughput optimization problem must be attacked. We then illustrate how we attack the problem via an efficient heuristic algorithm. The algorithm decomposes the end-to-end response time requirement for each type of composite service into atomic-service level response time assurance, and co-locate atomic services with similar response time assurance on machines with similar utilization characteristics. The algorithm exemplifies an integrated approach to optimizing service deployment and service composition. We demonstrate that the algorithm achieves a substantially higher throughput than a common baseline algorithm."
4030,"The use of Java Message Service (JMS) for enterprise applications communication and integration is increasing very quickly. However, although JMS is frequently used in business-critical environments, applications are typically developed with the assumption that the middleware being used is robust, which is not always the case. Robustness failures in such environments are particularly dangerous, as they may originate vulnerabilities that can be maliciously exploited with severe consequences for the systems subject of attack. This paper proposes an approach for the evaluation of the robustness of JMS middleware. Our approach is presented through a concrete example of evaluating the robustness of three well-known JMS solutions (JBoss MQ 3.2.8.SP1, JBoss MQ 4.2.1.GA, and Active MQ 4.1.1), in which several robustness and critical security related problems have been disclosed (including specification conformance disparities)."
4031,"We present a new approach for adapting Web processes to input data volatility while respecting possible coordination constraints between the process participants. We focus on modeling the decision making process of service managers that are responsible for accomplishing the activities of an abstractly defined process. Compared to previous approaches that either employ a process manager overseeing all service managers or have a coordination mechanism which must be perfectly observed by all service managers, our approach does not need any central component but endows each service manager with the ability to communicate with other service managers. We define a regret-based coordination mechanism in order to motivate managers to respond to the communication. Our experiments demonstrate that our approach performs better than the previous decentralized approach while addressing the scalability problem suffered by the globally optimal centralized approach."
4032,"Enterprise applications today are composed of multiple independently executing services and processes that collectively provide a solution to a business problem. These composite applications contain a heterogeneous collection of services that execute in a variety of runtimes making them difficult to manage while maintaining a business centric point of view, as opposed to a service point of view. This paper introduces a business centric monitoring framework to bridge the gap between the business and service levels in complex business applications. Our technical approach focuses on using business information invariants to define one or more monitor sets in order to relate service activity to business composite execution. We apply this framework to enable end-to-end monitoring of heterogeneous composite business applications. In this paper we present a prototype of our business centric monitoring approach using monitor sets for monitoring an order management composite implemented on IBM's Web Sphere Integration Developer (WID) v7 and tested on Web Sphere Business Monitor v7. This extends our earlier work on demonstrating end-to-end monitoring of BPEL service composites in Web Sphere Business Modeler v7. The new contribution of this work is (1)a prototype implementation of our monitoring approach on a different IBM product, WID v7, which supports heterogeneous service composites, and (2) demonstration of the effectiveness of this approach with respect to a (new) heterogeneous order management scenario. Our prototype implementation demonstrates the ease of design and deployment of our monitoring solution to attain a single end-to-end business centric view of a collection of heterogeneous services executing together."
4033,"Systematic reuse of software artifacts has been an elusive goal for several years. Service-oriented architecture (SOA) has been touted in recent years due its promise of fostering reuse. Even so, reuse with SOA continues to be limited due to the lack of formal techniques for extracting domain knowledge from existing reusable software assets. In this paper,we present an approach that extracts the domain knowledge and service abstractions from design diagrams of existing software solutions and represents it in a form that can be reused in new projects. We have implemented our approach and preliminary results indicate that both domain knowledge and service abstraction thus extracted can promote reuse of software assets to a large extent."
4034,"As IT services become more powerful and complex, service deployment gets more difficult and expensive. Service deployment, the process of making a service ready for use, often includes deploying multiple, interrelated software components into heterogeneous environments. Different technologies and tools try to address these complexities by describing the environments, abstracting the dependencies, and automating the process. Virtual appliances, a set of virtual machines including optimized operating systems, pre-built, pre-configured, ready-to-run applications and embedded appliance specific components, are emerging as a breakthrough technology to solve the complexities of service deployment. Virtual appliances provide a simple, unified and easy to use interface for service deployment by encapsulating entire custom environments, and resolving the execution policy constraints and inter-dependencies through pre-installing the software applications. The motivation of this paper is to prove virtual appliances offer a better service deployment mechanism. We start with an easy to understand model to describe the complexity of service deployment and introduce the architecture of a virtual appliance. We then analyze the deployment process of using traditional deployment mechanisms, and quantitatively and qualitatively compare the deployment time, operations and parameters of the traditional approach with the use of virtual appliances. The results show virtual appliances offer significant advantages for service deployment by making the deployment process much simpler and easier, even for the deployment of advanced enterprise services."
4035,"Cloud computing has been gaining popularity for quite some time in various areas, on the infrastructure, platform and application level. Recently, the possibility to provide high performance computing (HPC) as a service has been investigated in conjunction with the cloud computing paradigm. While this is a viable solution for applications that do not require HPC in the truest sense -- with supercomputers which offer paramount performance regarding computation, network interconnect and storage -- there are HPC applications which cannot be realized in this way. HPC as a service can be offered for these applications as well, but it requires a different approach than the usage of cloud computing. The enhancement of mostly best effort based HPC with long-term service level agreements (SLAs) is a potential solution. HPC providers then need not only to decide on which service levels to offer but need to closely investigate the framework conditions for these service levels. The scheduling of these service levels is a difficult task and we simulate a proposed algorithm for providing guarantees on waiting times and the implications on other jobs. We investigate the influence of setting a maximum allowed job size on prioritized jobs and conclude that it makes sense to restrict this size for both clients and provider."
4036,"The need for having interoperable systems despite the presence of heterogeneous platforms has resulted in tremendous growth and acceptance of Web services in recent years. This success of Web services technology is fueling research efforts towards generalization into a science for service oriented computing (SOC). However, beyond the basic publish-find-bind model there is not much consensus yet on what are the architectural building blocks of a service oriented architecture (SOA). In this paper, we present a semantic model for SOC that is based upon established and well proven techniques of object oriented paradigm"
4037,"With the rapid development of cloud computing technology, cloud-based team collaboration applications are becoming popular on the Web. Among all the required features for a typical team collaboration application, shared storage for referred documents or produced artifacts by the team is a must-have one. However, existing shared storage solutions for team collaboration applications are far from satisfaction. Some of them rely on self-built storage infrastructure, which could be a big burden, especially for those small or medium vendors. With the prevalence of personal cloud storage services, such as Dropbox and Google Drive, more team collaboration applications allow users to share files from their personal cloud-storage spaces through external shared links, which can partly solve the problem. However, this method is not convenient for team collaboration, neither safe enough. This paper presents an approach to leverage third-part personal cloud-storage services to provide shared storage for team collaboration applications. Compared to existing approaches, our approach provides sophisticated mechanisms to make sure it's more convenient and safer. It brings benefits in three folds: for users, it improves the utilization of personal cloud storage space, for vendors of personal cloud storage service, it helps attract users to use their services, for vendors of team collaboration applications, it reduces the burden of developing self-built storage infrastructure. The approach has been tested in kAct, a task-based team collaboration application provided by Kingdee, and the results are promising."
4038,Context aware services are the next generation of the service computing paradigm. In this paper we explore a case study of creating context-aware services for the telecommunications industry
4039,"Stream processing is an increasingly popular model for online data processing that can be partitioned into streams of elements. It is commonly used in data analytics services, such as processing Twitter tweets. Current stream processing frameworks boast high throughput and low average latency. However, lower tail latencies and better real-time performance are desirable to stream processing users. In practice, there are issues that can affect the performance of these applications and cause unacceptable violations of real-time constraints. Some examples of these issues are garbage collection pauses and resource contention. In this paper, we propose applying redundancy in the data processing pipeline to increase the resiliency of stream processing applications to timing errors. This results in better real-time performance and a reduction in tail latency. We present a methodology and apply this redundancy in a framework based on Twitter's Heron. Then, we then evaluate the effectiveness of this technique against a range of injected timing errors using benchmarks from Intel's Storm Benchmark. Furthermore, we also study the potential effects of duplication when applied at different stages in the topology. Finally, we evaluate the additional overhead that duplicating tuples brings to a stream processing topology. Our results show that redundant tuple processing can effectively reduce the tail latency by up to 63% and that the number of missed deadlines can also be reduced by up to 94% in the best case. Overall we conclude that redundancy through duplicated tuples is indeed a powerful tool for increasing the resiliency to intermittent runtime timing errors."
4040,"The service-oriented paradigm offers support for engineering service-based systems (SBSs) based on service compositions. The selection of services with the aim to fulfil the quality constraints for SBSs and to achieve the optimisation goals is a critical and challenging issue. In particular, when the quality-of-service (QoS) constraints for a SBS are severe, it is often difficult to find an optimal solution for the SBS. Exploiting the competition among service providers can help SBS developers obtain favourable QoS offers for the component services of SBSs and increase the possibility of finding optimal solutions for the SBSs. In this paper, we present a novel joint optimisation and negotiation approach named Iterative Negotiation for Service Composition (INSC) that supports effective and efficient QoS-aware service selection for SBSs. We evaluate INSC experimentally using example SBSs that are synthetically generated based on a real-world Web service dataset. The experimental results show that INSC can significantly and efficiently increase the possibility of finding optimal solutions in severe service composition scenarios."
4041,"Among the panoply of applications enabled by the Internet of Things (IoT), smart and connected health care is a particularly important one. Networked sensors, either worn on the body or embedded in our living environments, make possible the gathering of rich information indicative of our physical and mental health. Captured on a continual basis, aggregated, and effectively mined, such information can bring about a positive transformative change in the health care landscape. In particular, the availability of data at hitherto unimagined scales and temporal longitudes coupled with a new generation of intelligent processing algorithms can: (a) facilitate an evolution in the practice of medicine, from the current post facto diagnose-and-treat reactive paradigm, to a proactive framework for prognosis of diseases at an incipient stage, coupled with prevention, cure, and overall management of health instead of disease, (b) enable personalization of treatment and management options targeted particularly to the specific circumstances and needs of the individual, and (c) help reduce the cost of health care while simultaneously improving outcomes. In this paper, we highlight the opportunities and challenges for IoT in realizing this vision of the future of health care."
4042,"On-demand resource provisioning is with great challenge in cloud systems. The key problem is how to learn about the future workload in advance to help determine resource allocation. There are various prediction models developed to predict the future workload. The major problem of previous researches is that they assume that application workload has static pattern. In practice, so many application workloads have hybrid dynamic pattern overtime. To achieve high prediction accuracy, we find that it's essential to detect both workload pattern stage and the changes in the model parameters. In this paper, we present a Pattern Sensitive Resource Provisioning Scheme, named PSRPS. It can recognize application workload patterns and choose suitable prediction models for prediction online. Besides, when there is maladjustment in prediction models, PSRPS can switch prediction models or adjust the parameters of the model by itself to adaptively to guarantee prediction accuracy."
4043,"Typical solutions for Web services composition problem develop a single intermediary which mediates or choreographs computation and communication among the existing services to realize a target/goal service. However, such a centralized choreography mechanism can involve communication/computation overhead that can be reduced through its decentralized realization. With this as motivation, we study the problem of synthesizing a decentralized choreography strategy that will have an optimum overhead for service composition by developing a set of site-specific choreographers working concurrently to implement a desired goal service. Each communication/computation is quantified by a cost. We develop an algorithm that takes as input the existing services, the goal service, the costs and produces as an output a set of site-specific choreographers that optimally realize the goal service using the existing services. The contribution lies in the formulation of the optimal decentralized choreographer synthesis problem as well as its solution and extends our earlier work in two ways: decentralization of solution and its optimality."
4044,"Service contract matchmaking represents a promising path towards the definition of accurate service selection mechanisms. Basically, it consists in evaluating the degree of match between preferences explicitly defined by a service customer and the contractual terms offered by service providers. However, service contract matchmaking assumes that a service customer is always aware of the contractual terms on which to specify preferences. In reality, a service customer often specifies preferences only on a very limited set of common and well-known terms (e.g., price). This paper proposes a new approach to service contract selection that adopts the value in use concept to quantify the trade-off between benefits and sacrifices related to contractual terms on which the customer has omitted to specify preferences. Experimental activities in the car insurance domain demonstrate the effectiveness of the approach."
4045,"Many systems are built with very little foresight for incorporating changes resulting from new requirements. This paper provides a systematic way and normative guidance to analyze the variations in SOA solution design. During SOA solution design, two types of change- oriented analysis patterns, variation-oriented analysis (VOA) and event-driven change analysis (EDCA), are proposed to create a resilient design that is adaptive to changes. Specifically, a meta-data model and associated assets for solution design are introduced first to capture the information context for SOA solution design. Then a new change propagation analysis process is presented to summarize the variation identification, variation impact assessment, variation impact path identification, variation analysis and decision making. Based on these two techniques, this paper creates a set of normative guidance for the variation oriented design. These guidelines include but not limited to the artifact type, variation description, variation type, variation artifacts, and impact analysis."
4046,"Web service composition is a standard approach to create value-added services from existing ones. As the Web services on the Internet grows, there are more and more services providing identical functionalities while differing in their non-functional properties (NFPs). However, most of the existing techniques for NFP-aware service composition consider either only quantitative NFPs or only qualitative NFPs. In this paper, we present a service composition model considering both quantitative and qualitative NFPs. We propose two algorithms for conducting service composition. One combines global optimization with local selection into one mechanism. The other is a genetic algorithm based solution. We have conducted extensive experiments to evaluate the effectiveness of our proposals."
4047,"Protecting sensitive information while preserving the share ability and usability of data is becoming increasingly important in the outsourced business process industry. Particularly in the context of call-centers a lot of customer related sensitive information is stored in audio recordings. In this work, we address the problem of protecting sensitive customer information in audio recordings and Automatic Speech Recognition (ASR) transcripts. The high word error rates, spontaneous nature of communication and the variability in agent-customer interaction makes it harder and expensive to craft rules or build annotators to detect sensitive information. In this paper we propose a semi supervised method to model sensitive information as a directed graph which is automatically generated from ASR transcripts. Vocabularies specific to the nodes are generated using features of context sensitive clusters. The direction and weight of the edge capture the ordering and timing constraints respectively for these features. These constraints are learnt from the time stamps associated with ASR transcripts. The effectiveness of this approach is demonstrated by applying it to the problem of detecting and locating credit card transaction in real life conversations between agents and customer of a call center."
4048,"Cloud environments are being increasingly used for deploying and executing business processes and particularly service-based business processes (SBPs). One of the expected facilities of Cloud environments is elasticity at different levels. In this work, we argue the need of supporting elasticity at the service level. We propose a formal model for describing elasticity mechanisms and their strategies for SBPs. Our contribution consists in composing SBP models with models of controllers that describe the behavior of services and container replicas. In addition, we provide means for specifying both reactive and predictive elasticity strategies."
4050,"Computing resource provisioning through the use of the Cloud computing paradigm has triggered revolutions in modern day computing. It is a new paradigm for deploying services on rented machines. On the other hand, Service Oriented Architecture (SOA) has gained wide adoption among organizations due to the importance of collaborations and outsourcing. Therefore the Cloud's enormous capacity with comparable low cost makes it an ideal platform for SOA deployment. The overall correctness of the SOA deployed in the Cloud depends on the correctness of all individual participants. As the SOA usually spans multiple administration domains, concluding the faulty service and making the provider responsible become a challenging task. In this paper, we propose a novel design to achieve Trustworthy Service Oriented Architecture (TSOA) in the Cloud through enforcing strong accountability. In such system not only the root of a fault can always be concluded to the guilty participant(s), each conclusion is supported with non-disputable evidence. We also implemented a demonstrative system to show its effectiveness in real practice. Our testing figure indicates the cost of incorporating our design to SOA in the cloud is acceptable."
4051,"Service-Oriented Architectures (SOA), and Web Services (WS), the technology generally used to implement them, achieve the integration of heterogeneous technologies, providing interoperability, and yielding the reutilization of pre-existent systems. Model-driven development methodologies provide inherent benefits such as increased productivity, greater reuse, and better maintainability, to name a few. Efforts on achieving model-driven development of SOAs already exist, but there is currently no standard solution that addresses non-functional aspects of these services as well. This paper presents an approach to integrate these non-functional aspects in the development of web services, with an emphasis on security."
4052,"Different types of business collaborations exist in terms of the way that the collaboration is carried out. In this paper, we will look into the characteristics of different collaborations and analyze their policy requirements accordingly. Various inconsistencies between authorization policies from different business units are identified and suggestions are made according to different types collaborations."
4053,"The development of web services and web APIs offers software developers great opportunities for choosing reliable services. However, the quality of these web services are often not available. Existing quality of web service prediction methods adopts the recommender system related techniques to predict the service quality. In these approaches, the behaviour of service invokers do not change. In reality, the service invokers network conditions are changing all the time. This fact inspires us jointly to consider the stability of service invokers network environment when building a prediction model. In specific, a reliability model is adopted for stability calculation and a recommendation algorithm is proposed in this paper. The advantages of our proposed algorithm is confirmed via experiments on a real-life quality of web service data set and comparison with existing quality of web service predicting algorithms."
4054,"Service composition is the technique of creating new services by combining several existing services. Composite services can be also combined with other composite services to form nested or hierarchical services. Given that service composition depends on the interoperability created by using common network protocols and invocation interfaces, a composite service can have an impractically large number of variations depending of the number of available services and the composite's structure. It is hard to enumerate and maintain all variations possible. To solve this problem, we introduce a higher-order function that can take functions as parameters to allow function invocation. In concrete terms, we propose the following methods: (1) a hierarchical service composition description by introducing higher-order functions and (2) a method to implement (1) in an existing composite service execution system. As a test, we apply the proposals to Language Grid, and evaluate the results. They show that our methods can reduce the number of variations that need to be registered and managed even though their overheads are quite practical."
4055,This paper discusses the web service and scientific workflow abstractions to next generation ab initio computational nuclear physics resources as part of the Leadership Class Configuration Interaction (LCCI) Environment. These abstractions will rapidly and efficiently involve new collaborators and graduate students in productive research. The workflow infrastructure democratizes the access to the nuclear physics simulations executing on remote supercomputing resources. The paper focuses on employing an open community based workflow system in developing and deploying LCCI infrastructures. The paper also emphasizes on the enhancements made to infrastructure to add advanced workflow capabilities providing greater flexibility in handling parametric sweeps and provenance aware workflows. The paper discusses on how the provenance integration will not only capture execution trace but dynamically modify the workflow graph at run time to re-use retrospective execution results.
4056,"Remote monitoring and intelligent intervention of machines are essentials in a smart industrial environment. However, the heterogeneity of machines makes it difficult to construct solutions for this purpose. This work aims to describe how to provide smart industrial environments using Smart Gateways, RESTful Web Services, and Cloud Computing. In the proposed solution, a Smart Gateway is responsible for representing a given machines' resources as RESTful Web Services for clients and Web applications, sending monitored data to a persistence service in a Cloud, while also allowing for the remote control of these machines. As a proof of concept, a smart industrial environment was developed from the considerations of a real case study at an apparel factory. Functional testing and performance evaluation confirmed the feasibility of the solution in this case study."
4057,"The one-way messaging pattern, in which a message sender does not expect any response, is fast and convenient for many applications, but whenever reliable communication is needed, developers either use heavy-weight middleware, such as JMS, or implement request-response interactions, based on TCP. However, TCP is poorly adapted to one-way messaging, because it offers a streaming channel with no mechanisms to encapsulate or track messages. Moreover, TCP does not tolerate connection crashes, forcing developers to come up with their own custom, error-prone solutions, to recover from crashes. In this paper, we propose three TCP-based design patterns that address these limitations, and facilitate developing light-weight and reliable one-way message-based applications. Our solutions are correct, modular, and involve low programming complexity."
4058,"Today, the risk of a service related change is typically assessed at change record creation time by a Change Requester either manually or through answering a fixed set of questions. Assessing the risk of a change, thus, relies heavily on one person's opinion. Further, in the questionnaire method, a fixed set of questions implies that the change context is assumed to be the same regardless of the type of change being raised, whereas in practice, no two changes are truly identical. Such deficiencies of the standard practice to assess risk of a change may, thus, result in inaccurate assessment, which can lead to unmitigated risks, ultimately materializing as failures. We present a novel Risk Engine, which takes into account a rich, dynamic change context to calculate and mitigate the risk of a service related change in real-time with increased accuracy and reliability. We describe the recipe for creating the Risk Engine along with several user studies we have conducted to justify our design choices at each step. Our initial pilot results reveal that it is possible to drive the number of change failures down significantly using our novel Risk Engine."
4059,"Instant messaging systems such as ICQ and Wechat enable interpersonal and pervasive interaction via the communication networks as well as the Internet. Using the same architecture and principles, a specific system called IMAS was developed for trading exhibitions. IMAS supports Android environments, helps to establish and maintain business links, and mediates requirements of visitors and offers of exhibitors. Both visitors and exhibitors can benefit more from the system if they can not only establish contacts and make schedules with each other, but also use and mashup further information services that are provided by the backend systems of different exhibitors. This paper reports on our efforts to extend IMAS with data services and mashups thereof, presents some intermediate progress, and discusses some hard issues encountered during the extension."
4060,"Nowadays, faced with heterogeneous, isolated content service systems, people require peering or allied content services. Firstly, we analyze some existing content service hybrid schemes, such as CDN over P2P Architecture and Peering of CDN Architecture. Based on problem statement, we propose A Novel Web Services-based Content Delivery Service Peering Scheme (WS-CDSP), and we describe scheme architecture design, which support multiple-to-multiple loosely-coupled peering services through Web Service Information Endpoint and Web Service Management Endpoint. And then we focus on main system function design, Web Service Information Endpoint and Web Service Management Endpoint design mechanism. After that, We discuss CDN or VoD composite resource model based on CIM. As part of experiment work, we discuss WS-CDSP scheme implementation and comparative experiment, and then analyze the experiment results. At last, from our research experiences and related survey, we analyze the prospective research direction and challenges in this field."
4061,"Data processing on the cloud is increasingly used for offering cost effective services. In this paper, we present a method for resource allocation for data processing services over the cloud taking into account not just the processing power and memory requirements, but the network speed, reliability and data throughput. We also present algorithms for partitioning data, for doing parallel block data transfer to achieve better throughput and allocated cloud resources. We also present methods for optimal pricing and determination of Service Level Agreements for a given data processing job. The usefulness of our approach is shown through experiments performed under different resource allocation conditions."
4062,"In this work we look into the domain of process security from a service perspective. Most often process security has been enacted through service level agreements (SLA) and business agreements. However, in a multi-party environment such as business process outsourcing (BPO) where processes themselves are offered as a service, the qualitative nature of SLA makes their monitoring quite difficult and their implementation through various restrictions, quite costly. We present our approach wherein we provide security as a process represented using e-Contracts and enacted through workflows. We explore if security too could be offered as a service which could be enacted and monitored by the process participants themselves; thus ensuring more trust. Most of the process based systems employ either Task Based Model (TBAC) or Role Based Model (RBAC) for granting privileges that are needed for executing the individual activities of the workflow. Current approaches are either potentially weak from security perspective, as they grant even those permissions to user which are actually not needed by him for executing the tasks, or they have very high administrative overhead. In this paper, we propose to couple RBAC with TBAC and additionally enforce sequential and temporal constraints over them so that process participants get only 'Need to know information' with less administrative overhead. In this paper, we propose our extended e-contract framework for security (EC framework), and the architecture of a system which implements it. In the end we present a briefcase study presenting our process security model."
4063,"In order to coordinate multiple resource providers in grid environment to meet a common objective, support for negotiation is needed to establish a contract between the users and the resource providers that clearly states the QoS required, restrictions on resource utilization and penalties during violation of the objective. In this paper, we propose an architecture called GSMA (grid SLA management architecture) that supports entire operations in SLA lifecycle such as negotiation, creation, monitoring, violation, enforcement and destroy. We propose a deviation based resource ordering algorithm (DRS) and successfully automates the SLA negotiation process with backing up of resource support. The negotiation process is implemented based on WS Agreement specification. Simulation results against gridway meta scheduler shows improved performance interms of average SLA creation time, success rate and throughput."
4065,"Summary form only given. This tutorial will introduce the participants to the area of QoS specification and management for XML Web services. It will explain the importance of this topic and why the widely used basic Web service technologies are not enough. Further, it will give an overview of a number of languages developed for QoS specification for Web services, as well as a number of research infrastructures, industrial products, and standardization proposals that offer some forms of QoS management for Web services. The achieved results and open topics for future research will be critically analyzed."
4066,"Keane is one of the world's leading providers of business and IT services. Several Keane outsourcing engagements inherited applications that play an important role in sustaining vital business transactions for high caliber clients. However, the majority of these applications were in the form of legacy systems that were based on archaic designs. This experience report summarizes several projects that were undertaken by one Keane engagement, in Southfield, Michigan, which targeted the conversion of multiple monolithic components into a Web Service based architecture that strives to comply with the latest SOA standards."
4067,"Large-scale cooperation support for learners becomes even more important, when e-Learning is implemented in scalable, open, dynamic and heterogeneous environment. We have designed grid architecture and implemented grid middleware and higher CSCW services for establishing collaborative platform for e-Learning. A Learning Assessment Grid, abbreviated as LAGrid, is built on top of these services. This paper introduces the layered LAGrid architecture and presents how to implement grid middleware to support collaborative platform for e-Learning using Web services technologies. The work shows service-oriented grid middleware layer provides a good middleware platform for collaborative applications in large-scale cross-organization environment."
4068,"Service constraints are usage restrictions on service features that are imposed by service providers. Such constraints need to be verified prior to the execution of a service in order to ensure correct service execution. In the case of composite services, the set of applicable constraints is derived from the service constraints defined over the individual service components that are part of the service plan. During the execution of a composite service, a constraint-aware composite service execution model can be used to manage and eventually operationally verify the service constraints prior to the corresponding service's execution. In addition to service constraints, other constraints might be imposed to put externally-defined restrictions on composite services. Such externally-defined restrictions are likely to be defined and become or cease to be applicable after the composite service has been assemble and deployed. Such a situation requires adaptation of the plan to a set of externally-defined constraints. Current web service composition adaptation approaches only focus on adaptation to failure in functional capabilities or Quality of Service (QoS) properties which can be dealt with re-construction of the composite service. However, we argue that adaptation to external constraints does not necessarily require changes in the plan of a composite service. In this paper, we define a constraint-based composite service model that not only considers service constraints, but also adapts composite plans according to new constraints that might add new restriction to the composite service at run time. A publicly available test set generator is used to compare the proposed solution with other existing service adaptation solutions."
4069,"Non-functional property is of paramount importance for Web services to function. However, service selection with consideration of Quality of Service is a challenge. This paper presents an approach to service selection by measuring the similarity between the web service publication and the service request. We identify five tendencies of the nonfunctional dimensions in the service requests, and present a flexible matchmaking approach which enables users to preset the negotiable preferences. Unlike other service selection approaches, we obtain the relative distance to compute the similarity score for generating the ranking list. Specifically, we introduce a novel method for evaluating the similarity between two categorical values based on the semantics and set theory."
4070,"Context-awareness (CA) has been applied in different domains, particularly in ubiquitous computing, to provide better value-added services. CA has also been used, albeit sporadically, in business related applications. This paper discusses an on-going research effort in enhancing Business-to-Business (B2B) collaborations through context awareness, specifically to support the formation of short and dynamic connectivity between partners collaborating in a supply chain. Through an understanding of context awareness and how companies evaluate and select prospective suppliers, a B2B Context Model is proposed to discover and match suitable partners. The prospective partners are represented as services in a service-oriented architecture (SOA) paradigm."
4071,"With the development of cloud computing, there is a growing number of virtual machines (VMs) in the IaaS cloud. The VM owners can install different kinds of software on demand. However, if the software is not updated in time, it would be a great threat to the security of the cloud. But for the VM owners, it is a tedious task to keep all of the installed software up to date. In this paper we present a new software update model called UaaS (Update as a Service) to handle the VM (online or offline) update automatically. Multiple VMs share one software update service and multiple update strategies are provided for single software, which can be customized at any time. The ability of UaaS has been evaluated by our experiments, and the results show that UaaS can provide software update service successfully and complete update task for lots of VMs with multiple update strategies efficiently."
4072,"Cost and competitive pressures are forcing business organizations to reuse assets from repositories, rather than develop them from scratch. But this has been hampered by some issues that have not been addressed so far. First, there is a lack of a mechanism for the representation of business process assets as variants and versions in repositories. Second, there is no formal means to compare between different variants and versions of an asset and determine which is the best to select for reuse. Third, there is a lack of a technique to determine the extent to which a business process asset could be customized for reuse. In this paper, we address the above research issues by presenting an integrated approach for modeling, analyzing, and searching business process assets in a repository for enhancing reuse. We demonstrate our approach on a large repository of business process assets in the insurance domain."
4073,"With the growing popularity of Internet of Things (IoT) services being applied in several aspects of real-life applications, performance has become an important requirement. Meanwhile, the techniques for reliability enhancement such as virtual machine migration and recovery also have significant impact on end-to-end performance. This paper proposes a predictive approach of reliability-aware performance evaluation for recoverable IoT services using the modeling techniques of generalized stochastic Petri net (GSPN). Mathematical models formulating the dynamics of both server clusters and IoT systems are presented, and quantitative analyses of performance metrics are provided. Empirical experiments based on real-world data obtained from IoT services and cloud systems are conducted, and parameter settings as well as experimental results are discussed in detail."
4074,"IT services delivery is a complex ecosystem that engages 100000s of system administrators in service delivery centers globally managing 1000s of IT systems on behalf of customers. Such large-scale hosting environments require a flexible identity management system to provision necessary access rights, in order to ensure compliance posture of an organization. A popular and effective access control scheme is Role Based Access Control (RBAC). Ideally, a role should correspond to a business function performed within an enterprise. Several role mining algorithms have been proposed which attempt to automate the process of role discovery. In this paper, we represent the user-permission assignments as a bi-partite graph with users/permissions as vertices and user-permission assignments as edges. Given a user-permission bi-partite graph, most role mining algorithms focus on discovering roles that cover all the user-permission assignments. We show that by relaxing the coverage requirement, one can improve the accuracy of role detection. We propose a parameterized definition of a role based on graph theoretical properties, and demonstrate that the role parameters can be controlled to balance the accuracy and coverage of the roles detected. Finally, we propose a heuristic to illustrate the efficacy of our approach and validate it on real and artificial organizational access control data."
4075,"Up to now, developing software for the Semantic Web is much more complex than developing software for other data representation paradigms, such as relational databases. Software development for relational databases is dramatically simplified by so-called object-relational mappers. Due to the conceptual difference between relational databases and the Semantic Web, the design patterns for object-relational mapping cannot directly be used for linked data. In this paper we show how design patterns for object-relational mapping can be used to achieve the more complex object triple mapping, which will make developing Semantic Web-enabled software much easier."
4076,"This paper describes a multi-agent reinforcement learning model for the optimization of Web service composition. Based on the model, we propose a multiagent Q-learning algorithm, where each agent would benefit from the advice of other agents in team. In contrast to single-agent reinforcement learning, our algorithm can speed up convergence to optimal policy. In addition, it allows composite service to dynamically adjust itself to fit the varying environment, where the properties of the component services continue changing. Our experiments demonstrate the efficiency of our algorithm."
4077,"Business processes change over time in response to changing circumstances, it is crucial for process managers to discover and understand such changes from event logs. While existing techniques in process mining cannot efficiently detect and locate concept drifts in uncompleted event logs. Hence, an appropriate method, which could prompt to unravel the process evolution, may greatly assist organizations to manage the flexibility and change of business processes in the context of business process management (BMP). In this paper, we first proposed an extensible feature and employed it with the sliding window technique and heuristic miner for detecting and locating concept drifts in uncompleted event logs. Then, Genetic Process Mining (GPM) was improved by the Weight Heuristic Miner (WHM) and Differential Evolution (DE) for mining the new process model of an evolving process. Finally, experimental results on four event logs which are created based on a real-world business process have validated the proposed method."
4078,"We proposed service processing agent which enables end-users to use context aware services easily. In ubiquitous network environments in the future, context-aware services are expected because the context-awareness is one of the most important factors in ubiquitous network. Therefore, the flexible service composition based on user context is required. Our proposed approach is that a service processing agent searches and selects suitable service components based on the user context and binds them dynamically. Users can send a request to the service processing agent in usual Web service messaging. Through the study, implementation and performance measurement, we showed the effectiveness and practicality of our proposed method."
4079,"Today's computationally able mobile devices are capable of acting as service providers as opposed to their traditional role as consumers. To address the challenges associated with the development of these mobile services, we have developed Odin, a middleware which masks complexity, allowing rapid development of mobile services. Odin, however, does not allow cross-platform development, which is an important concern with today's wide variety of mobile devices. To solve this problem, we have designed Odin Tools - a model-driven toolkit for cross-platform development of mobile services. Leveraging appropriate metamodels, a prototype has been implemented in Eclipse and Marama that allows developers to model mobile services in a platform-independent manner. We are currently working on transformations between levels of the model hierarchy which will allow full Odin-based service implementations to be generated automatically."
4080,"While there are several metrics to measure business process performance, recently there is an additional requirement from businesses to evaluate business processes based on their impact on users. In this work, we evaluate business process performance using social media analytics. We view a marketing campaign as a business process and we evaluate its performance based on its impact on the Twitter. We propose a new way to calculate the ""follow"" relationship in Twitter based on the users' reaction to the marketing campaign process activities and we use time series and sentiment analysis for defining and measuring performance. We re-build the Twitter graph based on users' reactions to the marketing activities in time and we are using community detection algorithms to identify the size of the ""follow"" community and thus we define metrics to calculate the impact of the marketing/campaign process. We evaluate our approach using a dataset for a given politician. We re-construct the campaign process as a set of activities on specific topics (promotions) in time using LDA. Our results show that social media analytics can be used as a valid metric for assessing business processes performance."
4081,"In this paper we identify several typical business service modes in a general distribution system composed of a single distributor and multiple retailers. For different service models, the business process can be different and each member of the distribution system may observe different level of information. Each member makes decisions to minimize its long-run-average cost or maximize its profit. A reinforcement learning algorithm is applied to obtain the decision policies and system costs. By comparing the system costs in various business service modes, we show numerically the impact of different business service modes on the distribution system. The results provide managerial insights into actual decision making in distribution systems."
4082,"IT management costs increasingly dominate the overall IT costs. The main hope for reducing them is to standardize software and processes, as this leads to economies of scale in the management services. A key vehicle by which enterprises hope to achieve this is cloud computing, and they start to show interest in clouds outside the initial sweet spot of development and test. As business applications typically contain multiple images with dependencies, one is starting to standardize on multi-image structures. Benefits are ease of deployment of the entire structure and consistent later management services for the business applications. Enterprises have huge investments in their existing business applications, e.g., their web design, special code, database schemas, and data. The promises of clouds can only be realized if a significant fraction of these existing applications can be migrated into the clouds. We therefore present analysis techniques for mapping existing IT environments to multi-image cloud templates. We propose multiple matching criteria, leading to tradeoffs between the number of matches and the migration overhead, and present efficient algorithms for these special graph matching problems. We also present results from analyzing an existing enterprise environment with about 1600 servers."
4083,"The tutorial aims at providing a deep comprehension of the Web Service Composition problem and automated techniques to tackle it. Web Service Composition is currently one of the most hyped and addressed issue in the Service Oriented Computing. Starting from an analysis of current technologies and standards for Web Service Composition, the tutorial will lead the attendees to consider formal models at the base of current proposals, and techniques that can be fruitfully considered to address automatic composition synthesis in each of them. More in detail, attendees will consider: (i) basic technologies and standards for Web Service invocation and description (SOAP, UDDI, WSDL, ...); (ii) advanced technologies and standards for orchestration and inter-organizational process enactment, in particular WSBPEL and WS-CDL; (iii) models for Web Service composition; (iv) formal tools for both data-centric and process-centric synthesis, including query reformulation a'la Data Integration, transition-systems based formalisms, trace-based formalisms, logics of programs and processes. In particular, we will show how these formal tools can be applied for Automatic Web Service Composition; (v) current state-of-the-art research results in automatic service composition, drawing a comparison and defining a unifying framework."
4084,"Security is a critical issue. The wide use of Internet applications has tremendous benefits, but a security fail can expose confidential data. Web services is a software application that can be remotely accessed over Internet using interoperable standards based on XML. At this time, there are no broadly adopted specifications for Web services security and we present an extension to the SRP (Secure Remote Password) protocol in order to apply access control to a Web service as an independent layer. This additional independent layer can be built on any available Web service platform. We have used Apache AXIS for this end."
4085,"For protecting grid resources and grid applications, a behaviors-based trust model is introduced. The behaviors and trusts dynamically judge the secure levels of grid entities through the their history and actuality of behaviors. In this paper, the trust relationships among users, resources and applications are discussed, a new trust model based on behavior tracks is proposed by improving the components of reputation in traditional trust mode, and then an simple experimentation is given in our experimental grid using this model."
4086,"In this paper, we present some of our experiences from the architectural design phase of business information analysis service provisioning system, BISON. More specifically we discuss the issues, challenges, and our solution approaches to asynchronously scheduling the service requests in an optimal manner to enhance the user experience in the system"
4087,"In this work we examine how to optimally plan for multi-channel communication campaigns. We primarily consider proactive campaigns that can take advantage of predicted risk or other behavioral propensity measure relating to the intended communication recipient(s). These predicted or estimated measures identify portions of the population that the servicer/campaign originator is interested in targeting. For example, in the application of loan servicing various risk segments low, medium and high predicted risk can be delineated and targeted differently. Each group not only has a different propensity to pay back a loan or liability but also different levels of sensitivity/conversion rates (to current status) with regards to communication campaigns. First we derive risk models that can predict the payback risk associated with loans at different times. The prediction techniques are not covered in this work. We then formulate the risk-driven campaign optimization problem and sub-problems, and solve them using various efficient dynamic techniques. Two main sub-problems are proposed relating to either single-or multi-channel communication. Using empirical values of cost structures and response rates from pilots we conducted in financial services, we were also able to run simulations to delineate regimes of optimal operation that might be appealing to loan servicing entities. Our simulation results demonstrate the optimal operational conditions with respect to different risk profiles, and at the same time, show how these optimal operational conditions change with respect to user defined parameters such as preference of one communication channel over the other."
4088,"Cloud-based services have become a cornerstone of today's IT. The self-service feature inherent in Cloud systems allows customers to play a greater role in service procurement. However, this restricts the value propositions and Service Level Agreements (SLAs) that Cloud providers offer because Quality of Service (QoS) and Non Functional Property (NFP) requirements vary from customer to customer. In feature-rich SLA templates, the contract space gets large, objectives are confidential and preferences over QoS and NFP often conflict between providers and customers. Hence, an SLA-gap exists between the two and contemporary providers bind their offerings to the inflexible take-it-or-leave-it SLAs. In this work, we address this problem by presenting a robust and computationally inexpensive negotiation strategy, using which agents can efficiently create near-optimal SLAs under time constraints. Experimental evaluations validate that our strategy performs at par with state of the art learning and non-learning strategies against a variety of metrics including utility, social welfare, social utility and the Pareto-optimal bids. This enables a dynamic SLA negotiation mechanism on top of our OpenShift (PaaS) based Cloud system designed using Service Oriented Cloud Computing Infrastructure (SOCCI) architecture. Negotiated procurement of services is shown to improve satisfaction of participants and reducing the SLA-gap."
4089,"In this paper, we consider the change management process for enterprise IT services with the goal of improving the efficiency of this process, i.e., minimizing change completion time and maximizing the ""change capacity"". The change management process handles critical updates in the system that must be implemented by a set of executing personnel. In presence of such timing constraints and scheduling conflicts between change classes, we argue that addressing the application change management question optimally involves computing solutions to NP-hard problems. Using appropriate simulation models, we evaluate various batched scheduling alternatives to understand the effect of simple process re-engineering on the change completion time, and the change capacity of the system. In addition, we examine the benefit of executor cross-training and degree of conflicts on the performance of the system. Our results indicate that a simple longest queue based scheduling approach works well in a wide range of practical scenarios. Based on these results, we recommend process re-engineering based on longest queue based scheduling with a small degree of executor cross-training."
4090,"Service Oriented Architectures (SOA) mainly rely on the service composition mechanism which supports the reusability. Numerous researches focus on this thematic in order to satisfy all the objectives of quality fixed by the SOA community. One of these objectives is the notion of loose coupling between the composition's services. However this notion is intuitively understood and tangible, its definition lacks formalism. This unclear picture of the loose coupling is reflected in limited metrics which do not allow for its quantitative and objective evaluation. In this paper, we propose a set of different metrics which are associated with a clear definition of the loose coupling notion. These metrics act as guidelines for architects to develop their composite service. They are gathered in a formula which allows for evaluating the global coupling of a composition of services. Moreover, they are also used as comparison criteria for classifying existing composition models."
4091,"The current efforts on programming grid applications often rely on service-oriented approaches like grid services. This work presents HOC-SA -a service architecture for higher-order components, which provides the programmer with reusable and composable patterns of parallelism and is interoperable with the latest Globus toolkit implementations. We describe our implementation of HOC-SA using OGSA-DAI, a framework for integrating grids with distributed databases. We present a simple example application and report first measurements on our grid testbed."
4092,"In Serviced-Oriented Computing (SOC) environments, the trust level of a service or a service provider is a critical issue for a service client to consider, particularly when the client is looking for a service from a large set of services or service providers. However, a service may invoke other services offered by different providers forming composite services. The complex invocation relations significantly increase the complexity of trust evaluation in composite services. In this paper, we propose a novel algorithm for trust evaluation in composite services that takes all atomic invocations into account, which is essential for composite services selection and discovery."
4093,"Personalized service recommendation becomes increasingly essential because of the growing number of services. To enhance the performance of personalized service recommendation in collaborative tagging systems, not only tag information but also time and social relations information should be considered. In this paper, we propose a hybrid method aiming at taking advantage of tag, time and users' social relations information for a preferable service recommendation. We first improve a simple tag-based recommendation method by a time-decay function. Then we develop a temporal social-based recommendation method which analyzes user familiarity and user preference similarity between friends. Based on these two steps we integrate them as a temporal tag-and social-based (TTS) recommendation algorithm. Experiment results indicate that our method outperforms general tag-based and social-based recommendation methods."
4094,"Business processes have been identified as effective means to developing service-based applications. It is an important and challenging research problem to check consistency between conceptual and executable business processes. Most existing approaches analyze the consistency based on qualitative equivalence relations between business processes and only provide a ""true""/""false"" result. Thus, they fail to differentiate slight inconsistency scenarios from totally inconsistency ones. To address this problem, we leverage activity constraints, i.e., partial orders, mutual-exclusions, and independences, to analyze consistency, and measure the consistency degree (ranging from 0 to 1.0) between a conceptual business process and an executable one based on the rate of consistent activity constraints. We show the applicability of our approach by analyzing the consistency between public views and private processes of some real-life BPEL processes."
4095,"Prepaid charging, an essential option for the accounting of cloud services, provides effective financial control for both service providers and customers. However, it has to be supported by real-time credit checking and cost calculation. These real-time actions consume resources of the providers' network and impose high overhead. To tackle this issue, we present a scalable accounting solution in which an accounting component is hosted in every cluster constituting a cloud system. Each of our accounting component supervises service consumptions based on a calculated interval of a service bundle that is composed of all services hosted in a cluster and consumed by one customer simultaneously. Credit will be re-allocated when a customer's credit in one cluster is not enough to compensate further usage, and the allocation is performed based on service consumptions. This work is intended to reduce the cost of prepaid services and to ensure service provision is not hampered by the charging part. Additionally, we perform theoretical and experimental analyses that indicate this work can provide an inexpensive accounting solution for the long-lived services in storage clouds."
4096,"Specifying and monitoring service level agreements (SLA) has been the subject of intensive research. However methods for enforcing SLA have not addressed the specific issues of composite web services. Our work focuses on the problem of ensuring prearranged SLAs for service workflow via SLA aware workflow scheduling (SLAAWS). The novel scheduling algorithm takes into account the workflow structure, utilization of component services and quality of service (QoS) obligations defined in SLA."
4097,"Next-generation applications based on Web services impose additional requirements on the use of coordination protocols with various optimizations, such as the two-phase commit protocol (2PC). We analyse the well-known 2PC optimizations presumed commit and presumed abort, and present an improved 2PC that is suitable for Web services based applications. More specifically, the protocol allows every individual service provider to choose dynamically the most appropriate presumption for any distributed transaction. The protocol does not introduce extra overhead to the previous 2PC variants in terms of number of messages and log records, and it is easy to understand and realize."
4098,"The rapid growth of services and the Internet of Things vision lead to the future of Internet in which a massive number of services are available and connected to each other. In such service network, dependency between services potentially causes cascading failure, where the failure of one service can cause the failure of dependent services. Cascading failure tolerance is determined by the topology of the network and the degree of service interdependency. As to the former, we analyze cascading failure in scale-free, exponential, and random service networks. We find that scale-free topology has generally the highest tolerance. This is contrast to cascading failure in power network, where random topology provides better tolerance. For the latter, we find that the number of cascade failed nodes increases as the inverse of the average number of alternate services, e.g. Functionally equivalent services. This suggests that increasing the number of alternate services can significantly improve the network tolerance if each service only has few alternate services available."
4099,Monitoring provides well established means to track the exchanged messages in cross-organizational processes. Complex Event Processing (CEP) is a technology used by monitoring systems that aims to process incoming events and generate high-level information. Current implementation of CEP has performance issues when the processing of events involves calling a composition of services to generate the required information. In this paper we develop a conceptual framework that relies on a set of patterns that maximize the parallel processing of incoming events. These patterns are used to split the composition of services into a set of blocks that can be executed in parallel while ensuring an effective correlation of the intermediate results. Our approach significantly decreases the processing time without requiring too many resources. We implemented a prototype of our framework and conducted performance evaluation in comparison with existing software.
4100,"The LARIAT system developed at IBM Research uses information extraction applied to news feeds and other time-sensitive documents, along with historical and enterprise data, to provide a stream for B2B sales leads to different sales teams. This paper overviews the system and discusses lessons learned. LARIAT is contrasted with the IBM infoSage system from almost two decades ago. The experience with LARIAT is used as the basis for the design of a Solution-as-a-Service framework that will enable a richly extensible version of the capability, which could serve multiple B2B companies while affording economies of scale."
4101,"The sensor as a service is an emerging application of the services computing. However, how to implement such sensor services efficiently and reliably is an open issue. This paper presents an application framework, called Sensor Service Framework (SSF), that supports developers to build and deploy sensor services in the home network system (HNS). The SSF prescribes device-neutral features and APIs for the sensor devices to be deployed as Web services. Writing a small amount of code with the SSF, the developer can easily deploy any sensor device as a service in the HNS. The sensor service can provide a standardized access to heterogeneous sensor devices, as well as a context management service with user-defined conditions. We then present a sensor mashup platform (SMuP), which allows the dynamic composition of the existing sensor services. To support non-expert developers, we also implemented a GUI front-end, called Sensor Service Binder (SSB). The proposed technologies are implemented and evaluated in an actual HNS to demonstlate practical feasibility."
4102,"The service-centric applications are composed of third-party services. These services delivered by different vendors are usually black-box components which lack source code and design documents. It is difficult to evaluate their quality by static code analysis. Detecting anomalous services online is important to improve the reliability of these applications. This paper presents a framework for detecting anomalous services in the OSGi-based applications, followed by a method of monitoring services. We propose a method to monitor the resource utilization and interaction of services through tracing thread transfer. In addition, we detect anomalous services with XmR control charts. A prototype tool was implemented and applied in an application server. The experimental results show that our method 1) is of high accuracy for monitoring the resource utilization of the OSGi-based services; 2) does not introduce significant overhead; 3) can detect anomalous services effectively."
4103,"SOA-based solutions are typically modeled as business processes composed of loosely coupled services. Such an approach promises the flexibility to more easily customize the solution functionality in line with constantly changing customer requirements. The research issue that we address in this paper, therefore, is how to best accomplish this customization. Current approaches are typically manual and rather ad-hoc, involving repeated attempts to synchronize between (higher-level) design artifacts and (lower-level) source code to determine the configuration points. Alternatively, we propose a framework, Morpheus, which uses design artifacts to locate points of change in an SOA based solution via multi-level change propagation. First, we formally define the structure and semantics of the design artifacts, and the relationships among them. Second, we use these relationships to enumerate all possible changes in each design artifact; if two design artifacts share a relationship,then we also map a change in one design artifact to related changes in the other. Third, using these change relationships,we present an algorithm to traverse the design artifacts so as to propagate changes based on change requirements, ultimately resulting in the modifications needed to support the changed requirements. We illustrate our ideas on a simple yet realistic example in the insurance domain, and also present a prototype implementation."
4104,"This paper addresses the Terms of Service (ToS) in a contract between a service provider and a client. To generally handle any types of agreement between parties, several approaches have been proposed by researchers and proprietary vendors. However, the description of a typical ToS usually involves legal terms and expressions that are quite difficult both for developers and machines (programs) to represent and understand. To address this difficulty in the real world, we propose standardized semantic definitions of common ToS patterns that are supported by three-layer descriptions in a ToS: descriptions for developers, for lawyers, and for machines. We borrow this concept from the Creative Commons (CC) licenses, which are standardized licenses with three-layer descriptions for distributing copyrighted creative content, and apply it to licensed Web-based services. We have ported the primitive components in the CC licenses for ToS purposes, and combined them to produce 6 feasible licenses as a Service Commons, typical ToS patterns for free-of-charge services."
4105,"Facility network transformation (FNT) is a strategic approach involving assessing and optimizing the industrial facility networks such as new site selection, demand forecasting, performance evaluation in banking, retailing, etc. In practice, FNT requirements are often diverse, dynamic and industry specific, it's often difficult to implement a generic FNT service fully integrated with legacy systems. The heterogeneity of spatial information further calls for a loosely coupled architecture. An innovative spatial decision support system, iFAO (intelligent facility network analytics and optimization), is therefore developed based on service oriented architecture for FNT problems. In this paper, key FNT service patterns are identified and modeled to develop an industrial independent solution, and an SOA-based framework for iFAO is proposed correspondingly. Implementation of iFAO services is presented with a model-driven approach. With a real case in banking, it's illustrated how the SOA based iFAO services are integrated to solve the real industrial problems, especially for quick decisions on business strategy in the competitive and ever-changing marketplaces."
4106,"Mobile companions such as smart phones and PDAs carry a lot of sensitive data about their owners. With new services aimed at providing more targeted information retrieval through increased interactions with these devices, privacy concerns of individuals must be addressed. Existing mobile service computing solutions give users little control over the release of this information. In this paper, we present a privacy-aware information brokerage framework called MUPPET that incorporates three novel techniques to give users control over the release of their data. First, it introduces operation-focused access control, a purpose-based access control model that supports flexible and fine-grain policies using typed operation labels. Second, MUPPET includes a purpose detector that has a number of techniques to detect the active purpose in a pervasive environment. Third, our system allows reward-driven information exchange, a protocol for explicit communication and negotiation of justifications and rewards supporting tunable privacy policies based on ongoing evaluation of the information exchange. To validate our design, the MUPPET prototype has been integrated with a personalized coupon offering application for two different service providers in an experimental retail kiosk setting."
4107,"Digital video content via broadcast television, Internet and other content distribution networks provide limited interaction for fitness and wellness activities. The content delivery is one-way only and provides no personalization to the pace, programming and progress of the user's exercise routine. Furthermore, the content is to be viewed only on a screen which makes it awkward and incompatible with full-body activities such as yoga, pilates and T'ai chi. We present Cloud Mat, a system for context-aware personalization of fitness content with cloud-enabled connected surfaces. CloudMat provides real-time closed-loop feedback between the state of the user on the physical mat and the state of the content in the cloud service. Content is tagged with actuation signals where events are delegated from the screen to display on an electroluminescent lighting layer on the mat, which provides spatial guidance to the end-user. Through the sensor-layer embedded in the mat, the physical interface captures the pose and timing of the user activity and relays it to the Context-aware Personalization cloud service. This service coordinates sensing and actuation between the content stream and mat by generating pose templates and metadata files about the exercise routine to be delivered to the user. Through this interactive process between the physical mat and the content service, the feedback provided by the user performing the routine continuously adapts the pace and programming to maintain the desired user experience. We demonstrate the utility of the system and evaluate the system performance with a case study on interactive yoga."
4108,"While end-users enjoy the data service mashup with high convenience and flexibility, how to achieve the best performance with minimum maintenance cost is a challenging problem. In this paper, the data maintenance problem is analyzed, and the maintenance cost model for the data service mashup is built. The model measures the response cost and update cost of a group of data service mashups in terms of the request frequency and update frequency. Experiment shows our mashup platform with the model can reduce data maintenance cost."
4109,"Multi-processor system on chip (MPSoC) has been widely applied in embedded systems design. However, it has posed great challenges in designing and implementing prototype chip for diverse applications due to different instruction set architectures (ISA), programming interfaces and software tool chains. In order to solve the problem, we introduce SOA into MPSoC design, because it can provide flexibility and extensibility for MPSoC chip design at lower cost through adopting re-usable, self-contained modules in the design process. In this paper, we propose a service-oriented multi-processor SOMP, which integrates embedded processors and hardware IP cores as computing servants on a single chip. SOMP provides unified programming interfaces for users through utilizing diverse computing resources. In order to demonstrate the performance of SOMP, we implemented it on a Digilent Virtex5LX110T FPGA board and designed several sample test applications for verification purpose. The experimental results show that SOMP can improve the parallelism greatly and achieve 95.7% of the theoretical speedup on average."
4110,"We present a formal method to derive a set of web services from a given choreography, in such a way that the system consisting of these services necessarily conforms to the choreography. A formal model to represent orchestrations and choreographies is given, and we define several conformance semantic relations allowing to detect whether a set of orchestration models, representing some web services, leads to the overall communications described by a choreography."
4112,"The Internet market is highly competitive and is influenced by high expectation levels of Internet users, continuous advancement in the information technology, and high processing and storage capabilities of the hardware. In this work, we focus on the design and development of a replicated and highly available service registry for microservice architectures. The service registry key-value store comprises six nodes and supports a total of 216 microservices. Existing replicated service registries, like ZooKeeper and ETCD, are based on majority consensus strategies. Moreover, if these strategies fail to achieve majority consensus, then the service registries are bound to provide limited functionality. As part of this research, we propose a highly available data replication strategy for a replicated service registry (DRSR). DRSR exploits 1) a simple encoding scheme and 2) a mapping method for efficient distribution of the encoded values to the service registry nodes in order to overcome the limitations faced by existing strategies."
4113,"QoS value prediction of Web services is an important research issue for service recommendation, selection and composition. Collaborative Filtering (CF) is one of the most widely used methods which employs QoS values contributed by similar users to make predictions. Therefore, historical QoS values contributed by different users can have great impacts on prediction results. However, existing Web service QoS value prediction approaches did not take data credibility into consideration, which may impact the prediction accuracy. To address this problem, we propose a reputation-aware QoS value prediction approach, which first calculates the reputation of each user based on their contributed values, and then takes advantage of reputation-based ranking to exclude the values contributed by untrustworthy users. CF QoS prediction approach is finally used to predict the missing QoS values based on the purified dataset. Experimental results show that our approach has higher prediction accuracy than other approaches."
4114,"Mobile push notifications are an important feature of mobile computing services and it has been widely implemented in mobile systems. However, it also brings vulnerability to the systems on security and reliability. Formal specification and verification is an effective approach for understanding the properties of mobile push notifications and ensuring quality of the system development. Due to the dynamic interaction among multiple systems, security and mobility in mobile computing services, formally modeling and analyzing this type of services is a grand challenge. In this paper, we proposed an approach that models mobile computing services using a high level Petri nets and analyzes them through combining formal verification and testing techniques. The effectiveness of the approach has been demonstrated with case study of a mobile searching service with mobile push notifications built on Amazon simple notification service and Google mobile push notification services."
4115,"Humans naturally employ a process of iterative refinement when they search for information, clarifying their intentions and adjusting their goals in response to what they encounter. We propose to develop tools specifically to facilitate this process, and hypothesize that they will help users converge on their desired information more quickly. This work presents a layered extension to the Google Web search interface that illustrates the so-called ""retrieval by reformulation"" paradigm. The tool will serve as a launching point to a wider field of inquiry into user interaction with large information spaces."
4116,"Finding appropriate web APIs to develop mashup services is becoming difficult because of increasing number of web APIs offered from different sources. If we can recommend relevant web APIs for a mashup service based on its requirements, it will help software developers to find suitable APIs easily instead of searching from thousands of web APIs. Although there are many existing methods to recommend web APIs for mashup services, their recommendation accuracies and diversities are still not high. We will present a novel approach in this paper to produce better web API recommendation results in terms of accuracy and diversity. It is a matrix factorization based API recommendation method for Mashup services. It uses a two-level topic model for clustering Mashup services. We used a dataset from programmableWeb to perform experiments and compared results of our method with other existing methods. Its evaluation results show that our matrix factorization based recommendation archives better API recommendation accuracy and diversity for Mashup services."
4117,"Storage services based on public clouds provide customers with elastic storage and on-demand accessibility. However, moving data to remote cloud storage also raises privacy concerns. Cryptographic cloud storage and search over encrypted data have attracted attentions from both industry and academics. In this paper, we present a new approach to constructing efficient oblivious keyword search (OKS) protocol, which permits fast search (i.e., sub-linear time) and relatively short ciphertext, while providing provably strong privacy for both users and cloud storage service providers. Previous OKS protocols have ciphertext size linear in the number of keywords, which consume much storage space and relatively long searching time. We formally define a Disjunctively Oblivious Keyword Search (DOKS) protocol realizing oblivious keyword search with the ciphertext size constant in size of keywords, which is significantly less than that of previous OKS protocols. Our approach improves both the privacy and efficiency of existing OKS protocols. With DOKS, adversary cannot distinguish two search keywords submitted by users, and cannot know the relations between ciphertext of documents and search keywords. A search keyword cannot be reused by adversaries. Users can get the matching documents without revealing statistical information on search keywords."
4118,"This paper presents a game theoretic model and an iterative bidding framework for the service request prioritizing and scheduling problem in Software as a Service (SaaS) environments. We focus on a type of settings in which the service provider's existing capacity is over demanded and customers' service requests are constrained with completion times. To gain more profit, the provider needs to prioritize customers' requests based on their profitability and schedule as many profitable requests as possible. The prioritizing and scheduling problem is considered in a strategic setting in which customers' values on the service requests are their private information that is not known to the provider, which calls for economic based models. The key issue here is how to maximize economic social welfare across all customers given the presence of customers' self-interests. We propose an iterative bidding framework as a multilateral negotiation platform for the service provider and its customers to allocate compute capacity among service requests. The proposed bidding framework is evaluated by a computational study. The data from the experiments show that the proposed approach produces near optimal solutions with moderate information revelation. It also scales well to large size problem instances."
4119,"With the proliferation of Web services specifications, the question of which of the multiplicity of technologies to adopt arises. While sometimes the decision can be based purely on matching requirements to protocol capabilities, a harder consideration is whether the chosen specifications will enable wide interoperability with other systems, or lead to an architectural dead-end. We present a risk assessment of several contemporary WS-* protocols, enabling architects and developers to make sane choices for service interoperability."
4120,"Service component architecture (SCA) defines a component model that allows for a late binding with a concrete implementation. Similarly, the SCA policy framework defines a policy mechanism that uses intents, which are high-level declarations of desired nonfunctional features that are bound to concrete policies at a later time. The definitions of intents are simple, but the actual intent application is complex. Meeting policy requirements requires application of intents at many points in an assembly, and presumes domain knowledge about the semantics and constraints related to using a particular combination of intents. We propose pattern based policy configuration to resolve these difficulties. Our proposed configuration system has two features: policy intent patterns and pattern application rules. Domain knowledge is encoded as roles and role constraints in policy intent patterns. Pattern application rules can be used to define how patterns are applied automatically. This paper takes the original approach of thinking about policy application from a compositional perspective; this reduces the complexity of meeting nonfunctional requirements for SCA assemblies and provides a validation framework for applied intents."
4121,"In view of complex battlefield environment and technological factors, unmanned aerial vehicle's (UAV) decision-making cannot accurately reflect battlefield situation and make reasonable decisions by itself. To solve this problem, this paper presents a limited intervention collaborative decision-making mechanism between manned aerial vehicle(MAV) and UAV based on visualising fuzzy cognitive map (VFCM). By use of limited intervention trigger strategy, high efficiency and feasibility of decision-making can be ensured. Simulation indicates that limited intervention collaborative decision making of the MAV/UAV team is reasonable and feasible."
4122,"This paper focuses on clustering alerts around common root causes at the lower levels of the event management chain. The aim is to enable root-cause identification from a mixed event stream and to offer aggregated information for holistic problem solving. This end-to-end investigation spans feature selection and similarity assessment, clustering on heterogeneous feature maps, and evaluation of results. We compare feature values based on network information, user-defined similarity matrices, and textual analysis, and capture aspects of feature correlation in the event similarity function. Spectral clustering partitions the stream and serves to learn a more general similarity metric from a reference partitioning. Finally, we introduce two novel result visualization techniques and make a case study on one identified root-cause for which this framework outperforms both a time-pressured human operator and baseline clustering algorithms."
4123,"We describe a comprehensive methodology for discovering service similarity (substitutability) by testing. Our solution does not rely on the service descriptions provided by their authors, it does not assume the existence of ontologies, semantic tagging, or other representations, and it avoids common information retrieval techniques. The only information our method expects is that every service in the repository will be annotated with its inputs and outputs, and that each input and output will be associated with a domain from a published list."
4124,"Service Oriented Architectures ease integration of heterogeneous systems, such as sensor data and workflow systems. Systems are integrated since they model an overlapping part of the physical world, i.e., physical objects exchanged between different parties. For workflows handling physical objects, the correlation of sensor data with workflow states and workflow state changes are investigated in this paper. Further, the implications of the state or state change correlation on the workflow execution and the support by existing service infrastructures is discussed."
4125,"Orchestration and choreography language provide basic composition techniques and specification among services, but they don't give any secure manners or secure operation styles and specifications. For giving a general guide to implement secure orchestration and choreography, we give a formal approach for carrying out this goal. To this target, we address those by designing an extension of the Spi calculus with secure global calculus. We named our method SpiG4WSC calculus. The combination of strong practical needs for secure Web services composition and the theoretical foundations will lead to a bridge between practice and theories."
4126,"The application of ontology in Web services context for dynamic discovery, autonomous composition, invocation and monitoring has been deemed as very promising in semantic Web based enterprise application integration and m-commerce. There are many initiatives aiming to realize pervasive services to enable effective enactment of better quality of services, such as sensitiveness on service locations. The traditional WSDL is hardly qualified enough to tackle new challenges such as accurate representation of the non-functional properties of services. Hence, OWL-S came up with an effectual approach to facilitating semantic service description, dynamic composition, on-the-fly execution, and many other tasks. In this paper, we propose a practical geographic ontology based on geography markup language (GML) and extend OWL-S profile to formulate new geographic profiles. We also discuss specific scenarios where context-aware services invocation peers, which are advertised and coordinated via a newly designed GeoProfile, can be equipped with geographic information inherently to distinguish different peers."
4127,"Enterprise applications today are composed of multiple independently executing services and processes that collectively provide a solution to a business problem. These composite applications contain a heterogeneous collection of services that execute in a variety of runtimes making them difficult to manage while maintaining a business centric point of view, as opposed to a service point of view. This paper introduces a business centric monitoring framework to bridge the gap between the business and service levels in complex business applications. Our technical approach focuses on using business information invariants to define one or more monitor sets in order to relate service activity to business composite execution. We apply this framework to enable end-to-end monitoring of composite business applications. In this paper we present an initial prototype of our business centric monitoring approach using monitor sets for monitoring a simple loan application composite implemented on IBM's WebSphere Business Modeler, Process Server and Business Monitor. Our prototype implementation demonstrates the convenience, effectiveness and ease of design and deployment of our monitoring solution to attain a single end-to-end business centric view of a collection of heterogeneous services executing together. Our work also exposes potential challenges as we extend this work to support more powerful end-to-end monitoring."
4128,"BPEL(Business Process Execution Language) composite service evolves a lot in its lifetime. Regression testing must be performed to ensure the correctness of each evolved version. In this article, an approach is proposed to select test cases for regression testing based on data flow testing criterion. With XCFG(eXtended Control Flow Graph) modeling BPEL composite service, the approach improves the traditional data flow analysis to compute the def-use pairs in BPEL process, and then identifies the affected def-use pairs by comparing the def-use pairs and XCFG model in the evolved version with those in the baseline version, where related WSDL(Web Service Description Language) documents are incorporated for comparison. The data flow paths covering the affected def-use pairs are calculated for regression testing, and some of them can reuse the test cases in the baseline version, which are determined by analyzing the path condition of data flow paths between two versions. The proposed approach can detect three kinds of changes, including process change, binding change and interface change. Experimental study shows the effectiveness."
4129,"Efficient resources utilization and better system performance are always two important objectives that service providers pursue to enjoy a maximum profit. In this paper, through analyzing experimental measurements, we study the performance impact of interdependent soft resources on an n-tier application benchmark - the RUBiS system. Soft resources are vital factors that influence hardware resources usage and overall application performance. Improper soft configurations can result in correlated bottlenecks and make performance degradation, so tuning the configuration of soft resources is imperative. Based on the experimental measurements, SRConfig method is applied to predict the soft configurations through adopting the back propagation neural network in n-tier application. Experimental results validate the accuracy and efficacy of our method."
4130,"Cloud computing has enabled provisioning of scalable and virtualized resources to organizations in an ubiquitous and on-demand manner. Likewise, configurable process modeling has enabled organizations to reuse their existing knowledge by sharing a reference process model between different tenants that can be customized according to specific needs. Nevertheless, there are some limitations in this context, (i) it is not cost-effective to make use of real cloud infrastructure such as, Amazon EC2 for experimenting and analyzing the deployment of the best process variant, (ii) frequent changes in the configurations during experimentation using a real cloud setup involves various manual adjustments which is a time-consuming task, (iii) certain conditions prevailing in the internet-based cloud environments are beyond the control of the practitioners involved in such analysis. Thus, making use of simulation environments is one of the good alternative for testing and analyzing the best process variants with optimal resource allocation. Building upon our previous work, we propose in this paper, (i) a unified description model that allows the representation of process activities along with their possible cloud resource allocation alternatives, (ii) a methodology for simulating a configurable process along with its cloud resource consumption, and (iii) an extension of CloudSim framework in order to realize the simulation of the configurable resource allocation for the cloud-based business processes."
4131,"In cloud systems, services constituting a transaction may spread over a large number of servers or clusters. Theoretically, these services could consume cloud resources unlimitedly. To avoid financial loss due to resource overuse, clouds have to monitor the state of resources consumed by the services-collect values of consumption, and evaluate whether the combined usage of resources has excessed a pre-defined upper bound or not. The distributed nature of the services introduces a challenge to the monitoring system on how to summarise distributed state information with low cost. We present our resource state monitoring solution to capture the challenge introduced by services hosted in clouds. Our solution tracks the resource consumed by each service constituting a transaction individually whilst ensures the whole transaction does not overuse the allocated resource. It improves availability by avoiding single points of failure, and achieves scalability by minimising message exchanges. We performed experimental analyses that indicate this work can provide an inexpensive resource monitoring solution for transactions in clouds."
4132,"Workflow engines which require process instances to run to completion against their creation model are limited in their ability to adapt to model changes. The technique of process instance migration can be used to migrate instances to a new process model, but at any given time only a fraction of instances may be in a state which is compatible with the new model. We propose a continuous batch approach for migration which treats migration itself as a long running problem. The continuous migration approach schedules and orchestrates the evaluation and migration of long running processes over time, controlled by configurable policies while maintaining continuous process availability. We have built a long running process migration simulation to test our approach, looking at both migration success and cost. The simulation confirms the need for a long running migration approach and yields some interesting suggestions for tuning the configuration of the migration system."
4133,"Provenance has become an important concept for services computing in general, and for scientific workflows in particular. Provenance often contains confidential data and dependencies whose access needs to be protected. Provenance access control policies control who can access which provenance information. Correct specification of provenance access control policies is critical to ensure system security. However, due to the sheer size of provenance, it is often difficult to comprehend the full effects of an access control policy by manual inspection alone due to complex multi-step dependencies and their interactions. In this paper, we present automated analysis algorithms and complexity results for three provenance analysis problems. We have also developed incremental strategies for these algorithms for evolving provenance and access control policies."
4134,"The paper employs three types of agents namely resource brokering agents (RBAs), job agents (JAs), and resource monitoring agents (RMAs) for resource allocation in grid computing. RBA acts as a resource scheduler as well as a broker for the users to submit their jobs through JAs. JAs search for the resources. RMAs reside inside the nodes of the local cluster and inform the status of the resources to the local cluster server. The proposed model is evaluated in a simulated environment to test its operational effectiveness in various scenarios."
4135,"Web service composition is the process of integrating multiple independent Web Services into a coherent system that performs complex functions. This paper proposes an effective method for performing Web service composition and verification. We use Temporal Logic of Actions (TLA) to model Web services, composition patterns and users' requests. Based on the models, composition can be created by conducting TLA reasoning. The correctness of composition result can also be verified by checking whether the resulted TLA formula implements a user's request. A case study was conducted to illustrate the correctness of our approach."
4136,"The paper proposes ξ-calculus, a formalism for describing interactions in Service Oriented Architectures. The calculus treats interactions between services as a core concept to capture important architectural characteristics of the system. The focus of ξ-calculus is on the higher level abstraction rather than on the lower level details like parameter passing, use of stacks, closures etc. And hence it can be used to specify, study, and compare different service oriented systems from an architectural point of view. The paper also introduces a notion of interaction types suitable for the level of abstraction under consideration. These types are used to describe the type system of the calculus. The paper introduces a notion of design time non-functional capabilities and also present the application of the calculus in a real business scenario."
4137,"This paper presents a privacy disclosure recommendation approach based on a privacy cost model. The approach involves selecting appropriate credentials or attributes from users, and automatically building a new credential to fulfill service's authorization policies. The recommendation principles consider three aspects: (1) the selected user's attributes in the new credential satisfy the requested service's authorization policy, (2) hiding user's credentials and attributes to keep private during the request procedure, and (3) the total privacy cost of users is minimum. In addition, an automated tool is designed and implemented to derive a new credential. The correctness of our approach is demonstrated and validated by a practical case. Experimental results and complexity analysis show that our approach is efficient."
4138,"Traditional centralized Web Service Composition (WSC) approaches suffer from critical problems such as performance bottleneck and single point of failure. However, the analogy between Service and Agent Computing paradigms suggests that incorporating both technologies will likely lead to a more effective hybrid service model. In this work, we adopt an agent-based approach to WSC, in which Service Dependency Graph (SDG) is constructed as an AND/OR graph, distributed among the agent community members. Upon receiving a user composition request, agents perform internal reasoning and corporate through a communication protocol attempting to find a solution. Experiments are conducted on a public dataset and results show the effectiveness of the proposed approach in terms of communication cost."
4139,"As Web services are widely adopted, the nonfunctional requirements (i.e. QoS constraints) have become a significant factor in implementing trustworthy and predictable service applications. In this paper, we analyze the QoS publishing necessity, and categorize QoS parameters to refine their characteristics. Then we propose a conceptual publishing model, which is adaptive for the distributed and decentralized service-oriented computing environment. We further introduce the perception service quality measurement models, and extend P-E model for web services. The extended model takes both dynamic QoS parameters and customer expectations into consideration. A compensation mechanism is also incorporated into the model for trade-off when nonfunctional requirements cannot be fully satisfied."
4140,"Performance of distributed applications over a wide area network (WAN) has remained an important issue as WAN conditions change over time. Networking community has employed various approaches to improve application performance by adapting to changes in WAN conditions. Composite web services with components distributed over a WAN pose similar performance problems, however, with a different set of challenges and opportunities to improve. With their unique architecture, these composite web services can be partitioned and orchestrated in a decentralized fashion on a distributed infrastructure. In this paper, we investigate how different topologies generated by decentralized orchestration are affected differently by variations in WAN conditions. We present a system that exploits this and adapts to changes in WAN conditions. The system monitors the availability of network bandwidth, evaluates its effect on the performance of each topology using a performance model based on bandwidth availability, and then selects the optimal topology at runtime. The results from our simulation show that the adaptive system improves performance."
4141,"End-user service composition is a promising way to ensure flexible, quick and personalized information provision and utilization, and consequently to better cope with spontaneous business requirements. For end-users to compose services directly, issues like service granularity, service organization and business-level semantics are critical. End-users will certainly be at loss if they have to select from a long list of available Web services expressed in IT jargons. This article introduces the concept of personalized active service spaces and focuses on the use of business services, service dependency rules, and service personalization rules to support end-user service composition. It addresses two key issues in end-user composition: how to utilize the user preference and context to restrict the scope of applicable services for selection, and how to capture and utilize dependencies or usage patterns between services in order to provide guidance and enforce temporal/sequential restrictions on service invocations for end-user service compositions"
4142,"Although lots of IoT (Internet of Things) applications have been developed, the systematic method to construct IoT services is still obscure. In this paper, an Event-driven Service-oriented Architecture (EDSOA) for IoT services is discussed, where distributed events act as a primary mechanism for each IoT service to share independent meaningful events, to express its requirements and capabilities, and to decouple itself from other services. Such distributed events however do not provide powerful expressiveness to describe business logic in SOA because business activities are not completely independent each other. To fill the gap, we propose an information-centric session mechanism to describe service behavior working upon distributed events, called event session. This paper also discusses how to build an Event-driven SOA infrastructure, where we can use resource information to create IoT services, use independent and shared events to run the IoT services, and use event session to coordinate the IoT services. Some applications and experiments are given to show concept proof for such event-driven SOA."
4143,"This paper presents a distributed service management infrastructure called BISE. One distinguishing feature of BISE is its adoption of the peer-to-peer (P2P) model in support of realtime service managements. BISE offers significant advantages over existing systems in scalability, resilience, and manageability. Current P2P algorithms are mainly developed for the file-sharing applications running on desktops, which have characteristics dramatically different from enterprise data centers. This difference led us to design our own P2P algorithms specifically optimized for enterprise environments. Based on these algorithms, we implemented a P2P substrate called BiseWeaver (25,000 lines of Java code) as the core of BISE. Our evaluation on a set of distributed machines shows that BiseWeaver is efficient and robust, and provides timely monitoring data in support of proactive SLA management"
4144,"Distributed clouds have recently attracted many cloud providers and researchers as a topic of intensive interest. High energy costs and carbon emissions are two significant problems in distributed clouds. Due to the geographic distribution of data centers (DCs), there are a variety of resources, energy prices and carbon emission rates to consider in a distributed cloud, which makes the placement of virtual machines (VMs) for cost and carbon efficiency even more critical than in centralized clouds. Most previous work in this field investigated either optimizing cost without considering the amount of produced carbon or vice versa. This paper presents a cost and carbon emission-efficient VM placement method (CACEV) in distributed clouds. CACEV considers geographically varying energy prices and carbon emission rates as well as optimizing both network and server resources at the same time. By combining prediction-based A* algorithm with Fuzzy Sets technique, CACEV makes an intelligent decision to optimize cost and carbon emission for providers. Simulation results show the applicability and performance of CACEV."
4145,Provides a listing of current committee members.
4146,"Workflow management has already been worldwide accepted due to its capability to make organizations model and control work processes effectively and efficiently in a distributed environment. Currently, most of the workflow management systems (WfMSs) are lack of a coordination mechanism across different organizations. We first propose an intercultural collaboration workflow process definition and then propose an agent coordination workflow model based on the consideration of cultural factors in intercultural collaboration work process modeling."
4147,"This research paper demonstrates Web services based business process management system, developed to aid marketing of agricultural produce. For achieving relevance to real-life practice, we carefully follow the business process suggested in model act that is planned for implementation in all the Agricultural Produce Marketing Committees (APMC) throughout India. Our proposal exposes various business functionalities explained in the act, in the form of Web services that can also utilize the existing IT infrastructure of the APMCs. With the help of above mentioned and any other external Web services, a trader can execute a complete trading business process. We follow Web service orchestration specifications to achieve our objectives. We also demonstrate a complete business process that is initiated by a farmer with a mobile PDA involving trade of an agricultural produce in the market place."
4148,"A model of a knowledge intensive service system is introduced to study knowledge sharing in these systems. Simulation experiments based on the model are conducted to relate particular aspects of knowledge creation and sharing to a metric of service performance. The results indicate that certain network configurations of knowledge sharing among the service provider agents have performance advantages over others. In addition, the degree of fallibility associated with shared knowledge has a significant impact on performance. This paper is part of a research program that investigates the knowledge dynamics within service systems with particular focus on the influence of human, social and structural factors."
4149,"Architecture views are a popular technique used for documenting and describing software systems architecture through a number of views, each of which cater to a certain group of stakeholders. With the increasing adoption of service oriented architecture and the deployment of Web service based applications both internally and externally, organizations need an improved framework for representing and documenting the service architecture. To this end, we propose a well defined set of views that describe services within the enterprise, including primitive as well as composite service views. In this manner, the proposed model achieves consistency and coherence among different views, thus providing a unified view model for service oriented systems within an organization"
4150,"The ShanghaiGrid is an ongoing grid computing environment (GCE) based on grid service standards and the open grid services architecture. The ShanghaiGrid is a City Grid, which is to provide a general shared information grid platform. Based on this point, a workflow management system is very import especially for user-defined workflow. By using the workflow, users in GCE would compose several grid services as a new service and provide it to others. This enlarges the extensibility and business-originated usage in the GCE. This paper discusses the type of workflow, proposes a model of client-side workflow management system, and the design of several important workflow services. A service discovery process based on workflow is also given."
4151,"The end-to-end QoS negotiation for SLA establishment for composite services involves compound multi-party negotiations in which the composite service provider concurrently negotiates with multiple candidates for each atomic service, selecting the ones that best satisfy the atomic service QoS preferences while ensuring that end-to-end QoS requirements are also fulfilled. In order to negotiate with potential candidates, it is necessary to derive the atomic utility boundaries from the global utility boundary. Additionally, there needs to be a mechanism to update these boundaries in subsequent negotiation rounds based upon the individual negotiation outcomes. In this paper, we propose an algorithm that addresses both these requirements and evaluate it using an example scenario for composite service provisioning."
4152,"This paper investigates the multi-agent cooperation problems in Web services domain. For Pareto-optimal Nash equilibrium, reinforcement learning algorithms are used to solve the coordination problem in cooperative environments. Most previous works study the deterministic gain of a state. However, in practical service environments, the gain may be nondeterministic due to unstable Quality of Service (QoS). To avoid local optimal solution, we use an improved update function. The experimental results show that proposed reinforcement learning algorithm outperforms other learning methods."
4153,"The number of sensors deployed around the world is growing at a rapid pace when we are moving towards the Internet of Things (IoT). The widespread deployment of these sensors represents significant financial investment and technical achievement. These sensors continuously generate enormous amounts of data which is capable of supporting an almost unlimited set of high value proposition applications for users. Given that, effectively and efficiently searching and selecting the most related sensors of a user's interest has recently become a crucial challenge. In this paper, inspired by ant clustering algorithm, we propose an effective context-aware method to cluster sensors in the form of Sensor Semantic Overlay Networks (SSONs) in which sensors with similar context information gathered into one cluster. Firstly, sensors are grouped based on their types to create SSONs. Then, our meta-heuristic algorithm called Ant Clust has been performed to cluster sensors using their context information. Finally, a few useful adjustments have been applied to reduce the cost of sensor search process. Experiments show the scalability of Ant Clust in clustering sensors and significantly faster runtime on sensor search, when compared with existing systems."
4154,"Different ranking algorithms have been proposed to fulfil the need of ranking. The problem is that most of the existing algorithms and models are just applicable on a specific data. When the data is imbalanced and heterogeneous, finding the records belonging to the minority class is significant especially in failure cases. So considering ranking as a classification problem of predicting the specific relevance score for any category, we are going to propose a generic ranking service. In this model, a metric learning based ranking model is proposed which can be used on wide range of scientific data sets. A real world imbalanced and heterogeneous data set is used to prove the efficiency of model."
4155,"More and more enterprises rely on web services as an important channel for conducing business process and some changes to web services might occur for various reasons, efficiently managing services change becomes crucial. In this paper, we proposes a web service change management framework based on Bayesian Network, which enable representing the inside causal and probabilities semantics of dynamic changes to web services. The framework implement change management effectively in a heuristic way, by analyzing and identifying the detail of change as well as providing some meaningful knowledge to the following reaction. With the continuous evolution of the associated Bayesian Network, the efficiency and applicability of the web service change management network can be improved to a satisfied extent."
4156,"In this paper, we propose an approach to select top-k cloud services. Our approach combines the trust, determined by the reputation of the provider, and the QoS. We present different algorithms for processing such selection queries and evaluate them through a set of experiments."
4157,"Social tagging has become a popular solution for recognition and management of resources in web2.0 era. To solve the current problem of insufficient capacity of semantic description of web services, a multi-dimensional social tagging method for semantics of web services is proposed. Under the guidance of general social tagging model, users will annotate services manually from the domain the services attribute to, services theme and self reference, and simultaneously, the functional and nonfunctional properties of the services are automatically tagged, so that the semantic description coverage rate of web services will be enhanced, Moreover, a method of service semantics emerge automatically is proposed. Finally, experimental analysis will verify the feasibility and effectiveness of this method."
4158,"We address the infrastructure of chord-like networks union. Chord merger is not a good idea for security reason. In this paper we propose Hamming metric union in chord-like networks from the initial view of alleviating the security problem, in which intra-network and inter-network routing are served based on a two dimensional structure. We discuss the routing procedure in detail where network shifted neighbor selection and nearest destination Hamming weight forwarding are explored. It has been proved that this routing in the united chord network is more efficient than chord itself."
4159,"Facing fast changing and more diversified customers, specific customer oriented industries' executives should continuously make customer oriented strategic transformation decisions faster than competitors to obtain sustained growth. However, due to higher transformation cost per facility, increased frequency of transformation efforts, and greater complexity associated with rolling out quickly across large diverse facility networks, enterprises invest heavily yet often fail to deliver the required impacts. Executives in these industries are eager to have a quantitative method and tool to help them derive more scientific strategic transformation suggestions. In this paper, we describe a general analytics and optimization solution designed to provide strategic analysis services for those specific industries to adapt to the fast changing needs. The data model design is elaborated for efficient communication and quick solution development, and an implementation case study on intelligent facility network analytics and optimization services design in retailing industry is finally presented to illustrate how the proposed data model improves the flexibility of solutions across industries."
4160,"Semantic web service is proposed to support the automation of service discovery, composition and invocation. However, it is still difficult for common users to compose semantic web services or generate them from web services. In order to alleviate such pains, in this paper we present Flow Editor: a visual semantic web service composition tool based on OWL-S specification. It has several advantages, including visual service composition by drag-and-drop, semi-automatic semantic web service generation, visual control construct supports and so on. In practice, the tool is already applied to help users solve realistic problems by creating and exporting new on-demand services."
4161,"In the past ten years many assessment approaches have been proposed to help manage software process quality. However, few of them are configurable and real-time in practice. Hence, it is advantageous to find ways to monitor the current status of software processes and detect the improvement opportunities. In this paper, we introduce a web- based prototype system (Continuous SPA) on continuous assessing and monitoring software process, and perform a practical study in one process area: project management. Our study results are positive and show that features such as global management, well-defined responsibility and visualization can be integrated in process assessment to help improve the software process management."
4162,"Classroom scheduling problem has long been proven to be a NP-Complete problem. This paper presents a novel services-oriented scheduling approach. Regarding each type of resource as a service, Smart Class originally introduces SOA into this problem to efficiently facilitate educational programmers with the design space exploration (DSE) and greedy algorithms based approach. Furthermore, the Smart Class architecture can be dynamically coupled to different algorithms to fit in any specific demands. Typical case studies demonstrate that Smart Class provides a new efficient research paradigm to the traditional problem of class scheduling, providing high flexibility and scalability."
4163,"The development of the web technologies and the increasing of available services have introduced the issue of the selection of the most appropriate service among a set of candidate web services. First of all, the services offering a given functionality are discovered. Then, the service section process assists users in choosing the services that better meets their preferences. These preferences are generally, expressed as potentially objective functions often conflicting. Most of existing works trying to select the best web services are based either on a single evaluation criterion or, at best, on the use of an aggregation function like weighted sum of several quantitative evaluation criteria, or the use of the Pareto optimality notion. The work presented here addresses these shortcomings by introducing a new optimality notion based on two tests: (i) concordance and (ii) discordance tests. It presents an efficient algorithm to select only the best services using the introduced optimality notion. Moreover, the proposed algorithm exhibits encouraging results as supported by a series of experiments."
4164,"Cloud Computing, Mobile Technology, and Social Networking Services such as Facebook and Twitter has become an integral part of society during the event of an emergency or disaster. In the wake and aftermath of a disaster, a tremendous number of people used social networking sites to post and share information. The Department of Health and Human Services (HHS) had sponsored a challenge for software application developers to design a Facebook application to help people prepare for emergencies and to obtain support from friends and families during its aftermath. Lockheed Martin had responded to this challenge by creating a cloud and mobile based Facebook application called the Personal Emergency Preparedness Plan (PEPP). This paper discusses the design and integration of the PEPP Facebook App with the intention of serving as reference architecture for developing social networking applications using cloud computing and mobile technology."
4165,"Today's Industrial growth mainly depends on the real time and on line information rather than stored data. It is a challenging one for the communication industry to provide the required information continuously without delay. As the communication industry is considered the major factor is the interconnection between the different networks and the cooperation between the service providers in the form of QoS. The major issue is the mobility management (MM) to provide and maintain connectivity between the different networks. Here in this paper an approach is proposed for MM based on the session management, location management which are common to any type of network. By considering and having the control over the common control management techniques the flow of information is maintained without/ with delay based on the services offered i.e.., either online service or offline service. Moreover the proposed work also concentrates on the allocation of the transmission channel is purely based on the type of information transmission applications involved such as data, voice, voice &amp; data, multimedia information, video images, real time video applications etc. Based on the type of application and also the network and the service provider a common procedure is being followed so as to provide a seamless transmission."
4166,"Specifying and analysing non-functional properties (NFPs) is essential for driving architectural decisions and validating composite service designs. Only where NFPs have been specified can we choose between services with similar functionality that would better satisfy our non-functional requirements. Meanwhile, incorporating provenance functionality into service-oriented systems' design is becoming crucial for users, allowing them to query the generation methods and origins of the data the system outputs. This need is particularly evident in compositions of services, where audits of individual services do not provide a connected picture of the composition's processing history. Making provenance awareness (ability to answer provenance queries) an explicit NFP in composite service specifications would enable composite service designers to analyse whether they meet provenance-related requirements. In this paper, we discuss a framework for designing and analysing provenance awareness for service compositions. We envision this as a basis for analysing the impact of provenance on other NFPs such as performance and storage."
4167,"IP multicast has not been deployed because of hardware problems. So a new scheme called end system multicast or overlay multicast for group communication has been emerged. It supports IP multicast functions, which is located on application level. For developing it, we have been focused on efficient overlay tree construction among group members with low stretch and stress. However, we should consider a variety of transmission or receipt condition since a real Internet environment has users with various transmission/receipt rates. Thus, we make on-demand one-time source specific tree depending on required bandwidth information of group members when a member requests data transmission. Our mechanism provides satisfied data quality limited maximum transmission rate of the source to each group members. Furthermore, we manage a large group enough as distributing control information to cores that are designated members for maintaining host member information. Lastly, we prove that our tree guarantees data quality to each group members, and show low tree construction time is required. In addition, for evaluating group scalability, we analyze control information increasing rate via group size, and validate its scalability."
4168,"This paper studies heuristic search-based optimization of service compositions. We have investigated applying Genetic Algorithms (GA) to optimize service-oriented architectures (SOA) in terms of security goals and cost, we help software Engineers to map the optimized service composition to the business process model based on security and cost. Service composition security risk is measured by implementing the aggregation rules from the local security risk values of the aggregated services in the composition. We adapt the DREAD model for Security risk assessment by suggesting new categorizations for calculating DREAD factors based on a proposed service structure and service attributes. We implemented the YAFA-SOA Optimizer as an extension of an existing GA implementation to solve multi-objective optimization problems for varying number of objectives in the context of SOA. We evaluated the tool in a case study. The study results show that applying multi-objective GA is feasible to find the optimized security and cost in SOA-based systems. We were able to approve that adding security services to the generated composition reduces the risk severity of the generated composition and enhances its security in terms of confidentiality, integrity and availability (CIA). We found that the generated service composition risk severity is less than 0.5, which matches the validation results obtained from a security expert."
4169,"The IBM Corp invents, develops and manufactures IT products, including computer systems and software, system networks, storage hardware and microelectronic products. The IBM CIO Global Infrastructure Architecture and Strategy (CIO GIAS) team is responsible for running and optimizing IBM's internal IT operations which include more than 500,000 users connecting to over 20,000 servers across 170 countries. The solution described below, implemented between 2008-2010, is a response to managing software licenses in one of the world's largest distributed enterprises."
4170,"Hadoop distributed file system (HDFS) becomes a representative cloud storage platform, benefiting from its reliable, scalable and low-cost storage capability. HDFS has been utilized in BlueSky, one of the most prevalent e-Learning resource sharing systems in China, to store and share courseware majorly in the form of PowerPoint (PPT) files and video clips. Unfortunately, HDFS does not perform well for massive small files since huge numbers of small files imposed heavy burden on NameNode of HDFS, correlations between files were not considered for data placement, and no prefetching mechanism was provided to improve I/O performance. This paper introduces a novel approach to improve the efficiency of storing and accessing small files on HDFS. Characteristics of file correlations and access locality remaining among small files in the context of courseware are well considered for storing and accessing them. Firstly, all correlated small files of a PPT courseware are merged into a larger file to reduce the metadata burden on NameNode. Secondly, a two-level prefetching mechanism is introduced to improve the efficiency of accessing small files. The experimental results indicate that the proposed approach is able to effectively mitigate the load of NameNode and to improve the efficiency of storing and accessing massive small files on HDFS."
4171,"The development of enterprise-wide service-oriented architectures (SOA) is a complex task. In most cases, evolutionary approaches are used to deal with the arising complexity. However, most of the existing design methodologies and implementation strategies focus on more technical, service realization specific aspects. Challenges regarding the definition and the management of related service artifacts throughout the whole service lifecycle are neglected. Also the implementation of a lifecycle-encompassing information management infrastructure is not addressed adequately in research and industry. In this paper we introduce a common service management information model (coSIM) that builds a foundation for the management of services and service infrastructures during design-, run- , and change-time."
4172,"One of the most typical models of service market is a reverse auction, where service providers offer their services at a certain price and a user selects a combination of the services. In the first price auction, however, a service which is cheaper and has better quality is not necessarily selected. This causes unstable outcomes which are undesirable both for a user and service providers. A possible solution to this is the VCG (Vickrey-Clarke-Groves) mechanism, where the dominant strategy for a service provider is to report true cost of his service. In spite of this desirable property, implementing the VCG mechanism for service composition suffers from its computational cost. Calculation of payments to service providers based on the VCG mechanism requires iterative service selection, although each service selection can be NP- hard. Approximation algorithms cannot be applied because approximate solutions do not assure the desirable property of the VCG mechanism. Thus, we propose a dynamic programming (DP) based algorithm for service selection and VCG payment calculation. The proposed algorithm solves service selection in quasi-polynomial time and gives an exact solution. Moreover, we extend the algorithm to reuse matrix which is built during DP. This largely improves the performance of VCG payment calculation. Our experiment shows that the proposed algorithm can solve practical scale service composition."
4173,"It is common that multiple Web services coordinate to achieve a common goal. If two Web services happen to reside on the same runtime system, it is inefficient to communicate via Web services interface. This research proposes our short circuit switch (SCS), which is an intelligent Web services discovery and invocation engine that can be attached to deployed Web services. Our preliminary empirical experience shows that our short circuit switch engine exhibits high performance."
4175,"With the rapid deployment of e-services, many workflow-like e-business process definition languages come into existence. At the same time, ontology Web language for services (OWL-S) aims to build an ontology language to support the integration of various specifications. There has been some work on mapping WSDL (Web services description language) to OWL-S to build a connection between the Web service and service profile. However, in the sense of activity relationships, there has been no effort so far trying to build the OWL-S service model from a workflow process model. Therefore, we design and develop an innovative mapping tool to translate BPEL4WS (business process execution language for Web services) to OWL-S. Through this mapping, semantics in the traditional business process specifications can be enriched significantly to enable more flexible and automatic e-service functions by using existing OWL-S tools such as composition and discovery, especially the execution of workflow-based services."
4176,"Interoperability is the ability of software and hardware on various machines from various vendors to communicate with each other without significant changes to either side. SOA implementations have a wide variety of disparate products, such as portal, business process engine, enterprise service bus, application server, registry etc., from same or different vendors. These products need to interoperate with each other for the system to behave as a cohesive whole and provide the desired functionality. Usually open Web Service standards and specifications are used to connect these products in an SOA stack However, interoperability cannot be guaranteed due to various reasons like differences in the versions of Web Service standards and specifications supported, differences in error handling mechanisms, differences in protocol support etc. Before an enterprise decides on its SOA enabled products, the interoperability between them should be tested against at least a couple of business processes. We have built a methodology to verify the interoperability of the various products in an SOA stack based on a case study carried out for a customer."
4177,"Traditional concept of services focus on Web based information centric services. In this paper, we study virtual services which mean services over 3D Internet/virtual worlds. We focus on studying the learning applications as one type of killer applications of virtual services. We show that virtual learning services have rich opportunities of service innovation. Virtual services have additional dimensions beyond traditional Web services which have significant influence on both its research and industry applications. We not only give the problems of learning design in 3D virtual worlds, but also summarize patterns for virtual learning solutions and architecture of virtual learning. With several case studies, we show the applicability of those key concepts and methods in the design and development of virtual learning services."
4178,"When operating in volatile environments, service-based systems (SBSs) that are built through dynamic composition of component services must be monitored in order to guarantee the response times of the SBSs. In particular, the critical path of a composite SBS, i.e., the execution path in the service composition with the maximum execution time, should be prioritised in cost-effective monitoring as it determines the response time of the SBS. In volatile operating environments, the critical path of a SBS is probabilistic. As such, it is important to estimate the criticalities of the execution paths and the component services, i.e., the probabilities that they are critical, to decide which parts of the system to monitor. In this paper, we propose a novel approach to the identification of Probabilistic Critical Path for Service-based Systems (PCP-SBS). PCP-SBS takes into account the probabilistic nature of the critical path and calculates path criticalities in the context of service composition. We evaluate PCP-SBS experimentally using SBSs that are synthetically composed based on a real-world Web service dataset."
4179,"This paper presents a language-based approach to service deployment. The language is called Abacus, which is a service-oriented programming language for grid applications. In Abacus, a service is abstracted as a basic language construct, and service deployment is expressed by a deployment statement. This approach allows an Abacus application to automatically deploy a service at runtime without any configuration information. This paper compares the development detail in the Abacus environment with that in the manual deployment environment, and evaluates our deployment approach accordingly. On the one hand, this approach frees maintainers from annoying deployment work at low-level, which helps to enhance the productivity in building grid applications. On the other hand, this approach allows services to be deployed according to the application logic, helping to improve the utilization and the management of various grid resources"
4180,"We present methods that can provide a win for both the consumer and the producer of electricity. What distinguishes this paper from most work on demand-side management is our consumer-centric focus: we transform human and other-generated internal demand within the consumer to fit a prescribed shape for demand to be presented to an external utility. We recommend that the utility prescribe the shape. We also explain why setting tariffs by time-of-day, independent of the amount consumed, can put the utility at an economic disadvantage. Key contributions of our paper include (1) the design and exemplary operation of an estimator that selects a battery size likely to make feasible the periodic (e.g., daily) transformation of internal demand into a shape proportional to an input shape; (2) the design and exemplary operation of a system to perform the required reshaping based on various input forecasts and consumption history; (3) the suggestion that energy price discounts should be based on the daily shape of the demand, as opposed to tariffs based on time-of-day but independent of the amount of demand; and (4) a demonstration of the effectiveness of using forecasts to manage demand. We focus on the management of demand for electricity. However, our methods would apply in a variety of industries that use metered services for a commodity that can be stored. These commodities include water, natural gas and other fuels, and some internet services (e.g., streaming and cloud services, especially cloud storage)."
4181,"The emerging Web service technology has enabled the development of Internet-based applications that integrate distributed and heterogeneous systems and processes, which are owned by different organizations. However, while Web services are rapidly becoming a fundamental paradigm for the development of complex Web applications, several security issues still need to be addressed. Among the various open issues concerning security, an important issue is represented by the development of suitable access control models, able to restrict access to Web services to authorized users. We present an innovative access control model for Web services. The model is characterized by a number of key features, including identity attributes and service negotiation capabilities. We also discuss an architecture implementing the model and we propose the use of a certificate scheme able to support the exchange and verification of subject attributes."
4182,"Interpreting legacy XML documents is a great challenge for realizing the vision of the semantic Web (SW). This paper presents an algorithm to transform XML data into RDF- foundation language of the SW - automatically. Our approach maps element definitions stored in XML schema to RDF schema ontology, where the ontology is used to describe the meaning and relationships between XML elements. The RDF results containing XML data at the semantic level while retaining their nesting structure make huge XML data source on the current Web be available for the SW."
4183,Service discovery has been recognised as an important aspect of service oriented computing. This is even more the case when developing service centric systems in which software systems are constructed based on the identification and composition of Web services that together can fulfil the functionality of the system being developed. In this paper we present a framework that supports the discovery of services that can provide the functionality and satisfy the properties and constraints of service-based systems during their design phase. Our framework makes use of linear composition of service operations in which more than one Web service operations can be combined to fulfil a functionality of the system when no single operation can be identified. The discovery process is based on a graph-matching algorithm. A prototype tool has been developed to demonstrate and evaluate the framework.
4184,"This paper describes the methodology and practices in applying cloud in China top banks' next generation banking system. The core banking system will help China top banks from world largest retail banks into integrated financial services providers. The services-oriented new system will require reliable, flexible and manageable infrastructure frameworks. The existing data centers do not meet the criteria. This paper describes a new approach for mapping application patterns onto private cloud based technology services. The application patterns will be mapped into deployment patterns, which will help data centers to provide orchestration just-in-time. Meanwhile, Key Performance Indicators are identified for manage the non-functional requirements of application components. The frameworks, together with key algorithms are described in the paper. The paper comes from real design projects in China top banks, which have over 4 thousand million accounts. This is the first time to use private cloud in such a large enterprise core business."
4185,"Modern Internet of Things (IoT)-aware business processes consist of various geographically dispersed sensor devices. Large amounts of raw data acquired from sensors need to be regularly transmitted to the targeted processes in enterprise data centers, which results in a significant increase in network traffic and latency. It is necessary to execute such processes in a distributed way. A major challenge for distributed business processes is to design an optimal fragmentation and deployment scheme to improve the overall performance of the process. To tackle this challenge, we propose a novel location-based fragmentation algorithm to partition a process, and apply the Kuhn-Munkres algorithm to solve the optimal deployment of process fragments. These distributed fragments can collaborate together to complete a common goal by using a topic-based publish/subscribe infrastructure. This approach can reduce network traffic and save the process execution time. In our experiment, an integrated monitoring process is used to illustrate the effectiveness of the proposed solution. The results show that the performances of distributed execution outperform the centralized one."
4186,"We present design and implementation of CyPhyS+, a comprehensive, low-cost and standard complaint Cyber Physical System for remote health monitoring of elderly staying in old age homes. It is an end-to-end 6LoWPAN based healthcare system, which provides reliable and secured medical data acquisition, on the fly medical data analytics and visualization in real-time. It is designed for low power operations and resource constraint environment. The paper describes system architecture, design and implementation details of software and hardware sub-systems. As part of the system we developed power efficient Bluetooth-6LoWPAN mote, Health Mote. The paper describes novel energy efficient end-to-end multi-stage message reliability scheme. The experimental results prsented show the efficacy of the approach. For ensuring robust operational network, CyPhyS+ employs a low power and application aware SNMP based network monitoring for 6LoWPAN multihop network. In addition, CyPhyS+ incorporates a 128-bit AES, CBC-MAC based encryption and authentication mechanism. Other notable features of CyPhyS+ include its compliance with open FHIR/HL7 medical standards, support for Restful architecture based web services, and medical data analytics of ECG. We report on the extensive field trials that have been carried out across various parts of the city."
4187,"Service composition with end-to-end QoS constraints have been proven to be an NP-hard problem and various evolutionary algorithms such as Artificial Bee Colony (ABC) are widely adopted to look for an approximately-optimal solution in the restricted time. The advantage of ABC algorithm is its simplicity (i.e., only three control parameters, and simple heuristic rules for exploiting the solution space), and our previous work has verified its effectiveness in solving the service composition problem. This paper focuses on the enhancement of traditional ABC neighborhood strategy for local search, with the objective of better optimality and faster convergence rate. The work is shown in two perspectives. Firstly, an approximate-mapping based local search strategy is proposed, where the discrete solution space of service composition problems are approximately transformed into a continuous space in which a locally optimal neighboring solution is precisely found, in this way, the superiority of traditional ABC could still hold in service composition problem. Secondly, we adopt the Von Neumann neighborhood topology, which has been proven to have better performance than other topologies, to further improve the quality of local search. Experiment results show that our Approximate-Mapping Von Neumann algorithm (AMV) is more effective than other service composition algorithms such as genetic algorithm and Threshold-Based Algorithm (TBA)."
4188,"Cloud environments are being increasingly used for deploying and executing business processes to provide a high level of performance with low operating cost. Nevertheless, due to the lack of explicit and formal description of the resource perspective in existing business processes, cloud resources cannot be efficiently and optimally managed. The aim of the present paper is to offer a formal definition of the resource perspective in business processes as a step towards ensuring correct and optimal Cloud resource allocation in business process modeling. Concretely, we propose a formalism based on the Event-B for specifying Cloud resource allocation policies in business process models. This formal specification is used to formally validate the consistency of Cloud resource allocation for process modeling at design time, and to analyze and check its correctness according to users needs and resource properties. In order to show its feasibility, our approach has been tested using a real use case study from an industrial partner."
4189,"By introducing a virtual computing session abstraction to network computing infrastructure, and slightly extending its underlying remote display protocol, we propose VICOS (virtual computing session): a framework of session management which provides session-level manageability, and allows multiple user to access the computing service on the server simultaneously. The virtual computing session abstraction also supports user roaming among desktops, i.e. user mobility. This framework isolates user's computing session from network or desktop failure. The session-level management mechanism provided by this framework improves the service availability and reliability. We have implemented our VICOS prototype in Linux. We demonstrate that our Linux VICOS prototype can provide general-purpose computing service with low overhead."
4190,"Service Oriented Architectures are nowadays used in a wide range of organizations to support critical daily operations. Although the underlying services should behave in a secure manner, they are often deployed with bugs that can be maliciously exploited. The characteristics of service-based environments open the door to security challenges that must be handled properly, including services under the control of multiple providers and dynamism of interactions and compositions. This paper presents an extensible tool able to widely test such infrastructures for vulnerabilities. The tool is based in an iterative process that uses interface monitoring to automatically monitor and discover the existing services, resources and interactions, and applies different testing approaches depending on the level of access to each existing services. Two case studies has been developed do demonstrate the tool, and results show that the tool can effectively be used in different service-based scenarios, under different access conditions to the target services."
4191,"The paper describes a case study conducted in collaboration with a leading provider of global transportation solutions. The organisation is a multimodal cargo solutions provider, offering worldwide transportation via land, sea, and air, as well as logistics services. The company engaged in an initiative to develop an enterprise portal aiming to leverage leading edge technologies to improve customer service, streamline internal business processes, consolidate distributed applications, and enable the organisation to continue to expand globally. Following a feasibility study, which included analysing the current system, establishing the original system requirements, identifying new and emerging requirements, and analysing suitable portal products that would meet these organisational requirements, a portal suite was purchased. The organisation was then faced with the challenge of deploying a solution and a number of portal applications that would quickly, provide business benefits to the organisation, therefore giving stakeholders an early return on investment. The paper describes the design and development approach adopted during the initial phases of the portal development life cycle focusing, in particular, on the development and implementation of two service portal applications."
4192,"Service-oriented thinking is one of the fastest growing paradigms in information technology, with relevance to many other disciplines. Service-oriented analytic workflows can bring together various analytic computing tools and compute resources offered as services to answer complex research questions. The current healthcare system in United States is experiencing fundamental transformation as it moves from a volume-based business to a value-based business. One strategy that healthcare organizations start to deploy is leveraging their healthcare data to gain insights for optimizing their operation. Therefore it is perfectly logical to extend the application of service-oriented analytic workflows to population health studies, as these rely on both medical expertise and processing of large data sets to serve end users of various backgrounds and skill sets. However, in the practical application of such service oriented approach, the user often finds it difficult to choose the right services or workflows that can help them to find the answers to their questions. To tackle this problem, we propose a heuristic recommendation method based on the feature significance. The user submits an enquiry, then based on which, the system will recommend the services and compositions that are likely to produce meaningful answers. In this paper, we will elaborate the interactions between different roles in a service oriented analytic system, develop the modeling to illustrate the relations among enquiry, features, services and workflows, propose the algorithm for service recommendation, architect the system and show a reference implementation of a prototype."
4193,"Energy consumption is emerging as a new crucial issue of the Cloud Computing environments such as data centers. The problem of power consumption is more challenging especially in the context of scientific workflows deployment in the Cloud as they trigger intensive computational tasks and data manipulation steps which begets excessive data movement operations over communication networks. For instance, it was revealed that network devices consume up to one-third of the total energy consumption of Cloud data centers. In this paper, we propose an energy-aware approach for scientific workflows scheduling in the Cloud. In the first step, we propose a Workflow Partitioning for Energy Minimization (WPEM) algorithm that allows reducing the network energy consumption of the workflow and the total amount of data communication while achieving a high degree of parallelism. In the second step, we use the heuristic of Cat Swarm Optimization to schedule the generated partitions in order to minimize the workflow's overall energy consumption and execution time. We evaluated the proposed approach using three real cases of data intensive workflows and compare it with other algorithms from literature. The experimental results show that our proposal allows to reduce remarkably the network energy consumption of the tested workflows (up to 96% of network energy consumption saving for memory intensive workflows) and the overall energy consumption of the workflows while ensuring a reasonable execution time and using less Cloud resources."
4194,"Video can be streamed live with different applications (e.g. YouTube Live, Periscope). Typically, the video content is adapted for end users based on receiving client's capabilities, and network bandwidth. The adaptation is realized with different video representations, which are created by transcoding the original video content. When video is streamed live, transcoding has to be completed within real time constraints, which is a computationally demanding process. Particularly, live transcoding should be enabled efficiently by a content distributor to minimize resource provisioning costs. The contribution of this paper is an architecture for predicting live video transcoding performance on a Docker-based platform. Particularly, cloud resource management for live video transcoding has been focused on. A model was trained based on measurements in different transcoding configurations. Offline evaluation results indicate that live transcoding speed or CPU usage can be predicted with 3-8 % accuracy. When video is transcoded on virtual machines based on predictions in a prototype system (live), live transcoding speed prediction accuracy is within a similar range as the offline performance, but worse for CPU usage prediction (5-15%). In most cases the specified range for transcoding speed and CPU usage can be achieved at least with a precision of 76%."
4195,"This paper studies the quality of web service prediction problem. We formalize the QoS prediction problem by incorporating multiple contextual characteristics via collective matrix factorization that simultaneously factor the user-service quality matrix and contextual information matrices. Using the service category and location context, we develop three context-aware QoS prediction models and algorithms to demonstrate the advantages of this modeling technique. The advantages of our proposed models are demonstrated via experiments on real-life data sets."
4196,"Web services are loosely-coupled business applications willing to cooperate in distributed settings within different groups called communities. Communities aim to provide better visibility, efficiency, market share and total payoff. There are a number of proposed mechanisms and models on aggregating web services and making them cooperate within their communities. However, forming optimal and stable communities as coalitions to maximize individual and group efficiency and income has not been addressed yet. In this paper, we propose an efficient coalition formation mechanism using cooperative game-theoretic techniques. We propose a mechanism for community membership requests and selections of web services in the scenarios where established communities already exist. Moreover, we analyze the scenarios where communities are not established yet and web services can form multiple communities. The ultimate objective is to develop a mechanism for web services to form stable groups allowing them to maximize their efficiency and generate near-optimal (welfare-maximizing) communities. The theoretical and simulation results show that our algorithms provide web services and community owners with applicable and near-optimal decision making mechanisms."
4197,"With the development of Service oriented Computing (SOC), more and more functionally similar Web services are deployed over the Web. Quality of service (QoS) aspects (e.g., availability, response time, etc.) are thus crucial for selecting among functionally similar Web services. Moreover, the skyline has been considered as an important concept for selecting Web service based on QoS. In this paper, we propose WS-Sky, a Web service selection framework based on QoS. Our framework leverages two variants of the notion of skyline to effectively and efficiently select Web services that better fit the user needs. Our experimental evaluation on both real-world and synthetic datasets demonstrates that WS-Sky assists users to find the most relevant Web services in a flexible way, and allows users to control the size of the retrieved Web services."
4198,This paper proposes a hybrid scheduling scheme to combine the priority queueing and Packet General Processor Sharing (PGPS) algorithm for Multimedia Cloud Services in Software Defined Network (SDN). The network calculus theory is employed to develop modeling and analysis techniques for evaluating the QoS performance of the proposed scheduling scheme. Both analytical and numerical results obtained in this paper show that the proposed scheme can provide QoS guarantee for meeting diverse requirements of multimedia applications.
4199,"Poor Data Quality is a serious problem affecting enterprises. Enterprise databases are large and manual data cleansing is not feasible. For such large databases it is logical to attempt to cleanse the data in an automated way. This has led to the development of commercial tools for automatic cleansing. However, offering data cleansing as a service has been a challenge because of the need to customize the tool for different datasets. This is because current commercial systems lack the ability to incorporate the unique exceptions of different data sources. This makes the migration of underlying data cleansing algorithms from one dataset to another difficult. In this paper we specifically look at the address standardization task. We use Ripple Down Rules (RDR) framework to lower the manual effort required in rewriting the rules from one source to another. The RDR framework allows us to incrementally patch the existing rules or add exceptions without breaking other rules. We compare the RDR approach with a conditional random field (CRF) address standardization system and an existing commercially available data cleansing tool. We demonstrate that RDR is an effective knowledge acquisition method and that its adoption for data cleansing can allow data cleansing to be offered as a service."
4200,"As organizational structures shift from centralized and hierarchical structures to decentralized and loosely coupled networks, value creation and innovation processes are changing as well. Especially in the services sector, we observe a shift from closed and company-internal innovation processes to more open and collaborative forms of innovation (e.g. via various types of communities that emerge in enterprises' surrounding ecosystems). Communities of co-innovation or innovation networks can be described from a service systems perspective. In this work, a modelling approach to generally describe service systems is developed leveraging the St. Gallen Media Reference Model (MRM). A specific instantiation of this model is presented that allows for modelling service systems in terms of communities of co-innovation. The model is applied for the case of co-innovation in the enterprise software ecosystem of the SAP AG. The approach fosters understanding, organizing and managing such new networks of collaborative innovation in service science."
4201,"In large enterprises, there are an abundance of IT assets which provide employees with the productivity tools necessary to do their jobs. There are also large numbers of applications and services for running the 'back office'. A challenge to all enterprises is cataloging these services, in order that organization can effectively put the services into business use, retire unused services or duplicate services, and build new services to fill gaps in the catalog. Furthermore, it is necessary to understand the relationships among available services in order to combine them into convenient and useful packages/bundles which employees can immediately put into use. In this paper we will describe the process used in the IBM enterprise to create and distribute these bundles."
4202,"Over the last couple of year, the expansion of Web services has grown drastically. In practical applications, many uncertain attributes of Web service change as the complexity of the Web evolves and influence the QoS (quality of service). However, existing matchmaking models and algorithms do not take the uncertain attributes into account, which leads to inaccurately matched results. This paper presents an uncertainty-aware QoS match-making model, UQoSMM, which describes the uncertainty of QoS attributes under different conditions. This paper also proposes a QoS decision making model based on the non-parameter-test, NPTQOS. The decision making model includes a QoS stability comparison algorithm according to the history values of QoS attributes. A prototype has been designed to realize the uncertain attribute-based Web service selection."
4203,"Recent advancements in process centric systems, the concept of Services have been adopted widely over business processes. Services which are discrete reusable functional blocks have become the choice to achieve highest order of reuse. However, key to having such services is to identify them with right objectives. Current methods and techniques for service analysis are heuristic and rely heavily on the experience and intuition of the designer. We present the prerequisite formalism for service analysis from business process models and basic quantitative metrics."
4204,"Web service registries play an important role in service oriented applications. They constitute the market where service consumers and providers go to search and advertise Web services. With the proliferation of Web service registries, finding an adequate registry has become a complex task for a service requester. In this paper we propose a semantic model for Web services registry description (WSRD). WSRD descriptions depict the functionalities offered by services advertised by a given registry since they rely on these descriptions. We also propose a functionality-driven clustering approach for distributed Web service registries. This approach is based on a fuzzy clustering technique and allows structuring distributed registries based on their WSRD descriptions without any additional data. This clustering will be helpful for selecting an adequate registry for service requesters."
4205,"IT service providers are increasingly utilizing globally distributed resources to drive down costs, reduce risk through diversification and gain access to a larger talent pool. However, fostering effective collaboration among geographically distributed resources is a difficult challenge. In this paper, we present our initial attempt to quantify the increased overhead in leveraging distributed resources as one of the project costs. We associate this overhead cost measurement with metrics that measure communication quality, such as reduction in productivity and communication delay. These metrics can in turn be computed as functions of underlying project parameters. To achieve this goal, we first build a project communication model (PCM) to categorize different types of collaborative communication. We then represent communication efficiency and changes in resource availability in terms of information theoretic concepts such as reduced channel capacity, information encoding efficiency and channel availability. This analysis is used to help determine the cost associated with team formation and task distribution during the project planning phase."
4206,"Edge computing, as an emerging computing model, can offload delay-sensitive computing tasks from Internet of Thing (IoT) devices with limited computing resources and energy to the edge cloud. In the edge computing system, several servers are placed on the network edge near the IoT devices to process the offloaded tasks. A key issue in edge computing system is how to reduce the system cost while completing the offloaded tasks. In this paper, we study the task scheduling problem to reduce the cost of edge computing system. We model the task scheduling problem as an optimization problem, where the objective is to minimize the system cost while satisfying the delay requirements of all the tasks. Then, we prove that the proposed optimization problem is NP-hard. To solve this optimization problem effectively, we propose a task scheduling algorithm, called Two-stage Task Scheduling Cost Optimization (TTSCO). We validate the effectiveness of our algorithm by comparing with optimal solutions. The results show that the approximate ratio is less than 1.2 for 95% of the data sets we use. Performance evaluation shows that our algorithm can effectively reduce the cost of edge computing system while satisfying the delay requirements of all the tasks."
4207,"The evolution of workflow models has been characterized by profound flux ever since the formative days of flow charts. Despite, or because of, a proliferation of workflow-oriented standards, none can honestly claim to be ubiquitous. In a widely distributed grid, gaps will inevitably exist in the model of workflows deployed therein. Thus, there is a need to present a consistent view of disparate processes to high-level roles such as analysts. To that end, we propose virtualizing the workflow layer to reconcile such gaps, both in the modeling and the execution sense. Given a wide area network of workflow providers, we overlay it with one based on WS-BPEL, our reference model. For every provider, we build virtualization-enabling silos, realized as components driven by a SCA container. Further, we describe design patterns to facilitate bindings to other models at various levels. Finally, we take reality checks by identifying those patterns in such standards as BPMN, XPDL, WWF and ebBP."
4209,"The challenges in banking industry are forcing banks to renovate their core systems to survive fierce competition. A progressive renovation approach is preferable to total replacement because it is less intrusive and risky. In this paper, we present an incremental core banking renovation methodology based on the concept of business entities. We implemented this methodology to develop an SOA solution to dynamic product bundling for an Asian bank. This renovation project provided an opportunity for us to explore SOA design principles regarding service component modularity, service reusability, change management and integration, and also to empirically demonstrate the advantages of the business entity-centric approach in achieving well-designed SOA solutions."
4210,"We present a mobile distributed computing platform, WIPdroid, for distributed services computing in mobile environment with real-time communication capabilities. It is based on WIP, Web services session initiation protocol, for real-time service-oriented communication over IP. It integrates the emerging mobile endpoint platform, Android, from Google's initiative of open handset alliance (OHA). The WIP component of WIPdroid supports Web services based communication over IP. But most importantly, it transforms WIPdroid into a mobile SOA Web services endpoint. Applications developed on WIPdroid can be exposed and enabled by two-way Web services. This makes WIPdroid well suited for distributed computing and SOA based applications, such as communication enabled business process (CEBP) and software-as-a-service (SaaS). It also leads to a new type of dual-mode phone, with one communication mode from the binding with the physical mobile environment, e.g. GSM, CDMA, 3G, etc., and one fully web services enabled, service-oriented communication over IP mode from WIP. A prototype WIPdroid mobile computing platform is developed and tested. In order to support advance Web services capabilities on mobile endpoints, we developed an endpoint-edition of two-way Web services application proxy (2SAP) which has a small memory footprint (2-3 MB) and can be hosted on a mobile endpoint/handset, e.g. smart phone and PDA."
4211,"New value-added business, generated by Web service composition in BPEL, provides more powerful and flexible features. SOAP-based web services invoked in BPEL are heavy with redundant information and lower performance. RESTful Web service is brought forward to new value-added business as an alternative solution. However, it's a big challenge to compose RESTful Web services with SOAP-based Web Services in SOA BPEL business processes. BPEL is not designed to use RESTful Web services because these types of services do not use WSDL to describe themselves. This paper proposes a novel approach for integrating RESTful Web services and SOAP-based Web services in business processes described in BPEL. The approach is applied to a practical multimedia conference BPEL process, and the experimental data show the stability of the new RESTful BPEL process with a higher performance."
4212,"Software that continuously runs over a long period has been frequently reported encountering ""gradual degradation"" issues. As time progresses, software tends to exhibit degraded performance, deflated capacity, exhausted physical resource or deteriorated QoS (Quality of Service). Different from transient software anomalies, this issue is a chronic degrading process and usually persists until the software is eventually unavailable. We name it ""Software Degradation"" or ""Degradation"" for short. In this paper, we propose a framework GVAR, utilizing Granger Causalities to predict and diagnose software degradation. GVAR is evaluated via an 8-day experiment on a VoD (Video on Demand) platform Helix-Serv. The experimental results show that GVAR can predict the TTF (Time to Failure) of degraded software in an accuracy of 80.1%, remarkably outweighing the widely used ARMA and Sen's Slope Estimator approaches. Moreover, GVAR can guide diagnosing the potential root cause of degradation issues."
4213,"Automatic web services composition approaches aim at automatically composing services to satisfy users' request according to the information of users' input. As the number of web services is increasing, the performance of automatic service composition is faced with challenges. In this paper, we propose an approach based on backward chaining method to compose web services with high performance and good Qos, called Clustering Based Backward Chaining Method (CBBCM). We use k-means method to cluster services as the preliminary work. Services are clustered according to the semantic similarities of their output parameters. Skyline is used as an optimization to set services with good Qos on the top of each cluster. Then the adapted backward chaining method is applied to compose services. We evaluate our approach and the backward chaining method through experiment with different datasets. It is proved that our algorithm has a high performance and gets the composition results with better Qos."
4214,"Web service interactions have triggered the initiative to identify and solve mismatches from a behavioral aspect. Current approaches are limited since they mainly focus on control-flow but largely ignore data-flow. In this paper, we propose an approach to automatically generate scenarios and views for describing external behavior of Web services, i.e. the public process, considering both control-flow and data-flow. We define a scenario as a set of complete execution paths for a public process. Data dependencies are presented as a dependency graph, which is optimized into a minimal dependency graph. Then, a view is generated to describe a scenario for analysis purposes, and external behavior of a Web service is described as a finite set of views. Our approach is very useful for service modelers and users to better understand the external behavior of Web services, to identify and solve mismatches from a behavioral aspect, and thus to facilitate Web service interactions."
4215,"In this paper, we propose a formal framework for checking the consistency of security policies of services using Answer Set Programming (ASP). We illustrate that the formalisation of security policies of the service providers and the service consumers in ASP is an effective way for reasoning about the compatibility of policies to enable the dynamic discovery and invocation of services."
4216,"Process inter-operation is characterized as cooperative interactions among loosely coupled autonomous constituents to adaptively fulfill system-wide purpose. Issues of inconsistency can be anticipated in inter-operating processes given their independent management and design. To reduce inconsistency (that may contribute to failures) effective methods for statically verifying behavioral interoperability are required. This paper contributes a method for practical, semantic verification of interoperating processes (as represented with BPMN models). We provide methods to evaluate consistency during process design where annotation of the immediate effect of tasks and sub-processes has been provided. Furthermore, some guidelines are defined against common models of inter-operation for scoping traceability to possible causes of inconsistency. This supports subsequent resolution efforts."
4217,"Modern applications are increasingly dynamic and heterogeneous and their lifecycle is more and more governed by autonomic managers that are also getting more and more complex. The purpose of this paper is to present a service-oriented framework that facilitates the development and management of dynamically extensible autonomic managers. More precisely, we propose an architecture based on the opportunistic collaboration of very specialized and coherent modules called administration tasks. The current framework prototype has been implemented as a specialized Service-Oriented Component Model. It allows the dynamic integration of autonomic tasks and their management based on contextual evolutions."
4218,"Classical distributed computing projects generally use a specialized client/server model. Recent approaches, such as BOINC, favor instead the development of distributed computing platforms, relying on a generic client/server model. We propose a fully decentralized computing model, considering all participant as peers that can submit personalized computing tasks to any number of other peers currently offering their services, listed in a peer directory. Our model is built upon Chord, a particular distributed hash table. Chord allows load balancing of the number of keys per node, but offers no way to balance the bandwidth load of a frequently accessed key, such as a peer directory. Our model extends Chord with load balancing of those access-intensive keys. We present a modelization of the bandwidth and storage costs of our model and experimental performance results using a variable number of peers, tasks, tasks time, and a variable ratio of contributors and solicitors roles among peers."
4219,"The benefits provided by Web service protocols are well recognized. Deployments to date, however, have concentrated on new applications, and existing Web-based applications. A host of legacy applications and protocols continue to exist in their native forms, outdated, yet entrenched due to large installed bases. This paper details our observations in integrating a Web service infrastructure into the simple network management protocol (SNMP). SNMP has been in use for over a decade and a half, predominantly in network equipment embedded systems. Our Web service-based approach allows us to enhance our existing application with XML/SOAP interoperability, SSL/TLS security, and the potential to migrate both application and protocol layers to encompass future extensions and Web browser accessibility. The difficulty with SNMP, and many other legacy networking protocols, is that much of the extensive installed base is hosted on limited capability, legacy hardware. While the benefits of our scheme are quite tangible, the performance impact of adding these features is not well known. We examine two approaches, an integrated solution using a light-weight HTTP/SOAP stack, as well as a standard Java Web server implementation. Our tests reveal unanticipated performance results through both the integrated and proxy methods. We discuss the impact of these anomalies on the viability of our approach and address the broad issue of migrating Web services to legacy embedded architectures."
4221,"Gartner has determined that “Complex Event Processing” (CEP) is one of the emerging areas on the rise in the “hype cycle” and become dominant in the area of Business Process Integration and Management (BPIM) as well as other areas (such as: command and control). According to Gartner’s prediction, within 5-10 years this area will get to maturity. Activities in this industry have already started, and it is of the interest of this community to have early acquaintance with an area that may be dominant in several of the application types. The aim of this tutorial is to educate the audience about CEP and it’s relevant to the some of the other emergent areas such as: Web services, autonomic computing, and grid management, as well as to traditional data management areas. The tutorial will cover introduction to the area, model-based approaches, architectures, several use cases, relationships to other concepts. It does not require any prior knowledge in the CEP."
4222,"Recent years have witnessed a rapid growth in using Web services for data publishing and sharing among organizations. To improve the efficiency of software development and economize on human and material resources, service reuse is viewed as a powerful means which will not only reuse atomic services, but also reuse arbitrary granularities of Service Process Fragments (SPFs). However, effectively reusing arbitrary granularities of SPFs has not been solved yet. In this paper, we propose a novel method of SPF reuse, named SCKY, based on the Cocke-Kasami-Younger (CKY) algorithm. We first present an extended CKY to do SPF-query. Then we address how to do SPF-query by a probability CKY, i.e. return a SPF with maximum emergence probability. Finally, we explore the QoS Query of SPF. Through a set of experiments, the effectiveness and robustness of our approach are evaluated, where the dataset is constructed by the Web Service Challenge Testset Generator1 (CTG)."
4223,"Recently, collaborative filtering has been applied to QoS-aware Web service recommendation. However, it cannot make recommendations for users that have invoked only a very small number of services because of data sparsity. In addition, these methods do not know how confident they are in their recommendations. Based on the fact that QoS values of web services are usually subject to the locations of users, a few works assume that the additional knowledge of users' locations can be used to better deal with the data sparsity issue, since a user only needs to know the users near to him/her. On the other hand, the sparsity of user-service invocations forces the location-aware method to consider the QoS experiences of users not near enough, which may decrease its precision. In order to find a good trade-off between coverage and precision, we propose a random walk method combining location-aware and collaborative filtering method for web service recommendation. The random walk method allows us to define and to measure the confidence of a recommendation. To evaluate the performance of our proposed method, we conduct a set of comprehensive experiments using a real-world web service dataset, and compared the method with existing collaborative filtering methods."
4224,"As web-enabled software becomes the standard for business processes, the ways organizations, partners and customers interface with it have become a critical differentiator in the market place, i.e., API Economy. With the rapid proliferation of APIs, it is increasingly important for users to effectively manage objective APIs in kinds of API markets, e.g., ProgramableWeb (PW), Mashape, etc. In this paper, to facilitate the process of API management, we propose a graphbased recommendation approach called ATRec to automatically assign tags to unlabeled APIs by exploiting both graph structure information and semantic similarity. Specifically, ATRec first leverages the multi-type relations (i.e., among APIs, mashups, and mashup assigned tags) to construct a heterogeneous network, in which a Random Walk with Restart (RWR) model is applied to alleviate the total cold start problem where no API has ever been tagged. Furthermore, we apply the recommended API tags in two API management scenarios (API search, API recommendation). Comprehensive experiments based on a real dataset crawled from PW demonstrate the effectiveness of the proposed approach."
4225,"Locality Sensitive Hashing (LSH) is a widely used similarity search technique for many web services, such as content-based retrieval services for images and videos. Due to its popularity, much research effort has been devoted to improving the search quality, and the indexing and query performance of LSH. However, most existing variants of LSH can only run on single node, which limits their applicability to large-scale data. In this paper, we present a Shuffle-Efficient Similarity Search scheme based on LSH, which can be efficiently executed in distributed environments, to serve a massive amount of data. In SES-LSH, a shuffle efficient indexing scheme is proposed to reduce the data shuffle when constructing hash tables, and a location-aware querying scheme is proposed to improve the query performance. We have implemented a prototype of SES-LSH based on Spark, and several optimizations have been utilized to improve the fine-grained hash table operations of distributed LSH. Extensive experiments using large-scale real-world datasets show that SES-LSH is remarkably more efficient than existing methods."
4227,"In a short period the Web has become an important part of our lives. However, the full potential of the Web is still not realized. Two recent developments - Web services and the semantic Web - are steps in the direction of utilizing the full potential of the Web. Web services allow applications to utilize the Web for automatically extracting (and updating) information while the semantic Web enterprise promises to provide the infrastructure that allows intelligent Web services to be rapidly created and deployed. However, with this comes the task of transforming the traditional Web-based systems to Web-services over the semantic Web. In this paper, we demonstrate how an existing successful Web-based system for providing help to first responders of chemically hazardous emergencies (called E-plan) can be converted into a Web-services based model using the semantic Web and intelligent reasoning technologies. Our efforts can be regarded as a case study in converting monolithic Web-based applications to a more agile, rapidly deployable intelligent Web-services model."
4228,"Workflows often operate in volatile environments in which the component services' QoS changes frequently. Optimally adapting to these changes becomes an important problem that must be addressed by the Web service composition and execution (WSCE) system being utilized. We adopt the A-WSCE framework that utilizes a three-stage approach for composing and executing Web workflows. The A-WSCE framework offers a way to adapt by defining multiple workflows and switching among them in case of component failure or changes in the QoS parameters. However, the A-WSCE framework suffers from the limitations imposed by a simple strategy of periodically checking the QoS offerings of randomly picked providers in order to decide whether the current workflow is optimal. To address these limitations, we associate the value of changed information (VOC) with each workflow and utilize the VOC to update which workflow to execute. We empirically demonstrate the improved performance of the workflows selected using the new approach in comparison to the original framework."
4229,"XML plays an important role in building enterprise applications. However, most of the XML-based applications, particularly the emerging Web services, suffer from low performance caused by XML processing and thus bring negative user experience in terms of response time. We argue that by reducing the considerable overhead in garbage collection the XML processing performance can be improved. We begin by conducting a set of experiments to understand the XML parser's memory characteristics, such as heap composition, object size and type distributions, object lifetime, and so on. Then, we get the valuable findings for improving performance that XML processing, which violates the weak generational hypothesis, is a memory allocation intensive workload in which most objects are small and long-lived. The findings can benefit the design of XML parsing specific GC and related tools designed to improve XML processing performance for Web services"
4230,"Traditional, centralized orchestration of composite Web services often leads to inefficient routing of messages. To solve this problem, we present a novel scheme to execute composite Web services in a fully decentralized way. We introduce service invocation triggers, a lightweight infrastructure that routes messages directly from the producing service to the consuming one, enabling fully decentralized orchestration. An evaluation confirms that decentralized orchestration can significantly reduce the network traffic when compared with centralized orchestration"
4231,"The emergence of Web services represents a significant advance in the continuing evolution of e-business. In order to fully explore business opportunities provided by this paradigm, it is important to track its utilization. This can be done through the use of logging facilities. However, current Web logging approaches do not contemplate Web services utilization. This paper presents a Web services logging architecture based on SOAP intermediaries that captures comprehensive services usage information, which can be explored to improve B2B and B2C transactions by providing feedback on customer electronic behavior."
4232,"In this paper, we describe an approach to discover the control flow graph of Web services for Web services analysis, verification, and testing. For this purpose, three novel methods are proposed. First, we introduce a domain independent RDF Schemas for concise resource oriented functional specification of Web services operations. Secondly, we describe the use of RDF entailment to accurately derive the control flow from the functional specifications. We developed a transformation from RDF graph to SPARQL query to facilitate the RDF entailment which offers flexibility and extensibility over the direct graph matching approach. The third is a linkage based Web services modeling and analysis framework, within which we apply an improved Google PageRank algorithm to efficiently calculate test coverage potential using the derived control flow. We justify that the proposed linkage based Web services modeling and analysis framework is particularly suitable for testing Web services. A prototype of the proposed methods has been implemented and tested on some standard based Web services. Experimental results show that the control flow analysis is quite efficient and accurate, and the coverage based test results of the proposed approach are very promising."
4233,"Authorization and access control in Web services is complicated by the unique requirements of the dynamic Web services paradigm. Current authentication mechanisms for Web services do not differentiate between users in terms of fine-grained access privileges. This results in an all-or-nothing access which is not flexible enough for modern day business processes using Web services to execute. In this paper, we present a policy-based authorization framework to address this requirement. We have designed a profile of the well-known WS-policy specification tailored to meet the access control requirements in Web services by integrating WS-policy with an access control policy specification language, X-GTRBAC. The design of the profile is aimed at bridging the gap between available policy standards for Web services and existing policy specification languages for access control. The profile supports the WS-policy attachment specification, which allows separate policies to be associated with multiple components of a Web service description, and one of our key contributions is the design of an algorithm to compute the effective policy for the Web service given the multiple policy attachments. To allow Web service applications to use our solution, we have adopted a component-based design approach based on well-known UML notations. We have also prototyped our architecture, and implemented it as a loosely coupled Web service providing healthcare information services to physicians subject to applicable authorization policies."
4234,"Semantic web service discovery has attracted a lot of attention in the last decade. Research conducted in this area can be (mainly) summarized as follows: (1) ""monolith"" matchmaking algorithms (and systems), and (2) schema matching-based techniques. In this paper we describe a flexible approach that takes leverage of existing schema matchers, leading to a multiple choice strategy for semantic service discovery. The approach has been implemented and validated in using the data collection provided by the S3 (Semantic Service Selection) community, and led to promising preliminary results."
4235,"We present methods for optimally adapting Web processes to exogenous events while preserving inter-service constraints that necessitate coordination. For example, in a supply chain process, orders placed by a manufacturer may get delayed in arriving. In response to this event, the manufacturer has the choice of either waiting out the delay or changing the supplier. Additionally, there may be compatibility constraints between the different orders, thereby introducing the problem of coordination between them if the manufacturer chooses to change the suppliers. We focus on formulating the decision making models of the managers, who must adapt to external events while satisfying the coordination constraints, using Markov decision processes. Our methods range from being centralized and globally optimal in their adaptation but not scalable, to decentralized that is suboptimal but scalable to multiple managers. We also develop a hybrid approach that improves on the performance of the decentralized approach with a minimal loss of optimality"
4236,In this paper we introduce a new modeling tool for constraint handling in the area of workflow technology. The constraint handlers can be used to improve the quality of business processes but without changing already existing business logic. Todays workflow languages provide no possibility to model constraints and the actions in case the constraints get violated explicitly. Fault and event handling mechanisms to react to events not expected in normal executions are only provided by the BPEL language. Using BPEL as workflow language we integrate the constraint handling extension without changing any existing semantics in a smart way. In our approach we use this fault and event handling mechanisms to extend the BPEL language with a constraint handling mechanism. By integrating this constraint handling tool into the BPEL language we provide an approach for quality driven process modeling with the BPEL language.
4237,"This paper addresses a fundamental issue of Web service composition. We present a simple but powerful conceptual model that leads to a scalable approach to automatically constructing a composite Web service to meet its requirements by using as few services as possible. Our approach is based on a state space model that has a monotone property to allow efficient search along with efficient algorithms for pruning and simple parallelization. We provide both empirical and theoretical analyses of the proposed approach and show that it has time complexity of O(n
<sup>2</sup>
), for a repository with n services. However, the approach takes linear time for sequential compositions when service applicability is performed by service discovery and thus, it is shown to give asymptotically optimal performance. Although optimality in the number of services deployed is not guaranteed, our experiments on public benchmark data sets show correct optimized solutions 100% of the time, with a reduction in the average running time, compared to a well-performed planning-based system, of better than 35% over 207 composition problems."
4238,"With the advancement of Web services technologies, online businesses have the ability to offer their capabilities to larger, lesser known communities of potential collaborators. Universal Description, Discovery, and Integration (UDDI) specification and supporting technologies support open frameworks for businesses to store, advertise and retrieve pertinent services. Many researchers investigate approaches that ubiquitously create higher-level processes by composing services discovered in and retrieved from UDDI registries. However, there are few studies that consider the impact of registry performance on future service automation. This work focuses on evaluating the performance of UDDI registries considering variability and concurrent load of publish and inquiry requests."
4239,"With the proliferation of Web services, service engineers demand good automatic service composition algorithms that not only synthesize the correct work plans from thousands of services but also satisfy the quality requirements of the users. Our observation is that conventional approaches suffer from serious limitations in scalability and accuracy when addressing both requirements simultaneously. We have designed and implemented a tool QSynth to use QoS objectives of service requests as the search directives. This approach effectively prunes the search space and significantly improves the accuracy of the search results. Evaluations show that, compared to the state of the art, QSynth achieves superior scalability and accuracy with respect to a large variety of composition scenarios. Our design of QSynth won the performance championship of Web Services Challenge 2009."
4240,"Service-oriented systems facilitate business workflows to span multiple organizations (e.g. by means of Web services). As a side effect, data may be more easily transferred over organizational boundaries. Thus, privacy issues arise. At the same time, there are personal, business and legal requirements for protecting privacy and IPR and allowing customers to request information about how and by whom their data was handled. Managing these requirements constitutes an unsolved technical and organizational problem. We propose to solve the information request problem by attaching meta-knowledge about how data was handled to the data itself. We present our solution, in form of an architecture, a formalization and an implemented prototype for logging and collecting logs in service-oriented and cross-organizational systems."
4241,"We present a semantic optimized service discovery (SemOSD) approach capable of handling Web service search requests on a fine-grained level of detail where we augment semantic service descriptions with statistically built predictor functions. Our approach combines ontologies and mathematical functions built using statistical regression over previous Web service interactions. In the search requests we allow for arbitrary, independent and dependent constraints and user preferences expressed using objective functions. Our approach maps to standard operational research global optimization problem where algorithms of simulated annealing and differential evolution are used. It is capable of finding the optimal combination of service input and output parameters (a configuration) to a user request with rich preferences. Our approach is applied to an international package shipment scenario where real (Web) services are used and mined to create price prediction models. We show that the chosen regression method provides price prediction models of high accuracy and our approach supports expressive and complex search requests."
4242,"The composition of Web-based services is a process that usually requires advanced programming skills and vast knowledge about specific technologies. One such modern technology is Web services. In this work, we propose a framework for the smooth composition of Web services that are targeted to provide particular government services that usually require interaction between several agencies. The goal is to allow government agencies to smoothly register simple services or create new ones based on other already registered simple or composed services. Our work has the additional goal of reducing the overhead involved in the execution of composite services by providing efficient mapping schemes to bypass intermediary composed services and allowing direct invocation of the basic services in the composition."
4243,"This paper introduces a service selection model with the service location considered. The location of a service represents its position in the network, which determines the transmission cost of calling this service in the composite service. The more concentrated the invoking services are, the less transmission time the composite service costs. On the other hand, the more and more popular big data processing services, which need to transfer mass data as input, make the effect much more obvious than ever before. Therefore, it is necessary to introduce service location as a basic feature in service selection. The definition and membership functions of service location are presented in this paper. After that, the optimal service selection problem is represented as an optimization problem under some reasonable assumptions. A shortest-path based algorithm is proposed to solve this optimization problem. At last, the case of railway detection is studied for better understanding of our model."
4244,"Advances of the computing industry in SOA infrastructure have made service oriented applications practically attainable in the field of high performance computing for earth sciences. We have applied SOA to modeling and inversion of geophysical well logs. Parallelized log simulators are accessible as a Web service to a variety of interpretation workflows. We have developed an HPC management infrastructure based on SOAP. It is a fit-for-purpose system with a low runtime overhead and a novel flexible job submission mechanism, suitable for applications running on multi-cluster environments. We describe the system architecture and discuss performance benchmarks, including a novel way of estimating performance on heterogeneous clusters."
4245,"Web services provide an instantiation of the loosely coupled service–oriented architecture and facilitate the process of enterprise application integration by encapsulating information, software, and other resources. However, to exploit the true potential of web services, it is critical to develop technologies and tools for composing new services from existing ones. While numerous composition approaches have been developed in the past, very little has been done towards tooling. What is clearly lacking is an Integrated Development Environment (IDE) to ease the process of composition, thereby reducing development time and integration efforts. In this paper, we build on our previous work on service composition, and present an IDE for end–to–end composition of web services. We elaborate on the design of the IDE, describe its integration with existing technologies, and discuss its usability based on the findings of a user survey."
4246,"In this paper, we formalize the concept of a delegation network for Web intermediaries and present formal semantics for the responsibility of these actors. Key properties of the network are proven and method to judge an actor's responsibility is given. This work is important because it determines the accuracy of task execution and the feasibility of content reuse in the network"
4248,"The growth of the Internet has been accompanied by the growth of Web services (e.g. e-commerce, e-health). This proliferation of Web services and the increasing regulatory and legal requirements for personal privacy have fueled the need to protect the personal privacy of Web service users. We advocate a privacy policy negotiation approach to protecting personal privacy (Yee and Korba, 2003; ). We provided semiautomated approaches for deriving personal privacy policies (Yee and Korba, 2004). However, it is evident that approaches are also needed to ensure that providers of Web services comply with the privacy policies of service users. In this paper, we examine privacy legislation to derive requirements for privacy policy compliance systems. We then propose an architecture for a privacy policy compliance system that satisfies the requirements and discuss the strengths and weaknesses of our proposed architecture."
4249,"This paper presents the initial findings from a series of case studies involving the enterprise transformation to service oriented architecture. Ten large enterprises were studied to determine how they were able to convert their legacy IT architecture. Particular interest was paid to business models, governance, enterprise architecture, change management, risk management, and technology. These cases were used to form a predictive model of success factors in transformation."
4250,"Technologies involved in Web information distribution services are evolving to adapt themselves to new user requirements. Usually, these new technologies are used separately. ELIN (electronic newspaper initiative) project is a European Commission funded project that tries to integrate the newest standards and technologies involved in multimedia delivery applied to web newspapers. It has as objective the delivery of any type of media format to any kind of user terminal in an efficient way. In order to do that, it takes the approach of using MPEG standards: MPEG-4 for video delivery, MPEG-7 for data classification and MPEG-21 for data management and adaptation. So it integrates the totality of solutions provided for the MPEG group."
4251,"A major advantage of Service-Oriented Architectures (SOA) is composition and coordination of loosely coupled services. Because the development lifecycles of services and clients are decoupled, multiple service versions have to be maintained to continue supporting older clients. Typically versions are managed within the SOA by updating service descriptions using conventions on version numbers and namespaces. In all cases, the compatibility among services description must be evaluated, which can be hard, error-prone and costly if performed manually, particularly for complex descriptions. In this paper, we describe a method to automatically determine when two service descriptions are backward compatible. We then describe a case study to illustrate how we leveraged version compatibility information in a SOA environment and present initial performance overheads of doing so. By automatically exploring compatibility information, a) service developers can assess the impact of proposed changes; b) proper versioning requirements can be put in client implementations guaranteeing that incompatibilities will not occur during run-time; and c) messages exchanged in the SOA can be validated to ensure that only expected messages or compatible ones are exchanged."
4252,"With the fast development of Internet and rapid acceptance of Web Service technology, more and more service resources have emerged. Service composition which integrates the functionalities of different services is a promising technique for developing applications across multiple organizations. However, in a distributed, dynamic and autonomous environment, such as a service composition-based system, the availability and reliability are big concerns in terms of nonfunctional properties. In this paper, we propose a novel service composition method, ANGEL, with the target of the improvement of system availability. We adopt redundant mechanism in ANGEL and propose a model to improve the property of the service availability. We model the multiple services selection problem based on redundant mechanism as a nonlinear mixed integer programming problem and therefore, we propose two heuristic algorithms to select multiple feasible services that have the same functions, but with better availability. In order to maintain the availability of composite services, we further introduce monitor and detection mechanisms. Through the comprehensive experiments, we find that our proposed techniques can indeed achieve better availability as expected."
4253,Presents the welcome message from the conference proceedings.
4254,"The civil aviation system is a global enterprise that includes airframe, engine and component manufacturers, airlines, maintenance organizations, regulatory agencies, airports, air traffic control authorities and millions of service providers that must work together effectively to ensure cargo and passengers get to their destinations as scheduled, while traveling safely and efficiently. The system includes a bewildering array of commercial and custom developed systems for monitoring and controlling the operations of the participant."
4255,"In order to accurately forecast Quality of Service (QoS) of different Web Services, this paper proposes a novel QoS forecasting approach called MulA-LMRBF (Multi-step fore-casting with Advertisement and Levenberg-Marquardt improved Radial Basis Function) based on multivariate time series. Considering the correlation among different QoS attributes, we use phase-space reconstruction to map historical multivariate QoS data into a dynamic system, use Average Dimension (AD) to estimate the embedding dimension and delay time of reconstructed phase space. We also add the short-term QoS advertisement data of service provider to form a more comprehensive data set. Then, RBF (Radial Basis Function) neural network improved by the Levenberg-Marquardt (LM) algorithm is used to update the weight of the neural network dynamically, which improves the forecasting accuracy and realizes the dynamic multiple-step forecasting. The experimental results demonstrate that MulA-LMRBF is better than previous approaches in term of precision and is more suitable for multi-step forecasting."
4256,"The DynaSOAr framework presents a wholly service-oriented approach to grid and Internet-based computing that makes a clear and explicit separation of concerns between service-provision and resource-provision for each service invocation. The separation allows the dynamic deployment of code at runtime, in the form of a service implementation, between a service provider and an explicit resource provider. This paper presents work in progress towards an integrated tripartite security model and framework that enables the security constraints of each engaging party to be expressed, propagated, unified and enforced as part of a DynaSOAr service invocation."
4257,"Web service composition is to integrate existing web services to provide a compound service which satisfies specified requirement. However, traditional web service compositions fail to provide different compound services under various scenarios. In this paper, we propose an approach to compose services with context. A context ontology is defined to describe the scenario for user. An abstract service description is defined to describe current three kinds of services including WSDL/Restful/Web API. The goal is to correlate context and service composition to improve the quality of the compound service from real services."
4258,"Community of Web services (CWS) is a society composed by a number of functionally identical Web services. The communities always aim to increase their reputation level in order to obtain more requests. In this paper, we propose an effective mechanism dealing with reputation assessment for communities of Web services. The proposed mechanism is based on after-service feedbacks provided by the users to a run-time logging system. The proposed method defines the evaluation metrics involved in reputation assessment of a community, and supervises the logging system in order to verify the validity and soundness of the feedbacks provided by the users. In this paper, the proposed framework is described, a theoretical analysis of its assessment and its implementation along with empirical result discussions are provided. We also show how our model is efficient, particularly in very dynamic environments."
4259,"Operation services are reusable and shareable units of configuration code executed by configuration management tools (CMTs), achieving continuous deployment and continuous delivery. With the prevalence of DevOps (Development and Operations), thousands of operation services have been developed for various software systems, and they are publicly available through the online repositories of popular CMTs. However, locating and retrieving desired operation services is challenging since keyword-and tag-based search provided by a repository is with low precision. In this paper, we implement a hierarchical categorization approach based search service, named OSFinder, which searches and locates desired operation services more accurately. OSFinder first constructs a category hierarchy for operation services across multiple repositories, and then it classifies over 13,000 operation services into 90 categories based on machine learning technique, finally it provides a search for users. With OSFinder, a user can narrow down his search scope by tracking the category hierarchy in a top-down way, and then searches in a small group with keywords. The evaluation shows that OSFinder outperforms keyword-and tag-based search."
4260,"This paper discusses a solution based on Web services and service oriented architecture involving interorganizational information systems in the context of multichannel integration in retailing industry. Distribution intensive industries such as retailing have seen a proliferation of distribution channels. This is in part triggered by the intense competition as well as the retailers' drive to reach the customers through all possible channels. As the number of channels for a retailer increases, managing the dynamics of customer behavior in the multichannel environment becomes complex. Customers have an option of interacting with retailers across a number of channels. Acquiring and retaining customers requires that relationship management applications should be able to accommodate the various channels. In this paper, we provide an overview of the benefits of using Web services in multichannel integration and related retail processes. We propose a reference service oriented architecture for usage of Web services involving inter and intra company systems for multichannel integration."
4261,"Most approaches for Web service compositions focus on the technical level as they specify ""how-to"" achieve the composition instead of providing casual users with means to express ""what-to"" achieve their business objectives. This new mindset requires extending Web services with the concept of capability, which describes what a service can do to reduce the gap between Web services and business processes. In this paper, we propose a three-layer Web service capability model to capture objectives, to specify actions that achieve objectives, and establish links between capabilities. Based on this model, the capability matching process does not only discover Web services from high level requirements expressed by Semantics of Business Vocabulary and Business Rules (SBVR), but it also derives constraints on Web services."
4262,"With the advent of Web 2.0 application, and the increasing number of browsers and platforms on which the applications can be executed, cross-browser incompatibilities (XBIs) are becoming a serious problem for organizations to develop web-based software. Although some techniques and tools have been proposed to identify XBIs, they cannot assure the same execution when the application runs across different browsers as only explicit user activity is considered, and thus prone to generating both false positives and false negatives. To address this limitation, this paper describes X-Check, a platform that enables cross-browser testing as a service by leveraging record/replay technique. Comparing to existing techniques and tools, X-Check supports to detect cross-browser issues with high accuracy. It also provides useful support to developers for diagnosis and (eventually) elimination of XBIs. Our empirical evaluation shows that X-Check is effective, improves the state of the art."
4263,"Web service search has been a serious concern for service-oriented software development. The challenge is how to find the right Web services efficiently and effectively for a given development task. There have been many efforts on developing techniques or systems to search services. Though effective in certain scenarios, existing techniques do not form systems for public use; or are based on one query modal--keyword query, which cannot give the matched services accurately. In this paper, we introduce proposed multimodal query search, where users can use keyword and file as query or custom the query. The multimodal query is based on an innovative similarity measure approach, which incorporates both semantic information and structural information of Web services. Our experiments and test cases validate the effectiveness of the approach. Compared with the alternative system Seekda, it is able to obtain much higher search accuracy with keyword query (with a match rate of 2-4 times higher than that of Seekda). The custom search can achieve 100% top-3 match rate, while Seekda fails in most cases using keywords."
4264,"The performance of a service's QoS may evolve relatively frequently with its internal changes or the changes of dynamic Internet environment, especially when some""intentional"" deceptions are taken into consideration. Therefore, the service providers could not always deliver their services according to their ""promised"" quality. In view of this challenge, a history record-based service optimization method, named Hire Some, is investigated in this paper. This method aims at enhancing the credibility of service composition plan, taking advantage of a web service's QoShistory records, rather than using the tentative QoS values advertised by the service provider. At last, a case study and an experiment is presented for validating the method."
4265,"In data-centric environments, for example, in the field of scientific computing, the transmission of large amount of structured data to Web services is required. In service-oriented environments (SOA), the Simple Object Access Protocol (SOAP) is commonly used as the main transport protocol. However, the resulting 'by value' data transmission approach is not efficiently applicable in data-centric environments. One challenging bottleneck of SOAP arises from the XML serialization and deserialization when processing large SOAP messages. In this paper, we present an extended Web service framework which explicitly considers the data aspects of functional Web services. Aside from the possibility to integrate specialized data transfer methods in SOA, this framework allows the efficient and scalable data handling and processing within Web services. In this case, we combine the advantages of the functional perspective (SOA) and the data perspective to efficiently support data-centric environments."
4266,"With the development of deep learning and artificial intelligence, more and more research apply neural networks to natural language processing tasks. However, while the majority of these research take English corpus as the dataset, few studies have been done using Chinese corpus. Meanwhile, Existing Chinese processing algorithms typically regard Chinese word or Chinese character as the basic unit but ignore the deeper information into the Chinese character. In Chinese linguistic, strokes are the basic unit of Chinese character who are similar to letters of the English word. Inspired by the recent success of deep learning at character-level, we delve deeper to Chinese stroke level for Chinese language processing and developed it into service for Chinese text classification. In this paper, we dig the basic feature of the strokes considering the similar Chinese character components and propose a new method to leverage Chinese stroke for learning the continuous representation of Chinese character and develop it into a service for Chinese text classification. We develop a dedicated neural architecture based on the convolutional neural network to effectively learn character embedding and apply it to Chinese word similarity judgment and Chinese text classification. Both experiments results show that the stroke level method is effective for Chinese language processing."
4267,"We propose a new framework for composing Sensor-Cloud services based on dynamic features such as spatio-temporal aspects. To evaluate spatio-temporal Sensor-Cloud services, two new quality attributes are introduced. We present a heuristic algorithm based on A* to compose Sensor-Cloud services in terms of spatio-temporal aspects. In addition, a new spatio-temporal technique based on 3D R-tree to access Sensor-Cloud services is proposed. Analytical and simulation results are presented to show the performance of the proposed approach."
4268,"With the rapid development of Web2.0 and its related technologies, Mashup services (i.e., Web applications created by combining two or more Web APIs) are becoming a hot research topic. The explosion of Mashup services, especially the functionally similar or equivalent services, however, make services discovery more difficult than ever. In this paper, we present an approach to recommend Mashup services to users based on user interest and social network of services. This approach firstly extracts users' interests from their Mashup service usage history and builds a social network based on social relationships information among Mashup services, Web APIs and their tags. The approach then leverages the target user's interest and the social network to perform Mashup service recommendation. Large-scale experiments based on a real-world Mashup service dataset show that our proposed approach can effectively recommend Mashup services to users with excellent performance. Moreover, a Mashup service recommendation prototype system is developed."
4269,"Service oriented computing (SOC) allows resources on a network to be made available as services. For a business service, differentiated services can be provided based on the usage context, i.e., location, age, purpose and user profiles. In differentiated services, service outcomes depend on context. Currently there is no efficient technical solution for supporting differentiated service development. In this paper we present an approach to deliver differentiated services realized by configurable business processes so that service flexibility, manageability and reusability can be achieved."
4270,"QoS-aware service composition intends to integrate services from different providers and maximize the global QoS in order to increase the user's satisfaction degree while subjecting to dynamic context constraints. Current composition approaches only focus on optimizing a single process to maximize the satisfaction degree for one party. When multiple processes are performed concurrently by their selfish users in a dynamic resource-constrained environment, new issues will arise, i.e., undesirable competition for service resources, extra waiting and frequent change of contexts. To address these issues, this paper aims to optimize QoS-aware services composition for multiple selfish users if the communication among users is allowed. Firstly, we propose an extensional QoS-aware service selection model for each process. Then based on this model, we present fault handling mechanisms before and during the execution of concurrent composite services for concurrent processes based on a multi-issue negotiation protocol among agents, and an adaptive context-aware service re-selection mechanism for adjusting the service execution plan for each running composite service in the dynamic resource-constrained environment. Comparative experiments reveal our approach facilitates to increase the average satisfaction degree, reduce the average waiting time of multiple users, and make the satisfaction degrees among multiple users more evenly distributed in the dynamic resource-constrained environment."
4271,"Web services have many important advantages. But their great drawback, the invocation overhead, has not been a research focus. So far, only invocations of simple Web services were considered. But Web service composition may create additional performance problems. Our measurements demonstrate that Web service composition may reduce the maximal load of a system drastically. The reduction quotient increases quasi-exponentially with the number of service compositions. We call that phenomenon ""service congestion"" since it is not due to the combined payload of the composed services. Such detrimental performance effects can not be tolerated in many areas. For that reason, we propose an optimized service composition architecture as a solution. This service component architecture uses service connectors on top of standard Web service middleware. It optimizes automatically the local invocation of services with a Lookup&amp;Service bus. The result is that no service congestion occurs since local service invocations have the cost of local calls"
4272,"The vision of the Semantic Web is to reduce manual discovery and usage of Web resources (documents and services) and to allow software agents to automatically identify these Web resources, integrate them and execute them for achieving the intended goals of the user. Such a composed Web service may be represented as a workflow, called service flow. Current studies of Web services are not sufficient for automatic composition. This paper presents different types of compositional knowledge required for Web service discovery and composition: syntactic, semantic and pragmatic knowledge. As a proof of concept, we have implemented our framework in a cardiovascular domain which requires advanced service discovery and composition across heterogeneous platforms of multiple organizations. Within this framework, we describe (1) How to represent the compositional knowledge, which plays a role in service discovery and composition, in DAML-S; (2) How heterogeneous medical services intemperate in a composed medical service flow. (3) To solve this knowledge level integration, we build on an ontology integration method called SEMIO (Semantic Interoperability)."
4273,"Monitoring resource utilization is an essential task in utility computing (UC). Typically, a UC manager with adaptors is used to orchestrate and collect the monitoring results. However, large IT systems, such as those in data centers are required to monitor dynamic resources running in distributed platforms, thus demanding a complex UC manager structure. This paper introduces a novel approach that utilizes a platform specific aspect-oriented programming (AOP) tool to dynamically weave a monitoring Web service into a running resource to enable communication with a UC manager exposed with standard monitoring Web service interfaces. This AO-Web service approach provides a simple yet powerful and effective means for the dynamic monitoring of distributed resources running in heterogeneous platforms."
4274,"Organizing Web services into functionally similar clusters, is an efficient approach to discovering Web services efficiently. An important aspect of the clustering process is calculating the semantic similarity of Web services. Most current clustering approaches are based on similarity-distance measurement, including keyword, ontology and information-retrieval-based methods. Problems with these approaches include a shortage of high quality ontologies and a loss of semantic information. In addition, there has been little fine-grained improvement in existing approaches to service clustering. In this paper, we present a new approach to grouping Web services into functionally similar clusters by mining Web service documents and generating an ontology via hidden semantic patterns present within the complex terms used in service features to measure similarity. If calculating the similarity using the generated ontology fails, the similarity is calculated by using an information-retrieval-based term-similarity method that adopts term-similarity measuring techniques used by thesaurus and search engines. Another important aspect of high performance in clustering is identifying the most suitable cluster center. To improve the utility of clusters, we propose an approach to identifying the cluster center that combines service similarity with the term frequency-inverse document frequency values of service names. Experimental results show that our clustering approach performs better than existing approaches."
4275,"Nowadays, Web services are widely used because of their interoperability and reusability. Multiple Web services can be composed following some business logic specified by BPEL (Business Process Execution Language) scripts. Since BPEL scripts allow specifying concurrent workflow, typical concurrency problems, such as data race, atomicity violation and order violation, also commonly occur in BPEL scripts. These issues are hard to detect and reproduce due to their non-determinism and the special language features of BPEL. In this paper, we implement a tool to detect data races for WS-BPEL based on static analysis approach and constraints solver. Our system is based on three key concepts: (1) a preprocess model to record necessary information, (2) a thorough Happens-Before model of WS-BPEL concurrency, (3) constraint encoding to transfer Happens-Before relationship to constraints and check if there is a feasible solution (namely data races) by Z3-Str solver. We evaluate the usability and performance of our tool on 10 benchmark programs with effective results."
4276,"Nowadays vast amounts of data are being produced in continuous ways. They may come from sensors, smart meters, application logs, monitoring software etc. The data need to be processed in realtime to gain actionable insights. Services like smart grid load balancing, cloud platform maintenance, can be carried out in an efficient way. Stream processing is the programming paradigm that answers such demand. When talking about stream processing, we can easily recall several famous open-source software frameworks such as Spark Streaming, Samza, Flink and Storm. Although they provide distributed, robust, low-latency stream processing engines, it's still difficult for an end user to set up a usable stream processing application from scratch. Firstly, users are required to write code to define their business related stream processing logic. Secondly, the submission and update of the stream processing logic require service restart, therefore it may lead to service unavailability for minutes. Thirdly, extra operation effort are required for handling scaling and failover issues. In this paper, we present RTA, a released research service on realtime data processing. The RTA service fills the gap between the stream processing requester and the existing software stacks. It offers a SQL-like stream query language for defining stream processing logic definition over streaming data. It allows users easily define their stream processing logic without programming. In RTA service, stream processing logic is also treated as a type of input, which enables online logic update without service downtime. The RTA service also provides scalability, high availability and resource isolation for serving multiple tenants. In this paper, we also provide a comprehensive evaluation of our service through a case study."
4277,"With the increased number of web services advertised on the internet, it is becoming vital to resolve typical problems of service recommendation. Although service recommendation has been studied by researchers in recent years, existing methods have remarkable achievements on offering single service recommendation, not only considering functional features of web services but also non-functional features. However, the customers usually adopted composite services to satisfy complex and coarse-grained requirements. The traditional service recommendation does not have much concern about composite services. Through a long period of usage, the dependencies among composite services are hidden in historical usage records. In reality, these dependencies have great influence on the quality of service recommendation. To improve the effectiveness of service recommendation, this paper proposes a novel service recommendation approach based on service usage patterns. Firstly, the similar customer group of target customer is identified through the personal attribute based clustering and similarity of rating preference, Secondly, service usage patterns of the similar customer group are mined based on the variant of Generalized Sequential Patterns (GSP) algorithm, Thirdly, promising services are recommended for the target customer according to the matching degree between previously used services and service usage patterns, Finally, experimental results verify the efficiency and effectiveness of our approach."
4278,"Today, multimedia system are still widely realized as monolithic systems. But building such applications using Service-Oriented Architectures - especially for the Processing and Delivery of continuous Multimedia data streams - has been a controversial topic since years. As a result, applications in the multimedia domain cannot yet benefit from Web service architectures. Thus, building and maintaining large-scale multimedia applications remains a difficult, costly, time-consuming and challenging problem. In this paper we present our approach for building large scale multimedia systems and compare it with the current state of the art, concentrating on the selection and validation of multimedia service composition."
4279,"Although a data processing system often works as a batch processing system, many enterprises deploy such a system as a service, which we call the service-oriented data processing system. It has been shown that in-memory data processing systems suffer from serious memory pressure. The situation becomes even worse for the service-oriented data processing systems due to various reasons. For example, in a service-oriented system, multiple submitted tasks are launched at the same time and executed in the same context in the resources, compared with the batch processing mode where the tasks are processed one by one. Therefore, the memory pressure will affect all submitted tasks, including the tasks that only incur the light memory pressure when they are run alone. In this paper, we find that the reason why memory pressure arises is because the running tasks produce massive long-living data objects in the limited memory space. Our studies further reveal that the long-living data objects are generated by the API functions that are invoked by the in-memory processing frameworks. Based on these findings, we propose a method to classify the API functions based on the memory usage rate. Further, we design a scheduler called MURS to mitigate the memory pressure. We implement MURS in Spark and conduct the experiments to evaluate the performance of MURS. The results show that when comparing to Spark, MURS can 1) decrease the execution time of the submitted jobs by up to 65.8%, 2) mitigate the memory pressure in the server by decreasing the garbage collection time by up to 81%, and 3) reduce the data spilling, and hence disk I/O, by approximately 90%."
4280,"Web service orchestration is becoming widely spread for the creation of composite Web services using standard specifications such as BPEL4WS. The myriad of specifications and aspects that should be considered in orchestrated Web services are resulting in increasing complexity. This complexity leads to software infrastructures difficult to maintain with interwoven code involving different aspects such as security, fault tolerance, distribution, etc. In this paper, we present ZenFlow a reflective BPEL engine that enables to separate the implementation of different aspects among them and from the implementation of the regular orchestration functionality of the BPEL engine. We illustrate its capabilities and performance exercising the reflective interface through a decentralized orchestration use case."
4281,Provides a listing of current committee members and society officers.
4282,"Websites increasingly embed semantic data for search engine optimization. The most common ontology for semantic data, schema.org, is supported by all major search engines and describes over 500 data types, including calendar events, recipes, products, and TV shows. As of today, users wishing to pass this data to their favorite applications, e.g., their calendars, cookbooks, price comparison applications or even smart devices such as TV receivers, rely on cumbersome and error-prone workarounds such as reentering the data or a series of copy and paste operations. In this paper, we present Semantic Data Mediator (SDM), an approach that allows the easy transfer of semantic data to a multitude of services, ranging from web services to applications installed on different devices. SDM extracts semantic data from the currently displayed web page on the client-side, offers suitable services to the user, and by the press of a button, forwards this data to the desired service while doing all the necessary data conversion and service interface adaptation in between. To realize this, we built a reusable repository of service descriptions, data converters, and service adapters, which can be extended by the crowd. Our approach for linking services to websites relies solely on semantic data and does not require any additional support by either website or service developers. We have fully implemented our approach and present a real-world case study demonstrating its feasibility and usefulness."
4283,"In this paper, we present a REST Chart based approach to design and implement RESTful northbound API in SDN. In our approach, the RESTful API is modeled with Petri Net, and REST constraints, such as hypermedia driven, are enforced naturally. The RESTful structure of the designed northbound API can be checked almost automatically based on the described REST Chart model. The proposed approach has been applied to design and implement the northbound API of SDN in data center networks with OpenStack Neutron. To improve the protocol efficiency, we developed a structured caching mechanism for data network applications with SDN, that reduces the response time overheads by more than 65%, while it fully maintains the desired flexibility and scalability of the RESTful northbound API."
4284,"Many real-world applications are complex, involving many user ""choices"", such as different functionalities, different ways to achieve a goal, etc. Conventional automated service composition models do not consider such potential choices, or simply consider them independently. Also, existing service composition models do not model exceptions and automated composition approaches require that after an exception, the original system goal should still be achieved. This may not be feasible for some exceptions. Thus, the service composition model should also consider alternate goals after exceptions occur. In this paper, we first define the concept of multi-functionality and develop a holistic service composition model. Since most existing composition reasoning techniques can only handle a single functionality, we extend them and develop new algorithms for automated holistic service composition. A case study system is used to illustrate how our approach automatically generates a holistic workflow for a system with multiple functionalities."
4285,"The success of deep neural networks (DNN) in solving general machine vision problems has agitated a wave of its adoption in automated visual inspection solutions. Especially, DNN is able to learn by itself those relevant image features to reach a model that is robust to image quality variation, which promises very scalable solutions. The correlation between image acquisition hardware and image processing software, which is typical in traditional solutions, is alleviated. On this basis, we propose a novel visual inspection service architecture that is scalable, economic and reliable. The realization challenges of the visual inspection service are analyzed and the corresponding designs in model composition and model scheduling are presented. Special focus is placed on the runtime performance of inspection models and the efficient use of the computing resources of contemporary commodity servers."
4286,"The Web Services Choreography Description Language (WS-CDL) is a specification developed by the W3C that can be viewed as a blueprint for the development of end-point services. Considering that it is the W3C candidate recommendation for web service choreography, it is worth providing a systematic approach for its modeling, analysis and verification. The Unified Modeling Language (UML) is the de facto industry standard for modeling. Applying UML to model WS-CDL is obviously a promising solution to bring together academics and practitioners in through a unique standard language. This paper proposes to use different UML diagrams to model WS-CDL. Given the UML specification of WS-CDL, we then provide a systematic way of formally analyzing and verifying WS-CDL."
4287,"Context-aware service is a new service mode that can provide appropriate service automatically to improve service level based on context information. In context-aware service, service is provided based on the current scene of customer. The scene is identified by context, and scene transition is caused by the change of context. Current research focuses on modeling service with context directly. However, the fact is that the vast majority of context change does not cause the change of scene, and monitoring context to determine the scene directly is inefficient, especially of the multi-context application. We use event which is defined by context as the motivation of scene change and propose an event driven model of context-aware service: EDM. A case of smart home service is discussed to show how to use EDM to do the requirement analysis, design, and implementation of context-aware service. Experimental results show that this approach can effectively reduce the number of scene determination in multi-context application."
4289,"A service-oriented application is composed of several web services to provide complex functionality that a single web service cannot provide. A set of services along with their control flows can be frequently used in multiple applications. Such services form a service composition pattern which is well tested in the numerous adoptions. Reusing service composition patterns in service composition provides an efficient way to improve the quality of new applications. To facilitate the documentation of service composition patterns, we propose an approach to automatically recognize service composition patterns from various applications. We identify service composition patterns by locating a set of associated services commonly used by different applications and recovering the control flows among the set of associated services."
4290,"Handwritten Chinese character recognition (HCCR) is an important research field of pattern recognition, which has attracted extensive studies during the past decades. Recently convolutional neural network (CNN) based methods have achieved the state-of-the-art performance for handwritten Chinese character recognition. Nevertheless, handwritten Chinese character recognition is still limited to be effectively used in the actual environment due to the large-scale vocabulary and great diversity of handwriting style. In this paper, we constructed a handwritten Chinese character recognition service based on convolutional neural network, which tries to make effective use of handwritten based printed fonts and existing handwritten database. At the same time, the service can effectively collect more handwritten data to expand the training dataset, which makes it easy to adapt to the new handwriting styles. Meanwhile, We propose a multi-level recognition theory applied to online handwritten Chinese character recognition, which may improve the accuracy of handwritten Chinese character recognition and break the limitations of handwritten Chinese character recognition by identifying the structure of Chinese characters and possible stroke orders firstly. Furthermore, we try to apply the method of online character recognition to the offline character recognition based on the basic writing rules."
4291,"The ability to build new (complex) services by composing existing services is one of the key benefits of the Service Oriented Architecture paradigm. Existing approaches to automate composition requires pre-planning or prediction of the number of required services, making them unsuitable in dynamic composition scenarios. To address this gap, we present a consistency-based service composition approach, where composition problems are modeled in a generative constraint-based formalism. We illustrate how the configuration of service processes differs from established constraint-based configuration techniques and develop an algorithm to synthesis valid service process compositions. We also show that our technique scales well to non-trivial problems."
4292,"When moving from monolithic applications towards service-oriented multimedia frameworks, the composition of Web services to form complex multimedia workflows becomes a demanding problem. Especially mobile devices require a flexible composition strategy as they often have to move computationally complex or power-demanding tasks to powerful servers. Such a strategy also has to consider the changing environment due to movements of the device and it has to adapt to device-specific characteristics, e.g., the current battery level. Hence, mobile devices experience problems beyond the mere question of services' availability or successful execution. We propose the E
<sup>2</sup>
Mon algorithm that monitors the execution chain of Web services and gracefully recovers from failures of individual services and network-specific or device-specific alarms. The sophisticated control flow dynamically chooses the quality-optimal and cost-optimal composition of available services handling both successive and parallel service execution"
4293,"Service-oriented environments facilitate dynamic processes whose properties can be altered during runtime. The transactional support of such processes holds specific requirements that are not completely covered by existing specifications. In this paper, we introduce a life cycle model for transactional dynamic processes and analyze existing specifications with respect to their potentials to support such a model. Subsequently, we propose a framework resolving the weaknesses of existing specifications and allowing comprehensive transactional coordination of dynamic processes."
4294,"A major drawback of using SOAP for application integration is its enormous demand for network bandwidth. Compared to classical approaches like Java-RMI and CORBA, SOAP messages typically cause more than three times more network traffic. In this paper we will give a detailed survey of state of the art binary encoding strategies for SOAP and introduce a new experimental concept for SOAP compression, which makes use of the commonly available WSDL description of a SOAP Web service."
4295,"A Web service is defined as an autonomous unit of application logic that provides either some business functionality or information to other applications through an Internet connection. Web services are based on a set of XML standards such as universal description, discovery and integration (UDDI), Web services description language (WSDL), and simple object access protocol (SOAP). Recently there are increasing demands and discussions about Web services privacy technologies in the industry and research community. In general, privacy policies describe an organization's data practices what information they collect from individuals (e.g., consumers) and what (e.g., purposes) they do with it. To enable privacy protection for Web service consumers across multiple domains and services, the World Wide Web Consortium (W3C) published a document called ""Web services architecture (WSA) requirements"" that defines some specific privacy requirements for Web services as a future research topic. At this moment, there is still no standardized Web services privacy technology. This paper briefly overviews the research issues of Web services privacy technologies."
4296,"Web services gain momentum for developing flexible service-oriented architectures. Quality of service (QoS) issues are not part of the Web service standard stack, although non-functional attributes like performance, dependability or cost and payment play an important role for service discovery, selection, and composition. A lot of research is dedicated to different QoS models, at the same time omitting a way to specify how QoS parameters (esp. the performance related aspects) are assessed, evaluated and constantly monitored. Our contribution in this paper comprises: a) an evaluation approach for QoS attributes of Web services, which works completely service-and provider independent, b) a method to analyze Web service interactions by using our evaluation tool and extract important QoS information without any knowledge about the service implementation. Furthermore, our implementation allows assessing performance specific values (such as latency or service processing time) that usually require access to the server which hosts the service. The result of the evaluation process can be used to enrich existing Web service descriptions with a set of up-to-date QoS attributes, therefore, making it a valuable instrument for Web service selection"
4297,"Recently, the Web Services Interoperability Organization(WS-I) has announced to have completed its interoperability standards work. The latest deliverables include the so-called ""Basic Security Profile"" and the ""Reliable SecureProfile"". This gives rise to the question whether or not Web Services adopters can rely on interoperability of Web Services stacks, in particular in terms of security and reliability features. To answer this question, we thoroughly analyze two important Web Services stacks for interoperability of WS-Security and WS-Reliable Messaging features. Our analysis shows that security and reliability features are far from being implemented in an interoperable manner. Additionally, we reveal that some of those interoperability problems are not even covered by WS-I profiles and therefore conclude that WS-I's work has not yet resulted in Web Services interoperability."
4298,"Outlier detection has been shown to be a promising machine learning technique for a diverse array of felds and problem areas. However, traditional, supervised outlier detection is not well suited for problems such as network intrusion detection, where proper labelled data is scarce. This has created a focus on extending these approaches to be unsupervised, removing the need for explicit labels, but at a cost of poorer performance compared to their supervised counterparts. Recent work has explored ways of making up for this, such as creating ensembles of diverse models, or even diverse learning algorithms, to jointly classify data. While using unsupervised, heterogeneous ensembles of learning algorithms has been proposed as a viable next step for research, the implications of how these ensembles are built and used has not been explored."
4299,"Telecommunication system providers move their IP multimedia subsystems to virtualized services in the cloud. For such systems, dedicated hardware solutions provided a reliability of 99.999% in the past. Although virtualization offers more cost efficient usage of such services, it comes with higher complexity for providing reliable running software components due to the fragile computation stack. In order to hide the impact of such problematic behaviors, automatic mechanisms may help to detect degraded state anomalies in order to execute remediation actions. This work introduces IFTM as a framework for unsupervised anomaly detection in a distributed environment based on real-time monitoring data. The proposed approach consists of two key concepts using an automatic identity function and threshold learning to distinguish between normal and abnormal system behaviors. The evaluation is performed on a testbed running an open source implementation of the IP multimedia subsystem (Clearwater) executed on a replicated Openstack cloud environment. Results show the applicability of IFTM with high detection rates (98%) and low number of false alarms."
4300,"This paper studies the scenario where data in business documents is aggregated by different entities via the use of Web services in streamlined business processes. The documents are transported within the Simple Object Access Protocol (SOAP) messages and travel through multiple intermediary entities, each potentially makes changes to the data in the documents. The WS-security provides integrity protection by allowing portions of a SOAP message to be signed using eXtensible Markup Language (XML) signature scheme. This method however, has not considered the situation where a portion of data may be modified by another entity, therefore a need to allow the originating system to control which intermediary entity is authorized to change which portion of the data. The XML signature scheme also does not provide the final recipient the trust for the intermediary entity that makes the changes. In our paper, we study the security requirements for a streamlined business process, and proposes a novel scheme using sanitizable signature on SOAP messages to complement the XML signature to address not only integrity protection but also control of change as well as establishment of trust for intermediary entities. We show how the proposed scheme can be incorporated into the existing standards and be customizable to achieve flexible use of both the vanilla and sanitizable signatures as required in a business scenario. With the proposed technique, IT systems can be more loosely coupled and reap the benefits of distributed systems, such as delegation of work and encapsulation of business logic."
4301,"This paper analyzes the execution behavior of web services on devices with limited resources. The experiments compare web services in the Axis2 and CXF frameworks analyzing performance and power consumption. To determine which framework is better suited for service provision, a testing environment and a performance and energy evaluation between them are presented. We show that the Raspberry Pi can be useful in service-oriented applications for different types of tasks. Bringing together the best features of small devices and SoC, it is possible to provide diverse, mobile and green applications."
4302,"Online Social Networks (OSNs) have been used to enhance service provision and service selection, where trust is one of the most important factors for the decision making of service consumers. Thus, it is significant to evaluate the trustworthiness of the service providers along the social trust paths from a service consumer to a service provider. However, there are usually many social trust paths between an unknown service consumer and service provider. Thus, a challenging problem is how to effectively and effciently find those social trust paths that can yield trustworthy trust evaluation results based on the requirements of a service consumer particularly in the real-time OSN environments. In this paper, we first present a contextual trust-oriented social network structure and a concept of Quality of Trust (QoT). We then model the multiple social trust paths finding with end-to-end QoT constraints as the Multiple Constrained K Optimal Paths (MCOP-K) selection problem, which is NP-Complete. To deal with this challenging problem, based on the Monte Carlo method and our optimization search strategies, we propose a new efficient and effective approximation algorithm D-MCBA. The results of our experiments conducted on a real-world dataset of OSNs illustrate that D-MCBA can efficiently identify the social trust paths with better quality than our previously proposed MONTE K algorithm that is the most promising algorithm for the social trust path finding in OSNs."
4303,"Today's Web applications and their respective business processes reside under the control of different organizations. Establishing federations between these organizations, i.e. bringing these business processes together by transcending organizational and security borders, raises a new class of security questions concerning the management of trust relationships between the autonomous bodies that wish to work together. Based on the Webcomposition architecture model we provide a modeling approach for federated Web applications. In this paper we present a methodology for formalizing these models using the ambient calculus for use in further computation. Based on the results we help the users to identify and detect security related aspects in Web-based federations."
4304,"Wireless Web services, also called mobile services or M-services, provide access to Web services through wireless networks. In this paper, we propose novel access methods and multi-channel organization for mobile users to effectively access composite M-services in wireless broadcast networks. We define a few semantics for accessing broadcast based M-services and study their impact on access efficiency."
4305,"Aligning different ontologies from similar (or same) domains is an active field of current research. There are various solutions which process and analyze lexical, structural or semantic information to align ontologies. However, there are few solutions that focus on interpreting the concepts that entities are presented with and using them in relation to the semantics implied in an ontology. In this paper, the prototype (OACLAI) is presented to tackle this by combining lexical analysis with consequences from reasoners which reflect the semantics implied in an ontology. We evaluate OACLAI over the four real ontologies and compare it against the seven solutions. The experiments show that the accuracy of OACLAI is higher than those of others on average."
4306,"The design of maintenance mechanisms of distributed hash tables (DHTs) is usually specific to their initial graphs, and thus it is complicated and error-prone. Zhang and Liu propose in [4j the “distributed line graphs” (DLG) mechanism, a universal technique for designing DHTs based on arbitrary regular graphs while preserving the main features of the initial graphs. However, two important properties of DLG, the expandability and fidelity, have not been studied with detailed explanations or analysis. In this paper, we study the above properties of DLG transformations, and prove that (i) the DLG transformations are incrementally expandable, and (ii) DLG transformations from G
<sub>i</sub>
 to G
<sub>i+1</sub>
 keep fidelity."
4307,"QoS has become an important measure for web service selection. In this paper, we present an approach which can provide the approximate QoS value for users, and support finding the optimal web service. Firstly, it clusters the users based on location and network condition, then according to the QoS historical statistics of users in the same cluster, uses the linear regression algorithm to predict the QoS value based on invocation time and workload."
4308,"Web service directories are shared resources that have to accommodate a high number of concurrent read requests, whereas updates are relatively infrequent. To allow for the automatic composition of complex web services based on those contained in a directory, read requests may involve a series of queries which require a consistent view of the data. We have developed an efficient web service directory that is based on the Multiversion Generalised Search Tree (MVGiST), an integration of a multidimensional index structure with multiversion concurrency control. The MVGiST is able to index web services according to their input and output parameters, supports a high level of concurrent read requests, and guarantees consistency across multiple subsequent read queries. In this paper we evaluate the performance and scalability of the MVGiST and compare it with a traditional, locking-based concurrency control mechanism."
4309,"Regression testing (RT), testing software with previously used test cases, is a mainstream practice in software maintenance. Regression test selection (RTS) is to reduce the number of tests which need to be retested. Safe RTS techniques add the assurance that no modification-revealing test case will be left unselected. Several effective safe RTS techniques were developed for traditional applications, but none of them can be directly applied to Web services, even though there have been RT tools and techniques for Web services test-case generation, and ranking competing services. We have developed an approach to adapt Rothermel and Harrold's safe RTS technique to Web services. This approach was designed to be automated. In doing so, we have recognized a set of challenging issues that arise as a result of multiple concurrent modifications in distributed, autonomous, but still interconnected services. We believe not only these issues are common to any automated RTS approach, the needs for the solutions to these issues will also become more and more keen as composite Web services are getting more and more ubiquitous."
4310,Recent advances in Web services have made it practical to provide communication Web services to enable communication through SOA and package communication capability as services. This paper provides an appropriate implementation to deliver the multimedia conferencing communication components as Web services in order to be used simply by Web service clients in converged applications.
4311,"During the design procedure of BPEL (Business Process Execution Language) processes, there may be control dependency deadlocks among the activities in BPEL. We design a novel Agree/Refuse Matrix (ARM) to real-timely detect control dependency deadlocks. Online detection approach can be integrated into Active BPEL Designer to extend their functions and improve the accuracy of BPEL processes. Most importantly, this method's real-time property can help avoiding detecting the whole process from scratch caused by process's partial modifications."
4312,Today's mobile users have access to a wide range of Web-based services. This paper presents our m-Tableaux algorithm for enabling cost-efficient and optimised semantic reasoning to support pervasive service discovery. We present performance evaluation of the m-Tableaux optimisation strategies which clearly demonstrate its operational feasibility on a mobile device.
4313,"Web services provide a standard means of interoperating between different software applications, running on a variety of platforms and/or frameworks. While the concepts of Web services are aimed at providing a standard means to support interoperable machine-to-machine interaction over a network, they do not solve the problem of trust between service requesters and providers. A trusted component is defined as a reusable software element possessing specified and guaranteed property qualities. The highly reusable nature of a Web service emphasizes the need for a ""trust ensuring"" mechanism between the requester and the provider of the service. The focus of this paper is to suggest a fortified Web services architecture introducing the concept of contracts to increase the level of trust between the requester and the provider of the requested service. In order to achieve this goal a new language is introduced into the fortified Web services architecture: WS-Contract. WS-Contract is a machine-processable specification of the Web service semantics, formally supporting the different levels of contract information. It defines the pre-and post conditions of the Web services interface, the synchronization policy of the service and the QoS parameters that should be maintained between the requester agent and the provider agent. WS-Contract relies, wherever possible, on existing Web services standards to build the necessary ""trust ensuring"" mechanism for Web services"
4314,"With the number of smartphone applications (apps) growing explosively, it has a practical significance to provide personalized app recommendations. In this paper, we propose the GTRM, a new recommendation model which builds the top-N app list by optimizing the metric Group-oriented Mean Average Precision (GMAP). GMAP is an extension of the traditional metric Mean Average Precision (MAP) and it measures the precisions of top-N list in terms of the collective positions of related items rather than the position of individual item. Therefore, GTRM can recommend a more reasonable top-N app list by avoiding overfitting problem. The details of GMAP and GTRM are described. Extensive experiments on a real-world app dataset demonstrate the effectiveness of GTRM, and show that GTRM significantly outperforms the compared methods."
4315,"In this paper we propose a meta-model for nonfunctional property descriptions targeted to support the selection of Web Services. The approach is based on the explicit distinction between NFP offered by providers and requested by users, on the concept of policy that aggregates NFP descriptions into single entities with an applicability condition, and finally on a set of constraint operators, which is particularly relevant for NFP requests. The semantic meta-model embracing the above perspective is defined by a BNF syntax whose semantics is formalized by an ontology. The ontology has been formalized in OWL-DL and WSML to provide for logical syntax. The logic upon which the meta-model supports NFP-based selection is discussed in the paper."
4317,"In recent years, the issues in web service security have been widely investigated and various security standards have been proposed. But most of these studies and standards focus on the access control policies for individual web services and do not consider the access issues in composed services. Consider a simple service chain where service s
<sub>1</sub>
 accesses s
<sub>2</sub>
, and s
<sub>2</sub>
, in turn, accesses service s
<sub>3</sub>
. The information returned from s
<sub>3</sub>
 to s
<sub>2</sub>
 may be used to compute some results that are further returned to s
<sub>1</sub>
. The current web service security framework does not provide any mechanisms to control such an information flow, and hence, sensitive information may be leaked to s
<sub>1</sub>
 without the consensus of s
<sub>3</sub>
. In this paper, we propose an enhanced security model to facilitate the control of information flow through service chains. It extends the basic security models by introducing the concepts of delegation and pass-on. Based on these concepts, new certificates, certificate chain, delegation and pass-on policies, and how they are used to control the information flow are discussed."
4318,"With the increasing demand for express delivery, a courier needs to deliver many tasks in one day and it's necessary to deliver punctually as the customers expect. At the same time, they want to schedule the delivery tasks to minimize the total time of a courier's one-day delivery, considering the total travel time. However, most of scheduling researches on express delivery focus on inter-city transportation, and they are not suitable for the express delivery to customers in the “last mile”. To solve the issue above, this paper proposes a personalized service for scheduling express delivery, which not only satisfies all the customers' appointment time but also makes the total time minimized. In this service, personalized and accurate travel time estimation is important to guarantee delivery punctuality when delivering shipments. Therefore, the personalized scheduling service is designed to consist of two basic services: (1) personalized travel time estimation service for any path in express delivery using courier trajectories, (2) an express delivery scheduling service considering multiple factors, including customers' appointments, one-day delivery costs, etc., which is based on the accurate travel time estimation provided by the first service. We evaluate our proposed service based on extensive experiments, using GPS trajectories generated by more than 1000 couriers over a period of two months in Beijing. The results demonstrate the effectiveness and efficiency of our method."
4319,"Emerging grid applications desire not only high bandwidth but also the ability to control the topology and traffic engineering of the underlying networks, through Web service interfaces. To achieve that goal, we present an advanced user controlled lightpath provisioning (UCLP) system, where network resources and grid resources are both modeled as Web services and are seamlessly integrated into workflows."
4320,"In this paper we present a SOAP extension for protecting the privacy of users of a Web service. This extension allows a user to prove to a remote SOAP server to be member of a trusted group without revealing his/her identity. Our extension has been designed as to ensure interoperability among SOAP applications written in different programming languages. We developed also some implementations of our extension using different programming languages. Moreover, we conducted an extensive experimentation of our implementation to prove its feasibility in a real-world context. In sums, our work suggests that privacy can be added to Web services with very little impact on the application developer and without compromising the performance of Web services."
4321,"User interface (UI) design is an integral part of the software design process. The UI design not only outlines the look and feel of the system, but also helps in flushing out the requirements - by identifying what data is visible to and processed by different users. However, in any SOA methodology, UI design is typically considered out of scope. In this paper, we highlight the importance of UI design specification in the SOA landscape, from a service- identification perspective. Service identification, which is a key activity in any SOA-based development, involves specification of business requirements as a set of granular service definitions. We propose an approach for harvesting the UI design specification to define service requirements for the intended system; more specifically in terms of information and business service requirements. Our approach consists of the following steps: (1) capture user interface design in a format amenable to automated analysis, with appropriate references to data and process models, (2) identify requirements for information services from data that is displayed in the user interface, and (3) identify business service requirements from the UI navigation flow and the links between the UI and the business process model. To illustrate our approach, we present a case study using the Amazon associate Web services. The study demonstrates how the use of UI designs can lead to better service identification. The proposed approach can complement any existing SOA methodology that follows a top-down approach to identify services."
4322,"Many web service providers use commercial cloud computing infrastructures like Amazon for flexible and reliable service deployment. For these web service providers, the cost of cloud computing usage becomes a big part of their IT department cost. Facing the diverse pricing models including on-demand, reserved, and spot instance, it is difficult for web service providers to optimize their cost. This paper introduces a new cloud brokerage service to help web service providers to minimize their cloud computing cost for deadline-constrained batch jobs, which have been a significant workload in web services. Our cloud brokerage service associates each batch job with deadline, and always tries to use cheaper reserved instances for computation to maintain a minimum cost. We achieve this with the following two steps: (1) given a set of jobs' specifications, determine the scheduling of jobs, (2) given the scheduling and pricing options, find an optimal instance renting strategy. We prove that both problems in two steps are computation intractable, and propose approximation algorithms for them. Trace-based evaluation shows that our cloud brokerage service can reduce up to 57% of the cloud computing cost."
4323,"Web service provides a means to integrate functional components and for business IT solutions in an enterprise or cross-enterprise environment. Enterprise resources are consumed by Web services to provide desired capabilities expected by the users. However, beyond service provisioning, an enterprise needs to manage their Web services and related resources in an optimal manner in order to maximize the resource usage. This paper presents a framework of Web service management aimed for solving the resource allocation issues based on system dynamics models. Our approach combines the business operational consideration and IT infrastructure configuration. Dependency that is derived from impact analysis of different perspectives is an essential part of the model and can be built into control system. The resultant system embodies the capability of autonomic Web service management to certain degree if not fully. The reference architecture of this framework is accounted in this paper. We also demonstrate dynamical behavior of the system through simulation."
4324,"Interoperability promised by Web service makes it a most promising technology for the development of next generation distributed heterogeneous software systems. Services should be compliant at signature, behavioral and semantic level to make the interoperation successful and correct. Service adaptation provides an effective approach to bridge the incompatibility of services to make them interoperate as well as possible. In this paper, we aim to contribute to the definition of a methodology to develop adaptors that are capable of making two incompatible services interoperate not only successfully but also correctly at semantic level. To achieve this goal, we proposed service specifications for both atomic and composite services with semantic dependency between outputs and inputs specified; then we proposed adaptor specification consisting of three parts, which are message mapping, action mapping and treatment for non-mapping messages. Based on service and adaptor specifications, an incremental derivation approach of a concrete adaptor is given."
4325,"Business Process driven Service Oriented Architecture (SOA) allows for designing services that execute (or realize) atomic tasks of business processes. When developing and designing SOA applications using Model Driven Development (MDD), business processes and services are represented using specifications such as Unified Modeling Language (UML). UML based models help in specifying structural properties of services and the behavioral properties of a service composition, executing a business process. In a typical design scenario, architects develop process models (comprising of tasks) and service design independently and establish associations between tasks and the designed services. Service compositions representing business processes are verified with the designed services. Verification at design time is a manual activity and relies on the static information captured in these UML models. In this paper we describe a model-based approach that enables executing service compositions at design time. The process model and service model is transformed to an executable UML model and UML Action language (UAL) code fragments are automatically generated. UAL enables unambiguous specification of the behavior of service operations and their compositions. The generated executable model enables a precise behavior analysis of the process realization. We demonstrate the use of this approach by reporting on a reference model with 17 business processes and 127 business tasks realized using 13 Service interfaces."
4326,"Service composition based on state dependent services is a challenge if it is done in a decentralized way that is without a centralized coordinating partner knowing all involved parties. In particular, the challenge is the combination of services to a composite service, such that every party involved considers its view of the composite service to be acceptable. In the paper a protocol is proposed which incrementally derives proposals for composite services. These proposals are then evaluated by the involved parties locally and a consensus on the local decisions on accepting or rejecting the proposed composite service is derived."
4327,"Complex services are composed of simple services which typically need to be processed in a particular order. Two complex services only match if they agree on both, their simple services and their processing order. This matching semantics can be formalized by means of modelling complex services as finite state automata (FSAs), and analysing the intersection of the FSAs. However, computing the intersection of FSAs is computationally expensive, and thus, does not scale for large service repositories. This paper presents an approach for indexing and matching complex services using an abstraction that transforms the underlying FSA via its grammar into a form that can be indexed using available index mechanisms. Evaluation of this approach shows a performance gain of several orders of magnitude as compared to sequential matching."
4328,"Web service filtering is an efficient approach to address some big challenges in service computing, such as discovery, clustering and recommendation. The key operation of the filtering process is measuring the similarity of services. Several methods are used in current similarity calculation approaches such as string-based, corpus-based, knowledge-based and hybrid methods. These approaches do not consider domain-specific contexts in measuring similarity because they have failed to capture the semantic similarity of Web services in a given domain and this has affected their filtering performance. In this paper, we propose a context-aware similarity method that uses a support vector machine and a domain dataset from a context-specific search engine query. Our filtering approach uses a spherical associated keyword space algorithm that projects filtering results from a three-dimensional sphere to a two-dimensional (2D) spherical surface for 2D visualization. Experimental results show that our filtering approach works efficiently."
4329,"Service chaining, the act of stringing a sequence of services together to form a new service, is a key element of Web services. However, for Web services to reach its full potential the issue of testing service-chaining at the network level must be resolved. How can one map the microlevel service-service interactions to the macrolevel system performance? For instance, as service chains become longer and more complex, how do they affect end-user quality of service? Focusing on aggregate service chains $situations in which the user invokes a service that carries out the chain, without the user being aware of the individual services - we tackle these questions using a Java simulation tool to model service chaining, visualize network traffic and quantify service chain complexity. We demonstrate that one can orchestrate very complex service chains in a simple distributed manner and quantify how service chain complexity affects end-user quality of service and network loading."
4330,"This paper presents an event based functionality integration framework to approach the issue of service personalization and service mashups. In contrast to existing data integration approaches, the proposed framework addresses the mashup issue from a new perspective by extracting and reasoning the context through user generated event, while recommending and aggregating the contextual services dynamically in response to the user's functional requirements. An event hierarchy is proposed to retrieve contextual information and analyze underlying functionalities. The three layer system framework, service recommendation logic, and the functionality integration are also presented."
4331,"Sensors are pervasively deployed on mobile devices with the development of Internet of Things technology. Value-added services are innovated and developed by analyzing data streams from massive number of mobile sensors in online mode. Due to dynamic working condition of mobile sensors and the high data rate, back end analytic services confront incoming streams with large rate fluctuation and out-of-order time series. This puts forward special challenges in service implementation for commercial applications, where good reliability/scalability performance is a must. In this paper, a data ingestion and scheduling framework is proposed to enable large-scale tempo-spatial streams analysis in a reliable and cost-effective way. A case study on a real world application adopting this framework is introduced and its pilot result is presented."
4332,"Advances in server, network, and storage virtualization are enabling the creation of resource pools of servers that permit multiple application workloads to share each server in the pool. This paper proposes and evaluates aspects of a capacity management process for automating the efficient use of such pools when hosting large numbers of services. We use a trace based approach to capacity management that relies on i) a definition for required capacity, ii) the characterization of workload demand patterns, iii) the generation of synthetic workloads that predict future demands based on the patterns, and iv) a workload placement recommendation service. A case study with 6 months of data representing the resource usage of 139 workloads in an enterprise data center demonstrates the effectiveness of the proposed capacity management process. Our results show that when consolidating to 8 processor systems, we predicted future per-server required capacity to within one processor 95% of the time. The approach enabled a 35% reduction in processor usage as compared to today's current best practice for workload placement."
4333,"WS-Eventing is a W3C specification to enable publish-subscribe Web Services. This paper proposes a WS-Eventing extension for disseminating notifications by using a UDP multicast binding. To achieve this, some specification modifications will be done which do not affect legacy client implementations. We show that it is possible to extend WS-Eventing almost without losing backward compatibility. An exemplary proxy application illustrates that the extension can be embedded into open source and proprietary Web Service frameworks."
4334,"This article proposes a unified methodology for designing asynchronous SOA (Service-Oriented Architecture) based on the asynchronous messaging models and patterns. Conventional SOA focuses on synchronous messaging. Although asynchronous messaging provides much efficient and productive way to coordinate services, design of aSOA (asynchronous SOA) is far more complicated due to the variety of messaging and architecture while assuring behavioral consistency of architecture. This paper proposes a model-driven design methodology for aSOA. The methodology is based on aMEPs (Asynchronous Message Exchange Patterns) identified by classifying the messaging in terms of behavioral concerns. Based on the meta-model of aSOA, a set of aSOA patterns is generated by composing aMEPs. Then, an aSOA pattern is selected and transformed to a platform specific aSOA on top of Web services standards. We successfully implemented an aSOA on Apache Axis, which enables to asynchronous messaging of SOAP over SMTP. We demonstrated that conventional methods are subsets of the proposed methodology, which is the major contribution of this work."
4335,"In this paper, we propose a novel way of modeling Web services using semantic graph transformations. Each operation supported by a Web service is associated with a semantic annotation that describes the input and output messages using RDF graph patterns. The terms used in these patterns are defined in OWL ontologies that describe the application domain. A key difference between our model and existing semantic Web service models like OWLS is that it describes the inputs and outputs in terms of instance-based graph patterns, rather than in terms of concepts. This allows associating a rich set of constraints on the input and output data in terms of relations between instances. We also propose a composition model for Web service operations, that describes the conditions for composing services into workflows. The composition model includes the notion of semantic propagation, i.e. the semantic description of the output message of an operation depends on the semantics of the input message. We have developed a planner that uses this model to compose services, automatically. The planner uses DLP reasoning to aid plan search. We present performance results for the planner."
4336,"The growing synergy between Web services and grid-based technologies is enabling profound, dynamic interactions between applications dispersed in geographic, institutional, and conceptual space. Such deep interoperability requires the simplicity, robustness, and extensibility for which XML has been conceived, making it a natural lingua franca for the network. Along with these advantages, there is a degree of inefficiency that may limit the applicability of XML. Firstly, we investigate the limitations of XML for high-performance and high-interactive distributed computing. Our experimental results clearly show that focusing on parsers, that are routinely used to desterilize XML messages exchanged in these system, we can improve the performance of a generic end to end Web services based solution. Secondly we present a new parser, the cache parser, which uses a cache to reduce the parsing time sender and receiver side, by reusing information related to previously parsed documents/messages similar to the one under examination. Finally, we show how our fast parser can improve the global throughput of any application based on Web or grid services, or also JAXP-RPC. Experimental results demonstrate that our algorithm is 25 times faster than the fastest algorithm in the market and, if used in a WS scenario, can dramatically increase the number of requests per second handled by a server (up to 150% of improvement) bringing it close to a system that does not use XML at all"
4337,"Making your enterprise data and traditional Web services consumable through the World Wide Web is becoming increasingly important for a organization. REST (Representational State Transfer), a collection architecture principle, plays a key role in making your services easily integrated in both your enterprise applications and others via Web. This tutorial will teach you the basic of REST and the related application architecture, the Atom Publishing Protocol (APP) and how it stitches each resource in the service architecture, the best practices in constructing the REST services, and how to utilize and integrate other REST services with your existing Web service and application. Furthermore, it will also show you how to effectively use and consume the REST services from the IBM social software (Lotus Connections) in order to build your own social Web applications. You will not only learn the industrial products and trends in utilizing and making the consumable services using REST and APP, but also learn the details on how to make the most out of your own consumable services."
4338,"With a growing number of web services, discovering services that can match with a user's query becomes a challenging task. It's very tedious for a service consumer to select the appropriate one according to her/his needs. In this paper, we propose a non-logic-based matchmaking approach that uses the Correlated Topic Model (CTM) to extract topic from semantic service descriptions and model the correlation between the extracted topics. Based on the topic correlation, service descriptions can be grouped into hierarchical clusters. In our approach, we use the Formal Concept Analysis (FCA) formalism to organize the constructed hierarchical clusters into concept lattices according to their topics. Thus, service discovery may be achieved more easily using the concept lattice. In our approach, topic models are used as efficient dimension reduction techniques, which are able to capture semantic relationships between word-topic and topic-service interpreted in terms of probability distributions. In our experiment, we compared the accuracy of the our hierarchical clustering algorithm with that of a classical hierarchical agglomerative clustering. The comparisons of Precision@n and Normalised Discounted Cumulative Gain (NDCGn) values for our approach, Apache lucene and SAWSDL-MX2 Matchmaker indicate that the method based on CTM presented in this paper outperform all the others matchmakers in terms of ranking of the most relevant services."
4339,"Improving service quality has long been recognized as a key business strategy in consumer market. However, little is written in the literature about service among business organizations. The shortage of knowledge on business service (BS) poses a significant challenge in tracking and improving BS quality in today's e-commerce environment. To address this, this paper takes a first step to identify main BS dimensions in an e-commerce environment. It first provides a review on BS. Then the BS gap model and main BS dimension are identified. The paper concludes with a summary and future research suggestions."
4340,"Advances in mobile Internet technology have enabled the clients of Web services to be able to adjust to context changes, which need to rely on monitoring to QoS of Web services. Most contemporary QoS prediction methods exploit the QoS characteristics for one specific dimension, e.g., time or location, and do not exploit the complicated relations among multi-dimension of context. This paper proposes a learning approach to quality-of-service (QoS) prediction of web services via multi-dimensional context derived from the past invocation history. To validate our approach, large-scale experiments are conducted based on a real-world Web service dataset, WSDream. The results show that our proposed approach achieves higher prediction accuracy than other approaches."
4341,"Soft resources, which are system software components that use hardware or synchronize the use of hardware, are playing a critical role in the performance of multi-tier web systems, and thus it is quite important to tune the soft resource allocation for using the limited hardware resources to obtain maximum effectiveness. In this paper, we integrate both theoretical and experimental studies to the soft resource allocation problem. Specifically, we apply the queueing network model for formulating multi-tier web systems, and conduct experimental measurements based on the RUBiS benchmark system to obtain precise model parameters. Quantitative analysis is carried out, based on which an optimization model as well as an algorithm are put forward for soft resource allocation. The efficacy of our approach is validated by both theoretical analyses and experimental results."
4342,"This paper presents our design and development of a context-driven content adaptation planner, which dynamically transforms requested Web content into a proper format conforming to receiving contexts (e.g., access condition, network connection, and receiving device). Aiming to establish a semantic foundation for content adaptation, we apply description logics (DLs) to formally define context profiles and requirements and automate content adaptation decision. In addition, the computational overhead caused by content adaptation can be moderately decreased through the reduction of the size of adapted content."
4343,Service optimization and energy conservation requires a thorough understanding of the performance impact of different hardware configurations. In this paper we focus on the configuration of memory and investigate the impact of memory dynamic voltage and frequency scaling (DVFS) on the performance of services/applications. We propose a quantitative metric called frequency sensitivity (FS) and study memory FS of various benchmarks. Our experiments yield several insights for memory DVFS based performance tuning.
4344,"Data-flow requirements is an important aspect of service composition. Although several approaches have been proposed to specify data-flow requirements, they cannot be efficiently exploited in dynamic setting, for example, where the composition participants (i.e., component services) need to be dynamically replaced. In this paper, we propose a new modeling methodology for data-flow requirements, in which we explicitly distinguish a data model of a composite service from data models of component services. We show that following our methodology, a composition can be adjusted to various dynamic changes with significantly less effort. We implement our solution using planning techniques and make its basic evaluation on a scenario from the travel domain."
4345,"Ticketing system is an example of a Service System (SS) which is responsible for handling huge volumes of tickets generated by large enterprise IT (Information Technology) infrastructure components, and ensuring smooth operation. An issue is captured as summary on the ticket and once a ticket is resolved, the solution is also noted down on the ticket as resolution. Further the system maintains the provision of recording the time when a ticket is opened, acknowledged to user, resolved and/or closed, from which different QoS parameters could be obtained. For example, Resolution Time can be computed as the difference of resolution date and opening date of the ticket. QoS parameters are used to measure the performance of different aspects of a service. In case of impreciseness of observations of these parameters fuzzy sets seems to be an optimal tool to model them. To ensure better operation for services based on these QoS values we propose a two-stage analysis framework for QoS prediction of incoming tickets which includes fuzzy clustering of incident tickets based on QoS values and building a fuzzy regression model using this categorization and the textual contents of tickets. Further we carry out a fuzzy correlation analysis of different categories (clusters) of QoS parameters. Lastly we report on our experimental results."
4346,"In systems that require several services to collaborate, specifying coordination protocols is vital, but costly. Additionally, several properties, which are derived from laws, regulations, requirements, etc., must be satisfied. Coordination protocol composition approaches construct specific protocols in a cost effective manner in accordance with the composition intentions. However, existing composition approaches are insufficient in terms of satisfying properties. Existing approaches use concrete specifications to identify composition intentions and do not consider interference between compositions. Herein we propose a new composition approach in which a developer directly expresses his or her intentions as constraints via metadata, and then the system searches for optimal composition methods based on the constraints."
4347,"In this paper we address the problem of qualitative and quantitative analysis of timing aspects of Web service compositions defined as a set of BPEL4WS processes. We introduce a formalism, called Web service timed state transition systems (WSTTS), to capture the timed behavior of the composite Web services. We also exploit an interval temporal logic to express complex timed assumptions and requirements on the system's behavior. Building on top of this formalization, we provide techniques and tools for model-checking BPEL4WS compositions against time-related requirements. We also present a symbolic algorithm that can be used to compute duration bounds of behavioral intervals that satisfy such requirements. We perform a preliminary experimental evaluation of our approach and tools with the help of an e-Government case study"
4348,"In a recommender system, items can be rated across multiple fields by users with varying degrees of familiarity. Hence, the ratings in a recommender system should have different recommended weights. Ratings in fields where in the user has high or low familiarity should be given high or low recommended weights, respectively. However, current recommendation algorithms ignore this problem and use the ratings indiscriminately, thus affecting the accuracy of the recommendation system. In this paper, we provide a focused study of user-familiarity degree-aware recommendation and develop a user-familiarity degree-aware latent factor model for recommendations that considers both user familiarity and item features reflected by the tagging information. We also design a user-familiarity degree-aware probability matrix factorization model, which computes the degree of familiarity of a user with the items he/she has rated. By using the user-familiarity degree, different recommended weights are given to every rating to obtain precise recommendations. The experiment results on real-world datasets show that our algorithm significantly outperforms state-of-the-art latent factor models and effectively improves the accuracy of the recommendation results."
4349,"Data Mashup is a special class of mashup application that combines information on the fly from multiple data sources to respond to transient business needs. Data mashup is a difficult task that would require an important programming skill on the side of mashups' creators, and involves handling many challenging privacy and security concerns raised by data providers. This situation prevents non-expert users from mashing up data at large. In this paper, we present a declarative approach for mashing-up data. The approach allows data mashup creators to build data mashups without any programming involved. The approach builds the mashups automatically and takes into account the data's privacy concerns. We evaluate the efficiency of the approach via a thorough set of experiments. The results show that handling data privacy introduces only a negligible cost in the mashup building time."
4350,Transactions are a fundamental technology for building efficient and reliable web service based applications. Various models and protocols have been developed by academic and industrial research community in order to effectively manage web services transactions. We propose a novel abstract model for dynamically modeling distinct web services transaction protocols. Model-based testing techniques can be used on the abstract model in order to automatically generate test scenarios.
4351,"Web services are gaining momentum as a major vehicle to deliver business functionalities on the Web. More and more business organizations have begun to use Web services to facilitate user interactions and the collaboration among themselves. This essentially forms a large service space, which still keeps growing. Meanwhile, there may be functionality overlaps among different service providers. The concept of Quality of Web Service (QoWS) is emerging as a key feature in distinguishing between competing service providers. We present in this paper a systematic approach for efficient service selection by using QoWS as the major criterion. In particular, we adopt a relational approach that enables to store QoWS information in a relational DBMS and leverage standard relational operators for efficient service selection. We perform a preliminary set of experiments to evaluate the proposed service selection algorithms."
4352,"The ability to dynamically discover and invoke a Web service is a critical aspect of service oriented architectures. An important component of the discovery process is the matchmaking algorithm itself. In order to overcome the limitations of a syntax-based search, matchmaking algorithms based on semantic techniques have been proposed. Most of them are based on an algorithm originally proposed by M. Paolucci, et al. [19]. In this paper, we analyze this original algorithm and identify some correctness issues with it. We illustrate how these issues are an outcome of the greedy approach adopted by the algorithm. We propose a more exhaustive matchmaking algorithm, based on the concept of matching bipartite graphs, to overcome the problems faced with the original algorithm. We analyze the complexity of both the algorithms and present performance results based on our implementation of both these algorithms. We show that the complexity of our algorithm is equivalent to that of the original algorithm in spite of the improvements we have made to address the correctness issues."
4353,"Trust and reputation for web services emerges as an important research issue in web service selection. Current web service trust models either do not integrate different important sources of trust (subjective and objective for example), or do not focus on satisfying different user's requirements about different quality of service (QoS) attributes such as performance, availability etc. In this paper, we propose a Bayesian network trust and reputation model for web services that can overcome such limitations by considering several factors when assessing web services' trust: direct opinion from the truster, user rating (subjective view) and QoS monitoring information (objective view). Our comprehensive approach also addresses the problems of users' preferences and multiple QoS-based trust by specifying different conditions for the Bayesian network and targets at building a reasonable credibility model for the raters of web services."
4354,"Distributed software systems are the basis for innovative applications. The key for achieving survivable and maintainable distributed systems is agility because the non-deterministic nature of distribution would otherwise leave the system uncontrollable, especially in emerging mobile ad-hoc networks. A mobile ad-hoc network (MANET) is based on a self-organizing and rapidly deployed network of mobile services to collaborate without using any pre-existing fixed network infrastructure. Survivability is defined as the capability of a service to fulfil its mission in a timely manner, even in the presence of attacks, failures, or accidents. There are four key survivability properties: resistance, recognition, recovery and adaptation. Recovery, a hallmark of survivability, is the capability to maintain critical components and resource during attack, limit the extent of damage, and restore full services following attack. Exception handling is a way to deal with the recovery aspect of survivability. Resistance can be viewed as the process of limiting access to critical and vulnerable resources only to authorized users, programs, processes, or other systems. This paper bridges the analysis of secure business process and its recovery aspect in terms of exception handling in the context of access control requirements. We propose an integrated approach to engineer a survivable distributed system through dynamic regeneration of workflow specifications in the context of Business Process Execution Language for Web Services (BPEL) and eXtensible Access Control Markup Language (XACML)"
4355,"In the era of big data, data intensive applications have posed new challenges to the filed of service composition, i.e. composition efficiency and scalability. How to compose massive and evolving services in such dynamic scenarios is a vital problem demanding prompt solutions. As a consequence, we propose a new model for large-scale adaptive service composition in this paper. This model integrates the knowledge of reinforcement learning aiming at the problem of adaptability in a highly-dynamic environment and game theory used to coordinate agents' behavior for a common task. In particular, a multi-agent Q-learning algorithm for service composition based on this model is also proposed. The experimental results demonstrate the effectiveness and efficiency of our approach, and show a better performance compared with the single-agent Q-learning method."
4356,"We present a technique called Dynamic Distributed Service Coordination Protocol (DDSCP) that enables dynamic and distributed coordination for composed services and applications in telecommunication networks. Individual service components are modeled as Web services and DDSCP facilitates coordination among these components by dispatching executable processes to the service components that specify different steps that the service component must follow in response to (receipt of) specific messages and events. The collective and concurrent execution of these processes at different service components achieves overall goals of the service. The planning and creation of these processes is not our focus in this paper. We describe the structure and processing of different messages DDSCP, and describe how this protocol can work. Our model has several advantages over the existing service platforms for 3rd Generation Mobile Networks, such as Parlay/OSA, and the Web-service composition models. These advantages include introduction of flexibility among network components at finest level, ease of creation of highly customized services, easy integration with foreign components, reduced application complexity, increased reuse of application components, and possibility of increased user participation in managing her services, and thus reducing load on the network."
4357,"Dependability is one of the most important challenges for service-oriented architectures if their success shall continue in critical settings such as air traffic control or finance and banking. Replication of services and the underlying resources is one of the primary fault tolerance techniques for achieving dependability. While replication is well known in traditional fields (e.g. databases), it is rather in its infancy in service-oriented environments. Thus, in order to reduce the dependability gap we are currently facing in service-oriented environments, we contribute with a replication middleware for Web services which is built upon the Java-based Axis2 SOAP engine and provides a variant of primary-backup replication. Performance evaluations of our middleware implementation show the relatively low overhead of replication if the number of replicas is small."
4358,"Due to differences in consumer requirements, a Web service usually has multiple service variants for use in different business contexts. In such situations, delivering customizable services helps increase efficiency not only in service description and publication but also in service consumption. However, existing approaches for providing customizable services enforce the tight coupling between providers and consumers. Nor do they take into account recursive nature of service customization. Consequently, the approaches hamper the widespread use of customizable services in SOA. In this paper, we propose a language, namely Web Service Variability Description Language (WSVL), which formalizes the customization interface between providers and consumers using the XML technology to address these problems. We also describe a reference architecture for service deployment and a service engineering technique which together support the provisioning of WSVL-based customizable services. A proof-of-concept prototype system is introduced to demonstrate the feasibility of our approach."
4359,"With the rapid growth of the web services technologies, users often leverage various web services to perform their daily activities, such as on-line shopping. Due to the massive amount of web services available, a user faces numerous choices to meet their personal preferences when selecting the desired services from the web services with the similar functionality. Therefore, it becomes tedious and cumbersome tasks for users to discover and compose services. To reduce user's cognitive burden, it is critical to support automated service composition and make efficient recommendation of personalized services to achieve user's overall goals. However, existing approaches only offer users with limited options designed for the interest of a group of users without considering individual users' interests. To allow users to compose personalized services without much manual specification, we propose a machine learning approach that applies a learning-to-rank algorithm, RankBoost, to automatically learn user preferences and the prioritization of the preferences from users' historical data. Moreover, our approach uses the multi-objective reinforcement learning (MORL) algorithm to make trade-offs among user preferences and recommends a collection of services to achieve the highest objective. We conduct an empirical study to evaluate our approach by collecting the historical data from 12 subjects. The results demonstrate that our approach outperforms the two well-established baseline approaches by 100%-200% in terms of precision on recommending services."
4360,"One of the main benefits of web services is the dynamic composability, however how to achieve this is one of the current research challenges. Web service composition has been studied and, amongst other methods, the use of natural computing methods has been proposed previously. In this paper, we address the need for a fast response when computing the most suitable sequence of services. In particular, we propose a novel heuristic immune algorithm with an efficient encoding and mutation method. The algorithm involves two steps: an immune selection operation, which is maintaining antibody population diversity and a clonal selection. The use of a vaccine during the evolution provides heuristic information that accelerates the convergence. Our experimental results illustrate that the proposed heuristic immune algorithm is very effective in improving the convergence speed."
4361,"The efforts and findings of the last decades of research on the formalization and the verification of Web services have given a certain level of assurance on Web services. However new challenges such as high availability and security issues are not fully addressed. In fact, Web services are exposed to attacks that appear continuously. These issues have naturally paved the way to a new research topic that aims at providing new techniques for making Web services attack tolerant. In this paper, we present a state of the art of attack tolerance, especially for Web services. We also present our approach to address such issues through a combination of techniques leveraging in particular diversification. The paper ends with our promising results and a discussion to highlight the perspectives and research direction."
4363,"A common architecture in today's development of distributed systems is the service-oriented architecture (SOA) implemented using Web services. Until recently, it was difficult to build a SOA based grid/distributed system using Web services due to the inability to learn the state of services. The state of a Web service could only be accessed through specialized clients and/or services. Should the specialized client or service fail, the state can't be accessed. This paper shows the innovative resources via Web instances (RVWI) framework. RVWI grants to web services the ability to show the state of dynamic resources in their WSDL. This was achieved via software components called connectors which watch for any changes in a resource and updates the web service. The significance of this report is the support for resources which can change state between requests and the innovation is the improvement of state updates between the service and discovery services."
4364,"Security is today a relevant requirement for any distributed application, and in particular for these enabled by the Web such as e-health, e-commerce, and e-learning. It is thus crucial that the use of Web services, stand-alone or composed, provide strong security guarantees. Web services security encompasses several requirements that can be described along the well known security dimensions, that is: integrity, whereby a message must remain unaltered during transmission; confidentiality, whereby the contents of a message cannot be viewed while in transit, except by authorized services; availability, whereby a message is promptly delivered to the intended recipient, thus ensuring that legitimate users receive the services they are entitled to. Moreover, each Web service must protect its own resources against unauthorized access. This in turn requires suitable means for: identification, whereby the recipient of a message must be able to identify the sender; authentication, whereby the recipient of a message needs to verify the claimed identity of the sender; authorization, whereby the recipient of a message needs to apply access control policies to determine whether the sender has the right to use the required resources. In the tutorial we will first discuss the main security requirements underlying the interactions between clients and Web services and among the Web services themselves. Then we will describe how such security requirements are addressed by standards for Web services security recently developed or under development by various standardizations bodies. Standards that are covered include: WSS, that encompasses a large number of components addressing various security aspects; XACML, that is related to access control and has been recently extended with a profile for Web services access control; WS-Federation, Liberty Alliance and Shibboleth, that address the important problem of identity management in federated organizations. Issues related to the use of th..."
4365,"In SOA, services may become volatile and fail to deliver the quality of service as requested by users. In this paper, we present an approach for repairing failed services by replacing them with new services and ensuring the new service process still meets the user specified end-to-end QoS constraints. An iterative structural inspection algorithm is designed to produce reconfiguration regions that include one or more failed service. By reconfiguring only services in the selected regions, the business process will not be affected significantly. The algorithm may also utilize those available QoS constraints to relax the original constraints of a reconfiguration region and to provide more effective reconfiguration solutions. We also present the middleware components to support the service reconfiguration in the LLAMA framework."
4366,"Nowadays, the exploding number of functionally similar Web services has led to a new challenge of selecting the most relevant services using quality of service (QoS) aspects. Traditionally, the relevance of a service is determined by computing an overall score that aggregates individual QoS values. Users are required to assign weights to QoS attributes. This is a rather demanding task and an imprecise specification of the weights could result in missing some user desired services. Recent approaches focus on computing service skyline over a set of QoS aspects. This can completely free users from assigning weights to QoS attributes. However, two main drawbacks characterize such approaches. First, the service skyline often privileges services with a bad compromise between different QoS attributes. Second, as the size of the service skyline may be quite large, users will be overwhelmed during the service selection process. In this paper, we introduce a new concept, called alpha-dominant service skyline, to address the above issues and we develop a suitable algorithm for computing it efficiently. Experimental evaluation conducted on synthetically generated datasets, demonstrates both the effectiveness of the introduced concept and the efficiency of the proposed algorithm."
4367,"Actually Internet applications can provide not only information, but also, another way of getting distributed computing. Cooperative information systems are autonomous and heterogeneous systems, distributed geographically, but interconnected. Web Services provides a set of interoperable standards that can be used to connect distributed applications. On this environment, security is a critical issue, and an attack can expose systems services without authentication. An end-to-end connection, like the ones involved in such systems, usually requires that an authentication can be shared between different information systems. Web Services security model is not yet fully defined and a lot of proposals are emerging, delaying the adoption of this technology in many situations. In this paper we present multiplatform authentication control system based on an extension of SRP protocol, using SAML. Within this solution, authentication control can be leveraged, even with weak passwords and an authentication assertion can be exchanged with different cooperative information systems."
4368,"Today, there is a huge amount of security services that can be used to implement different security requirements in Web Service based systems. For example, identity management services are required for authentication and authorization whereas message logging services are necessary to achieve non-repudiation. However, the deployment and configuration of these security services usually requires expert knowledge about the systems and expert knowledge about security requirements and implementations which a person can only learn by experience. Furthermore, today's Web Service based systems become increasingly complex. Thus, implementing security requirements is a complex and error prone task, even for experts. For this paper, we analysed several service-based implementations for identity management and their differences in the service orchestration. We present an approach to derive the needed security services, their configuration, and their connections to the functional services, based on defined security requirements for a Web Service based system. Therefore, we evaluate the UML use case model of the system and apply service security pattern derived during the analysis of the identity management implementations."
4369,"This paper presents a novel Web-based service-oriented framework to support collaborative product commerce. The framework coordinates processes to design and manufacture a product in a loosely coupled environment using Web services. We have developed a service model using OWL. It provides facilities for companies to discover services that other companies provide. Choreography of Web services is achieved using the process grammar. Usage of process grammar helps to configure processes dynamically, facilitating collaboration among companies. The framework, therefore, provides a truly distributed architecture for management of service composition and interoperation, lending a unique functionality to the framework."
4370,"Optimizating semantic Web service compositions is known to be NP-hard, so most approaches restrict the number of services and offer poor scalability. We address the scalability issue by selecting compositions which satisfy a set of constraints rather than attempting to produce an optimal composition. Firstly, we define constraints within an innovative and extensible quality model designed to balance semantic fit (or functional quality) with quality of service (QoS) metrics. The semantic fit criterion evaluates the quality of semantic links between the semantic description of Web services parameters, whilst QoS focuses on non-functional criteria of services. Coupling these criteria allows us to further constrain and select valid compositions. To allow the use of this model in the context of millions of services as foreseen by the strategic EC-funded project SOA4All, we i) formulate the selection problem as a constraint satisfaction problem and ii) test the use of a stochastic search method. Finally we compare the latter with state-of-the-art approaches."
4372,"Interest in service oriented architecture (SOA) is rapidly increasing in the business world due to the many benefits it offers such as reliability, manageability, reusability, flexibility, efficiency, and interoperability. There are many security technologies and models being developed for SOA. They implement or encode specific aspects of authentication, authorization, encryption, trust, and access control respectively but none of them was entirely devoted to integrity. In this paper we propose service Clark-Wilson integrity model (SCWIM), a top down integrity model for SOA capable of describing sufficient conditions to protect data integrity in any SOA implementation. Based on the original Clark-Wilson integrity model, our model can form the basis for system security audits and assist SOA architects in developing systems that protect data integrity, as well as providing guidance for evaluating existing SOA systems."
4373,"The development of the Internet and Web 2.0 eased communication among people and triggered the flourish of Web based communities. Among them, there are blog, social networks, music networks, and recommendation network, etc. There are also communities of massively multiplayer online games. In particular, 3D based virtual worlds, like Second Life, are highly welcomed by people. Users contribute to these communities while at the same time obtain value from the communities instead of passive browsing. The relationships among the community members and community services could be represented by different types of networks. The topology of networks influence the way how people interact and information spread within the community. In this paper, we propose a mechanism to facilitate the communities to grow into scale-free networks. Scale-free communities have good properties that could potentially speed up its accumulation of value for both the members and the community itself. We extend WSDL into scale-free Web services using Web services resource framework."
4374,"In this paper we report on our work towards a novel system that aims at the combination of classical failure handling methods for composite service execution and support for automatic handling of semantic application level failures. In particular, we formulate the notion of Control Flow Intervention; a generalization for handling those two types of failure categories. When combined with automatic service composition it constitutes an optimistic forward recovery method. The method adopts expressive semantic service annotations provided by OWL resp. OWL-S."
4375,"A telecom operator (""service provider"", SP) offers various services to subscribed customers by partnering with various third party providers (""content provider"", CP). The SP acts as a liaison between subscribers and partners. One of the main functions of the SP, therefore, is to match the ""demand"" of the subscribers with the ""supply"" of the CPs. Such a matching is a prerequisite for efficient service selection while ensuring customer satisfaction, and is useful for optimisation, such as resource allocation and load balancing. The ""demand"" requirements and ""supply"" guarantees can be concretized using service level agreements (SLAs). SLAs can be expressed formally using standards such as WSLA or WS-Agreement. We present a system that automates the task of finding a matching between these two sets, subscriber-SP and SP-CP, of SLAs. First, the SLAs are normalised to a common denominator, then composed if required, and finally the matching engine computes and outputs the map. The matching algorithm, which is of central importance, compares logical expressions involving predicates. The logical expressions are first converted into CNF-form, and instead of naive O(m times n) comparisons, we develop a more efficient approach to solve the problem. As a proof-of-concept, we have implemented a prototype as an Eclipse plugin"
4376,"Unlicensed taxis are widely considered as major obstacles to city traffic regulation and public safety. Thus, many governments have issued restrictions for car-hailing services and alleged that the use of unlicensed vehicles was illegal. However, it is very challenging that traffic administrative enforcements face limited manpower to prohibit unlicensed taxis, due to costly and time-consuming procedure of on-site evidence collection. In this paper, we propose an effective service to incorporate human mobility mechanism into unlicensed taxis detection from massive city-wide vehicles. We first extract 276 spatio-temporal features, which are grouped into two categories, including daily behaviors and sustainable behaviors to capture the mobility characteristics of unlicensed taxis. Second, we investigate the detection accuracy of three machine learning techniques, viz. support vector machines, decision tree, and logical regression. We illustrate our approach using real-world vehicle license plate recognition dataset in Xiamen, China, which contains 336 million passing records for 6.2 million vehicles filmed by 439 devices in August 2016. Experimental results reveal that LR outperforms SVM and DT in prediction accuracy and F-score measurement, while SVM is capable of identifying the largest number of unlicensed taxis."
4377,"The Sloan Digital Sky Survey (SDSS) science database describes over 230 million objects and is over 1.6 TB in size. The SDSS Catalog Archive Server (CAS) provides several levels of query interface to the SDSS data via the SkyServer website. Most queries execute in seconds or minutes. However, some queries can take hours or days, either because they require non-index scans of the largest tables, or because they request very large result sets, or because they represent very complex aggregations of the data. These ""monster queries"" not only take a long time, they also affect response times for everyone else - one or more of them can clog the entire system. To ameliorate this problem, we developed a multiserver multiqueue batch job submission, execution, and tracking system for the CAS called CasJobs. The transfer of very large result sets from queries over the network is another serious problem. Statistics suggested that much of this data transfer is unnecessary; users would prefer to store results locally in order to allow further joins and filtering. To allow local analysis, a system was developed that gives users their own personal databases (MyDB) at the server side. Users may transfer data to their MyDB, and then perform further analysis before extracting it to their own machine. MyDB tables also provide a convenient way to share results of queries with collaborators without downloading them. CasJobs is built using SOAP XML Web services and has been in operation since May 2004."
4378,"Channel passing mechanisms enable dynamically determining destinations of message transferring. WS-CDL, a language developed by W3C for the specification of Web services choreographies, adopts channel passing to support dynamic Web services composition. A choreography can be projected into individual services or orchestration skeletons. It is a challenge to ensure the services generated from a choreography always have sufficient and correct channels to complete their collaboration. In fact, WS-CDL is not ready for rigorous validation and implementation with respect to channel passing, since it provides no structure for specifying explicitly which role should firstly initialize which channel variable. Here we propose an algorithm to uncover these implicit assumptions, that is implemented as an extension to Pi4SOA. With the help of the algorithm, some existing methods for verification and implementation can be applied on choreographies written in WS-CDL. In addition, we propose an approach to detect design defects in choreographies, and show how a defect is found from the main sample choreography in WS-CDL Primer. It seems that choreographies with channel passing are error prone. Methods and tools are necessary to support designers in this field. Also, we suggest improving the situation by adding a syntactical construct to WS-CDL."
4379,"Survivability is crucial for computer systems that support critical infrastructures of our society. For those in paradigm of SOA where the computing settings are intrinsically open, traditional survivability model and safeguard mechanisms are no longer applicable. To rise to the challenge, we propose a formal definition and a corresponding framework to evaluate the survivability of SOA systems. We treat survivability as a multidimensional QoS property, and then give a holistic evaluation method based on the Hidden Markov Model."
4380,"Reliability is an essential quality requirement for web services. Existing techniques for measuring reliability of web services mainly focus on failures caused by code-based defects. For data-centric web services, the reliability of services can be significantly affected by the quality of data used to provide services. However, the impact of data quality on the reliability of web services has rarely been explored. We present an approach to estimate data quality and to incorporate data quality with the reliability of software components in the reliability estimation of web services. To demonstrate the proposed approach, we present a case study on a government web service. In this study we observed that more than 60% of service failures reported over a three-month period were caused by invalid data. The results show that by taking into account data quality, the reliability estimate of the web service is more accurate than the traditional reliability measurement."
4381,"The message-based communication among services in Service Oriented Architecture (SOA) is vulnerable to various security attacks, and has to be well protected by security mechanisms, which may sacrifice service performance due to limited system resources. In this paper, an adaptive tradeoff model for service performance and security in service- based systems is presented. This model can be used to adjust security configurations of services to provide sufficient protection and satisfy service performance requirements for SOA-based systems simultaneously. The construction of this model includes the development of a set of metrics to quantitatively measure the performance and security of services, the development of a tradeoff objective function incorporating service performance and security, and the parameter estimation through experiments. An example of service-based secure voice communication system is used to illustrate the construction of this model."
4382,"Many popular web service networks are content-rich in terms of heterogeneous types of entities and links, associated with incomplete attributes. Clustering such heterogeneous service networks demands new clustering techniques that can handle two heterogeneity challenges: (1) multiple types of entities co-exist in the same service network with multiple attributes, and (2) links between entities have diverse types and carry different semantics. Existing heterogeneous graph clustering techniques tend to pick initial centroids uniformly at random, specify the number k of clusters in advance, and fix k during the clustering process. In this paper, we propose Service Cluster, a novel heterogeneous service network clustering algorithm with four unique features. First, we incorporate various types of entity, attribute and link information into a unified distance measure. Second, we design a Discrete Steepest Descent method to naturally produce initial k and initial centroids simultaneously. Third, we propose a dynamic learning method to automatically adjust the link weights towards clustering convergence. Fourth, we develop an effective optimization strategy to identify new suitable k and k well-chosen centroids at each clustering iteration. Extensive evaluation on real datasets demonstrates that Service Cluster outperforms existing representative methods in terms of both effectiveness and efficiency."
4383,"For users' convenient access to Web services, this paper presents a method for generating XForms based user interfaces from WSDL files. The proposed method uses an XML schema in a WSDL file, and consists of three steps: processing a WSDL file, generating XForms codes, and embedding them into a host language document. Specifically, to generate XForms codes, rules for generating XForms codes, which support complex types and map simple types into appropriate input controls, are proposed. Experimental results with a large volume of WSDL files show that the proposed method generates XForms-based user interfaces from most of WSDL files successfully."
4384,"Collaborative Filtering (CF) has been increasingly employed as an effective vehicle for providing personalized service recommendations in service computing. CF exploits historical user-service interaction information to predict the preference of service users. A key challenge faced by CF is to handle new users with no previous interaction information. We present a novel strategy that integrates Matrix Factorization (MF) with decision tree learning to bootstrap service recommendation systems. The proposed strategy first employs MF to partition existing users into a set of user groups. In practice, only a small amount of user-service interaction information is observed. The MF based user partitioning scheme also provides a way to estimate the missing interaction information based on the group structure. The tree learning algorithm then leverages these estimated information and exploits user groups as class labels to learn a decision tree. Few highly discriminative services are identified as tree nodes to adaptively query a new user based on the interaction results with the prior services in the tree. Through a short and intuitive bootstrapping process, the new user is classified into one of the user groups, via which the user's preference is predicted. We conduct a set of experiments on real-world service data to demonstrate the effectiveness of the proposed bootstrapping strategy."
4385,"Today's business process orchestration languages such as WS-BPEL and BPML have high-level constructs for specifying flow of control and data, but facilities for allocating tasks to humans are largely missing. This paper presents SoftAlloc, a work allocation language with soft constraints, and explains the requirements and trade-offs that led to its design, in particular, what soft constraints are, and how they enable business process definitions to capture allocation rules, best practices, and organizational goals without rendering the business processes too strict. SoftAlloc combines with virtually any business process language and any conceivable legacy system, while guaranteeing polynomial performance. We present the design, the formal definition, and an evaluation of SoftAlloc."
4386,"Recently, Web services have become a new technology trend for Enterprise Application Integration (EAI) and more and more applications based on Web services are emerging. One of the problems in using Web services in business applications such as logistics is services composition automatically and efficiently. In this paper, we present a Dynamic, Demand-Driven Web services Engine called D3D-Serv to implement composite service functionality that is used to dynamically build composite services from existing services according to different business logics and requirements. In this D3D-Serv framework, the most challenging function to implement is dynamic selection of service providers at run time. The highly dynamic and distributed nature of Web services often makes some service providers overloaded at certain times while others idle. To solve this problem, we propose an efficient services selection and execution strategy that is based on the queuing theory and can provide guarantees for the QOS (Quality of Service) under provider's limited resources. Preliminary experimental results have shown that this algorithm is effective."
4388,"Traditional methods for implementing highly resilient systems are costly; and their methods using specialized hardware and high-speed interconnects do not extend to cloud.&nbsp;&nbsp;We demonstrate an architecture that rethinks the traditional approach and uses cloud's agility to provision additional capability as failures occur.&nbsp;&nbsp;This architecture continues serving requests with the best possible response even as the primary service deteriorates and even fails completely.&nbsp;&nbsp;It offloads requests according to their need to access the primary service; first redirecting requests with less need to lower-grade secondary capability that is provisioned on-demand on the cloud in order to reserve primary capacity for requests that need it (e.g., first redirect web-browsing requests to a slightly out-of-date cached dataset, and reserve the primary, most-consistent data service for check-out requests).&nbsp;&nbsp;Our approach 1) triages access to the primary service based on request needs in times when it is overwhelmed, and 2) provisions on-demand cloud capability to offload and continue serving other requests as needed.&nbsp;&nbsp;The result is a highly resilient system architecture that scales via cloud."
4389,"Logs, which record valuable system runtime information, have been widely employed in Web service management by service providers and users. A typical log analysis based Web service management procedure is to first parse raw log messages because of their unstructured format; and then apply data mining models to extract critical system behavior information, which can assist Web service management. Most of the existing log parsing methods focus on offline, batch processing of logs. However, as the volume of logs increases rapidly, model training of offline log parsing methods, which employs all existing logs after log collection, becomes time consuming. To address this problem, we propose an online log parsing method, namely Drain, that can parse logs in a streaming and timely manner. To accelerate the parsing process, Drain uses a fixed depth parse tree, which encodes specially designed rules for parsing. We evaluate Drain on five real-world log data sets with more than 10 million raw log messages. The experimental results show that Drain has the highest accuracy on four data sets, and comparable accuracy on the remaining one. Besides, Drain obtains 51.85%~81.47% improvement in running time compared with the state-of-the-art online parser. We also conduct a case study on an anomaly detection task using Drain in the parsing step, which determines the effectiveness of Drain in log analysis."
4390,"In this paper, we study Web services methods and approaches to enable real-time communication services over IP. This approach extends Web services methodologies from service integration to a service-oriented approach for communication. In particular, we describe the generic Web service based application session management, WS-Session, the two-way full duplex Web service interaction framework, and most importantly, the development of Web service initiation protocol (WIP) which is a full featured Web service and SOA based communication framework for multimedia and voice communication over IP. We show that WIP provides a service oriented architecture which can be extended seamlessly from a P2P endpoint to an endpoint with advanced call control and switching capabilities typically requiring the assistance of a dedicated PBX. A prototype of WIP system is fully developed. Architectural design and system implementation of Web service based communication endpoints are studied and applied to realize service-oriented communication over IP. Advances of WIP indicate the beginning of a full Web service and SOA based communication paradigm that can reshape the converged communication over IP."
4391,"Web services fail to deliver on the promise of ubiquitous deployment and seamless interoperability due to the lack of a uniform, standards-based approach to all aspects of security. In particular, the enforcement of access policies in a service oriented architecture is not addressed adequately. We present a novel approach to the distribution and enforcement of credentials-based access policies for Web services (3PAC) which scales well and can be implemented in existing deployments."
4392,"In this paper, we describe our work in progress on Web services recommendation for services composition in a Mashup environment, by proposing a new approach to assist end-users based social interactions capture and analysis. This approach uses an implicit social graph inferred from the common composition interests of users. We describe the transformation of users-services interactions into a social graph and a possible means to leverage that graph to derive service recommendation. As this work is in progress, this proposal was implemented within a platform called SoCo where preliminary experiments show interesting results."
4393,"Application-caching occurs when applications assume control for data caching rather then relying on middleware to manage their data. This paradigm is a good fit for""main-memory caching"" in which data storage and retrieval issues are relatively straight-forward. However, the caching requirements of certain workloads exceed even modern main-memory capacity. Although disk-offload offers the potential to overcome this capacity constraint, it poses a problem for the application-caching paradigm since application data may now be resident in either main-memory or on disk. In this paper, we therefore examine how disk-offload middleware can best support the application-caching paradigm for data-centric web-services. We present two disk-offload caching architectures, evaluate their performance, and draw conclusions as to their relative effectiveness."
4394,"The concept of trust in web services mainly deals with the degree of belief that a client or a group of clients have over services functioning satisfactorily and providing the expected results. With services being invoked in composition with each other, computing the trust of the composition and selecting services that deliver the highest trust for it becomes a desired goal. In this paper we demonstrate how using Bayesian networks and its supporting queries, we can select the set of services among all candidates that would provide highest global trust."
4395,"To keep up with the trend of globalization and informatization, an increasing number of enterprises decide to run their business process in a service-based manner with the help of Web Service technology. In order to manage such service-based business process (SBP), it is vital that the dependencies among the internal process and the exposed external services are correctly developed and maintained. SBP is dynamic by nature, therefore it is necessary to develop a practical and robust method to verify the correctness of SBP. In an SBP, complex dependencies exist not only between internal process and involved services but also within their components (activities, data, operations, etc.). The complex dependencies make the correctness verification for SBP a challenging task. In this work, we develop a correctness verification approach to handle this task. A Petri net based model is proposed with a hierarchical structure to cover the characteristics of SBPs. This model can support the control flow patterns that are necessary for SBPs. A set of correctness properties for SBP are identified which any SBP developers shall consider, and the respective verification methods are developed."
4396,"With the development of information technology, more and more web services have emerged, thereby making it difficult for customers to find their favorite services quickly and accurately. To overcome this difficulty, recently the collaborative filtering (CF) technique has been widely employed for personalized service recommendation, meanwhile improving the profits of service providers. Although the CF-based web service recommender systems have shown their potential, they appear to be vulnerable to shilling attack problems. Therefore, in this paper we analyze a general form of web service shilling attacks and four kinds of classical attack models, e.g., average attack, bandwagon attack, random attack, and segment attack are thoroughly investigated. Furthermore, we also study the impact of distribution-aware Pareto attack models. To demonstrate how shilling attacks alter the recommendation results, this paper analyzes 1) the variation of Quality-of-Service (QoS) prediction values of target services, 2) the QoS value prediction shifts of services with short response time which are more likely recommended, and 3) the comparison of prediction shift caused by classical attack models and Pareto attack models. The experimental results on WS-DREAM dataset revealed several interesting findings about the predictions of QoS values of target service correlated to different attack models. It is expected that this work can provide some insight for future vulnerability analysis of CF-based web service recommender systems."
4397,"In our earlier work, competitive Web services market was proposed to address one of the major business concerns, namely ""competitiveness"", of the current Web services research. This paper aims to attack the issues of how to model and analyze competitive Web services market, which ranges from the service node at the micro level to the competitive composite service providers at the macro level. Moreover, in order to tackle and contain the randomness embedded in a Web services system, we apply the Markov chain theory to analyze the availability of Web services market. Moreover, we analyze the stochastic performance of the Web services market using queuing theory and propose the adaptive control methodology to improve the performance."
4398,"Due to the inherent stochastic nature of services execution environment within service oriented systems, a runtime adaptation of the given composition may be required. We address a runtime service adaptation mechanism based on conditional retries for the orchestrated web services. The conditional retry may be issued while a concrete service within composition is executed. The retry could either invoke the same concrete service or a functionally equivalent web service that implements the same task. We use dynamic programming to determine the optimal time instances at which the current request should be terminated before request replication. The calculation takes into account different QoS parameters like services' response -- time distributions and cost - relating parameters, and the solution optimizes the expected revenue of composite service provider. We illustrate the benefits of our approach by numerical calculations, and discuss the impact of considered QoS parameters to the solution at hand."
4399,This paper presents a solution to performance issues in the quality of service aware selection of Web services using techniques of parallelism and mechanisms of inference provided by Semantic Web. The results point to a significant improvement in the speed of searching Web services and thus makes the use of semantic resources viable in distributed systems to provide better quality of service to the clients.
4401,"On a crowdsourcing platform, in order to cheat for rewards or sabotage the crowdsourcing processes, spam workers submit numerous random and erroneous answers to the tasks published by honest requesters. This type of behaviours extremely reduces the enthusiasm of honest users, which may lead a crowdsourcing platform to a failure. To defend the threats from spam workers, reputation-based defense mechanisms and verification-based defense mechanisms have been proposed in crowdsourcing environments. However, reputation-based defense fails to indicate the trust level of a worker who boosts its reputation by transacting with his/her accomplices. In addition, verification-based defense is costly and ineffective when facing a large number of spam workers with ""good"" reputations. Thus, it is a challenging problem to effectively defend the threats from spam workers. In this paper, we propose a new trust vector-based threat defense model CrowdDefense. Firstly, we build a Crowdsourcing Trust Network (CTN) consisting of requesters, workers and their transaction relations. Then, we analyze three threat patterns of spam workers. Based on the analysis, we infer the trust relation between a worker and a requester who are indirectly linked with each other. Moreover, we compute a worker's trust relations with different requesters, and present them in a new Worker Trust Vector (WTV) that indicates the worker's global trust. As spam workers always succeed in the transactions with their accomplices, they cannot obtain comprehensively good trust scores in their WTVs and thus are prevented from participating in tasks. The experiments demonstrate that CrowdsDefense significantly outperforms the state-of-the-art approaches in terms of preventing spam workers from participating in the tasks published by honest requesters."
4402,"The growing number of Additive Manufacturing Web (AMW) services, offered by different providers over the Internet, makes it challenging for consumers to compare these AMW services to select a service of their choice. In addition, it is even more challenging for consumers to compare these AMW services against their personal preferences. This is because, consumers personal preferences on multiple non-functional attributes such as price, material, accuracy and schedule, should be considered for AMW service selection. The decentralized nature of AMW services coupled by the need to consider consumers personal preferences during AMW service selection, requires a system that will serve as a broker between AMW services and consumers. In this paper, we propose a service broker system for AMW services that provides consumers with a single point of access to a large number of AMW services from many additive manufacturing service providers. This broker system also incorporates the first real application of service selection with fuzzy logic based personalized preferences and trade-off. We develop a method to generate fuzzy membership functions for each non-functional attribute. This makes it easy for consumers to specify their fuzzy membership functions. Finally, we present an application case study to demonstrate the feasibility of brokerage in AMW services and also evaluate our method in terms of performance."
4403,"Web services proxy is an extendable intermediate that can monitor and control XML communication with good performance. It adopts a plug-in architecture enhanced by characteristic techniques named ""MFI4P"" and ""XPath centric architecture"". MFI4P efficiently controls plug-in libraries over various XML operations by optimizing transformation cost of XML between the libraries. The XPath centric architecture performs XML processing better and makes it easy to develop plug-in by using streaming XPath and ""two phase rule evaluation"". Experimental results show many types of management functions can be built as plug-ins and XML processing overhead is minimized in many cases."
4404,"Transaction concept plays a key role in business success. However, existing transaction models are not applicable in service grid environment. Though many new models have been suggested, they still have some deficiencies, e.g., major extension to services are required, and they are hard to implement, to name a few. In this paper, a new model is proposed based on the analysis of existing work and the characteristics of service grid environment. Besides, model implementation issues are also covered"
4405,"Push notification is an important approach to distribute interesting information to users timely. With the fast development of mobile devices and mobile applications, push notification is getting more and more popular. The convergence of mobile and IoT also bring new challenges on how the system can handle the mixed push channels designed for M2M communication and human interaction, and enable the effective interaction with both human and IoT devices involved. IoT devices may push notifications of sensor data in a high frequency. To enable push notification for both of mobile devices and IoT devices, the push notification system is required to achieve high throughput to handle the frequent notifications, and support content matching to filter out the undesired notifications. To enable effective push notification with both human and IoT devices involved, the system is required to understand the users' interests for notifications with the IoT devices providing the users' contexts. That is to say users need an intelligent push notification system. In this paper we propose a high performance context-aware push notification system for the converged mobile and IoT messaging. We designed high performance content matching engine as the core to enable efficient message dispatching for push notification according to highly personalized interest to ensure IoT messages to be involved in push notification. A user's interest of notifications is highly related with his context. Therefore, based on the content-matching engine, a framework for efficient context information fusion is built to support various types of context-aware push notification, towards intelligent push notification. Also we designed shared connection scheme to reduce the resource cost. Based on the content-match engine and context-aware features, the proposed push notification service can support both of group push notification and bi-directional push notification. Tests are conducted for performance evaluation."
4406,"Although service composition has received a great deal of attention and there are many solutions to the problem of composition, finding potential service compositions efficiently is still challenging because of the fact that the number of candidate services can be very large. In this paper, a tree-based method of Web service composition is proposed, by which all potential candidate composite services that meet the user's request can be created. Moreover, these composite services are ordered according to the QoS attribute concerned by the user, so the user can find the best and arbitrary top-n composite services."
4407,"Current Infrastructure-as-a-Service (IaaS) clouds offer both on-demand and reservation instance purchasing options. Users can combine these two options dynamically to serve time-varying demands while minimizing their instance acquisition costs. However, when future demands are unknown, it is far from trivial for cloud users to make optimal instance purchasing decisions. To deal with this problem, a carefully designed online algorithm can be employed to guide users in acquiring instances without any prior knowledge of future demands while guaranteeing a competitive ratio. In this paper, we propose an instance reselling model, in which a cloud user can temporarily rent out its idle reserved instances to other users through pay-as-you-go model. We also design online instance acquisition strategies which achieve a better competitive ratio than previous methods. Through extensive simulations based on both synthetic data and real-world traces, we show that our online algorithm under the proposed reselling model can outperform previous models and achieve significant cost savings."
4408,"Existing Web service access control models focus on individual Web services, and do not consider service composition. In composite services, a major issue is information flow control. Critical information may flow from one service to another in a service chain through requests and responses and there is no mechanism for verifying that the flow complies with the access control policies. In this paper, we propose an innovative access control model to empower the services in a service chain to control the flow of their sensitive information. Our model supports information flow control through a back-check procedure and pass-on certificates. We also introduce additional factors such as the carry-along policy, security class, and transformation factor, to improve the protocol efficiency. A formal analysis is also presented to show the power and complexity of our protocol."
4409,"One of the critical challenges for service oriented computing systems is the capability to guarantee scalable and reliable service provision. This paper presents Reliable GeoGrid, a decentralized service computing architecture based on geographical location aware overlay network for supporting reliable and scalable mobile information delivery services. The reliable GeoGrid approach offers two distinct features. First, we develop a distributed replication scheme, aiming at providing scalable and reliable processing of location service requests in decentralized pervasive computing environments. Our replica management operates on a network of heterogeneous nodes and utilizes a shortcut-based optimization to increase the resilience of the system against node failures and network failures. Second, we devise a dynamic load balancing technique that exploits the service processing capabilities of replicas to scale the system in anticipation of unexpected workload changes and node failures by taking into account of node heterogeneity, network proximity, and changing workload at each node. Our experimental evaluation shows that the reliable GeoGrid architecture is highly scalable under changing service workloads with moving hotspots and highly reliable in the presence of both individual node failures and massive node failures."
4410,"Recent years have witnessed a growing interest in context-aware recommender system (CARS), which explores the impact of context factors on personalized Web services recommendation. Basically, the general idea of CARS methods is to mine historical service invocation records through the process of context-aware similarity computation. It is observed that traditional similarity mining process would very likely generate relatively big deviations of QoS values, due to the dynamic change of contexts. As a consequence, including a considerable amount of deviated QoS values in the similarity calculation would probably result in a poor accuracy for predicting unknown QoS values. In allusion to this problem, this paper first distinguishes two definitions of Abnormal Data and True Abnormal Data, the latter of which should be eliminated. Second, we propose a novel CASR-TADE method by incorporating the effectiveness of True Abnormal Data Elimination into context-aware Web services recommendation. Finally, the experimental evaluations on a real-world Web services dataset show that the proposed CASR-TADE method significantly outperforms other existing approaches."
4411,"In this paper, we focus on the link predication problem in social networks. Our approach is based on the observation that there is a large amount of social behavior taking place every day which contains substantial information about user intrinsic characteristics that influence the dynamics of social networks. In order to obtain a deeper understanding of user behavior, we introduce the concept of latent factor to capture the motivation behind social activities. Since user relationships are often asymmetric, we also take into account bilateral user wishes with respect to friend as preferences, which is beyond traditional approaches or overall measurements. Two combination modes are proposed, independent fusion and interdependent fusion, to integrate these hybrid metrics with traditional measurements for link inference. In order to quantify the sensitivity of each element in metrics we use information theory. Experimental results on several real datasets show that our approach has better performance than previous methods."
4412,"We address the problem of large-scale data integration, where the data sources are unknown at design time, are from autonomous organisations, and may evolve. Experiments are described involving a demonstrator system in the field of health services data integration within the UK. Current Web services technology has been used extensively and largely successfully in these distributed prototype systems. The work shows that Web services provide a good infrastructure layer, but integration demands a higher level ""broker"" architectural layer; the paper identifies eight specific requirements for such an architecture that have emerged from the experiments, derived from an analysis of shortcomings which are collectively due to the static nature of the initial prototype. The way in which these are being met in the current version in order to achieve a more dynamic integration is described."
4413,"Quality of service (QoS) management in compositions of services requires careful consideration of QoS characteristics of the services and effective QoS management in their execution. A Web service is a software system that supports interoperable application-to-application interaction over the Internet. Web services are based on a set of XML standards such as simple object access protocol (SOAP). The interactions of SOAP messages between Web services form the theoretical model of SOAP message exchange patterns (MEP). Web Services Business Process Execution Language (WSBPEL) defines an interoperable integration model that facilitates automated process integration in intra- and inter-corporate environments. A service-level agreement (SLA) is a formal contract between a Web services requestor and provider guaranteeing quantifiable issues at defined levels only through mutual concessions. Based on a prior research work on message detail record (MDR), this paper further proposes a SOAP message tracking model for supporting QoS end-to-end management in the context of WSBPEL and SLA. This paper motivates the study of QoS management in a Web service composition framework with the evolution of a distributed toolkit in an industrial setting."
4414,"In recent years, the evolving of IoT (Internet of Things) has resulted in the deployment of massive numbers of sensors in various fields, such as the Energy and Utility (E&amp;U) industry. These sensors are continuously producing a huge amount of time series data, which creates a correspondingly huge demand for time series data analysis, such as pattern searching. However, analysis on time series data is currently implemented as custom applications, a strategy that suffers from low efficiency and high maintenance costs. Hence there is a need to provide a service for analysis on time series data that reduces maintenance costs and enhances query efficiency. Existing time series data management services lack the capability to perform pattern searches on the massive amount of data from sensors. This paper presents Time Series analytics as a Service (TSaaaS), a scalable analytic service for time series data in IoT scenarios. We designed pattern searching in TSaaaS that can support efficient and effective searching on truly massive amounts of time series data with very little overhead on the IoT system. To simplify access to the TSaaaS, we created a group of RESTful web interfaces. TSaaaS is implemented as an extension to the Time Series Database service in the IBM cloud platform offering (Codename: BlueMix), which is a new product to accelerate IoT application development. TSaaaS targets a future release of the Time Series Database service. We have conducted proofs of concept (PoC) of TSaaaS with real-world customers from power meter management and bridge monitoring in China. The pilot results and other experiments show that for a selection of pattern cases provided by customers, pattern searches via our service are 10-100 times faster than the existing techniques, while the additional storage cost for the service provider accounts for only about 0.4% of original time series data."
4415,"Large-scale scientific data management and analysis usually relies on many distributed scientists with diverse expertise. In recent years, such a collaborative effort is often composed and automated into a dataflow-oriented process, a so-called scientific workflow. However, existing scientific workflow tools are single user-oriented and do not support collaborative scientific workflow composition, execution, and management among multiple distributed scientists. In this paper, we report our study of collaboration protocols towards building a tool supporting collaborative scientific workflow composition. Based on a scientific collaboration ontology, we propose a collaboration model supported by a set of collaboration primitives and patterns. The collaboration protocols are then applied to support effective concurrency control in the process of collaborative workflow composition."
4416,"Web services (WS) technology is becoming pervasive in the development of distributed systems and is an appealing vehicle for service presentation and horizontal integration. On the other hand, model integrated computing (MIC) offers a means of system integration in the vertical direction by using domain-specific modeling, and then synthesizing the software system from the high-level model using a model-specific generator. This paper presents a meta-modeling approach to WS to explore the application of MIC in WS development and its contribution."
4417,"The growth of the Internet has been accompanied by the growth of Web services (e.g. e-commerce, e-health) leading to the need to protect the personal privacy of Web service users. However, it is also important to be able to measure a Web service in terms of how well it protects personal privacy. Such a capability would benefit both users and developers. Users would benefit from being able to choose (assuming that such measures were made public) the service that has the greatest ability to protect user privacy (this would in turn encourage Web service providers to pay more attention to privacy). Developers would benefit by being able to incrementally measure and modify their services during development until certain target levels of privacy protection are reached. This paper presents an approach for measuring how well a Web service protects personal privacy and illustrates the approach with an example"
4418,"Accurate and lightweight evaluation of web service security properties is a key problem, especially when business processes are dynamically built by composing atomic services provided by different suppliers at runtime. In this paper, we tackle this problem by proposing a security certification approach that virtually certifies a composite service for a set of security properties, starting from certificates awarded to the component services."
4419,"This work is motivated by the increasing importance and business value of data in the fields of business process management, scientific workflows as a field in eScience, and Internet of Things, all of which profiting from the recent advances in data science and Big data. We introduce a management life cycle that renders data as first-class citizen in service choreographies and defines the functions and artifacts necessary for enabling transparent and efficient data exchange among choreography participants. The inherent goal of the life cycle, functions and artifacts is to help decouple the data flow, data exchange and management from the control flow in service compositions and choreographies. This decoupling enables peer-to-peer data exchange in choreographies and provides the means for more sophisticated data management and exchange, as well as data exchange and provisioning optimization."
4420,"IT services need an automatic and flexible ability to react to dynamic changes in their environment. Managing change effectively and reducing the negative effects of day-today operations has become one of the most important tasks in IT service management, which require hiring highly skilled IT professionals with correspondingly high labor costs. There is a challenge to select, implement and integrate the right resources quickly and effectively. Although there is a growing body of research into IT management, many techniques are either too narrow (focusing on a single component rather than the entire system), or they address only configuration data collection and integration. Instead, one needs to scale or respond to special domain knowledge, collaboration and the right data for helping IT professionals to improve their work. In this paper, we propose a knowledge-sharing based collaborating management system for IT service management. It aims to bridge the gap between domain experts' knowledge and manageable systems. We developed a proof-of-concept of an impact analysis service based on knowledge-sharing; it establishes an IT service management collaboration paradigm and ecosystem, leveraging the expert's rich knowledge and experience to improve the management quality and reduce the cost. A case study driven by customers demonstrates that collaboration with knowledge sharing is effective both at constructing useful system analysis services and in using those services to improve system management."
4421,"Copyright and Reprint Permissions: Abstracting is permitted with credit to the source. Libraries may photocopy beyond the limits of US copyright law, for private use of patrons, those articles in this volume that carry a code at the bottom of the first page, provided that the per-copy fee indicated in the code is paid through the Copyright Clearance Center. The papers in this book comprise the proceedings of the meeting mentioned on the cover and title page. They reflect the authors' opinions and, in the interests of timely dissemination, are published as presented and without change. Their inclusion in this publication does not necessarily constitute endorsement by the editors or the Institute of Electrical and Electronics Engineers, Inc."
4422,"With the development of mobile network and corresponding techniques, more and more works focus on providing efficient services based on mobile devices. Furthermore, motivated by IoT, studies of local distributed mobile devices attract attentions of both industry and academia in recent years. However, existing storage systems cannot manage data and support the QoS of mobile services well. This paper presents LKSM, a light weight key-value storage system, which can be deployed on either one node or multiple nodes. To the best of our knowledge, it is the first attempt to propose key-value store in this scenario. We carefully analyze the challenges when designing the system on mobile cluster, and further propose RDS for addressing. With the help of RDS, LKSM achieves the goal of lower latency, better scalability, and higher availability. We organize LKSM using a log-structured merge-tree, and implement it based on LevelDB, an open source key-value storage system proposed by Google. Experiments on physical smartphones demonstrate that LKSM presents much higher performance compared with the ported LevelDB on mobile devices."
4423,"The service-oriented architecture (SOA) makes application development easier, because applications can be built from existing services with a bottom-up methodology. However, it is difficult to determine if a desired new service can be built from existing services. Not only the functional consistency of the existing services, but also the consistency of their non-functional (such as security) aspects must be verified. Message protection is an aspect of security. Every service needs an appropriate security policy defining the protection of messages exchanged between the parties to the service. Because of the intricacy of the Web services security policy language, it is difficult to verify the consistency of the security policies. We are developing a method to verify the consistency of security policies by abstracting them. Each security policy is abstracted, and then attached as a security type to the corresponding service in the application model. The security type denotes a security level for message protection. The security developer defines the possible abstraction methods. In this paper, we define the constraint of abstraction methods based on the semantics of the policy language. And also we state verifying the consistency of security types by using information flow analysis."
4424,"Web sites serve content both through Web services as well as through user-viewable Web pages. While the consumers of Web-services are typically 'machines', Web pages are meant for human users. It is highly desirable (for reasons of security, revenue, ownership, availability etc.) for service providers that content that will undergo further processing be fetched in a prescribed fashion, preferably through a supplied Web services. In fact, monetization of partnerships within a services ecosystem normally means that Web site data translate into valuable revenue. Unfortunately, it is quite commonplace for arbitrary developers to extract or leverage information from websites without asking for permission and or negotiating a revenue sharing agreement. This may translate to significant lost income for content providers. Even in cases where Web site owners are happy to share the data, they may want users to adopt dedicated Web service APIs (and associated API-servers) rather than putting a load on their revenue-generating websites. In this paper, we introduce a mechanism that disables automated Web scraping agents, thus forcing clients to conform to the provided Web Services."
4425,"Service composition is a widely used method in ubiquitous computing that enables accomplishing complex tasks required by users based on elementary (hardware and software) services available in ubiquitous environments. To ensure that users experience the best Quality of Service (QoS) with respect to their quality needs, service composition has to be QoS-aware. Establishing QoS-aware service compositions entails efficient service selection taking into account the QoS requirements of users. A challenging issue towards this purpose is to consider service selection under global QoS requirements (i.e., Requirements imposed by the user on the whole task), which is of high computational cost. This challenge is even more relevant when we consider the dynamics, limited computational resources and timeliness constraints of ubiquitous environments. To cope with the above challenge, in this paper we present QASSA, an efficient service selection algorithm that provides the appropriate ground for QoS-aware service composition in ubiquitous environments. The contribution of QASSA is three-fold. First, it formulates service selection under global QoS requirements as a set-based optimisation problem, benefiting from recent proposals in the domain of multi-objective optimisation. Second, QASSA resolves this problem in an efficient way using clustering techniques, namely the K-Means algorithm. Third, QASSA is devised in two versions, viz., centralised and distributed versions, so that it can be executed on top of centralised and decentralised infrastructures in ubiquitous environments. Results of experimental studies are presented to illustrate the timeliness and optimality of QASSA."
4426,"One of the major goals of Web services is to make easier their composition to form more complex services, modeled as workflows. A key role in the Web services composition is the selection of a proper service for each activity in the workflow. In general, this requires the exchange of sensitive information of users, requiring the composition, as well as of involved service providers. So far this problem has been investigated in the setting of orchestrated service composition, under the assumption of the presence of a broker coordinating the composition. However, a promising alternative approach is the one of choreography, where each service involved in the service composition has to locally manage service selection and invocation. In this paper, we propose a framework to enforce user and provider requirements in the scenario of service choreography in a privacy-preserving way, that is, without the releasing of any private information of users and providers. To achieve this result we make use of different privacy-preserving protocols. As it will be shown in the paper, the proposed solution does not implies relevant overhead."
4427,"WSDL provides the potential for Web services to enrich consumers' lives. However, it has had only limited success in enterprise environments and even less in the mass market. Apart from the difficulties and high costs involved in current approaches, another reason is the low precision for Web Services discovery using the most widely known tool: search engines. Seeking for effective ways to change the situation, we conducted an experiment to examine better approaches of using general-purpose search engines to discover Web Services. We used nine different approaches for publishing Web Services and two groups of total 18 queries for retrieving them using Yahoo and Google search engines. The queries were fired to each search engine daily over a week and the top 100 search results returned from every search are collected and analyzed. The results show that for both search engines, embedding a WSDL specification in a Web page that provides semantic description of the service yield the best results."
4428,"Web services composition design, verification and monitoring are active and widely studied research directions. Little work however has been done in integrating these related dimensions using a unified formalism. In this paper we propose a declarative event-oriented framework, called DISC, that serves as a unified framework to bridge the gap between the process design, verification and monitoring. Proposed framework allows for a composition design to accommodate various aspects such as data relationships and constraints, Web services dynamic binding, compliance regulations, security or temporal requirements and others. Then, it allows for instantiating, verifying and executing the composition design and for monitoring the process while in execution. The effect of run-time violations can also be calculated and a set of recovery actions can be taken, allowing for the self-healing Web services composition."
4429,Presents the welcome message from the conference proceedings.
4430,"Since there are many Web services on the Internet, personalized Web service selection and recommendation is very important. In this paper, we present a new similarity measure for Web service similarity computation and propose a normal recovery collaborative filtering (NRCF) method for personalized Web service recommendation."
4431,"SWSD practices cannot be defined independently of the situation in which they are applied. To address this challenge, we propose an approach for guiding the construction and execution of situational SWSD methods by reusing and integrating different existing SWSD method components suited to the specific situation on hand."
4433,"Today Web services have grown in context of both business to business (B2B) and business to customer (B2C) applications. Web services are the most popular mode of implementing service oriented architecture (SOA). With this growth and acceptance in the industry, the role of security is crucial. Most of the existing security mechanisms in Web services like XML encryption, digital signatures, user tokens etc. provide security on one basic assumption that source of the request is legitimate. But a typical denial of service attacker can use these sources as reflectors and play around with the contents of a Web service body to create an attack scenario. In this paper, we propose PreSODoS - a framework to detect and prevent XML based denial of service (XDoS) attacks on Web services based applications. The framework relies on content introspection to detect any XDoS possibility. We use a Patricia trie based representation so that the schemas and the request messages can be compared and validated in a performance efficient manner. PreSODoS is capable of detecting any repetitive request message and sense an attack scenario and trigger corresponding prevention mechanisms"
4434,"In recent years, a number of scientific workflow management systems (SWFMSs) have been developed to help domain scientists synergistically integrate distributed computations, datasets, and analysis tools to enable and accelerate scientific discoveries. As more scientific research projects become collaborative in nature, there is a compelling need of dedicated services to support collaborative scientific workflows on the Internet. This paper reviews the state of the art of the field of scientific workflows towards the support of collaborative scientific workflows, identifies critical research challenges, and presents our ongoing research work aiming to study how to create services supporting collaborative scientific workflows."
4435,"This paper investigates the problem of reputation management in composite services. Our focus is on developing a method of distribution of reputation received by a composite service to its component services. The proposed method enables the composite service to provide a fair distribution of reputation values so that a component service is neither penalized nor awarded for the bad and good performances respectively, of other component services. Experiment results show that the proposed technique propagates the ""fair share"" of reputation from the composite service to its component services."
4436,"Services computing is playing a critical role in recent years in many fields and we observe a rapidly increasing number of web accessible services and their compositions nowadays. However, our earlier empirical study reveals that, overall the public available services are under-utilized, and when they are used, they are used mostly in an isolated manner. This phenomenon inspires us to further explore a methodology to help consumers understand the usage pattern of the service ecosystem, including interactions among services, and the evolution of these interactions. Based on the derived usage pattern, this methodology also introduces a service recommendation method that suggests both services and their compositions, in a time-sensitive manner. We firstly construct an evolution network model from the historical usage of the services in the ecosystem. Then a rank-aggregation-based link prediction method is proposed to predict the evolution of the ecosystem. Based on this link prediction method, we can recommend services and compositions of interest to service developers. Through an experiment on the real-world mashup-service ecosystem, i.e., Programmable Web, we demonstrated that our approach can effectively recommend services and compositions with better precision than the methods we compared."
4437,"Web services allow devices running on different platforms to communicate with one another using standardized definitions and access ways. Due to recent developments in mobile networks and devices, many researches are on going to apply Web services to mobile network environments. In this paper, we propose an efficient method that discovers services based on a proposed mobility-based clustering in mobile ad-hoc networks. In order to maintain stable clusters and select a proper service discovery architecture, the proposed method uses the mobility of nodes including direction. Experimental results under different mobility models show that the proposed method outperforms a conventional clustering and service discovery method."
4438,"This paper analyses the well known web services representations based on web services descriptions and more generally on the content of WSDL files to evaluate their interest for discovery, clustering and recommendation tasks.Unfortunately, this analysis shows that these representations are very basic and do not lead to good results. Therefore,we introduce a new representation called symbolic reputation which is computed from relationships between web services.Different implementation issues are discussed and the results considering real world web services are analysed to determine the usefulness of the introduced representations for web services discovery, clustering and recommendation."
4439,"Due to complex environment of the coal mine, it's necessary to monitor the information of underground environment, device and miner instantly in order to ensure the safety of coal mine production. However, the exiting coal mine can not meet the requirements of coverage without blind spots as it is developed by the wired network. This paper proposes a RESTful Web services mashup augmented coal mine safety monitoring and control automation using ZigBee wireless sensor network, which can collect the underground temperature, humidity methane values and personal position through sensor nodes in the coal mine, and also collects the personnel position information inside the mine, and then implement a RESTful Application Programming Interface (API) on sensor nodes to provide access to sensors and actuators, allowing for them to be easily combined with other enterprise information resources based on the success of mashup applications. We also illustrated three different of scenarios for RESTful Web service mashups representing for coal mine safety monitoring and control automation. Finally, we give the conclusions."
4440,"With the overload of information on the Web, Recommender Systems (RSs) are becoming increasingly popular and have been employed to provide suggestions to meet different requirements. RSs are utilized in a variety of areas including movies, music, social tags, user group and products as Web services evoked on the Internet either as mobile Apps or PC-based applications. However, it is challenging to achieve personalized recommendations instead of offering up too many lowest common denominator recommendations. Understanding how products relate to each other is important because it has great impact on the performance. Furthermore, the personalized sequential behavior, which is closely related to a particular product, is essential for recommender systems. Most models simply integrate features from users and items without considering potential product bundle relationships between products exposed by users' personalized sequential behaviors. In this paper, a novel method based on Factorizing Personalized Markov Chain (FPMC) is proposed to comprehensively explore the latent bundle relations from users perspective, along with the hidden correlative semantics between products obtained from logic regression method, which provides a unified view to describe the user preferences, product/item features, and the user sequential patterns in timely manner. The involved semantic features are extracted using deep learning models. We evaluate our method on real-world Amazon datasets and our framework significantly outperforms other baseline models, especially on sparse datasets. The experimental results show that our approach qualitatively captures personalized behaviors with superior recommendation performance."
4441,"Software agents controlling production devices must maintain an up-to-date view of the physical world state in order to efficiently reason and plan their actions. Especially in a factory automation system, the world state undergoes rapid evolution, and the world view must remain synchronized with the changes. This paper discusses two approaches to updating the world view based on event notifications sent by web services representing production devices in a manufacturing system. One of the approaches is based on separately specified update rules, and one automatically uses the semantic web service descriptions formulated in OWL-S. While this paper specifically focuses on the factory automation domain, the approaches presented are applicable to other domains as well."
4442,"Web service technologies are playing an increasingly important role in service-oriented architecture design and application convergence over Mobile ad hoc networks (MANET). Due to the decentralized administration and dynamic wireless connectivity problems, accomplishing reliable service discovery in MANET faces a large number of challenges. In order to relieve the communication inefficiency among service providers and clients caused mainly by the unpredictable node mobility, this paper proposes a cross-layer service discovery scheme which enables improved network efficiency and reduced resource consumption. Firstly a network-layer based underlay framework is presented. It specifically establishes a reliability-oriented source routing mechanism which is equipped with a novel reliability-maximized path selection metric and a backup path support fast route recovery strategy. The cross-layer design is prudentially realized by piggybacking service discovery procedures on the reliability enhanced underlay routing mechanism. Simulation analysis verifies that the proposed scheme improves service discovery reliability by achieving low rediscovery frequency, and guarantees high network efficiency by providing reduced service discovery delay and control overhead."
4443,"In the past a few years, the Web has undergone a tremendous change towards a highly user-centric environment. Millions of users can participate and collaborate for their own interests and benefits. Service oriented computing and Web services have created great potential opportunities for the users to build their own applications. Then, it is a pressing issue that, the users can compose services without too complex tasks and efforts. In this paper, we introduce a user-oriented approach which aims to simplify service composition. We leverage the plentiful information residing in service tags, both from service descriptions (such as WSDL) and the annotations tagged by users. Employing some mining algorithms, a direct acyclic graph is built up to represent potential composition opportunities. With a simple and intuitive search, it allows users to explore the space of potentially composable services and achieve service composition in a heuristic manner. We have developed a composition advisor to provide recommendations guiding and assisting the users. It also lets the users discover and make use of services without having to understand too many details of individual candidate services. To enable the users to accomplish service composition in a more interactive access channel, we finally provide a user-friendly prototype based on Web browsers. It undoubtedly reduces the complexity and lowers the entry barrier for the users, and makes them better play their role in the service-oriented Web environment."
4444,"Availability of a wide variety of Web services over the Internet offers opportunities of providing new value added services built by composing them out of existing ones. By integrating individual existing Web services the technology enables the provision of advanced and sophisticated services, such as allowing users to use different types of resources and services simultaneously in a simple procedure. However the management and maintenance of a large number of Web services is not easy and, in particular, needs appropriate authorization policies to be defined so as to realize reliable and secure Web Services. The required authorization policies can be quite complex, resulting in unintended conflicts, which could result in information leaks or prevent access to information needed. This paper proposes a logic based approach using for specifying authorization policies and detecting conflicts resulting from the combination of various kinds of authorization and constraint policies used in Web services environments. The method not only enables static detection of policy conflicts but also yields information that is helpful for correcting the policies. An automated induction-based theorem prover SPIKE is used as verification back-end."
4445,"Service compositions inherently require multiple services each with its domain-specific functionality. Therefore, how to mine matching patterns between services in relevant domains and compositions becomes crucial to service recommendation for composition. Existing methods usually overlook domain relevance and domain-specific matching patterns, which restrict the quality of recommendations. In this paper, a novel approach is proposed to offer domain-aware service recommendation. First, a K Nearest Neighbor variant (vKNN) based on topic model Latent Dirichlet Allocation (LDA) is introduced to cluster services into semantically coherent domains. On top of service domain clustering results by vKNN, a probabilistic matching model Domain Router (DR) based on Extreme Learning Machine (ELM) is developed for decomposing a requirement to relevant domains. Finally, a comprehensive Domain Topic Matching (DTM) model is built to mine relevant domain-specific matching patterns to facilitate service recommendation. Experiments on a large-scale real-world dataset show that DTM not only gains significant improvement at precision rate but also enhances the diversity of results."
4446,"The emergence of Web services has changed the Internet a lot, and greatly facilitated the development of service based software systems. How to select appropriate services and compose them according to given context to satisfy a user's requirement is a big challenge. This paper proposes a novel Genetic Algorithm (GA) method to synthesis web services in a context-aware environment. We first present a context space model to illustrate both contexts and services in a formal way, we utilize GA to compose context-aware services according to users' preference. We transform the problem of service composition to a multi-objective optimization problem. To resolve the conflict and dependencies among services in GA process, we propose a service similarity tree (SST) model to measure the similarity between services. Finally, we design a simulation experiment to evaluate our method. The experiment result shows that our method is a promising one to solve service composition problem in a context-aware environment."
4447,"Due to the rapid growth in both the number and diversity of Web services on the web, it becomes increasingly difficult for us to find the desired and appropriate Web services nowadays. Clustering Web services according to their functionalities becomes an efficient way to facilitate the Web services discovery as well as the services management. Existing methods for Web services clustering mostly focus on utilizing directly key features from WSDL documents, e.g., input/output parameters and keywords from description text. Probabilistic topic model Latent Dirichlet Allocation (LDA) is also adopted, which extracts latent topic features of WSDL documents to represent Web services, to improve the accuracy of Web services clustering. However, the power of the basic LDA model for clustering is limited to some extent. Some auxiliary features can be exploited to enhance the ability of LDA. Since the word vectors obtained by Word2vec is with higher quality than those obtained by LDA model, we propose, in this paper, an augmented LDA model (named WE-LDA) which leverages the high-quality word vectors to improve the performance of Web services clustering. In WE-LDA, the word vectors obtained by Word2vec are clustered into word clusters by K-means++ algorithm and these word clusters are incorporated to semi-supervise the LDA training process, which can elicit better distributed representations of Web services. A comprehensive experiment is conducted to validate the performance of the proposed method based on a ground truth dataset crawled from ProgrammableWeb. Compared with the state-of-the-art, our approach has an average improvement of 5.3% of the clustering accuracy with various metrics."
4448,"We present the Web service description framework (WSDF), which provides both a representation mechanism and a runtime system architecture for semantically enriched Web Services. We analyze existing languages such as BPEL4WS and OWL-S before addressing their deficiencies in our proposal. Our approach allows a client to invoke a service based solely on a shared ontology, i.e. without prior knowledge on the API, providing an important building block towards a global, flexible information infrastructure. Another main point is that WSDF can be applied to clients and services written in a conventional object oriented programming language. This is achieved by lifting data structures to an ontology level in which rich logical statements about services can be formalized. We also present a detailed system architecture that covers planning, invocation, and the automatic processing of the service results, which is accomplished using the observer design pattern and by asserting the result in the respective model. Furthermore, the required annotations can be specified conveniently by placing comments in the source code."
4449,"The main issues for the fulfillment service level agreements (SLA) are concerned with problem of variability of QoS properties (vQoS). Indeed, the QoS properties may evolve frequently either because of internal changes or because of workload fluctuations. To solve the vQoS problem, we first introduced three variability operators: replicate, delete and replace. These operators will be used to reconfigure CWS when the SLA contract is violated. The first two operators are used to add and remove Web service instances, while the last one is used to substitute some faulty Web services. Then, we proposed an incremental approach for modeling and verifying the composites services (CWSs) reconfiguration using Event-B. We start by abstractly specifying the main requirements and then we refine them through several steps to model CWSs. The consistency of each model and the relationship between an abstract model and its refinements are obtained by formal proofs. Finally, we used ProB model-checker to trace possible design errors. We have exploited the LTL for dynamic reconfigurations to characterize the correct behavior of CWSs reconfiguration."
4450,"In this paper we describe tool support for a model-based approach to verifying compositions of Web service implementations. The tool supports verification of properties created from design specifications and implementation models to confirm expected results from the viewpoints of both the designer and implementer. Scenarios are modeled in UML, in the form of message sequence charts (MSCs), and then compiled into the finite state process (FSP) algebra to concisely model the required behavior. BPEL4WS implementations are mechanically translated to FSP to allow an equivalence trace verification process to be performed. By providing early design verification and validation, the implementation, testing and deployment of Web service compositions can be eased through the understanding of the behavior exhibited by the composition. The tool is implemented as a plug-in for the Eclipse development environment providing cooperating tools for specification, formal modeling and trace animation of the composition process."
4451,"Reliability is an essential quality requirement for service-oriented systems. A number of models have been developed for predicting reliability of traditional software, in which code-based defects are the main concern for the causes of failures. Service-oriented software, however, shares many common characteristics with distributed systems and web applications. In addition to residual defects, the reliabilities of these types of systems can be affected by their execution context, message transmission media, and their usages. We present a case study to demonstrate that the reliability of a service varies on an hourly basis, and reliability forecasts should be recalibrated accordingly. In this study, the failure behavior of a required external service, used by a provided service, was monitored for two months to compute the initial estimates, which then continuously re-computed based on the learning of the new failure patterns. These reliabilities are integrated with the reliability of the component in the provided service. The results show that with this progressive re-calibration we provide more accurate reliability forecasts for the service."
4452,"Communications among Web services are asynchronous. Asynchronous models of service compositions face the problem that the performance of verification is bring down with states explosion. We describe an approach, which utilize the interaction-independence of component services in the composition and forms all component services into different groups. Preliminary experiment results show that the verification of service compositions upon these groups can decrease the size of exploration states and hence improve the performance."
4453,"Service-oriented Architectures support the provision, discovery, and usage of services in different application contexts. The Web Service specifications provide a technical foundation to implement this paradigm. Moreover, mechanisms are provided to face the new security challenges raised by SOA. To enable the seamless usage of services, security requirements can be expressed as security policies (e.g. WS-Policy and WS-SecurityPolicy) that enable the negotiation of these requirements between clients and services. However, the codification of security policies is a difficult and error-prone task due to the complexity of the Web Service specifications. In this paper, we introduce our model-driven approach that facilitates the transformation of architecture models annotated with simple security intentions to security policies. This transformation is driven by security configuration patterns that provide expert knowledge on Web Service security. Therefore, we will introduce a formalised pattern structure and a domain-specific language to specify these patterns."
4454,"The behavioral analysis for Web services provides a priori detection of errors to ensure successful interactions in services invocation and composition, and the behavioral substitution of Web services is one of the most important issues in such analysis. In this paper, we propose to formalize the behavior of a Web service by π-calculus. Based on the formalization, we introduce two notions of behavioral substitution of Web services namely strong and weak simulation. Furthermore, we propose a derivative approach to analyzing the behavioral substitution of services according to the given notions, which is implemented based on an existing tool of π-calculus. The proposed approach takes advantage of formalization and theory of π-calculus, so that the formalized services can be naturally analyzed and the behavioral substitution of them can be easily determined."
4455,"Configuration management is a complex task, even for experienced system administrators, which makes self-managing systems a desirable solution. Self-management implies the need for a model based on which configuration changes may be decided. In previous work, we described a method for constructing a state-transition model of application behavior, by observing the application in simulation. This method relied on an expert to manage the (simulated) application in order to collect the necessary observations for constructing the model. However, that method was agnostic about (a) the size of the system space space as implied by the granularity of the observations, and (b) the sufficiency of the actual observations collected for understanding the application in a variety of configurations and environments. In this paper, we replace the (expensive) expert domain knowledge with automatic approaches to ensuring coverage of the application, and demonstrate the superiority of this approach. We present empirical data regarding state space and granularity to explore the use of state models for understanding applications."
4456,"QoS-aware service composition intends to maximize the QoS of a composite service when selecting service providers. This paper proposes a service composition scheme that uses a combination of Integer Programming, case-based reasoning, and, genetic algorithms techniques. The scheme reduces the service composition costs by reusing existing compositions. Experiments show that, compared with solutions purely based on Integer Programming, the proposed scheme is effective in reducing the time for carrying out service composition."
4457,"Modern scientific computations are usually data intensive, involving large-scale, heterogeneous and structured scientific datasets. Modeling, organizing, and processing scientific data have become key challenges for scientific workflow management systems (SWFMSs). In contrast to business data, which is usually relational and stored in databases, scientific data is often hierarchically organized and collection oriented. Although several data models have been proposed for SWFMSs, none of them provides a formal data model with a set of well-defined operators. In this paper, we take a first step towards formalizing a collection-oriented data model, called collectional data model, to model hierarchical collection oriented scientific data, and a set of well-defined operators to manipulate and query such data. We then apply the collectional data model to VIEW, a dataflow-based scientific workflow composition framework, whose workflow constructs are extended to support collections. We implement our techniques and validate them by a case study in a biological simulation project."
4458,"We present an automated approach to generate functional conformance tests for semantic Web services. The semantics of the Web services are defined using the inputs, outputs, preconditions, effects (IOPEs) paradigm. For each Web service, our approach produces testing goals which are refinements of the Web service preconditions using a set of fault models. A novel planner component accepts these testing goals, along with an initial state of the world and the Web service definitions to generate a sequence of Web service invocations as a test case. Another salient feature of our approach is generation of verification sequences to ensure that the changes to the world produced by an effect are implemented correctly. Lastly, a given application incorporating a set of semantic Web services may be accessible through several interfaces such as 1) direct invocation of the Web services, or 2) a graphical user interface (GUI). Our technique allows generation of executable test cases which can be applied through both interfaces. We describe the techniques used in our test generation approach. We also present results which compare two approaches: an existing manual approach without the formal IOPEs information and the IOPEs-based approach reported in this paper. These results indicate that the approach described here leads to substantial savings in effort with comparable results for requirements coverage and fault detection effectiveness."
4459,"The past decade has witnessed a fast growth of web-based services, making discovery of user desired services from a large and diverse service space a fundamental challenge. Service clustering has been demonstrated as a promising solution by automatically detecting functionally similar services so that they can be searched and discovered together. In this way, both the efficiency and accuracy of service discovery can be improved. However, the autonomous nature of service providers leads to highly diverse usage of terms in their respective service descriptions. Furthermore, a typical service description is comprised of very limited terms due to the small number of (and focused) functionalities offered by the service. These unique characteristics make service descriptions different from regular text documents, which poses additional challenges when clustering large-scale services. Recent works show that service clustering can benefit from discovery and use of functionality-related latent factors to represent services as opposed to a large and diverse set of terms. Nonetheless, how to determine the total number of latent functional factors and sparsely assign them to each service description arises as a central challenge, especially for a large service space where there is no easy way to enumerate the types of different functionalities. In this paper, we propose a machine learning method that automatically learns the number of latent functional factors in a service space. It also enforces the sparsity constraint, which allows each service to be represented by a small number of latent functional factors. The sparsity constraint is in line with the fact that most real-world services only provide limited functionalities. We conduct extensive experiments on two sets of real-world service data to demonstrate the effectiveness of the proposed service clustering approach."
4460,"Differential deserialization (DDS) is an optimization technique that exploits similarities between incoming SOAP messages to reduce deserialization time. DDS works by checkpointing the state of the SOAP deserializer at various points while deserializing a message, and using those checkpoints to avoid full deserialization of similar messages. DDS can improve performance in many cases, but its benefit is limited by the potentially significant memory and processing overhead associated with its checkpointing mechanism. Differential checkpointing (DCP) substantially reduces memory use, but still requires significant processing overhead. In this paper, we introduce lightweight checkpointing (LCP), a checkpointing approach that significantly reduces the cost of both DDS and DCP, in terms of both memory use and processing time. LCP statically determines locations in the incoming message where it would be most efficient to create checkpoints. LCP creates checkpoints much faster than both our original DDS checkpointing mechanism and our DCP approach. LCP also has significantly smaller memory requirements. For example, in some of our test cases, LCP requires only 10% of the memory that DCP requires, and only 3% of the memory that our original approach required. In terms of processing time, deserialization with LCP is approximately 50% to 60% faster than without differential deserialization, when approximately half the message is unchanged from the previous message"
4461,"In this paper, we develop an XML document retrieval system supporting a multimedia Web service for a digital museum. It can support unified retrieval on XML documents based on both document structure and image content. To achieve it, we perform the indexing of XML documents describing Korean porcelains used for a digital museum, based on not only their basic unit of element but also their image color and shape features. In addition, we provide a similarity measure for a unified retrieval to a composite query, based on both document structure and image content. Finally, we implement our XML document retrieval system designed for a digital museum Web service and analyze its performance in terms of retrieval time, insertion time, storage overhead, as well as recall and precision measure."
4462,"Web Services communicate through XML-encoded messages and suffer from substantial overhead due to verbose encoding of transferred messages and extensive (de)serialization at the end-points. We demonstrate that response caching is an effective approach to reduce Internet latency and server load. Our Tantivy middleware layer reduces the volume of data transmitted without semantic interpretation of service requests or responses and thus improves the service response time. Tantivy achieves this reduction through the combined use of caching of recent responses and data compression techniques to decrease the data representation size. These benefits do not compromise the strict consistency semantics. Tantivy also decreases the overhead of message parsing via storage of application-level data objects rather than XML-representations. Furthermore, we demonstrate how the use of aspect-oriented programming techniques provides modularity and transparency in the implementation. Experimental evaluations based on the WSTest benchmark suite demonstrate that our Tantivy system gives significant performance improvements compared to non-caching techniques."
4463,"The procedure of picking services bound to abstract tasks is usually called service selection in Service Oriented Architecture. In recent years, most studies focus on improving the Quality of Service (QoS) of the composed service. These techniques, however, are facing a new challenge, brought by the big data era, namely, the time and money wasted in data transmission, called transmission cost, cannot be optimized locally like QoS. To address this challenge, in this paper, we study and formalize the problem of transmission cost aware service selection, named TcSS. Owing to the insufficient service transfer rates, we propose a framework on a relaxation problem by making use of the service network ontology structure. The entire framework comprises two stages, an off-line stage to arrange the service network information from logs and an online stage to satisfy the service selection requirement efficiently. The solution of the relaxation problem is an approximation of the original TcSS with the approximate ratio guarantees. Finally, extensive experiments on real data establish the effectiveness and efficiency of our approach."
4465,"What Is Innovation? It’s not always about inventing something entirely new. Innovation occurs at the intersection of invention and insight. It’s about the application of invention - the fusion of new developments and new approaches to solve problems. (Sam Palmisano, Delivered at the Council on Competitiveness Annual Meeting, Washington, D.C., October 30, 2003). In the 21st Century, we are approaching fundamental limits of technology that will drive new paradigms for software and systems. Software complexity is driving a rethinking of software development. There are external forces including government and societal, that are significantly influencing the technology agenda. In addition, the changing business environment demands new approaches to use of IT - transformation must be fueled by innovation. Can we manage innovation? Can we create a culture of innovation? Can we work with customers and partners to drive innovation? Various innovation approaches are being deployed to enhance IBM’s innovation ecosystem. Lessons learned and future innovation drivers will be presented."
4466,"In Service-Oriented Computing (SOC) environments, the trustworthiness of each service provider is critical for a service client when selecting one from a large pool of service providers. The trust value of a service provider is usually in the range of [0, 1] and is evaluated from the ratings given by service clients, which represent the subjective belief of service clients on the satisfaction of delivered services. So a trust value can be taken as a subjective probability, by which one party believes that another party can perform an action in a certain situation. Hence, subjective probability theory should be adopted in trust evaluation. In addition, in SOC environments, a service provider usually can invoke the services from other service providers forming a composite service. Thus, the global trust of a composite service should be evaluated based on both the subjective probability property of trust and complex invocation structures. In this paper, we first interpret the trust dependency caused by direct service invocations as conditional probability. Then, on the basis of trust dependency, we propose a Subjective probability based deductive (SELECTIVE) approach to evaluate the subjective global trustworthiness of a composite service. All these processes follow subjective probability theory and keep the subjective probability property of trust in evaluations. Our experimental results demonstrate that when compared with existing approaches our proposed SELECTIVE approach can yield more reasonable results."
4467,"Since current heterogeneous Intrusion Detection Systems (IDSs) have not been designed to work in a cooperative manner, sharing security information among them poses a serious challenge especially in large-scale High Speed Networks (HSN) environment. The integration become more difficult when we should reduce computing and memory costs incurred by the high speed IDSs communication. Fortunately Web Services technology represents a good choice for IDSs integration thanks to its characteristics such as platform transparency and loose coupling. In this context, this paper presents a lightweight RESTful Communication model for coordinating different high speed distributed IDSs. Experimental results show an important gain in terms of data exchanged size and transmission time."
4468,"Accurate estimation of quality of online services is both an important and difficult problem, since a service has many interdependent quality attributes influenced by several contextual factors. It is even more challenging as quality ratings come from sources with unknown reliability, each source may rate a service on different quality aspects. Although several solutions have been proposed, there is little work addressing all these issues thoroughly. In this paper, we show that domain knowledge on service structure and related constraints, such as causal dependencies among quality attributes and contextual factors, while widely available, can be exploited to effectively address the above issues in a theoretically-sound framework. Theoretical analysis shows that computational cost of the approach is acceptable, and accurate evaluation of service quality requires a reasonable number of user feedback, provided services have a small number of quality attributes and contextual factors."
4469,"Process knowledge, such as tasks involved in a process and the control flow and data flow among tasks, is critical for designing business processes. Such process knowledge enables service composition which integrates different services to implement business processes. In the current state of practice, business processes are primarily designed by experienced business analysts who have extensive process knowledge. It is challenging for novice business analysts and non-professional end-users to identify a complete set of services to orchestrate a well-defined business process due to the lack of process knowledge. In this paper, we propose an approach to extract process knowledge from existing commercial applications on the Web. Our approach uses a Web search engine to find websites containing process knowledge on the Internet. By analyzing the content and the structure of relevant websites, we extract the process knowledge from various websites and merge the process knowledge to generate an integrated ontology with rich process knowledge. We conduct a case study to compare our approach with a tool that extracts ontologies from textual sources. The result of the case study shows that our approach can extract process knowledge from online applications with higher precision and recall comparing to the ontology learning tool."
4470,"Service-Oriented Computing is a paradigm that uses services as building blocks for building distributed applications. The primary motivation for orchestrating services in the cloud used to be distributed business processes, which drove the standardization of the Business Process Execution Language (BPEL) and its central notion that a service is a business process. In recent years, there has been a transition towards other motivations for orchestrating services in the cloud, e.g., XaaS, RMAD. Although it is theoretically possible to make all of those services into WSDL/SOAP services, it would be too complicated and costly for industry adoption. Therefore, the central notion that a service is a business process is too restrictive. Instead, we view a service as a technology neutral, loosely coupled, location transparent procedure. With these ideas in mind, we introduce a new approach to services orchestration: Ozy, a general orchestration container. We define this new approach in terms of existing technology, and we show that the Ozy container relaxes many traditional constraints and allows for simpler, more feature-rich applications."
4471,"A novel benchmark, WSBen, for testing Web services discovery and composition is presented. WSBen includes: (1) a collection of synthetic Web services (WSDL) files with diverse characteristics and sizes; (2) test discovery and composition queries and solutions; and (3) external files for statistical analysis and AI planners. Users can fine-tune the generated WSDL files using various parameters such as skewness or matching type. It is our hope that WSBen provides useful insights for researchers evaluating the performance of Web services discovery and composition algorithms and software"
4472,"OAuth is an open security standard that enables users to provide specific and time bound rights to an application to access protected user resources, stored on some external resource server, without needing them to share their credentials, with the application. Using OAuth, a client application gets one access token for further use through an HTTP redirect response from the resource server once the user authenticates the resource access. Unlike websites, for locally installed packaged web applications the main security challenge is to handle the redirect response appropriately. This paper proposes a novel method to execute OAuth flow from such applications with the help of web runtime framework that manages the life cycle of these applications. We compare our approach with other two approaches for OAuth flow handling proposed in the literature. Experimenting with different categories of packaged web applications, we found our approach blocking all illegal OAuth flow executions. Our approach also gives better OAuth response handling time and power consumption performance."
4473,"Substantial savings from asset reuse result when the right assets are identified in the very early stages of a client engagement. Unfortunately, advanced identification approaches (known by having high precision and recall, such as behavior-based approaches) cannot be adopted in these early stages, because at these early stages, there is no many details nor much understanding about the client functional requirements. On the other hand, unstructured keyword-based identification approaches are known of having low precision and recall. To overcome this problem, we argue that assets descriptions should have explicit information about the business activities realized by the assets. To be able to capture this information in a machine understandable format, this paper proposes a model for describing reusable assets functional scopes using component business maps (CBMs), in which the asset scope is represented as a hierarchy of CBM elements. Adopting this scope model, the paper proposes a rapid identification approach for reusable assets that retrieves assets based on their CBM projections. We believe the proposed approach provides better precision and recall when compared to unstructured keyword-based approaches."
4474,"As the World Wide Web continues to grow unbounded, users expect intelligent processing and accurate coverage of all its domains. To allow for the same, we present a novel approach to identify and extract key information from web pages with commendable accuracy. We extract important information such as the Title, Main Image, Description, Keywords and FavIcon from a webpage where available, using only the HTML responses without any explicit webpage rendering. The algorithm was modelled to be fast without compromising on its accuracy, is fully automatic, language independent and runs without any human supervision or training. We test our algorithm extensively on over one hundred thousand webpages and successfully extract the key information for 97% of them with an impressive average extraction time of less than 500 milliseconds per webpage."
4476,"A service ecosystem consists of services and their compositions (i.e., mashups) and evolves as a complex network system. It is driven by continuously emerged new services and the mashups of old services and new ones. Complex network analysis can be a powerful tool to study the static structure as well as the evolution of a service ecosystem. This paper presents a methodology to study such a system and an empirical study of Programmable Web. To the best of our knowledge, Programmable Web is the largest and most active Web APIs and mashups collection and consists of 4337 services and 6092 service compositions by Nov-2011. We conduct a comprehensive network analysis to quantitatively characterize the static structure and dynamic evolution of the ecosystem. The findings of this paper not only can help understand the current usage pattern and the evolution trace of the ecosystem, but also are applicable to other Web service systems."
4477,"For automatically composing Web services in a correct manner, information about their behaviors (an abstract model) has to be published in a repository. This abstract model must be sufficient to decide whether two, or more, services are compatible (the composition is possible) is possible without including any additional information that can be used to disclose the privacy of these services. The compatibility property is defined by different variants of the well known soundness property on open workflow nets. These properties guarantee the absence of livelocks, deadlocks and other anomalies that can be formulated without domain knowledge. In this paper we address the automatic abstraction of Web services and the checking of their compatibility using their abstract models only. To abstract Web services, we use the symbolic observation graph (SOG) approach that preserves necessary information for service composition and hides private information. We show how the SOG can be adapted and used so that the verification of different variants of compatibility can be performed on the composition of the abstract models (SOGs) of Web services instead of the original composite service."
4478,"One of the main ideas of Service-Oriented Computing (SOC) is the delivery of flexibly composable services provided on world-wide markets. For a successful service discovery, service requests have to be matched with the available service offers. However, in a situation in which no service that completely matches the request can be discovered, the customer may tolerate slight discrepancies between request and offer. Some existing fuzzy matching approaches are able to detect such service variants, but they do not allow to explicitly specify which parts of a request are not mandatory. In this paper, we improve an existing service matching approach based on Visual Contracts leveraging our preliminary work of design pattern detection. Thereby, we support explicit specifications of service variants and realize gradual matching results that can be ranked in order to discover the service offer that matches a customer's request best."
4479,"We describe a complete and largely automated method for the development of systems from Web services, which comprises the encapsulation of services, as well as their composition, verification and subsequent implementation in a model-driven manner. The paper follows the steps of the method: In a first phase, we import WSDL descriptions automatically as UML~2.x activities and provide them as building blocks, with some optional, manual adaptations. In a second phase, these building blocks can be used to compose an application that orchestrates Web services. The building blocks have behavioral contracts that enable automated, incremental verification based on compositional model checking. We demonstrate the approach by a subscription-based service to receive SMS messages."
4480,"This paper presents a methodology and a set of tools for the modelling, validation and testing of Web service composition, conceived and developed within the French national project WebMov. This methodology includes several modelling techniques, based mainly on some variations of Timed Extended Finite State Machines (TEFSM) formalism, which provide a formal model of the BPEL description of Web services composition. These models are used as a reference for the application of different test generation and passive testing techniques for conformance and robustness checking. The whole WebMov methodology is integrated within a dedicated framework, composed by a set of tools that implement the model representation, the test generation and passive testing algorithms. This framework also permits the interaction of these tools to achieve specific modelling and testing activities in a complementary way. A case study based on a real service, a Travel Reservation Web Service, is presented as well as the results of the application of the proposed WebMov methodology and tools."
4481,"Automation of web service composition is one of the most interesting challenges facing the semantic web today. Despite approaches which are able to infer partial order on services, data flow (i.e., the way data is exchanged among services) remains implicit and difficult to be inferred and automatically generated. Since web services have been enhanced with formal semantic descriptions, it becomes conceivable to exploit and reason on their semantic links (i.e., semantic matching between their functional output and input parameters) to infer data flow. Our approach has been directed to meet the main challenges facing the latter problem i.e., how to effectively i) guarantee whether a data flow is well-formed and ii) infer data flow between services based on their Description Logics (DL) descriptions. To this end, we apply constructive DL reasoning abduction, contraction and introduce the non standard DL reasoning join to model and infer data flow in compositions. The preliminary evaluation results showed high efficiency and effectiveness of the proposed approach."
4482,"Many researchers propose that, not only functional but also non-functional properties, also known as quality of service (QoS), should be taken into consideration when consumers select services. Consumers need to make prediction on quality of unused web services before selecting. Usually, this prediction is based on other consumers' experiences. Being aware of different QoS experiences of consumers, this paper proposes a collaborative filtering based approach to making similarity mining and prediction from consumers' experiences. Experimental results demonstrate that this approach can make significant improvement on the effectiveness of QoS prediction for web services."
4483,"Electronic Contract (eContract) has been recognized as a good combination of technical specification and legal documentation for establishing and regulating virtual organizations built for dynamic collaborations. This paper presents a design and implementation of an eContract service with an aim of providing a trusted collaboration platform for collaborators. The implemented service uses Web services technologies to facilitate its collaborators not only to contribute resources in an eContract, but also to negotiate and instantiate them through eContract. This paper describes the interface and protocols for the eContract service. The architecture, interface and protocols designed for the service are demonstrated using an example of providing universal connectivity service for a telepresence application in the context of eResearch domain."
4484,"A central element of emerging service oriented architectures (SOA) is the ability to develop new applications by composing enterprise functionality encapsulated in the form of services - whether within a given organization or across multiple ones. Semantic service annotations, including annotations of both functional and non-functional attributes, offer the prospect of facilitating this process and of producing higher quality solutions. A significant body of work in this area has aimed to fully automate this process, while assuming that all services already have rich and accurate annotations. In this article, we argue that this assumption is often unrealistic. Instead, we describe a mixed initiative framework for semantic Web service discovery and composition that aims at flexibly interleaving human decision making and automated functionality in environments where annotations may be incomplete and even inconsistent. An initial version of this framework has been implemented in SAP's guided procedures, a key element of SAP's enterperise service architecture (ESA)"
4486,"The similarity-based aggregation of XML documents is a proven method for reducing network traffic. However, when used in conjunction with XML security standards, a lot of pitfalls, but also optimization potentials exist. In this paper, we investigate these issues, showing how to exploit similarity-based aggregation for rapid distribution of digitally signed XML data. Using our own implementation in two different experimental settings, we provide both a thorough evaluation and a security proof for our approach. By this we prove both feasibility and security, and we illustrate how to achieve a network traffic reduction of up to 82% in total."
4487,"Modeling and deployment of e-contracts is a challenging task because of the involvement of both technological and business aspects. There are several frameworks and systems available in the literature. Some works mainly deal with the automatic handling of paper contracts and others provide monitoring and enactment of contracts. Because contracts evolve, it is useful to have a system that models and enacts the evolution of e-contracts."
4488,"Data Providing Services (DPSs) have the sole purpose of retrieving data from existing sources according to their input parameters while also providing a semantic description of the data they provide using a parametrized view over a domain ontology. A layered model of viewing DPSs is proposed consisting of the data acquisition, syntactic and semantic layers. It is shown that by defining all three layers, a DPS may be generated and managed exclusively by its declarative definition. This will increase the agility and efficiency with which DPSs may be deployed and managed. As a development model, a set of reusable messages are created, these messages are to be semantically annotated using a view over the domain ontology and are syntactically represented such that they may be exported to XML Schema. These messages are used within the DPS definition where their views over the domain ontology are parametrized and the data acquisition layer is defined to acquire data from the source."
4489,Geospatial processing often involves complex and complicated geospatial data types. It is extremely inefficient if not infeasible to require scientist users maintain type compatibility of the ports of Web services that are connecting to each other. This study proposes to extend the type checking system of Kepler scientific workflow system to help scientist users composite geospatial Web services more effectively.
4490,"A growing number of domains are adopting semantic models as a centralized gateway to heterogeneous data sources, or directly for modeling and managing relevant information. In such contexts, it is crucial to grant access to the semantic model and its data only to the authorized users. In this paper, we present a fine-grained access control model specifically tailored to semantic models. One of the relevant features of the model is the granularity of the resources that can be protected. Access control can be enforced at the level of both the model's concepts and the concepts' instances by means of a query rewriting strategy. The proposed model has been implemented adopting the XACML standard and the SeRQL query language; services exposed by the implementation can be used to trans- paretly integrate authorization into existing systems."
4492,"The idea of the future internet of services is to combine several services of numerous service providers to new value-added services or applications. To sell these services on so-called service marketplaces the providers have to ensure a high quality of service execution. But how could a provider of a composed service ensure the quality of the whole application, if single services of other providers fail or do not reach the required level of quality? This article describes an approach for modeling adaptation in web service compositions to ensure a guaranteed quality of service for the whole composite service. A special adaptation mechanism is the rebinding of single services while the process is executed if the services fail or could not reach the needed QoS level. We will present a solution for modeling these rebinding concepts in BPEL processes and the infrastructure to support this adaptation mechanism at run-time."
4493,"Negotiation of service level agreements (SLAs) is very important for maintaining quality of service (QoS) of composite Web services-based business processes. The process of negotiation involves specification of negotiation parameters, exchanging offers to conduct the actual negotiation process, and then finally generating the formal SLA if the negotiating parties come to a consensus. We propose a negotiation broker (NB) middleware framework to facilitate automated negotiations of SLAs for Web services in a service oriented architecture (SOA). High level business goals, contexts, preferences, constraints, and values of the negotiation issues are expressed as a policy specification by each of the negotiating parties. The NB maps the policy specifications to low level negotiation strategy models and parameters in order to conduct the negotiation locally as a trusted broker. We present a model and an example of the high level negotiation policy specification. We also present our NB framework including a prototype implementation to illustrate the mapping of the policy to a time-dependent negotiation strategy model."
4494,"Nowadays, many engineering studies are conducted to securely exploit depleted oil fields for CO2 Storage.These studies follow complex workflows of data processing services described by geologists. If no explicit semantics is applied to describe these workflows, it is not possible to share them between geologists by reusing existing ones or for composing new ones. The focus of our work is to make the semantics explicit in order to facilitate the geologists daily work. In this article, we first explain how geologists operate today. Then, we enrich such workflows with semantic indexes through ontology based characterizations."
4495,"Cloud computing is evolving as a key computing platform for sharing resources that include infrastructures, software, applications, and business processes. Virtualization is a core technology for enabling cloud resource sharing. However, most existing cloud computing platforms have not formally adopted the service-oriented architecture (SOA) that would make them more flexible, extensible, and reusable. By bridging the power of SOA and virtualization in the context of cloud computing ecosystem, this paper presents seven architectural principles and derives ten interconnected architectural modules to form a reusable and customizable cloud computing open architecture (CCOA). Two case studies on infrastructure and business cloud are used to deliver business and practical value of infrastructure and business process provisioning services over the Internet. We also present some potential value-added services of the proposed CCOA to guide strategic planning and other consulting practices of cloud computing."
4496,"This article proposes a new architectural principle for LISA (LInked Service Architecture) based on the Linked Data, ROA (Resource-Oriented Architecture), and meta-level service broker. LISA employs a two-layer architecture separating the concerns into meta-level of linking and base level of provisioning of services. A meta-level broker can dynamically coordinate services, and make the provisioning at the base level truly decentralized."
4497,"By employing Web services technology, the grid system is evolving to be more manageable service infrastructure, including lifetime management, discovery of characteristics, and notifications. Reliable messaging is one of the key issues addressed for quality of services in Web services. In this paper, we propose a reliable message scheme designed for mobile environments in the context of a Web services architecture; Web services-Wireless Reliable Messaging (WS-WRM). We also consider the federation issues with emerging specifications proposed by leading Web services standard groups. We eventually intend to extend the reliability to mobile end-nodes in a more efficient way. In this paper, we address the design issues and describe the detailed scheme of messaging architecture."
4498,"Service computing is a popular development paradigm in information technology. The functional properties of Web services assure correct functionality of cloud applications, while the nonfunctional properties such as reliability might significantly influence the user-perceived availability evaluation. Reliability rankings provide valuable information for making optimal cloud service selection from a set of functionally-equivalent candidate services. There existed several approaches that can conduct reliability ranking prediction for Web services. Those approaches acquire different rankings with different preference functions. It is arduous to determine whether there exists the best one in them, and what is the best one if not. This paper proposes a learning approach to reliability ranking prediction for Web services which utilizes past service invocation logs to train preference function. To validate the proposed approach, large-scale experiments are conducted based on a real-world Web service dataset, WSDream. The results show that our proposed approach achieves higher prediction accuracy than the existing approaches."
4499,"To automate the analysis of service descriptions, [?] proposed a simple and expressive business protocol model (the specification of possible message exchange sequences) based on state machines, supporting rich timing constraints. Furthermore, developers of client applications need to be aware not only of functional aspects but also of non-functional aspects including privacy. In fact, the major concerns of a client are the disclosure of its personnel data conveyed during the message exchange. The aim of the paper is to study the ability of business protocol to handle the privacy and its time-related properties."
4500,"The tutorial aims at providing a deep comprehension of the Web service composition problem and automated techniques to tackle it. Web service composition is currently one the most hyped and addressed issue in the service oriented computing. Starting from an analysis of current technologies and standards for Web service composition, the tutorial will lead the attendees to consider formal models at the base of current proposals, and techniques that can be fruitfully considered to address automatic composition synthesis in each of them. More in detail, attendees will consider: (i) basic technologies and standards for Web service invocation and description (SOAP, UDDI, WSDL,...); (ii) advanced technologies and standards for orchestration and inter-organizational process enactment, in particular WS-BPEL and WS-CDL; (iii) models for Web service composition; (iv) formal tools for both data-centric and process-centric synthesis, including query reformulation a'la data integration, transition-systems based formalisms, trace-based formalisms, logics of programs and processes. In particular, we will show how these formal tools can be applied for automatic Web service composition; (v) current state-of-the-art research results in automatic service composition, drawing a comparison and defining a unifying framework"
4501,"With REST becoming a popular paradigm for Web services, more and more use cases are applied to it, including some that require transactional guarantees. We propose a RESTful transaction model that satisfies both the constraints of transactions as well as those of the REST architectural style. We provide formal proof of consistency and recoverability in the proposed framework and show the robustness of its properties in the presence of concurrent transactions."
4502,"As software consumption is shifting to mobile platforms, enterprises are looking for efficient ways to reuse their existing legacy systems by exposing their functionalities as services. Mining services from legacy code is therefore an important problem for the enterprises. In this paper we present a technique for mining service candidates from the database applications. Central to our mining technique is the specification and identification of data-access patterns which specify how a program interacts with the databases. In addition to finding service candidates which are internal functions in the source code, we also provide an algorithm to expose the function as a stateless service by generating a wrapper function around the internal function. We demonstrate the effectiveness of our technique on two open source applications and twelve industrial applications."
4503,"Selecting the optimal service from a set of functionally equivalent services is non-trivial. Previous research has addressed this issue making use of Quality of Service (QoS) attributes of the candidate services. In doing this, researchers have however assumed that the customers' preference of the various QoS attributes varies linearly with the actual attribute values. In this work, we put forward a technique that overcomes this restriction and compares functionally equivalent services on the basis of the customers' perception of the QoS attributes rather than the actual attribute values. We utilize the 'mid-level splitting' method to track the customer's preference vis-a-vis the actual attribute values. Further, we utilize the 'Hypothetical Equivalents and In equivalents Method' to assign weights, reflecting the importance, to the attributes on the basis of the customer preference. The whole procedure is demonstrated using a simple running example."
4504,"As in linguistics the semantics co-exist with pragmatics to provide complete and unambiguous meaning to utterances, in the same way the pragmatics should augment the semantics in intelligent applications. Service-oriented computing is an area where such intelligent applications are greatly facilitated. In this paper we propose a pragmatic methodology to Web service discovery which utilizes both the pragmatics and the semantics. This methodology aims to solve a very basic problem of existing semantic discovery approaches: the inability of selecting the most appropriate service among many semantically equivalent Web services."
4505,"Service-oriented computing has emerged as a new component-based software development paradigm in a network-centric environment. By using a standard description language and protocol, services can be used to wrap legacy software systems to be integrated beyond the enterprise boundary across heterogeneous platforms. Nevertheless, the challenges come in tandem with the opportunities because of the inherent dynamic characteristics within a distributed environment. In particular, there is a need for dynamic adaptation for provisioned services to accommodate the ever-changing business requirements externally as well as the computing resource status internally, while maintaining the continuousness of service provisioning. We present a dynamic Web service provisioning approach based on .NET common language runtime, one of the two primary Web services platforms, exploring the runtime code manipulation at the intermediate language (IL) level rather than at the source code level. Meanwhile, we show how the service provisioning can be adapted in a modularized way by complementing the conventional service-oriented architecture (SOA) with a repository of adaptation aspects. Moreover, we demonstrate how dynamic service provisioning can be used for nonfunctional property assurance."
4506,"There is a growing demand for provisioning of different levels of quality of service (QoS) on scalable Web servers to meet changing resource availability and satisfy different client requirements. The proportional differentiation model is getting momentum because of its fairness and differentiation predictability. It states that QoS of different traffic classes should be kept proportional to their pre-specified differentiation parameters, independent of the class loads. In this paper, we present a processing rate allocation scheme for providing proportional response time differentiation on Web servers. A challenging issue is how to achieve processing rates for different request classes in the implementation. We propose a process allocation strategy, which dynamically and adaptively changes the number of processes allocated for handling different request classes while ensuring the ratios of process allocation specified by the processing rate allocation scheme. We implement the process allocation strategy at application level on Apache Web servers. Experimental results show that the processing rate can be achieved by the adaptive process allocation strategy and the Web servers can provide predictable and controllable proportional response time differentiation."
4507,"Personalization or privacy control? is a question one frequently asks himself/herself nowadays with the wide spread of service provision for users on the move. The right to protect intimacy or the right of privacy is a permanent and genuine right of any person. Context-awareness, offering to users services that react proactively to user environment and service conditions, although desirable, breaks into the private sphere. In the Internet world Web Services are usually employed as building blocks in such context-aware applications. In this paper, the reflection of user privacy preferences in the provision of context-aware Web Services is addressed. A user privacy preferences language for context-aware Web Service environments, namely Consumer Privacy Language (CPL), is proposed along with an adaptation mechanism for SOAP messages. The adaptation remains transparent to the end-user, who can profit from Web Services offering both context- and privacy-awareness."
4508,"In today's service computing environments, user needs and expectations are constantly changing. New services emerge while old ones become obsolete and need to be replaced. In such settings, composite services need to be adaptive to changes in user requirements and the environment. This paper proposes a conceptual framework for modeling compositional adaptation for services founded on a requirements monitoring facility. This facility helps maintain adherence between user requirements changes and the dynamics of service composition structure and quality attributes. Specifically, user requirements are represented as goals and soft goals, service composition structure is represented with a CSP-like grammar, and the adaptation mechanism is based on AI planning. The proposed approach is evaluated in a service simulation environment of real-world supply-chain adaptation scenarios."
4509,"In the process of web service composition, the check and prevention of semantic incompatibility is one of the most important issues. In this paper, a controlled Petri net (CtlPN)-based model for web service composition is proposed. Meanwhile, the optimal controller is constructed, such that the appropriate vectors of controllable place and arc are appended in the key transition which can lead to deadlock states. In addition, for the semantic incompatibility case, a policy based on appending optimal controller is presented. It is proved that our policy can be a good solution. Finally, the proposed controller is transformed as the activity of BPEL."
4510,"Much effort is being made by the IT industry towards the establishment of a Web services infrastructure and the refinement of its component technologies to enable the sharing of heterogeneous application resources. Traditional roles of the service provider, service requestor and service broker and their interactions are now being improved upon to enable more effective services. The implementation of the Web service broker is currently limited to being an interface to the service repository for service registration, browsing and/or programmatic access. In this work, we have extended the functionality of the Web services broker to include constraint specification and processing, which enables the broker to find a good match between a service provider's capabilities and a service requestor's requirements. This paper presents the extension made to the Web Services Description Language to include constraint specifications in service descriptions and requests, the architecture of a constraint-based broker, the constraint matching technique, some implementation details, and preliminary evaluation results."
4512,"In this paper, we explore the use of domain-independent and domain-specific ontologies to find matching service descriptions. The domain-independent relationships are derived using an English thesaurus after tokenization and part-of-speech tagging. The domain-specific ontological similarity is derived by an inference on the semantic annotations associated with Web service descriptions. Matches due to the two cues are combined to determine an overall semantic similarity score. By combining multiple cues, we show that better relevancy results can be obtained for service matches from a large repository, than could be obtained using any one cue alone."
4513,"The way Internet is used changes: demand grows for critical services that cross several provider networks; guaranteeing a required end-to-end quality of service (QoS) across several networks becomes a challenge. Some critical services (e.g. video-conference, VPN etc.) can not be satisfied in a best effort fashion. The use of QoS contracts (service level agreements, SLAs) is effective for management of such services. However, the problem of meeting end-to-end QoS requirement remains: no centralized entity can compute overall QoS for chains of contracts, and a fortiori, such contract chains can not be optimized centrally. Thus, the following problem has to be solved: given an end- to-end QoS request and collections of available SLAs on each participating domain, establish an end-to-end contract committing a chain of providers and giving optimal service under most reliable guarantees available."
4515,"As one of the key characteristics of software as a Service (SaaS), multi-tenancy aims to support massive customers by sharing application instances and databases. To achieve the high economies of scale, one of the most issues needing to be solved in the real industry is that, given a fixed number of nodes, how to optimally place on-boarding tenants to maximize the total supported number of tenants without violating their SLA requirements. This paper focuses on this problem, which is called On-line Tenant Placement Problem (OTPP). In order to calculate the resource consumption of on-boarding tenants, a novel resource consumption estimation model for multi-tenant pattern is proposed in this paper. Based on this model, we explore the complexity of OTPP. A robust heuristic is proposed for the OTPP. The simulation experimental results show the high effectiveness and the good efficiency of our algorithm."
4516,"On a crowdsourcing platform consisting of requestersand workers, it is a challenge to recommend suitableworkers for a human intelligence task (HIT) published by a requester. A suitable worker is the one who has a high probability of submitting a correct answer for the published HIT. However, there are four problems that make the existing methods be less effective in recommending suitable workers. First, on most of crowdsourcing platforms, the great majority of workers have good reputations and thus are regarded as homogenous workers who have equal opportunities to be recommended. Secondly, dishonest workers may gain recommendations by counterfeiting good reputations and overstating personal skills. Moreover, the classical data sparsity and cold start problems co-exist in crowdsourcing environments. To effectively differentiate homogenous workers, we firstlycalculate a worker's performance in different types of HITspublished by different requesters. Aiming to improve the accuracy of predicting a worker's performance, we propose a metric which separately considers two requesters' similarities in transacting with the common workers they trust and in transacting with the common workers they distrust. Afterwards, targeting dishonest behaviours, we propose a transaction-based trust model. Targeting the data sparsity problem, we propose a new trust sub-network extraction algorithm (TSE) to discover more requesters who can provide trustworthy opinions for generating recommendations. Furthermore, we propose two strategiesfor solving the cold start problem. Finally, by incorporatingthe similarity metric, the new trust model, the new trustsub-network extraction algorithm and the new strategies, wepropose a novel trust-aware worker recommendation methodCrowdRec. The experimental results illustrate that CrowdRecsignificantly outperforms CF and three state-of-the-art trustbased recommendation methods both in terms of accuracy and coverage."
4517,"The next generation intelligent devices need to grow and evolve with the user. Mobile user framework solution allows us to move towards this goal. In this paper, we describe a service framework that captures user's interest and intent which is mined through latent analysis of content. Applications using such a framework can provide adaptive and customized services to the user. We provide details of four demonstrator applications that use this framework, utilizing user interest and intent information to provision targeted advertisements, recommend products and songs. We provide details of our evaluation with subjective and objective analysis of the system."
4518,"In composite web services one can only either hide the identities of the participants or provide end-to-end confidentiality via encryption. For a designer of inter-organizational business processes this implies that she either needs to reveal her suppliers or force her customers to reveal their information. In this paper we present a solution to the encrypted data modification problem and reconciliate this apparent conflict. Using a generic sender-transformer-recipient example scenario, we illustrate the steps required for applying XML transformations to encrypted data, present the cryptographic building blocks, and give an outlook on advantages and weaknesses of the proposed encryption scheme. The transformer is then able to offer composite services without itself learning the content of the messages."
4519,"We present a new approach for checking the compatibility of policy descriptions. At present, policies are widely used for explicitly expressing non-functional properties, capabilities, constraints and requirements of Web services. Policies are crucial in the negotiation phase of service discovery and selection. Typically, a potential service consumer has its own policy that specifies the conditions the service has to fulfill. Ideally, an automatic negotiation process identifies a mutually agreeable policy for both the Web service consumer and provider. WS-Policy defines policy intersection as a ""first approximation"" for determining the compatibility of policies. However, policy intersection has a major weakness: It is a purely syntactic approach neglecting the semantics of the involved policy assertions. In addition, it is unspecific on how to include domain-specific processing. In this paper we present a new solution that overcomes these deficits by introducing an entailment relation that reflects the semantics of assertions and policies. This paper not only discusses the formal foundations but also introduces the required algorithms such as ""semantic policy differencing""."
4520,"Sustainable success of service oriented applications relies on capabilities to manage possible service failures. To substitute a failed service with some other equivalent service is unavoidable in recovering a suspended application due to failure of a constituent service. In this paper, we report a rule based approach to Web service substitution in order to secure availability of services. Availability provides delivery assurance for each Web service so that Simple Object Access Protocol (SOAP) messages cannot be lost undetectably, especially in a Web service composition. The rules are written in Semantic Web Rule Language. The rules are a formal representation of a categorization-based scheme to identify exchangeable Web services. This scheme not only tackles the issue of heterogeneity of domain ontology in describing the Web services, it also adapts itself by learning newly discovered ontology instances. A technical framework of Web service substitution using rule based deduction is demonstrated. Experiments on service substitution based on the proposed framework achieve a best precision of 85%."
4521,"Web service selection based on quality of service (QoS) has been a research focus in an environment where many similar web services exist. Current methods of service selection usually focus on a single service request at a time and the selection of a service with the best QoS at the user's own discretion. The selection does not consider multiple requests for the same functional web services. Usually, there are multiple service requests for the same functional web service in practice. In such situations, conflicts occur when too many requesters select the same best web service. This paper aims at solving these conflicts and developing a global optimal service selection method for multiple related service requesters, thereby optimizing service resources and improving performance of the system. It uses Euclidean distance with weights to measure degree of matching of services based on QoS. A 0-1 integral programming model for maximizing the sum of matching degree is created and consequently, a global optimal service selection algorithm is developed. The model, together with a universal and feasible optimal service selection algorithm, is implemented for global optimal service selection for multiple requesters (GOSSMR). Furthermore, to enhance its efficiency, Skyline GOSSMR is proposed. Time complexity of the algorithms is analyzed. We evaluate performance of the algorithms and the system through simulations. The simulation results demonstrate that they are more effective than existing ones."
4523,"The growing need for an integrated view of scientific information from different sources has led to the need for scientific information integration, and on the other hand, SOA is one most prevailing technology for its advantages on solving integration problems. In this paper, we argue that the deployment of SOA in an organization should be business domain-specific, and propose an approach called CAFISE-S, which introduces SOA into scientific information integration from a business view-aspect. Business service is put forward as basic elements in CAFISE-S to model business context and IT services coherently in a semantic way. Based on business service, CAFISE-S provides a business domain-specific modeling method for specification of information services, and then supports business-oriented publication, management and usage of information services. The implementation of CAFISE-S platform and an application of CAFISE-S in a real-world project of scientific information integration are also presented in this paper."
4524,Provides an abstract of the keynote presentation and may include a brief professional biography of the presenter. The complete presentation was not made available for publication as part of the conference proceedings.
4525,"With the popularity of cloud computing and micro service architectures, various service ecosystems including services, venders, and service-based processes continuously emerge on Internet or in an enterprise. Semantics of services from different venders may be described by distributed domain ontologies. Distributed knowledge brings difficulty to competition and cooperation among services, and hampers the evolution of a service ecosystem. In this paper, we propose a distributed knowledge based evolution model (DKEM) to promote competition and cooperation among services from different venders. DKEM considers stability as key factor in competition, and a stability evaluation model is designed to compute stability of services, venders, and service-based processes according to service invocation histories. Based on the evaluation model, two evolution patterns are given, and they can automatically explore new and more stable cooperation among services by means of runtime self-adaption mechanism. A prototype system for DKEM is implemented and a series of experiments show that DKEM is effective for competition and cooperation among services with distributed knowledge, and, evolved processes have higher stability and response efficiency."
4526,This paper reports on our experiences with combining Heart of Gold and Language Grid technology to provide more language resources available on Web. Heart of Gold is known as middleware architecture for integrating deep and shallow natural language processing components. The language grid is an infrastructure built on top of the Internet to provide distributed language services. Having Heart of Gold available as Web services in the language grid environment would contribute to interoperability among language services.
4527,"Though 20 years have passed since the birth of CSCW, the original goal of it is not reached as well as people expected. This situation is mostly due to the supporting technology especially the infrastructure. Today, great changes have taken place in technology, including grid computing and Web services. These technologies, we think, significantly affect the application of CSCW. In this paper, a framework called CoFrame is proposed to answer the challenges faced by CSCW. Based on the emerging grid and Web service technologies, CoFrame provides some general yet flexible cooperation related services and organizes them into different layers. The elaborately designed services and architecture make CoFrame adaptive to diverse requirements of different domains. The paper details the framework and demonstrates its application with a case study in e-learning."
4528,"Pervasive computing environments augment physical spaces with a large number of devices and services that help users perform different kinds of tasks. Users in these environments interact with one or more Web services using various devices to achieve their goals. One of the problems in these environments is discovering and coordinating different Web services for achieving the user's goals. Users may not be aware of which services and devices are available in an unfamiliar environment and how to interact with them in order to achieve their goals. In order to simplify a user's interaction with the environment, we present a novel approach of modeling and managing a user's interaction with the environment based on workflows. We have built a prototype, using the popular business workflow language (BPEL) that models various processes in pervasive environments as workflows. We found that this approach improves the usability of these environments. It also increases flexibility in changing the model of interaction without having to touch individual services and applications. This approach is particularly useful in helping visitors in public spaces like malls, museums, supermarkets and hospitals."
4529,"Recently, there has been a growing interest in web service composition and the related security issues. In this paper, we propose a framework for the decentralized execution of composite web services capable to ensure the correctness as well as the security of the execution. Our framework relies on a data structure, called container, which is passed among the web services participating in the composition. The container is encrypted and authenticated in such a way to ensure the correctness of the execution flow as well as a set of relevant security requirements."
4530,"In this work, we present a framework for the semantic composition of web services based on Statecharts and uniform community service descriptions. Our model is a two step process. In the first step, we derive the execution model of the user's query. The execution model is specified in Statecharts formalism; whereas the user's query is described in OWL-S. Therefore, a mapping from Statecharts formalism to OWL-S is developed. In the second step, we instantiate the developed execution model through invocation of available e-services instances. Hence, and a result, we obtain an execution plan (said also strategy) satisfying user constraints. The key features of the proposed framework could be summarized as follows. First, unlike other existing languages, using OWL-S enables the semantic description of e-services. These semantics are taken into consideration in our composition strategy. Second, the user constraints (or preferences) are taken into account during composition and are expressed as a finite set of logical formulas with the Knowledge Interchange Format (KIF) language."
4531,"The specification, design and implementation of web service applications need to address three major aspects: Orchestration of Services, Conversation and Choreography. In distributed computing, abstractions such as scripts have been used to abstract patterns of communication hiding low level details. In this paper, we demonstrate an approach of integrating orchestration with scripting to depict a pattern of communication or conversations among various agents."
4532,"caGrid has accumulated a repository of biomedical services, however, how a cancer researcher can find proper services in the caGrid when needed remains a big challenge. This research aims to enhance the cyber infrastructure of caGrid, by developing a mechanism that turns caGrid services into semantic-aware interoperable services. We proposed a service semantics model, and developed a technique that automatically extracts semantic metadata from static WSDL service descriptions. Such semantic information is stored as loosely coupled annotations that can be queried using semantic Web techniques, to enhance services discovery and composition. We also proposed a two-phase discovery technique that helps users quickly identify interested service operations. This paper also reports our examinations over available techniques and recommends a feasible infrastructure for biomedical service reuse. A prototyping system is developed as a proof of concept."
4533,"Pervasive Information System (PIS) represents a new generation of Information Systems (IS) available anytime, anywhere in a pervasive environment. In this paper, we propose to enhance PIS transparency and efficiency through a context-aware intentional service prediction approach. This approach allows anticipating user's future needs, offering and recommending him the most suitable service in a transparent and discrete way. We detail in this paper our service prediction mechanism and present encouraging experimental results demonstrating our proposition."
